---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Clustering"
date: 2020-10-20T13:00:00Z
draft: false
weight: 60
output: 
  html_document:
    keep_md: true
    self_contained: true
slug: clustering
---


```{r setup, include=FALSE}
library(knitr)  
library(reticulate)  
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(fig.path = "")
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)

# Hook from Maelle Salmon: https://ropensci.org/technotes/2020/04/23/rmd-learnings/
knitr::knit_hooks$set(
  plot = function(x, options) {
    hugoopts <- options$hugoopts
    paste0(
      "{", "{<figure src=", # the original code is simpler
      # but here I need to escape the shortcode!
      '"', x, '" ',
      if (!is.null(hugoopts)) {
        glue::glue_collapse(
          glue::glue('{names(hugoopts)}="{hugoopts}"'),
          sep = " "
        )
      },
      ">}}\n"
    )
  }
)

```

```{python, eval = FALSE, include = FALSE}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/W3CRK9/AppData/Local/r-miniconda/envs/r-reticulate/Library/plugins/platforms'
os.environ["PROJ_LIB"] = r'C:\Users\W3CRK9\AppData\Local\r-miniconda\pkgs\proj4-4.9.3-hfa6e2cd_9\Library\share'
os.environ['GDAL_DATA'] = r"C:\Users\W3CRK9\AppData\Local\r-miniconda\envs\r-reticulate\Library\share\gdal"
```

Nous allons continuer avec le jeu de données de [résultat des élections US 2020 au niveau des comtés](#preprocessing)

Jusqu'à présent, nous avons fait de l'apprentissage supervisé puisque nous
connaissions la variable à expliquer/prédire. Ce n'est plus le cas avec
l'apprentissage non supervisé.

Le *clustering* est un champ d'application de l'apprentissage non-supervisé.
Il s'agit d'exploiter l'information disponible pour regrouper des observations
à la structure commune ensemble.

L'objectif est de créer des classes d'observations pour lesquelles:

* au sein de chaque classe, les observations sont homogènes (variance infra-classe minimale)
* les classes ont des profils hétérogènes, c'est-à-dire qu'elles se distinguent l'une de l'autre (variance inter-classes maximale)

En Machine Learning, les méthodes de classification sont très utilisées pour
faire de la recommandation. En faisant, par exemple, des classes homogènes de 
consommateur, il est plus facile d'identifier et cibler des comportements 
propres à chaque classe. 
Ces méthodes ont également un intérêt en économie et sciences sociales parce qu'elles permettent
de regrouper des observations sans *a priori* et ainsi interpréter une variable
d'intérêt à l'aune de ces résultats. Ce [travail (très) récent](https://www.insee.fr/fr/statistiques/4925200)
utilise par exemple cette approche.


Les méthodes de *clustering* sont nombreuses.
Nous allons exclusivement nous pencher sur la plus intuitive: les k-means. 

# Données

```{python}
import pandas as pd
import geopandas as gpd
import os
import zipfile
import urllib.request

def download_url(url, save_path):
    with urllib.request.urlopen(url) as dl_file:
        with open(save_path, 'wb') as out_file:
            out_file.write(dl_file.read())
    
from pathlib import Path
Path("data").mkdir(parents=True, exist_ok=True)


download_url("https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip", "data/shapefile")
with zipfile.ZipFile("data/shapefile", 'r') as zip_ref:
    zip_ref.extractall("data/counties")

shp = gpd.read_file("data/counties/cb_2019_us_county_20m.shp")
shp = shp[~shp["STATEFP"].isin(["02", "69", "66", "78", "60", "72", "15"])]
shp

df_election = pd.read_csv("https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv")
df_election.head(2)
population = pd.read_excel("https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.xls?v=290.4", header = 2).rename(columns = {"FIPStxt": "FIPS"})
education = pd.read_excel("https://www.ers.usda.gov/webdocs/DataFiles/48747/Education.xls?v=290.4", header = 4).rename(columns = {"FIPS Code": "FIPS", "Area name": "Area_Name"})
unemployment = pd.read_excel("https://www.ers.usda.gov/webdocs/DataFiles/48747/Unemployment.xls?v=290.4", header = 4).rename(columns = {"fips_txt": "FIPS", "area_name": "Area_Name", "Stabr": "State"})
income = pd.read_excel("https://www.ers.usda.gov/webdocs/DataFiles/48747/PovertyEstimates.xls?v=290.4", header = 4).rename(columns = {"FIPStxt": "FIPS", "Stabr": "State", "Area_name": "Area_Name"})


dfs = [df.set_index(['FIPS', 'State']) for df in [population, education, unemployment, income]]
data_county = pd.concat(dfs, axis=1)
df_election = df_election.merge(data_county.reset_index(), left_on = "county_fips", right_on = "FIPS")
df_election['county_fips'] = df_election['county_fips'].astype(str).str.lstrip('0')
shp['FIPS'] = shp['GEOID'].astype(str).str.lstrip('0')
votes = shp.merge(df_election, left_on = "FIPS", right_on = "county_fips")


df_historical = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false', sep = "\t")
df_historical = df_historical.dropna(subset = ["FIPS"])
df_historical["FIPS"] = df_historical["FIPS"].astype(int)
df_historical['share'] = df_historical['candidatevotes']/df_historical['totalvotes']
df_historical = df_historical[["year", "FIPS", "party", "candidatevotes", "share"]]
df_historical['party'] = df_historical['party'].fillna("other")

df_historical_wide = df_historical.pivot_table(index = "FIPS", values=['candidatevotes',"share"], columns = ["year","party"])
df_historical_wide.columns = ["_".join(map(str, s)) for s in df_historical_wide.columns.values]
df_historical_wide = df_historical_wide.reset_index()
df_historical_wide['FIPS'] = df_historical_wide['FIPS'].astype(str).str.lstrip('0')
votes['FIPS'] = votes['GEOID'].astype(str).str.lstrip('0')
votes = votes.merge(df_historical_wide, on = "FIPS")
```

## Principe

L'objectif des kmeans est de partitioner l'espace d'observations en trouvant des points (*centroids*) qui permettent de créer des centres de gravité autour pour lesquels les observations proches peuvent être regroupés dans une classe homogène

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)

{{% panel status="hint" title="Hint" icon="fa fa-lightbulb" %}}
L'objectif des *kmeans* est de trouver un ensemble une partition des données $S=\{S_1,...,S_K\}$ telle que 
$$
\arg\min_{S} \sum_{i=1}^K \sum_{x \in S_i} ||x - \mu_i||^2
$$
avec $\mu_i$ la moyenne des $x_i$ dans l'ensemble de points $S_i$
{{% /panel %}}

```{python}
import matplotlib.pyplot as plt
```



{{% panel status="exercise" title="Exercise 1: principe du kmeans" icon="fas fa-pencil-alt" %}}
1. Importer les données (l'appeler `df`) et de restreindre aux variables `'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', "Percent of adults with a bachelor's degree or higher, 2015-19"` et bien-sûr  `'per_gop'`
2. Faire un kmeans avec $k=4$
3. Créer une variable supplémentaire stockant le résultat de la typologie
4. Choisir deux variables et représenter le nuage de point en colorant différemment
en fonction du label obtenu
5. Représenter la distribution du vote pour chaque *cluster*

{{% /panel %}}

```{python importdata}
import numpy as np
import pandas as pd
import geopandas as gpd

xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', "Percent of adults with a bachelor's degree or higher, 2015-19"]

df2 = votes[xvars + ["per_gop"]].dropna()
```


```{python kmeans}
from sklearn.cluster import KMeans
model = KMeans(n_clusters=4)
model.fit(df2[xvars])

votes['label'] = model.labels_
```

Sur une carte cela donne:

```{python mapkmeans}
votes.plot(column = "label", cmap = "inferno")
```

```{python, include = FALSE}
import seaborn as sns

p = sns.scatterplot(data=votes, x="Median_Household_Income_2019", y="Unemployment_rate_2019", hue = "label", palette="deep", alpha = 0.4)
p.set(xscale="log")
```

```{python}
p
plt.show()
```


```{python}
p2 = sns.displot(data=votes, x="label", hue="label", alpha = 0.4)
```

```{python}
p2
plt.show()
```


## Choisir le nombre de classes

Le nombre de classes est fixé par hypothèse du modélisateur. Il y a un arbitrage
entre biais et variance. Un grand nombre de classes implique une variance
infra-classe très faible. Avec beaucoup de classes, on tend à sur-apprendre, ce
qui est mauvais pour la prédiction (même s'il n'est jamais possible de déterminer
le vrai type d'une observation puisqu'on est en apprentissage non supervisé). 

Si le nombre de classes à fixer est inconnu (il n'y a pas d'hypothèses de
modélisation qui justifient plus ou moins de classes), il existe des méthodes
statistiques:

* Méthode du coude (*elbow method*): on prend le point d'inflexion de la courbe
de performance du modèle. Cela représente le moment où ajouter une classe
(complexité croissante du modèle) n'apporte que des gains modérer dans la 
modélisation des données
* Score de silhouette: mesure de similarité entre un point et les autres points
du cluster par rapport aux autres clusters. Moins succinctement:

> Silhouette value is a measure of how similar an object is to its own cluster
> (cohesion) compared to other clusters (separation). The silhouette ranges
> from −1 to +1, where a high value indicates that the object is
> well matched to its own cluster and poorly matched to neighboring
> clusters. If most objects have a high value, then the clustering
> configuration is appropriate. If many points have a low or negative
> value, then the clustering configuration may have too many or too few clusters
> 
> Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))

Le package `yellowbrick` fournit une extension utile à `scikit` pour représenter
facilement la performance en *clustering*.

Pour la méthode du coude, la courbe
de performance du modèle marque un coude léger à $k=4$. Le modèle initial
semblait donc approprié.

```{python elbow, echo = TRUE}
from yellowbrick.cluster import KElbowVisualizer
visualizer = KElbowVisualizer(model, k=(2,12))
visualizer.fit(df2[xvars])        # Fit the data to the visualizer
visualizer.show()        # Finalize and render the figure
```

`yellowbrick` permet également de représenter des silhouettes mais 
l'interprétation en est moins aisée:
  
```{python silhouette, include = FALSE, echo=TRUE}
from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(2, 2, figsize=(15,8))
j=0
for i in [3, 4, 6, 10]:
    j += 1
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(j, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    ax[q-1][mod].set_title("k = " + str(i))
    visualizer.fit(df2[xvars])
```

```{python}
fig
```
