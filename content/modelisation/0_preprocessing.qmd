---
title: "Préparation des données pour construire un modèle"
title-en: "Preprocessing before building machine learning models"
author: Lino Galiana
tags:
  - scikit
  - machine learning
  - US election
  - preprocessing
  - Modelisation
  - Exercice
categories:
  - Modélisation
  - Exercice
description: |
  Afin d'avoir des données cohérentes avec les hypothèses de modélisation, il est fondamental de prendre le temps de préparer les données à fournir à un modèle. La qualité de la prédiction dépend fortement de ce travail préalable qu'on appelle _preprocessing_. Ce chapitre présente les enjeux et les illustre par le biais de la librairie `Scikit Learn`, qui rend ce travail moins fastidieux et plus fiable. 
description-en: |
  In order to obtain data that is consistent with modeling assumptions, it is essential to take the time to prepare the data to be supplied to a model. The quality of the prediction depends heavily on this preliminary work, known as _preprocessing_. This chapter presents the issues involved and illustrates them using the `Scikit Learn` library, which makes this work less tedious and more reliable.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/artisan.jfif
echo: false
---

{{< badges
    printMessage="true"
>}}

:::: {.content-visible when-profile="fr"}

::: {.callout-tip collapse="true"}

## Compétences à l'issue de ce chapitre

- Comprendre l’importance cruciale du preprocessing pour garantir la cohérence entre les données et les hypothèses de modélisation, en s’appuyant sur l’écosystème robuste de Scikit-Learn ;
- Explorer la structure des données à l’aide de Pandas pour sélectionner les variables pertinentes avant modélisation ;
- Transformer les données continues selon les besoins du modèle : standardisation (distribution centrée-réduite) ou normalisation (norme unité), selon le contexte de l’algorithme ;
- Encoder les variables catégorielles via LabelEncoder, OrdinalEncoder ou OneHotEncoder pour les rendre utilisables en modélisation ;
- Gérer les valeurs manquantes grâce à l’imputation (moyenne, médiane, mode ou méthodes plus sophistiquées), au lieu de supprimer systématiquement les observations ;
- Détecter et traiter les valeurs aberrantes (outliers), en les identifiant via leur distribution, puis en les retirant si elles nuisent à la qualité du modèle ;
- Comprendre que, du point de vue de l'implémentation en Scikit, le preprocessing constitue un apprentissage : les paramètres estimés (moyenne, variance) sur l’ensemble d’entraînement peuvent être ré-appliqués à tout nouveau jeu de données, assurant que la distribution des données post-processing coïncide avec celle des données d'apprentissage.

:::

::::

  
::::: {.content-visible when-profile="en"}

::: {.callout-tip collapse="true"}

## Skills you will acquire in this chapter

- Understand the critical role of preprocessing in aligning data with modeling assumptions, using the robust Scikit-Learn ecosystem  
- Use Pandas to explore data structure and select relevant features before modeling  
- Transform continuous variables to fit model requirements: standardization (zero mean, unit variance) or normalization (unit norm), depending on the algorithm  
- Encode categorical variables using `LabelEncoder`, `OrdinalEncoder`, or `OneHotEncoder` to make them usable in machine learning models  
- Handle missing data through imputation (mean, median, mode, or more advanced techniques) rather than simply dropping observations  
- Detect and manage outliers by analyzing their distribution and removing them when they negatively impact model performance  
- Understand that in Scikit-Learn, preprocessing is part of the learning process: parameters like mean or variance are estimated from the training set and reapplied to new data, ensuring consistency in distribution between training and prediction phases  



:::

::::

::: {.content-visible when-profile="fr"}
# Introduction

L'[introduction de cette partie](/content/modelisation/index.qmd) présentait les enjeux de l'adoption d'une approche algorithmique plutôt que statistique pour modéliser des processus empiriques. L'objectif de ce chapitre est d'introduire à la méthodologie du _machine learning_, aux choix qu'impliquent une approche algorithmique sur la structuration du travail sur les données. Ce sera également l'occasion de présenter l'écosystème du _machine learning_ en `Python` et notamment la librairie centrale dans celui-ci: [`Scikit Learn`](https://scikit-learn.org/stable/).

L'objectif de ce chapitre est de présenter quelques éléments de 
préparation des données. Il s'agit d'une étape fondamentale, à ne
pas négliger. Les modèles reposent sur certaines hypothèses, généralement
relatives à la distribution théorique des variables, qui y sont intégrées.

Il est nécessaire de faire correspondre la distribution empirique
à ces hypothèses, ce qui implique un travail de restructuration des données.
Celui-ci permettra d'avoir des résultats de modélisation plus pertinents. 
Nous verrons dans le chapitre sur les *pipelines* comment industrialiser
ces étapes de _preprocessing_ afin de se simplifier la vie pour appliquer
un modèle sur un jeu de données différent de celui sur lequel il a été estimé. 

Ce chapitre, comme l'ensemble de la partie _machine learning_, est une introduction pratique illustrée dans une perspective de prédiction électorale. En l'occurrence, il s'agit de prédire les résultats des élections américaines de 2020 au niveau comté à partir de variables socio-démographiques. L'idée sous-jacente est qu'il existe des facteurs sociologiques, économiques ou démographiques influençant le vote mais dont on ne connaît pas bien les motifs ou les interactions complexes entre plusieurs facteurs.
:::

::: {.content-visible when-profile="en"}

# Introduction

The [introduction to this section](/content/modelisation/index.qmd) discussed the importance of adopting an algorithmic rather than a statistical approach for modeling empirical processes. The goal of this chapter is to introduce machine learning methodology and the choices that an algorithmic approach entails for structuring data work. This will also be an opportunity to introduce the Python machine learning ecosystem, particularly its core library: [`Scikit-Learn`](https://scikit-learn.org/stable/).

The aim of this chapter is to present some data preparation elements. This is a fundamental step that should not be overlooked. Models are based on certain assumptions, usually related to the theoretical distribution of variables, which are integrated into them.

It is necessary to align the empirical distribution with these assumptions, which requires a restructuring of the data. This will lead to more relevant modeling results. In the chapter on *pipelines*, we will see how to industrialize these preprocessing steps to simplify applying a model to a dataset different from the one on which it was estimated.

This chapter, like the entire machine learning section, is a practical introduction illustrated from an electoral prediction perspective. Specifically, it involves predicting the results of the 2020 U.S. elections at the county level based on socio-demographic variables. The underlying idea is that there are sociological, economic, or demographic factors influencing voting behavior, but the motivations or complex interactions between these factors are not well understood.

:::



::: {.content-visible when-profile="fr"}
## Présentation de l'écosystème `Scikit`

`Scikit Learn` est aujourd'hui la librairie de référence dans l'écosystème du
_Machine Learning_. Il s'agit d'une librairie qui, malgré les très nombreuses
méthodes implémentées, présente l'avantage d'être un point d'entrée unifié.
Cet aspect unifié est l'une des raisons du succès précoce de celle-ci. `R` n'a 
bénéficié que plus récemment d'une librairie unifiée,
à savoir [`tidymodels`](https://www.tidymodels.org/).

Une autre raison du succès de `Scikit` est son approche opérationnelle : la mise
en production de modèles développés via les _pipelines_ `Scikit` est peu coûteuse.
Un [chapitre spécial de ce cours](/pipeline-scikit) est dédié aux _pipelines_.
Avec Romain Avouac, nous proposons un [cours plus avancé](https://ensae-reproductibilite.github.io/website/) 
en dernière année d'ENSAE où nous présentons certains enjeux relatifs
à la mise en production de modèles développés avec `Scikit`. 

Le guide utilisateur de `Scikit` est une référence précieuse,
à consulter régulièrement. La partie sur le *preprocessing*, objet de ce chapitre, est
disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).

:::

::: {.content-visible when-profile="en"}

## Introduction to the `Scikit` ecosystem

`Scikit Learn` is currently the go-to library in the machine learning ecosystem. It is a library that, despite its many implemented methods, offers the advantage of a unified entry point. This unified approach is one of the reasons for its early success. `R` only recently gained a unified library, namely [`tidymodels`](https://www.tidymodels.org/).

Another reason for `Scikit`'s success is its operational focus: deploying models developed through `Scikit` pipelines is cost-effective. A [dedicated chapter of this course](/pipeline-scikit) covers pipelines.
Together with Romain Avouac, we offer a [more advanced course](https://ensae-reproductibilite.github.io/website/) in the final year at ENSAE, where we present some challenges related to deploying models developed with `Scikit`.

The `Scikit` user guide is a valuable reference to consult regularly. The section on *preprocessing*, the focus of this chapter, is available [here](https://scikit-learn.org/stable/modules/preprocessing.html).

:::




:::: {.content-visible when-profile="fr"}

::: {.callout-note}
## `Scikit Learn`, un succès français ! 🐓🥖🥐

`Scikit Learn` est une librairie _open source_ issue des travaux de l'[Inria](https://www.inria.fr/fr) 🇫🇷. Depuis plus de 10 ans, cette institution publique française développe et maintient ce _package_ téléchargé 2 millions de fois par jour. En 2023, pour sécuriser la maintenance de ce _package_, une  _start up_ nommée [`Probabl.ai`](https://probabl.ai/) a été créée autour de l'équipe des développeurs.euses de `Scikit`.

Pour découvrir la richesse de l'écosystème `Scikit`, il 
est recommandé de suivre le
[`MOOC scikit`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),
développé dans le cadre de l'initiative [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn).

:::

::::

:::: {.content-visible when-profile="en"}

::: {.callout-note}
## `Scikit Learn`, a French Success! 🐓🥖🥐

`Scikit Learn` is an open-source library originating from the work of [Inria](https://www.inria.fr/fr) 🇫🇷. For over 10 years, this French public institution has developed and maintained this package, which is downloaded 2 million times a day. In 2023, to secure the maintenance of this package, a startup named [`Probabl.ai`](https://probabl.ai/) was created around the team of `Scikit` developers.

To explore the depth of the `Scikit` ecosystem, it is recommended to follow the
[`Scikit MOOC`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),
developed as part of the [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn) initiative.
:::

::::


::: {.content-visible when-profile="fr"}

## Préparation des données

L'exercice 1 permet, à ceux qui le désirent, d'essayer de le reconstituer pas à pas. 

Les _packages_ suivants sont nécessaires pour importer et visualiser
les données d'élection :

:::

::: {.content-visible when-profile="en"}

## Data preparation

Exercise 1 allows those interested to try to recreate it step by step.

The following packages are needed to import and visualize the election data:

:::


```{python}
#| eval: false
#| echo: true
!pip install --upgrade xlrd
!pip install geopandas
```


::: {.content-visible when-profile="fr"}

Les sources de données étant diverses, le code qui construit la base finale est directement fourni. 
Le travail de construction d'une base unique
est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,
pour [réviser `Pandas`](/content/manipulation/02a_pandas_tutorial.qmd):

:::

::: {.content-visible when-profile="en"}

The data sources are varied, so the code that builds the final dataset is provided directly.
Building a single dataset can be somewhat tedious, but it’s a good exercise, which you can try,
to [review `Pandas`](/content/manipulation/02a_pandas_tutorial.qmd):

:::

{{< include "01_preprocessing/_exo1.qmd" >}}

::: {.content-visible when-profile="fr"}
Néanmoins, avant de se concentrer sur la préparation des données, nous
allons passer un peu de temps à explorer la structure des données 
à partir de laquelle nous désirons construire une modélisation. Ceci 
est indispensable afin de comprendre la nature de celles-ci et choisir
une modélisation adéquate. 

Ce code introduit une base nommée `votes` dans l'environnement. Il s'agit d'une base rassemblant les différentes sources. Elle a l'aspect
suivant :
:::

::: {.content-visible when-profile="en"}

However, before focusing on data preparation, we will spend some time exploring the structure of the data from which we want to build a model. This is essential to understand its nature and choose an appropriate model.

This code introduces a dataset named `votes` into the environment. It is a combined dataset from different sources and appears as follows:
:::


```{python}
#| echo: true
votes.loc[:, votes.columns != "geometry"].head(3)
```

::: {.content-visible when-profile="fr"}
La carte choroplèthe suivante permet de visualiser rapidement les résultats
(l'Alaska et Hawaï ont été exclus).
:::

::: {.content-visible when-profile="en"}

The following choropleth map provides a quick visualization of the results (Alaska and Hawaii are excluded).
:::


```{python}
#| warning: false
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Code pour reproduire cette carte"
from plotnine import *

# republican : red, democrat : blue
color_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}

(
  ggplot(votes) +
  geom_map(aes(fill = "winner")) +
  scale_fill_manual(color_dict) +
  labs(fill = "Winner") +
  theme_void() +
  theme(legend_position = "bottom")
)
```


:::: {.content-visible when-profile="fr"}

::: {.callout-important}
## Le piège territorial

Comme cela a été évoqué dans le [chapitre consacré à la cartographie](/content/visualisation/matplotlib.qmd), les cartes choroplèthes peuvent donner une impression fallacieuse
que le parti Républicain a gagné largement en 2020 car ce type de représentation graphique donne plus d'importance aux grands espaces plutôt qu'aux espaces denses. Ceci explique que ce type de carte ait pu servir 
de justification pour contester les résultats du vote.

Il existe des représentations à 
privilégier pour ce type de phénomènes où la densité est importante. L'une des représentations à privilégier est les
ronds proportionnels (voir @inseeSemiologie, _"Le piège territorial en cartographie"_). Les cercles proportionnels permettent ainsi à l'oeil de se concentrer sur les 
zones les plus denses et non sur les grands espaces. Cette fois, on voit bien
que le vote démocrate est majoritaire, ce que cachait l'aplat de couleur. 

Le [GIF "Land does not vote, people do"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF), qui avait eu un certain succès en 2020, propose un autre mode de visualisation.
La carte originale a été construite avec `JavaScript`. Cependant,
on dispose avec `Python` de plusieurs outils
pour répliquer, à faible coût, cette carte 
grâce à
l'une des surcouches à `JavaScript` vues dans la partie [visualisation](/content/visualisation/index.qmd). 

:::

::::

::: {.content-visible when-profile="en"}

:::: {.callout-important}
## The Territorial Trap

As mentioned in the [chapter on mapping](/content/visualisation/matplotlib.qmd), choropleth maps can give a misleading impression that the Republican Party won by a large margin in 2020 because this type of graphic representation gives more importance to large areas rather than dense areas. This explains why this type of map has been used as justification for contesting the election results.

There are alternative representations better suited to such phenomena where density is significant. One such representation is proportional circles (see @inseeSemiologie, _"The Territorial Trap in Cartography"_). Proportional circles allow the eye to focus on denser areas rather than large open spaces. With this representation, it becomes clear that the Democratic vote is in the majority, which was obscured by the flat color fill.

The [GIF "Land does not vote, people do"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF), which gained popularity in 2020, offers another visualization approach. The original map was created with `JavaScript`. However, with `Python`, we have several tools to replicate this map at a low cost using one of the `JavaScript` overlays discussed in the [visualization section](/content/visualisation/index.qmd).

::::
:::



```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Code pour reproduire cette carte interactive"
import numpy as np
import pandas as pd
import geopandas as gpd
import plotly
import plotly.graph_objects as go


centroids = votes.copy()
centroids.geometry = centroids.centroid
centroids['size'] = centroids['CENSUS_2020_POP'] / 10000  # to get reasonable plotable number

color_dict = {"republican": '#FF0000', 'democrats': '#0000FF'}
centroids["winner"] =  np.where(centroids['votes_gop'] > centroids['votes_dem'], 'republican', 'democrats') 


centroids['lon'] = centroids['geometry'].x
centroids['lat'] = centroids['geometry'].y
centroids = pd.DataFrame(centroids[["county_name",'lon','lat','winner', 'CENSUS_2020_POP',"state_name"]])
groups = centroids.groupby('winner')

df = centroids.copy()

df['color'] = df['winner'].replace(color_dict)
df['size'] = df['CENSUS_2020_POP']/6000
df['text'] = df['CENSUS_2020_POP'].astype(int).apply(lambda x: '<br>Population: {:,} people'.format(x))
df['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']

fig_plotly = go.Figure(
  data=go.Scattergeo(
  locationmode = 'USA-states',
  lon=df["lon"], lat=df["lat"],
  text = df["hover"],
  mode = 'markers',
  marker_color = df["color"],
  marker_size = df['size'],
  hoverinfo="text"
  )
)

fig_plotly.update_traces(
  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}
)

fig_plotly.update_layout(
  title_text = "Reproduction of the \"Acres don't vote, people do\" map <br>(Click legend to toggle traces)",
  showlegend = True,
  geo = {"scope": 'usa', "landcolor": 'rgb(217, 217, 217)'}
)

fig_plotly.show()
```


::: {.content-visible when-profile="fr"}
# La démarche générale

Dans ce chapitre, nous allons nous focaliser sur la préparation
des données à faire en amont du travail de modélisation.
Cette étape est indispensable pour s'assurer de la cohérence
entre les données et les hypothèses de modélisation mais aussi
pour produire des analyses valides scientifiquement. 

La démarche générale que nous adopterons dans ce chapitre, et qui sera ensuite raffinée dans les prochains chapitres, est la suivante : 

![Illustraton de la méthodologie du _machine learning_](/content/modelisation/img/pipeline1.png){#fig-ml-pipeline}

La @fig-ml-pipeline illustre la structuration d'un problème de _machine learning_. 

Tout d'abord, on découpe l'ensemble des données disponibles en deux parties, __échantillons d'apprentissage__ et de __validation__. Le premier sert à entraîner un modèle et la qualité des prédictions de celui-ci est
évaluée sur le deuxième pour limiter
le biais de surapprentissage. Le chapitre suivant approfondira
cette question de l'évaluation des modèles. A ce stade de notre
progression, on se concentrera dans ce chapitre
sur la question des données. 

La librairie `Scikit` est particulièrement pratique parce qu'elle propose énormément d'algorithmes de _machine learning_ avec quelques points d'entrée unifiée, notamment les méthodes `fit` et `predict`. Néanmoins, l'unification va au-delà de l'entraînement d'algorithmes. Toutes les étapes de préparation de données qui sont intégrées dans `Scikit` proposent ces deux mêmes points d'entrée. Autrement dit, les préparations de données sont construites comme une estimation de paramètres qui peut être réappliquée sur un autre jeu de données. Par exemple, cette préparation de données peut être une estimation de moyenne et variance pour normaliser des variables. La moyenne et la variance seront évaluées sur l'échantillon d'apprentissage et les mêmes moyennes et variances pourront être réappliquées sur un autre jeu de données pour le normaliser de la même façon. 
:::

::: {.content-visible when-profile="en"}

# General Approach

In this chapter, we will focus on data preparation to be done before modeling work. This step is essential to ensure consistency between the data and modeling assumptions and to produce scientifically valid analyses.

The general approach we will adopt in this chapter, which will be refined in subsequent chapters, is as follows:

![Illustration of machine learning methodology](/content/modelisation/img/pipeline1.png){#fig-ml-pipeline}

@fig-ml-pipeline illustrates the structure of a machine learning problem.

First, the available dataset is split into two parts: __training sample__ and __validation sample__. The former is used to train a model, and its prediction quality is evaluated on the latter to limit overfitting bias. The next chapter will delve deeper into model evaluation. At this stage of our progress, we will focus in this chapter on data issues.

The `Scikit` library is particularly convenient because it offers many machine learning algorithms with a few unified entry points, especially the `fit` and `predict` methods. However, the unification extends beyond algorithm training. All data preparation steps integrated into `Scikit` offer these same entry points. In other words, data preparations are built like parameter estimations that can be reapplied to another dataset. For example, this data preparation might be a mean and variance estimation for normalizing variables. The mean and variance are calculated on the training sample, and the same values can be reapplied to another dataset to normalize it in the same way.
:::


::: {.content-visible when-profile="fr"}
# Explorer la structure des données

La première étape nécessaire à suivre avant de se lancer dans la modélisation
est de déterminer les variables à inclure dans le modèle.

Les fonctionnalités de `Pandas` sont, à ce niveau, suffisantes pour explorer des structures simples.
Néanmoins, lorsqu'on est face à un jeu de données présentant de
nombreuses variables explicatives (*features* en machine learning, *covariates* en économétrie),
il est souvent judicieux d'avoir une première étape de sélection de variables,
ce que nous verrons par la suite dans la [partie dédiée](/content/modelisation/4_featureselection.qmd).  

Avant d'être en mesure de sélectionner le meilleur ensemble de variables explicatives,
nous allons en prendre un nombre restreint et arbitraire.
La première tâche est de représenter les relations entre les données,
notamment la relation des variables explicatives
à la variable dépendante (le score du parti républicain)
ainsi que les relations entre les variables explicatives. 
:::

::: {.content-visible when-profile="en"}

# Exploring data structure

The first necessary step before diving into modeling is to determine which variables to include in the model.

`Pandas` functionalities are sufficient at this stage for exploring simple structures.
However, when dealing with a dataset with numerous explanatory variables (*features* in machine learning, *covariates* in econometrics), it is often wise to start with a variable selection step, which we will cover later in the [dedicated section](/content/modelisation/4_featureselection.qmd).

Before selecting the best set of explanatory variables, we will start with a small and arbitrary selection.
The first task is to represent the relationships within the data, particularly the relationship between explanatory variables and the dependent variable (the Republican Party’s score), as well as relationships among explanatory variables.
:::


{{< include "01_preprocessing/_exo2.qmd" >}}


::: {.content-visible when-profile="fr"}
# Transformer les données

Les différences d'échelle ou de distribution entre les variables peuvent 
diverger des hypothèses sous-jacentes dans les modèles.

Par exemple, dans le cadre
de la régression linéaire, les variables catégorielles ne sont pas traitées à la même
enseigne que les variables ayant valeur dans $\mathbb{R}$. Une variable
discrète (prenant un nombre fini de valeurs) devra être transformée en suite de
variables 0/1 (des _dummies_) par rapport à une modalité de référence pour être en adéquation
avec les hypothèses de la régression linéaire.
On appelle ce type de transformation
*one-hot encoding*, sur laquelle nous reviendrons. Il s'agit d'une transformation,
parmi d'autres, disponibles dans `Scikit` pour mettre en adéquation un jeu de
données et des hypothèses mathématiques. 

L'ensemble de ces tâches de préparation de données s'appelle le *preprocessing* ou le _feature engineering_. L'un des intérêts
d'utiliser `Scikit` est qu'on peut considérer qu'une tâche de _preprocessing_
est, en fait, une tâche d'apprentissage. En effet, le _preprocessing_ 
consiste à apprendre des paramètres d'une structure 
de données (par exemple estimer moyennes et variances pour les retrancher à chaque
observation) et on peut très bien appliquer ces paramètres
à des observations qui n'ont pas servi à construire
ceux-ci. Autrement dit, cette préparation de données s'intègre très bien dans le _pipeline_ @fig-ml-pipeline. 
:::

::: {.content-visible when-profile="en"}

# Transforming data

Differences in scale or distribution between variables can diverge from the underlying assumptions in models.

For example, in linear regression, categorical variables are not treated the same way as variables with values in $\mathbb{R}$. A discrete variable (taking a finite number of values) must be transformed into a sequence of 0/1 variables (dummies) relative to a reference category to meet the assumptions of linear regression. This type of transformation is known as *one-hot encoding*, which we will revisit. It is one of many transformations available in `Scikit` to align a dataset with mathematical assumptions.

All these data preparation tasks fall under preprocessing or feature engineering. One advantage of using `Scikit` is that preprocessing tasks can be considered learning tasks. Preprocessing involves learning parameters from a data structure (e.g., estimating means and variances to subtract from each observation), and these parameters can then be applied to observations not used to calculate them. In other words, this data preparation fits seamlessly into the pipeline shown in @fig-ml-pipeline.
:::


:::: {.content-visible when-profile="fr"}
## _Preprocessing_ de variables continues

Nous allons voir deux processus très classiques de *preprocessing* pour des variables continues : 

1. La **standardisation** transforme des données pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$.

2. La **normalisation**  transforme les données de manière à obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1.

Il en existe d'autres, par exemple le `MinMaxScaler` pour renormaliser les variables en fonction des bornes minimales et maximales des valeurs observées. Le choix de la méthode à mettre en oeuvre dépend du type d'algorithmes choisis par la suite: les hypothèses des k plus proches voisins (knn) seront différentes de celles d'une _random forest_. C'est pour cette raison que, normalement, on définit des _pipelines_ complets, intégrant à la fois _preprocessing_ et apprentissage. Ce sera l'objet des prochains chapitres.

::: {.callout-caution}
Pour les statisticiens.ennes,
le terme _normalization_ dans le vocable `Scikit` peut avoir un sens contre-intuitif.
On s'attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que $X \sim \mathcal{N}(0,1)$.
C'est, en fait, la **standardisation** en `Scikit` qui fait cela.

:::
::::

:::: {.content-visible when-profile="en"}

## Continuous Variable Preprocessing

We will cover two very common preprocessing steps for continuous variables:

1. **Standardization** transforms data so that the empirical distribution follows a $\mathcal{N}(0,1)$ distribution.

2. **Normalization** transforms data to achieve a unit norm ($\mathcal{l}_1$ or $\mathcal{l}_2$). In other words, with the appropriate norm, the sum of elements equals 1.

There are other options, such as `MinMaxScaler` to rescale variables according to observed minimum and maximum bounds. The choice of method depends on the algorithms chosen later: the assumptions of k-nearest neighbors (knn) differ from those of a random forest. For this reason, complete pipelines are usually defined, integrating both preprocessing and learning, which will be discussed in upcoming chapters.

:::: {.callout-caution}
For statisticians,
the term _normalization_ in `Scikit` terminology can have a counterintuitive meaning.
One might expect normalization to transform a variable so that $X \sim \mathcal{N}(0,1)$.
In `Scikit`, this is actually **standardization**.

::::

:::


::: {.content-visible when-profile="fr"}
### Standardisation

La standardisation consiste à transformer des données pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour être performants, la plupart des modèles de _machine learning_ nécessitent souvent d'avoir des données dans cette distribution. Même lorsque ce n'est pas indispensable, par exemple avec des régressions logistiques, cela peut accélérer la vitesse de convergence des algorithmes.
:::

::: {.content-visible when-profile="en"}

### Standardization

Standardization involves transforming data so that the empirical distribution follows a $\mathcal{N}(0,1)$ distribution. For optimal performance, most machine learning models often require data to follow this distribution. Even when it’s not essential, as with logistic regressions, it can speed up the convergence rate of algorithms.
:::


{{< include "01_preprocessing/_exo3.qmd" >}}


:::: {.content-visible when-profile="fr"}

C'est une illustration d'un problème classique en _machine learning_, le _data drift_, qui arrive lorsqu'on essaie d'extrapoler à des données dont la distribution ne correspond plus à celle des données d'apprentissage. Ce type de situation arrive typiquement lorsqu'on a entraîné un algorithme sur un échantillon biaisé de la population ou lorsqu'on a des séries temporelles non stationnaires. Il est donc important de bien réfléchir à la constitution de l'échantillon d'apprentissage et aux possibilités d'extrapolation sur une population plus large : la validité externe du modèle - préparation des données ou algorithme d'apprentissage - peut être nulle si cette étape a été réalisée de manière hâtive.

::: {.callout-important}
## Le _data drift_

Le _data drift_ désigne un changement dans la distribution des données au fil du temps, entraînant une dégradation des performances d’un modèle de _machine learning_ qui, par construction, a été entraîné sur des données passées. 

Ce phénomène peut survenir à cause de variations dans la population cible, de changements dans les caractéristiques des données ou de facteurs externes. 

Il est crucial de détecter le _data drift_ pour ajuster ou réentraîner le modèle, afin de maintenir sa pertinence et sa précision. Les techniques de détection incluent des tests statistiques et le suivi de métriques spécifiques.

:::
::::

:::: {.content-visible when-profile="en"}

This is an illustration of a classic problem in machine learning, called _data drift_, which occurs when we attempt to extrapolate to data whose distribution no longer corresponds to that of the training data. This situation typically arises when an algorithm has been trained on a biased sample of the population or when we have non-stationary time series data. It is, therefore, essential to carefully consider the construction of the training sample and the potential for extrapolation to a broader population: the external validity of the model—whether data preparation or learning algorithm—can be null if this step was rushed.

::: {.callout-important}
## Data Drift

Data drift refers to a shift in data distribution over time, leading to a degradation in the performance of a machine learning model that, by design, was trained on past data.

This phenomenon can occur due to changes in the target population, shifts in data characteristics, or external factors.

Detecting data drift is crucial to adjust or retrain the model, ensuring its relevance and accuracy. Detection techniques include statistical tests and monitoring specific metrics.
:::
::::


::: {.content-visible when-profile="fr"}
### Normalisation

La **normalisation** est l'action de transformer les données de manière
à obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire.
Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1.
Par défaut, la norme utilisée par `Scikit` est une norme  $\mathcal{l}_2$.
Cette transformation est particulièrement utilisée en classification de texte ou pour effectuer du *clustering*.

Au passage, ceci est l'occasion de découvrir comment découper ses données en plusieurs échantillons grâce à la fonction [`train_test_split`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) de `Scikit`. Nous allons faire un échantillon de 70% des données pour estimer les paramètres de normalisation (phase d'apprentissage) et extrapoler aux 30% de données restantes. Cette répartition est assez classique mais est bien-sûr adaptable selon les projets. L'avantage d'utiliser [`train_test_split`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) plutôt que de faire soi-même les échantillonnages avec la méthode `sample` de `Pandas` est que la fonction de `Scikit` permettra d'aller beaucoup plus loin dans le paramétrage de l'échantillonnage, notamment si on désire de la stratification, tout en étant fiable. Faire ceci de manière manuelle est fastidieux et risqué car potentiellement complexe à mettre en oeuvre sans erreur. 
:::

::: {.content-visible when-profile="en"}

### Normalization

**Normalization** is the process of transforming data to achieve a unit norm ($\mathcal{l}_1$ or $\mathcal{l}_2$).
In other words, with the appropriate norm, the sum of elements equals 1.
By default, `Scikit` uses an $\mathcal{l}_2$ norm.
This transformation is especially useful in text classification or clustering.

This is also an opportunity to explore how to split data into multiple samples using the [`train_test_split`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) function in `Scikit`. We will create a 70% sample of the data to estimate normalization parameters (training phase) and extrapolate to the remaining 30%. This split is fairly standard but, of course, adaptable depending on the project. The advantage of using [`train_test_split`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) instead of manually sampling with `Pandas`’ `sample` method is that `Scikit`’s function allows for much more control over sampling, particularly if stratification is desired, while being reliable. Doing this manually can be tedious and risky, as it is potentially complex to implement without errors.
:::


{{< include "01_preprocessing/_exo4.qmd" >}}



::: {.content-visible when-profile="fr"}
## Encodage des valeurs catégorielles

Les données catégorielles doivent être recodées sous forme de valeurs numériques pour être intégrés aux modèles de *machine learning*.

Cela peut être fait de plusieurs manières avec `Scikit` :

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur numérique `[0,1,2]`. Cette approche a l'inconvénient d'introduire un ordre dans les modalités, ce qui n'est pas toujours souhaitable.
* `OrdinalEncoder`: une version généralisée du `LabelEncoder` qui a vocation à s'appliquer sur des matrices ($X$),
alors que `LabelEncoder` s'applique plutôt à un vecteur ($y$).

En ce qui concerne le _one hot encoding_, il est possible d'utiliser plusieurs méthodes :

* `pandas.get_dummies` effectue une opération de *dummy expansion*.
Un vecteur de taille *n* avec *K* catégories sera transformé en matrice de taille $n \times K$
pour lequel chaque colonne sera une variable *dummy* pour la modalité *k*.
Il y a ici $K$ modalités et il y a donc multicolinéarité.
Avec une régression linéaire avec constante,
il convient de retirer une modalité avant l'estimation.

* `OneHotEncoder` est une version généralisée (et optimisée) de la *dummy expansion*. C'est la méthode recommandée.


## Imputation

Les données peuvent souvent contenir des valeurs manquantes, autrement dit des observations de notre _DataFrame_ contenant un `NaN`. Ces trous dans les données peuvent être à l'origine de _bugs_ ou de mauvaises interprétations lorsque l'on passe à la modélisation.
Pour y remédier, une première approche peut être de retirer toutes les observations présentant un `NaN` dans au moins une des ses colonnes.
Cependant, si notre table contient beaucoup de `NaN`, ou bien que ces derniers sont répartis sur de nombreuses colonnes,
c'est aussi prendre le risque de retirer un nombre important de lignes, et avec cela de l'information importante pour un modèle car les valeurs manquantes sont rarement [réparties de manière aléatoire](https://stefvanbuuren.name/fimd/sec-MCAR.html).

Même si dans plusieurs situations, cette solution reste tout à fait viable, il existe une autre approche plus robuste appelée *imputation*. Cette méthode consiste à remplacer les valeurs manquantes par une valeur donnée. Par exemple :

- Imputation par la moyenne : remplacer tous les `NaN` dans une colonne par la valeur moyenne de la colonne ;
- Imputation par la médiane sur le même principe, ou par la valeur de la colonne la plus fréquente pour les variables catégorielles ;
- Imputation par régression : se servir d'autres variables pour essayer d'interpoler une valeur de remplacement adaptée.

Des méthodes plus complexes existent mais dans de nombreux cas, les approches ci-dessus peuvent suffire pour donner des résultats beaucoup plus satisfaisants.
Le package `Scikit` permet de faire de l'imputation de manière très simple ([documentation ici](https://scikit-learn.org/stable/modules/impute.html)).


## Gestion des valeurs aberrantes (_outliers_)

Les valeurs aberrantes (_outliers_ en anglais) sont des observations qui se situent significativement à l'extérieur de la tendance générale des autres observations dans un ensemble de données. En d'autres termes, ce sont des points de données qui se démarquent de manière inhabituelle par rapport à la distribution globale des données.
Cela peut être dû à des erreurs de remplissage, des personnes ayant mal répondu à un questionnaire, ou
parfois simplement des valeurs extrêmes qui peuvent biaiser un modèle de façon trop importante.

A titre d'exemple, cela va être 3 individus mesurant plus de 4 mètres dans une population,
ou bien des revenus de ménage dépassant les 10M d'euros par mois sur l'échelle d'un pays, etc.

Une bonne pratique peut donc être de systématiquement regarder la distribution des variables à disposition,
pour se rendre compte si certaines valeurs s'éloignent de façon trop importante des autres.
Ces valeurs vont parfois nous intéresser, si, par exemple, on se concentre uniquement sur les très hauts revenus (top 0.1%)
en France. Cependant, ces données vont souvent nous gêner plus qu'autre chose, surtout si elles n'ont pas de sens dans le monde réel.

Si l'on estime que la présence de ces données extrêmes, ou *outliers*, dans notre base de données vont être problématiques plus qu'autre chose,
alors il est tout à fait entendable et possible de simplement les retirer.
La plupart du temps, on va se donner une proportion des données à retirer, par exemple 0.1%, 1% ou 5%,
puis retirer dans les deux queues de la distribution les valeurs extrêmes correspondantes.

Plusieurs packages permettent de faire ce type d'opérations, qui sont parfois plus complexes si on s'intéresse aux outlier sur plusieurs variables.
On pourra notamment citer la fonction `IsolationForest()` du package `sklearn.ensemble`.
:::
::: {.content-visible when-profile="en"}

## Encoding Categorical Values

Categorical data must be recoded into numeric values to be integrated into machine learning models.

This can be done in several ways with `Scikit`:

* `LabelEncoder`: transforms a vector `["a","b","c"]` into a numeric vector `[0,1,2]`. This approach has the drawback of introducing an order to the categories, which is not always desirable.
* `OrdinalEncoder`: a generalized version of `LabelEncoder` designed to apply to matrices ($X$), while `LabelEncoder` applies mainly to a vector ($y$).

For one-hot encoding, several methods are available:

* `pandas.get_dummies` performs a dummy expansion.
A vector of size *n* with *K* categories will be transformed into a matrix of size $n \times K$, where each column represents a dummy variable for category *k*.
There are $K$ categories, resulting in multicollinearity.
In linear regression with a constant,
one category should be removed before estimation.

* `OneHotEncoder` is a generalized (and optimized) version of dummy expansion. This is the recommended method.


## Imputation

Data often contains missing values, that is, observations in our _DataFrame_ containing a `NaN`. These gaps can cause bugs or misinterpretations when moving to modeling.
One initial approach could be to remove all observations with a `NaN` in at least one column.
However, if our table contains many `NaN`s, or if these are spread across numerous columns,
we risk removing a significant number of rows, and, with that, losing important information, as missing values are rarely [randomly distributed](https://stefvanbuuren.name/fimd/sec-MCAR.html).

While this solution remains viable in many cases, a more robust approach called *imputation* exists. This method involves replacing missing values with a specified value. For example:

- Mean imputation: replacing all `NaN`s in a column with the column's average;
- Median imputation on the same principle, or using the most frequent column value for categorical variables;
- Regression imputation: using other variables to interpolate an appropriate replacement value.

More complex methods are available, but in many cases, the above approaches can provide much more satisfactory results.
The `Scikit` package makes imputation very straightforward ([documentation here](https://scikit-learn.org/stable/modules/impute.html)).


## Handling Outliers

Outliers are observations that significantly deviate from the general trend of other observations in a dataset. In other words, they are data points that stand out unusually from the overall data distribution.
This may be due to data entry errors, respondents who incorrectly answered a survey, or simply extreme values that may bias a model too much.

For example, these could be 3 individuals measuring over 4 meters in height within a population or household incomes exceeding 10 million euros per month at a national level.

It is good practice to routinely examine the distribution of available variables
to check if some values deviate too significantly from others.
Sometimes these values will interest us, for instance, if we are focusing solely on very high incomes (top 0.1%) in France. However, often these values will be more of a hindrance, especially if they don’t make sense in the real world.

If we find that the presence of these extreme values or *outliers* in our dataset is more problematic than helpful,
it is reasonable to simply remove them.
Most of the time, we set a percentage of data to remove, such as 0.1%, 1%, or 5%,
then remove the corresponding extreme values in both tails of the distribution.

Several packages can perform these operations, which can become complex if we examine outliers across multiple variables.
The `IsolationForest()` function in the `sklearn.ensemble` package is particularly noteworthy.
:::



::: {.content-visible when-profile="fr"}
## Exercice d'application

::: {.callout-caution}
## Attention aux nouvelles modalités !

Les _transformers_ créent un _mapping_ entre des modalités textuelles et des valeurs numériques. Cela présuppose que les données sur lesquelles a été construit ce _mapping_ intègrent l'ensemble des valeurs possibles pour les modalités textuelles. 

Néanmoins, si de nouvelles modalités apparaissent, le classifieur ne saura pas comment celles-ci doivent être transformées en valeurs numériques. Cela provoquera une erreur pour `Scikit`. Cette erreur technique est logique puisqu'il faudrait mettre à jour non seulement le _mapping_ mais aussi l'estimation des paramètres sous-jacents. 

:::

::::

:::: {.content-visible when-profile="en"}

## Application exercise

::: {.callout-caution}
## Be careful with new categories!

Transformers create a mapping between text categories and numeric values. This assumes that the data used to build this mapping includes all possible values for the text categories.

However, if new categories appear, the classifier will not know how to transform these into numeric values, which will cause an error in `Scikit`. This technical error makes sense, as it would require updating not only the mapping but also the estimation of underlying parameters.

:::
::::

{{< include "01_preprocessing/_exo5.qmd" >}}



::: {.content-visible when-profile="fr"}
# Références {.unnumbered}
:::

::: {.content-visible when-profile="en"}
# Reference {.unnumbered}
:::

::: {#refs}
:::
