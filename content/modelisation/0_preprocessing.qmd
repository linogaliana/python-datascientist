---
title: "Pr√©paration des donn√©es pour construire un mod√®le"
tags:
  - scikit
  - machine learning
  - US election
  - preprocessing
  - Modelisation
  - Exercice
categories:
  - Mod√©lisation
  - Exercice
description: |
  Afin d'avoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,
  il est absolument fondamental de prendre le temps de
  pr√©parer les donn√©es √† fournir √† un mod√®le. La qualit√© de la pr√©diction
  d√©pend fortement de ce travail pr√©alable qu'on appelle _preprocessing_.
  Beaucoup de m√©thodes sont disponibles dans `scikit`, ce qui rend ce travail
  moins fastidieux et plus fiable. 
bibliography: ../../reference.bib
image: featured_preprocessing.png
echo: false
---

{{< badges
    printMessage="true"
>}}

# Introduction

L'[introduction de cette partie](/content/modelisation/index.qmd) pr√©sentait les enjeux de l'adoption d'une approche algorithmique plut√¥t que statistique pour mod√©liser des processus empiriques. L'objectif de ce chapitre est d'introduire √† la m√©thodologie du _machine learning_, aux choix qu'impliquent une approche algorithmique sur la structuration du travail sur les donn√©es. Ce sera √©galement l'occasion de pr√©senter l'√©cosyst√®me du _machine learning_ en `Python` et notamment la librairie centrale dans celui-ci: [`Scikit Learn`](https://scikit-learn.org/stable/).

L'objectif de ce chapitre est de pr√©senter quelques √©l√©ments de 
pr√©paration des donn√©es. Il s'agit d'une √©tape fondamentale, √† ne
pas n√©gliger. Les mod√®les reposent sur certaines hypoth√®ses, g√©n√©ralement
relatives √† la distribution th√©orique des variables, qui y sont int√©gr√©es.

Il est n√©cessaire de faire correspondre la distribution empirique
√† ces hypoth√®ses, ce qui implique un travail de restructuration des donn√©es.
Celui-ci permettra d'avoir des r√©sultats de mod√©lisation plus pertinents. 
Nous verrons dans le chapitre sur les *pipelines* comment industrialiser
ces √©tapes de _preprocessing_ afin de se simplifier la vie pour appliquer
un mod√®le sur un jeu de donn√©es diff√©rent de celui sur lequel il a √©t√© estim√©. 

Ce chapitre, comme l'ensemble de la partie _machine learning_, est une introduction pratique illustr√©e dans une perspective de pr√©diction √©lectorale. En l'occurrence, il s'agit de pr√©dire les r√©sultats des √©lections am√©ricaines de 2020 au niveau comt√© √† partir de variables socio-d√©mographiques. L'id√©e sous-jacente est qu'il existe des facteurs sociologiques, √©conomiques ou d√©mographiques influen√ßant le vote mais dont on ne conna√Æt pas bien les motifs ou les interactions complexes entre plusieurs facteurs.  


## Pr√©sentation de l'√©cosyst√®me `Scikit`

`Scikit Learn` est aujourd'hui la librairie de r√©f√©rence dans l'√©cosyst√®me du
_Machine Learning_. Il s'agit d'une librairie qui, malgr√© les tr√®s nombreuses
m√©thodes impl√©ment√©es, pr√©sente l'avantage d'√™tre un point d'entr√©e unifi√©.
Cet aspect unifi√© est l'une des raisons du succ√®s pr√©coce de celle-ci. `R` n'a 
b√©n√©fici√© que plus r√©cemment d'une librairie unifi√©e,
√† savoir [`tidymodels`](https://www.tidymodels.org/).

Une autre raison du succ√®s de `Scikit` est son approche op√©rationnelle : la mise
en production de mod√®les d√©velopp√©s via les _pipelines_ `Scikit` est peu co√ªteuse.
Un [chapitre sp√©cial de ce cours](/pipeline-scikit) est d√©di√© aux _pipelines_.
Avec Romain Avouac, nous proposons un [cours plus avanc√©](https://ensae-reproductibilite.github.io/website/) 
en derni√®re ann√©e d'ENSAE o√π nous pr√©sentons certains enjeux relatifs
√† la mise en production de mod√®les d√©velopp√©s avec `Scikit`. 

Le guide utilisateur de `Scikit` est une r√©f√©rence pr√©cieuse,
√† consulter r√©guli√®rement. La partie sur le *preprocessing*, objet de ce chapitre, est
disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).


::: {.note}
## `Scikit Learn`, un succ√®s fran√ßais ! üêìü•ñü•ê

`Scikit Learn` est une librairie _open source_ issue des travaux de l'[Inria](https://www.inria.fr/fr) üá´üá∑. Depuis plus de 10 ans, cette institution publique fran√ßaise d√©veloppe et maintient ce _package_ t√©l√©charg√© 2 millions de fois par jour. En 2023, pour s√©curiser la maintenance de ce _package_, une  _start up_ nomm√©e [`Probabl.ai`](https://probabl.ai/) a √©t√© cr√©√©e autour de l'√©quipe des d√©veloppeurs.euses de `Scikit`.

Pour d√©couvrir la richesse de l'√©cosyst√®me `Scikit`, il 
est recommand√© de suivre le
[`MOOC scikit`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),
d√©velopp√© dans le cadre de l'initiative [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn).

:::

## Pr√©paration des donn√©es

L'exercice 1 permet, √† ceux qui le d√©sirent, d'essayer de le reconstituer pas √† pas. 


Les _packages_ suivants sont n√©cessaires pour importer et visualiser
les donn√©es d'√©lection :

```{python}
#| eval: false
#| echo: true
!pip install --upgrade xlrd
!pip install geopandas
```

Les sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni. 
Le travail de construction d'une base unique
est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,
pour [r√©viser `Pandas`](/content/manipulation/02a_pandas_tutorial.qmd)   :

::: {.exercise}
## Exercice 1 (optionnel): construire la base de donn√©es

__Cet exercice est OPTIONNEL__

1. T√©l√©charger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_02_sldl_500k.zip)
2. Exclure les Etats suivants : "02", "69", "66", "78", "60", "72", "15"
3. Importer les r√©sultats des √©lections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv
)
4. Importer les bases disponibles sur le site de l'USDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique
dans les 4 bases
5. *Merger* ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques
6. *Merger* aux donn√©es √©lectorales √† partir du code FIPS
7. *Merger* au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est
recommand√© d'utiliser la m√©thode `str.lstrip` pour les retirer
8. Importer les donn√©es des √©lections 2000 √† 2016 √† partir du [MIT Election Lab](https://electionlab.mit.edu/data)?
Les donn√©es peuvent √™tre directement requ√™t√©es depuis l'url
<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>
9. Cr√©er une variable `share` comptabilisant la part des votes pour chaque candidat. 
Ne garder que les colonnes `"year", "FIPS", "party", "candidatevotes", "share"`
10. Faire une conversion `long` to `wide` avec la m√©thode `pivot_table` pour garder une ligne
par comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.
11. Merger √† partir du code FIPS au reste de la base. 

:::

Si vous ne faites pas l'exercice 1, pensez √† charger les donn√©es en executant la fonction `get_data.py` :

```{python}
#| echo: true
#| output: false
import requests

url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'
r = requests.get(url, allow_redirects=True)
open('getdata.py', 'wb').write(r.content)

import getdata
votes = getdata.create_votes_dataframes()
```


N√©anmoins, avant de se concentrer sur la pr√©paration des donn√©es, nous
allons passer un peu de temps √† explorer la structure des donn√©es 
√† partir de laquelle nous d√©sirons construire une mod√©lisation. Ceci 
est indispensable afin de comprendre la nature de celles-ci et choisir
une mod√©lisation ad√©quate. 

Ce code introduit une base nomm√©e `votes` dans l'environnement. Il s'agit d'une
base rassemblant les diff√©rentes sources. Elle a l'aspect
suivant :

```{python}
#| echo: true
votes.loc[:, votes.columns != "geometry"].head(3)
```


La carte choropl√®the suivante permet de visualiser rapidement les r√©sultats
(l'Alaska et Hawa√Ø ont √©t√© exclus). 

```{python}
#| warning: false
#| echo: true
#| message: false
#| code-fold: true
#| code-summary: "Code pour reproduire cette carte"
from plotnine import *

# republican : red, democrat : blue
color_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}

(
  ggplot(votes) +
  geom_map(aes(fill = "winner")) +
  scale_fill_manual(color_dict) +
  labs(fill = "Winner") +
  theme_void() +
  theme(legend_position = "bottom")
)
```

::: {.important}
## Le pi√®ge territorial

Comme cela a √©t√© √©voqu√© dans le [chapitre consacr√© √† la cartographie](/content/visualisation/matplotlib.qmd), les cartes choropl√®thes peuvent donner une impression fallacieuse
que le parti R√©publicain a gagn√© largement en 2020 car ce type de repr√©sentation graphique donne plus d'importance aux grands espaces plut√¥t qu'aux espaces denses. Ceci explique que ce type de carte ait pu servir 
de justification pour contester les r√©sultats du vote.

Il existe des repr√©sentations √† 
privil√©gier pour ce type de ph√©nom√®nes o√π la densit√© est importante. L'une des repr√©sentations √† privil√©gier est les
ronds proportionnels (voir @inseeSemiologie, _"Le pi√®ge territorial en cartographie"_). Les cercles proportionnels permettent ainsi √† l'oeil de se concentrer sur les 
zones les plus denses et non sur les grands espaces. Cette fois, on voit bien
que le vote d√©mocrate est majoritaire, ce que cachait l'aplat de couleur. 

Le [GIF "Land does not vote, people do"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF), qui avait eu un certain succ√®s en 2020, propose un autre mode de visualisation.
La carte originale a √©t√© construite avec `JavaScript`. Cependant,
on dispose avec `Python` de plusieurs outils
pour r√©pliquer, √† faible co√ªt, cette carte 
gr√¢ce √†
l'une des surcouches √† `JavaScript` vues dans la partie [visualisation](/content/visualisation/index.qmd). 
:::



```{python}
#| output: false
#| echo: true
#| code-fold: true
#| code-summary: "Code pour reproduire cette carte interactive"
import numpy as np
import pandas as pd
import geopandas as gpd
import plotly
import plotly.graph_objects as go


centroids = votes.copy()
centroids.geometry = centroids.centroid
centroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number

color_dict = {"republican": '#FF0000', 'democrats': '#0000FF'}
centroids["winner"] =  np.where(centroids['votes_gop'] > centroids['votes_dem'], 'republican', 'democrats') 


centroids['lon'] = centroids['geometry'].x
centroids['lat'] = centroids['geometry'].y
centroids = pd.DataFrame(centroids[["county_name",'lon','lat','winner', 'CENSUS_2010_POP',"state_name"]])
groups = centroids.groupby('winner')

df = centroids.copy()

df['color'] = df['winner'].replace(color_dict)
df['size'] = df['CENSUS_2010_POP']/6000
df['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '<br>Population: {:,} people'.format(x))
df['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']

fig_plotly = go.Figure(
  data=go.Scattergeo(
  locationmode = 'USA-states',
  lon=df["lon"], lat=df["lat"],
  text = df["hover"],
  mode = 'markers',
  marker_color = df["color"],
  marker_size = df['size'],
  hoverinfo="text"
  )
)

fig_plotly.update_traces(
  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}
)

fig_plotly.update_layout(
  title_text = "Reproduction of the \"Acres don't vote, people do\" map <br>(Click legend to toggle traces)",
  showlegend = True,
  geo = {"scope": 'usa', "landcolor": 'rgb(217, 217, 217)'}
)

fig_plotly.show()
```



# La d√©marche g√©n√©rale

Dans ce chapitre, nous allons nous focaliser sur la pr√©paration
des donn√©es √† faire en amont du travail de mod√©lisation.
Cette √©tape est indispensable pour s'assurer de la coh√©rence
entre les donn√©es et les hypoth√®ses de mod√©lisation mais aussi
pour produire des analyses valides scientifiquement. 

La d√©marche g√©n√©rale que nous adopterons dans ce chapitre, et
qui sera ensuite raffin√©e dans les prochains chapitres,
est la suivante : 

![Illustraton de la m√©thodologie du _machine learning_](/content/modelisation/img/pipeline1.png){#fig-ml-pipeline}

La @fig-ml-pipeline illustre la structuration d'un probl√®me de _machine learning_. 

Tout d'abord, on d√©coupe
l'ensemble des donn√©es disponibles en deux parties, __√©chantillons d'apprentissage__ et de __validation__. Le premier sert √† entra√Æner
un mod√®le et la qualit√© des pr√©dictions de celui-ci est
√©valu√©e sur le deuxi√®me pour limiter
le biais de surapprentissage. Le chapitre suivant approfondira
cette question de l'√©valuation des mod√®les. A ce stade de notre
progression, on se concentrera dans ce chapitre
sur la question des donn√©es. 

La librairie `Scikit` est particuli√®rement pratique parce qu'elle propose √©norm√©ment d'algorithmes de _machine learning_ avec quelques points d'entr√©e unifi√©e, notamment les m√©thodes `fit` et `predict`. N√©anmoins, l'unification va au-del√† de l'entra√Ænement d'algorithmes. Toutes les √©tapes de pr√©paration de donn√©es qui sont int√©gr√©es dans `Scikit` proposent ces deux m√™mes points d'entr√©e. Autrement dit, les pr√©parations de donn√©es sont construites comme une estimation de param√®tres qui peut √™tre r√©appliqu√©e sur un autre jeu de donn√©es. Par exemple, cette pr√©paration de donn√©es peut √™tre une estimation de moyenne et variance pour normaliser des variables. La moyenne et la variance seront √©valu√©es sur l'√©chantillon d'apprentissage et les m√™mes moyennes et variances pourront √™tre r√©appliqu√©es sur un autre jeu de donn√©es pour le normaliser de la m√™me fa√ßon. 



# Explorer la structure des donn√©es

La premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation
est de d√©terminer les variables √† inclure dans le mod√®le.

Les fonctionnalit√©s de `Pandas` sont, √† ce niveau, suffisantes pour explorer des structures simples.
N√©anmoins, lorsqu'on est face √† un jeu de donn√©es pr√©sentant de
nombreuses variables explicatives (*features* en machine learning, *covariates* en √©conom√©trie),
il est souvent judicieux d'avoir une premi√®re √©tape de s√©lection de variables,
ce que nous verrons par la suite dans la [partie d√©di√©e](/content/modelisation/4_featureselection.qmd).  

Avant d'√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,
nous allons en prendre un nombre restreint et arbitraire.
La premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,
notamment la relation des variables explicatives
√† la variable d√©pendante (le score du parti r√©publicain)
ainsi que les relations entre les variables explicatives. 

::: {.exercise}
## Exercice 2 (optionnel) : Regarder les corr√©lations entre les variables

__Cet exercice est OPTIONNEL__

1. Cr√©er un DataFrame `df2` plus petit avec les variables `winner`, `votes_gop`, `Unemployment_rate_2019`,
`Median_Household_Income_2019`,
`Percent of adults with less than a high school diploma, 2015-19`,
`Percent of adults with a bachelor's degree or higher, 2015-19`
2. Repr√©senter gr√¢ce √† un graphique la matrice de corr√©lation. Vous pouvez utiliser le _package_ `seaborn` et sa fonction `heatmap`.
3. Repr√©senter une matrice de nuages de points des variables de la base `df2` avec `pd.plotting.scatter_matrix`
4. (optionnel) Refaire ces figures avec `Plotly` qui offre √©galement la possibilit√© de faire une matrice de corr√©lation. 

:::

```{python}
#| output: false
#| echo: true

# 1. Cr√©er le data.frame df2.
df2 = votes.set_index("GEOID").loc[: , ["winner", "votes_gop",
          'Unemployment_rate_2019', 'Median_Household_Income_2019',
          'Percent of adults with less than a high school diploma, 2015-19',
          "Percent of adults with a bachelor's degree or higher, 2015-19"]]
```

La matrice construite avec `seaborn` (question 2) aura l'aspect suivant :

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

corr = df2.drop("winner", axis = 1).corr()

mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
fig = plt.figure()

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
# More details at https://seaborn.pydata.org/generated/seaborn.heatmap.html
sns.heatmap(
    corr,          # The data to plot
    mask=mask,     # Mask some cells
    cmap=cmap,     # What colors to plot the heatmap as
    annot=True,    # Should the values be plotted in the cells?
    vmax=.3,       # The maximum value of the legend. All higher vals will be same color
    vmin=-.3,      # The minimum value of the legend. All lower vals will be same color
    center=0,      # The center value of the legend. With divergent cmap, where white is
    square=True,   # Force cells to be square
    linewidths=.5, # Width of lines that divide cells
    cbar_kws={"shrink": .5}  # Extra kwargs for the legend; in this case, shrink by 50%
)

plt.show(fig)
```

Alors que celle construite directement avec `corr` de `Pandas`
ressemblera plut√¥t √† ce tableau :

```{python}
#| output: false
#| echo: true
# Construction directement avec pandas √©galement possible
g2 = df2.drop("winner", axis = 1).corr().style.background_gradient(cmap='coolwarm').format('{:.2f}')
```

```{python}
g2
```

Le nuage de point obtenu √† l'issue de la question 3 ressemblera √† :

```{python}
#| echo: true
# 3. Matrice de nuages de points
pd.plotting.scatter_matrix(df2)
```


Le r√©sultat de la question 4 devrait, quant √† lui,
ressembler au graphique suivant :

```{python}
#| echo: true
# 4. Matrice de corr√©lation avec plotly
import plotly
import plotly.express as px
htmlsnip2 = px.scatter_matrix(df2)
htmlsnip2.update_traces(diagonal_visible=False)
htmlsnip2.show()
```


# Transformer les donn√©es

Les diff√©rences d'√©chelle ou de distribution entre les variables peuvent 
diverger des hypoth√®ses sous-jacentes dans les mod√®les.

Par exemple, dans le cadre
de la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me
enseigne que les variables ayant valeur dans $\mathbb{R}$. Une variable
discr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©e en suite de
variables 0/1 (des _dummies_) par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation
avec les hypoth√®ses de la r√©gression lin√©aire.
On appelle ce type de transformation
*one-hot encoding*, sur laquelle nous reviendrons. Il s'agit d'une transformation,
parmi d'autres, disponibles dans `Scikit` pour mettre en ad√©quation un jeu de
donn√©es et des hypoth√®ses math√©matiques. 

L'ensemble de ces t√¢ches de pr√©paration de donn√©es s'appelle le *preprocessing* ou le _feature engineering_. L'un des int√©r√™ts
d'utiliser `Scikit` est qu'on peut consid√©rer qu'une t√¢che de _preprocessing_
est, en fait, une t√¢che d'apprentissage. En effet, le _preprocessing_ 
consiste √† apprendre des param√®tres d'une structure 
de donn√©es (par exemple estimer moyennes et variances pour les retrancher √† chaque
observation) et on peut tr√®s bien appliquer ces param√®tres
√† des observations qui n'ont pas servi √† construire
ceux-ci. Autrement dit, cette pr√©paration de donn√©es s'int√®gre tr√®s bien dans le _pipeline_ @fig-ml-pipeline. 

## _Preprocessing_ de variables continues

Nous allons voir deux processus tr√®s classiques de *preprocessing* pour des variables continues : 

1. La **standardisation** transforme des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$.

2. La **normalisation**  transforme les donn√©es de mani√®re √† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.

Il en existe d'autres, par exemple le `MinMaxScaler` pour renormaliser les variables en fonction des bornes minimales et maximales des valeurs observ√©es. Le choix de la m√©thode a mettre en oeuvre d√©pend du type d'algorithmes choisis par la suite: les hypoth√®ses des k plus proches voisins (knn) seront diff√©rentes de celles d'une _random forest_. C'est pour cette raison que, normalement, on d√©finit des _pipelines_ complets, int√©grant √† la fois _preprocessing_ et apprentissage. Ce sera l'objet des prochains chapitres.

::: {.caution}
Pour les statisticiens.ennes,
le terme _normalization_ dans le vocable `Scikit` peut avoir un sens contre-intuitif.
On s'attendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que $X \sim \mathcal{N}(0,1)$.
C'est, en fait, la **standardisation** en `Scikit` qui fait cela.

:::


### Standardisation

La standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour √™tre performants, la plupart des mod√®les de _machine learning_ n√©cessitent souvent d'avoir des donn√©es dans cette distribution. M√™me lorsque ce n'est pas indispensable, par exemple avec des r√©gressions logistiques, cela peut acc√©l√©rer la vitesse de convergence des algorithmes.

::: {.exercise}
## Exercice 3: Standardisation

1. Standardiser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation. Cette transformation est √† appliquer √† toute la colonne ; les prochaines questions se pr√©occuperont du sujet de d√©coupage d'√©chantillon et d'extrapolation. 

*Note : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.*


2. Cr√©er `scaler`, un `Transformer` que vous construisez sur les 1000 premi√®res lignes de votre DataFrame `df2`  √†  l'exception de la variable √† expliquer `winner`. V√©rifier la moyenne et l'√©cart-type de chaque colonne sur ces m√™mes observations.

*Note : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs `.mean_` et `.scale_`*

On peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de
donn√©es et qu'on peut r√©utiliser sur un autre, √† condition que les
dimensions co√Øncident.

3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `Median_Household_Income_2019`.

*Note : Une fois appliqu√©s √† un autre `DataFrame`, on peut remarquer que la distribution n'est pas exactement centr√©e-r√©duite dans le `DataFrame` sur lequel les param√®tres n'ont pas √©t√© estim√©s. C'est normal, l'√©chantillon initial n'√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n'ont pas de raison de co√Øncider avec les moments de l'√©chantillon complet.*


:::

Avant standardisation, notre variable a cette distribution:

```{python}
(
  ggplot(df2, aes(x = "Median_Household_Income_2019")) +
  geom_histogram() +
  labs(
    x = "2019 Median household income (standardized)",
    y = "Density (number observations)"
    )
)
```

Apr√®s standardisation, l'√©chelle de la variable a chang√©. 

```{python}
# 1. Standardisation de Median_Household_Income_2019 et histogramme
import matplotlib.pyplot as plt
from sklearn import preprocessing

df2['y_standard'] = preprocessing.scale(
  df2['Median_Household_Income_2019']
)

(
  ggplot(df2, aes(x = "y_standard")) +
  geom_histogram() +
  labs(
    x = "2019 Median household income (standardized)",
    y = "Density (number observations)"
    )
)
```

On obtient bien une moyenne √©gale √† 0 et une variance √©gale √† 1, aux approximations num√©riques pr√™t :

```{python}
pd.DataFrame(
  {
    "Statistique": ["Mean", "Variance"],
    "Valeur": [df2['y_standard'].mean().round(), df2['y_standard'].var()]
  }
)
```

A la question 2, si on essaie de repr√©senter les statistiques obtenues dans un tableau lisible, on obtient

```{python}
# 2. Cr√©er un scaler

df_exo3 = df2.drop("winner", axis=1)

first_rows = df_exo3.head(1000)

mean_before = np.array(first_rows.mean(axis=0))
std_before = np.array(first_rows.std(axis=0))

# Initialize and apply the scaler
scaler = preprocessing.StandardScaler().fit(first_rows)
scaled_data = scaler.transform(first_rows)

mean_after = scaled_data.mean(axis=0)
std_after = scaled_data.std(axis=0)

# Create DataFrame to store results
result_df = pd.DataFrame({
    "Variable": df_exo3.columns,
    "Mean before Scaling": mean_before,
    "Std before Scaling": std_before,
    "Mean after Scaling": mean_after,
    "Std after Scaling": std_after
})
```

```{python}
from great_tables import *
(
  GT(result_df)
  .fmt_nanoplot("Mean before Scaling", options = {"interactive_data_values": False})
  .fmt_nanoplot("Std before Scaling")
  .fmt_nanoplot("Mean after Scaling")
  .fmt_nanoplot("Std after Scaling")
)
```

On voit tr√®s clairement dans ce tableau que la standardisation a bien fonctionn√©. 

Maintenant, si on construit un _transformer_ formel pour nos variables (question 3)

```{python}
# 3. Appliquer le scaler √† toutes les autres lignes
standarisation = scaler.fit(df_exo3.head(1000))
standarisation
```


On peut extrapoler notre standardiseur √† un ensemble plus large de donn√©es. Si on regarde la distribution obtenue sur les 1000 premi√®res lignes (question 3), on retrouve une √©chelle coh√©rente avec une loi $\mathcal{N(0,1)}$ pour la variable de ch√¥mage:

```{python}
#| fig-cap: "Taux de ch√¥mage standardis√© sur des observations qui ont servi √† l'entra√Ænement"
#| label: "fig-exo3-q3"
X1 = pd.DataFrame(scaler.fit_transform(df_exo3[1000:]))
X1.columns = df_exo3.columns

X2 = pd.DataFrame(scaler.transform(df_exo3[:1000]))
X2.columns = df_exo3.columns

(
  ggplot(X1, aes(x = "Unemployment_rate_2019")) +
  geom_histogram() +
  labs(x = "Unemployment rate (standardized using first 1000 rows)")
)
```

En revanche on voit que cette distribution ne correspond pas √† celle qui permettrait de normaliser vraiment le reste des donn√©es. 
```{python}
#| fig-cap: "Taux de ch√¥mage standardis√© sur des observations qui n'ont pas servi √† l'entra√Ænement"
#| label: "fig-exo3-q3bis"
(
  ggplot(X2, aes(x = "Unemployment_rate_2019")) +
  geom_histogram() +
  labs(x = "Unemployment rate (standardized using other rows)")
)
```

C'est une illustration d'un probl√®me classique en _machine learning_, le _data drift_, qui arrive lorsqu'on essaie d'extrapoler √† des donn√©es dont la distribution ne correspond plus √† celle des donn√©es d'apprentissage. Ce type de situation arrive typiquement lorsqu'on a entra√Æn√© un algorithme sur un √©chantillon biais√© de la population ou lorsqu'on a des s√©ries temporelles non stationnaires. Il est donc important de bien r√©fl√©chir √† la constitution de l'√©chantillon d'apprentissage et aux possibilit√©s d'extrapolation sur une population plus large : la validit√© externe du mod√®le - pr√©paration des donn√©es ou algorithme d'apprentissage - peut √™tre nulle si cette √©tape a √©t√© r√©alis√©e de mani√®re hative. 


::: {.important}
Le data drift d√©signe un changement dans la distribution des donn√©es au fil du temps, entra√Ænant une d√©gradation des performances d‚Äôun mod√®le de _machine learning_ qui, par construction, a √©t√© entra√Æn√© sur des donn√©es pass√©es. 

Ce ph√©nom√®ne peut survenir √† cause de variations dans la population cible, de changements dans les caract√©ristiques des donn√©es ou de facteurs externes. 

Il est crucial de d√©tecter le data drift pour ajuster ou r√©entra√Æner le mod√®le, afin de maintenir sa pertinence et sa pr√©cision. Les techniques de d√©tection incluent des tests statistiques et le suivi de m√©triques sp√©cifiques.
:::

### Normalisation

La **normalisation** est l'action de transformer les donn√©es de mani√®re
√† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire.
Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.
Par d√©faut, la norme utilis√©e par `Scikit` est une norme  $\mathcal{l}_2$.
Cette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du *clustering*.

::: {.exercise}
## Exercice 4 : Normalisation

1. A l'aide de la fonction [`train_test_split`](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) de `Scikit`
1. Normaliser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.
2. V√©rifier que la norme $\mathcal{l}_2$ est bien √©gale √† 1.

:::


```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df2.drop(columns = "winner"), df2['winner'], test_size=0.3)
```

```{python}
#| eval: false
# 1. Normalisation de Median_Household_Income_2019 et histogrammes
scaler = preprocessing.Normalizer().fit(df2)
X1 = scaler.transform(df2.dropna(how = "any"))

f, axes = plt.subplots(2, figsize=(10, 10))
sns.histplot(df2["Median_Household_Income_2019"] , color="skyblue", ax=axes[0])
sns.histplot(X1[:,col_pos] , color="olive", ax=axes[1])
```

```{python}
#| eval: false
# 2. V√©rification de la norme L2
np.sqrt(np.sum(X1**2, axis=1))[:10] # L2-norm
```



## Encodage des valeurs cat√©gorielles

Les donn√©es cat√©gorielles doivent √™tre recod√©es
sous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de *machine learning*.
Cela peut √™tre fait de plusieurs mani√®res :

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur num√©rique `[0,1,2]`.
Cette approche a l'inconv√©nient d'introduire un ordre dans les modalit√©s, ce qui n'est pas toujours souhaitable

* `OrdinalEncoder`: une version g√©n√©ralis√©e du `LabelEncoder` qui a vocation √† s'appliquer sur des matrices ($X$),
alors que `LabelEncoder` s'applique plut√¥t √† un vecteur ($y$)

* `pandas.get_dummies` effectue une op√©ration de *dummy expansion*.
Un vecteur de taille *n* avec *K* cat√©gories sera transform√© en matrice de taille $n \times K$
pour lequel chaque colonne sera une variable *dummy* pour la modalit√© *k*.
Il y a ici $K$ modalit√©s et il y a donc multicolin√©arit√©.
Avec une r√©gression lin√©aire avec constante,
il convient de retirer une modalit√© avant l'estimation.

* `OneHotEncoder` est une version g√©n√©ralis√©e (et optimis√©e) de la *dummy expansion*.
Il a plut√¥t vocation √† s'appliquer sur les *features* ($X$) du mod√®le

## Imputation

Les donn√©es peuvent souvent contenir des valeurs manquantes, autrement dit des cases de notre _DataFrame_ contenant un `NaN`.
Ces trous dans les donn√©es peuvent √™tre √† l'origine de _bugs_ ou de mauvaises interpr√©tations lorsque l'on passe √† la mod√©lisation.
Pour y rem√©dier, une premi√®re approche peut √™tre de retirer toutes les observations pr√©sentant un `NaN` dans au moins l'une des colonnes.
Cependant, si notre table contient beaucoup de `NaN`, ou bien que ces derniers sont r√©partis sur de nombreuses colonnes,
c'est aussi prendre le risque de retirer un nombre important de lignes, et avec cela de l'information importante pour un mod√®le car les valeurs manquantes sont rarement [r√©parties de mani√®re al√©atoire](https://stefvanbuuren.name/fimd/sec-MCAR.html).

M√™me si dans plusieurs situations, cette solution reste tout √† fait viable, il existe une autre approche plus robuste appel√©e *imputation*.
Cette m√©thode consiste √† remplacer les valeurs vides par une valeur donn√©e. Par exemple :

- Imputation par la moyenne : remplacer tous les `NaN` dans une colonne par la valeur moyenne de la colonne ;
- Imputation par la m√©diane sur le m√™me principe, ou par la valeur de la colonne la plus fr√©quente pour les variables cat√©gorielles ;
- Imputation par r√©gression : se servir d'autres variables pour essayer d'interpoler une valeur de remplacement adapt√©e.

Des m√©thodes plus complexes existent, mais dans de nombreux cas,
les approches ci-dessus peuvent suffire pour donner des r√©sultats beaucoup plus satisfaisants.
Le package `Scikit` permet de faire de l'imputation de mani√®re tr√®s simple ([documentation ici](https://scikit-learn.org/stable/modules/impute.html)).

## Gestion des outliers


Les valeurs aberrantes (_outliers_ en anglais) sont des observations qui se situent significativement √† l'ext√©rieur de la tendance g√©n√©rale des autres observations dans un ensemble de donn√©es. En d'autres termes, ce sont des points de donn√©es qui se d√©marquent de mani√®re inhabituelle par rapport √† la distribution globale des donn√©es.
Cela peut √™tre d√ª √† des erreurs de remplissage, des personnes ayant mal r√©pondu √† un questionnaire, ou
parfois simplement des valeurs extr√™mes qui peuvent biaiser un mod√®le de fa√ßon trop importante.

A titre d'exemple, cela va √™tre 3 individus mesurant plus de 4 m√®tres dans une population,
ou bien des revenus de m√©nage d√©passant les 10M d'euros par mois sur l'√©chelle d'un pays, etc.

Une bonne pratique peut donc √™tre de syst√©matiquement regarder la distribution des variables √† disposition,
pour se rendre compte si certaines valeurs s'√©loignent de fa√ßon trop importante des autres.
Ces valeurs vont parfois nous int√©resser, si par exemple on se concentre uniquement sur les tr√®s hauts revenus (top 0.1%)
en France. Cependant, ces donn√©es vont souvent nous g√™ner plus qu'autre chose, surtout si elles n'ont pas de sens dans le monde r√©el.

Si l'on estime que la pr√©sence de ces donn√©es extr√™mes, ou *outliers*, dans notre base de donn√©es vont √™tre probl√©matiques plus qu'autre chose,
alors il est tout √† fait entendable et possible de simplement les retirer.
La plupart du temps, on va se donner une proportion des donn√©es √† retirer, par exemple 0.1%, 1% ou 5%,
puis retirer dans les deux queues de la distribution les valeurs extr√™mes correspondantes.

Plusieurs packages permettent de faire ce type d'op√©rations, qui sont parfois plus complexes si on s'int√©resse aux outlier sur plusieurs variables.
On pourra notamment citer la fonction `IsolationForest()` du package `sklearn.ensemble`.

<br>

Pour plus de d√©tails sur ces deux derniers points, il est recommand√© d'aller voir l'exemple *Pour aller plus loin* en bas de la page.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 5 : Encoder des variables cat√©gorielles</h3>
```

1. Cr√©er `df` qui conserve uniquement les variables `state_name` et `county_name` dans `votes`.
2. Appliquer √† `state_name` un `LabelEncoder`
*Note : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.*

3. Regarder la *dummy expansion* de `state_name`
4. Appliquer un `OrdinalEncoder` √† `df[['state_name', 'county_name']]`
*Note : Le r√©sultat du _ordinal encoding_ est coh√©rent avec celui du label encoding*

5. Appliquer un `OneHotEncoder` √† `df[['state_name', 'county_name']]`

*Note : `scikit` optimise l'objet n√©cessaire pour stocker le r√©sultat d'un mod√®le de transformation. Par exemple, le r√©sultat de l'encoding One Hot est un objet tr√®s volumineux. Dans ce cas, `scikit` utilise une matrice Sparse.*


```{=html}
</div>
```
:::

```{python}
#1. Cr√©ation de df
df = votes[["state_name",'county_name']]
```


```{python}
#2. Appliquer un LabelEncoder √† stat_name
label_enc = preprocessing.LabelEncoder().fit(df['state_name'])
np.column_stack((label_enc.transform(df['state_name']),df['state_name']))
```


```{python}
# 3. dummy expansion de state_name
pd.get_dummies(df['state_name'])
```



```{python}
# 4. OrdinalEncoder
ord_enc = preprocessing.OrdinalEncoder().fit(df)
# ord_enc.transform(df[['state', 'county']])
ord_enc.transform(df)[:,0]
```


```{python}
# 5. OneHotEncoder
onehot_enc = preprocessing.OneHotEncoder().fit(df)
onehot_enc.transform(df)
```


# R√©f√©rences

::: {#refs}
:::
