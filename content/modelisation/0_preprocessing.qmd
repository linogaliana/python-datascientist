---
title: "Pr√©paration des donn√©es pour construire un mod√®le"
date: 2023-07-15T13:00:00Z
draft: false
weight: 10
slug: preprocessing
tags:
  - scikit
  - machine learning
  - US election
  - preprocessing
  - Modelisation
  - Exercice
categories:
  - Mod√©lisation
  - Exercice
type: book
description: |
  Afin d'avoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,
  il est absolument fondamental de prendre le temps de
  pr√©parer les donn√©es √† fournir √† un mod√®le. La qualit√© de la pr√©diction
  d√©pend fortement de ce travail pr√©alable qu'on appelle _preprocessing_.
  Beaucoup de m√©thodes sont disponibles dans `scikit`, ce qui rend ce travail
  moins fastidieux et plus fiable. 
plotly: true
bibliography: ../../reference.bib
image: featured_preprocessing.png
---


::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/modelisation/0_preprocessing.qmd")
```
:::

Ce chapitre utilise le jeu de donn√©es pr√©sent√© dans l'[introduction
de cette partie](index.qmd) :
les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines de 2020 au niveau des comt√©s
crois√©es √† des variables socio-d√©mographiques.
Le code de consitution de la base de donn√©es
est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/master/content/modelisation/get_data.py).
L'exercice 1 permet, √† ceux qui le d√©sirent, d'essayer de le reconstituer pas √† pas. 

Le guide utilisateur de `scikit` est une r√©f√©rence pr√©cieuse,
√† consulter r√©guli√®rement. La partie sur le *preprocessing* est
disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).

L'objectif de ce chapitre est de pr√©senter quelques √©l√©ments de 
pr√©paration des donn√©es. Il s'agit d'une √©tape fondamentale, √† ne
pas n√©gliger. Les mod√®les reposent sur certaines hypoth√®ses, g√©n√©ralement
relatives √† la distribution th√©orique des variables qui y sont int√©gr√©es.

Il est n√©cessaire de faire correspondre la distribution empirique
√† ces hypoth√®ses ce qui implique un travail de restructuration des donn√©es.
Celui-ci permettra d'avoir des r√©sultats de mod√©lisation plus pertinents. 
Nous verrons dans le chapitre sur les *pipelines* comment industrialiser
ces √©tapes de _preprocessing_ afin de se simplifier la vie pour appliquer
un mod√®le sur un jeu de donn√©es diff√©rent de celui sur lequel il a √©t√© estim√©. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> scikit-learn </h3>
```

`scikit-learn` est aujourd'hui la librairie de r√©f√©rence dans l'√©cosyst√®me du
_Machine Learning_. Il s'agit d'une librairie qui, malgr√© les tr√®s nombreuses
m√©thodes impl√©ment√©es, pr√©sente l'avantage d'√™tre un point d'entr√©e unifi√©.
Cet aspect unifi√© est l'une des raisons du succ√®s pr√©coce de celle-ci. `R` n'a 
b√©n√©fici√© que plus r√©cemment d'une librairie unifi√©e,
√† savoir [`tidymodels`](https://www.tidymodels.org/).

Une autre raison du succ√®s de `scikit` est son approche op√©rationnelle: la mise
en production de mod√®les d√©velopp√©s via les _pipelines_ `scikit` est peu co√ªteuse.
Un [chapitre sp√©cial de ce cours](/pipeline-scikit) est d√©di√© aux _pipelines_.
Avec Romain Avouac, nous proposons un [cours plus avanc√©](https://ensae-reproductibilite.github.io/website/) 
en derni√®re ann√©e d'ENSAE o√π nous pr√©sentons certains enjeux relatifs
√† la mise en production de mod√®les d√©velopp√©s avec `scikit`. 

Le coeur de l'√©quipe de d√©veloppement de `scikit-learn` est situ√©
√† l'[Inria](https://www.inria.fr/fr) üá´üá∑. 
Pour d√©couvrir la richesse de l'√©cosyst√®me `scikit`, il 
est recommand√© de suivre le
[`MOOC scikit`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),
d√©velopp√© dans le cadre de l'initiative [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn).

```{=html}
</div>
```
:::

## Construction de la base de donn√©es

Les sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni. Le travail de construction d'une base unique
est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,
pour [r√©viser `pandas`](#pandas)   :

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 1 : Importer les donn√©es des √©lections US</h3>
```

__Cet exercice est OPTIONNEL__

1. T√©l√©charger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_02_sldl_500k.zip)
2. Exclure les Etats suivants: "02", "69", "66", "78", "60", "72", "15"
3. Importer les r√©sultats des √©lections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv)
4. Importer les bases disponibles sur le site de l'USDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique
dans les 4 bases
5. *Merger* ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques
6. *Merger* aux donn√©es √©lectorales √† partir du code FIPS
7. *Merger* au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est
recommand√© d'utiliser la m√©thode `str.lstrip` pour les retirer
8. Importer les donn√©es des √©lections 2000 √† 2016 √† partir du [MIT Election Lab](https://electionlab.mit.edu/data)?
Les donn√©es peuvent √™tre directement requ√™t√©es depuis l'url
<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>
9. Cr√©er une variable `share` comptabilisant la part des votes pour chaque candidat. 
Ne garder que les colonnes `"year", "FIPS", "party", "candidatevotes", "share"`
10. Faire une conversion `long` to `wide` avec la m√©thode `pivot_table` pour garder une ligne
par comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.
11. Merger √† partir du code FIPS au reste de la base. 

```{=html}
</div>
```
:::

Si vous ne faites pas l'exercice 1, pensez √† charger les donn√©es en executant la fonction `get_data.py` :

```{python}
#| echo: true
#| message: false
#| warning: false

#!pip install --upgrade xlrd #colab bug verson xlrd
#!pip install geopandas

import requests

url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'
r = requests.get(url, allow_redirects=True)
open('getdata.py', 'wb').write(r.content)

import getdata
votes = getdata.create_votes_dataframes()
```

Ce code introduit une base nomm√©e `votes` dans l'environnement. Il s'agit d'une
base rassemblant les diff√©rentes sources. Elle a l'aspect
suivant:

```{python}
#| echo: false
votes.head(3)
```


La carte choropl√®the suivante permet de visualiser rapidement les r√©sultats
(l'Alaska et Hawa√Ø ont √©t√© exclus). 

```{python}
#| warning: false
#| echo: true
#| message: false

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# republican : red, democrat : blue
color_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}

fig, ax = plt.subplots(figsize = (12,12))
grouped = votes.groupby('winner')
for key, group in grouped:
    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])
plt.axis('off')

```


Les cartes choropl√®thes peuvent donner une impression fallacieuse 
ce qui exiplique que 
ce type de carte a servi 
de justification pour contester les r√©sultats du vote.
En effet, un biais
connu des repr√©sentations choropl√®thes est qu'elles donnent une importance
visuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces
peu denses et influencent donc moins la variable d'int√©r√™t (en l'occurrence
le taux de vote en faveur des r√©publicains/d√©mocrates). Une repr√©sentation √† 
privil√©gier pour ce type de ph√©nom√®nes est les
ronds proportionnels (voir @inseeSemiologie, _"Le pi√®ge territorial en cartographie"_). 


Le [GIF "Land does not vote, people do"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF)
qui avait eu un certain succ√®s en 2020 propose un autre mode de visualisation.
La carte originale a probablement √©t√© construite avec `JavaScript`. Cependant,
on dispose avec `Python` de plusieurs outils
pour r√©pliquer, √† faible co√ªt, cette carte 
gr√¢ce √†
l'une des surcouches √† JavaScript vue dans la partie [visualisation](#visualisation). 

En l'occurrence, on peut utiliser `plotly` pour tenir compte de la population:

{{< chart data="people_vote" >}}

La Figure a √©t√© obtenue avec le code suivant:

```{python}
#| message: false
#| warning: false
#| output: false
import plotly
import plotly.graph_objects as go
import pandas as pd
import geopandas as gpd


centroids = votes.copy()
centroids.geometry = centroids.centroid
centroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number

color_dict = {"republican": '#FF0000', 'democrats': '#0000FF'}
centroids["winner"] =  np.where(centroids['votes_gop'] > centroids['votes_dem'], 'republican', 'democrats') 


centroids['lon'] = centroids['geometry'].x
centroids['lat'] = centroids['geometry'].y
centroids = pd.DataFrame(centroids[["county_name",'lon','lat','winner', 'CENSUS_2010_POP',"state_name"]])
groups = centroids.groupby('winner')

df = centroids.copy()

df['color'] = df['winner'].replace(color_dict)
df['size'] = df['CENSUS_2010_POP']/6000
df['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '<br>Population: {:,} people'.format(x))
df['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']

fig_plotly = go.Figure(data=go.Scattergeo(
    locationmode = 'USA-states',
    lon=df["lon"], lat=df["lat"],
    text = df["hover"],
    mode = 'markers',
    marker_color = df["color"],
    marker_size = df['size'],
    hoverinfo="text"
    ))

fig_plotly.update_traces(
  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}
)

fig_plotly.update_layout(
        title_text = "Reproduction of the \"Acres don't vote, people do\" map <br>(Click legend to toggle traces)",
        showlegend = True,
        geo = {"scope": 'usa', "landcolor": 'rgb(217, 217, 217)'}
    )
```

```{python}
#| output: false
#| echo: false
fig_plotly.write_json("people_vote.json")
fig_plotly.write_image("featured_preprocessing.png")
```

Les cercles proportionnels permettent ainsi √† l'oeil de se concentrer sur les 
zones les plus denses et non sur les grands espaces.

## Explorer la structure des donn√©es

La premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation
est de d√©terminer les variables √† inclure dans le mod√®le.

Les fonctionnalit√©s de `pandas` sont, √† ce niveau, suffisantes pour explorer des structures simples.
N√©anmoins, lorsqu'on est face √† un jeu de donn√©es pr√©sentant de
nombreuses variables explicatives (*features* en machine learning, *covariates* en √©conom√©trie),
il est souvent judicieux d'avoir une premi√®re √©tape de s√©lection de variables,
ce que nous verrons par la suite dans la [partie d√©di√©e](4_featureselection.qmd).  

Avant d'√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,
nous allons en prendre un nombre restreint et arbitraire.
La premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,
notamment la relation des variables explicatives
√† la variable d√©pendante (le score du parti r√©publicain)
ainsi que les relations entre les variables explicatives. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 2 : Regarder les corr√©lations entre les variables</h3>
```

1. Cr√©er un DataFrame `df2` plus petit avec les variables `winner`, `votes_gop`, `Unemployment_rate_2019`,
`Median_Household_Income_2019`,
`Percent of adults with less than a high school diploma, 2015-19`,
`Percent of adults with a bachelor's degree or higher, 2015-19`
2. Repr√©senter gr√¢ce √† un graphique la matrice de corr√©lation avec `heatmap` de `seaborn`.
3. Repr√©senter une matrice de nuages de points des variables de la base `df2` avec `pd.plotting.scatter_matrix`
4. (optionnel) Refaire ces figures avec `plotly` qui offre √©galement la possibilit√© de faire une matrice de corr√©lation. 

```{=html}
</div>
```
:::

```{python}
#| output: false
#| echo: false

# 1. Cr√©er le data.frame df2.
df2 = votes.set_index("GEOID").loc[: , ["winner", "votes_gop",
          'Unemployment_rate_2019', 'Median_Household_Income_2019',
          'Percent of adults with less than a high school diploma, 2015-19',
          "Percent of adults with a bachelor's degree or higher, 2015-19"]]
```

```{python}
#| echo: false
#| message: false
#| warning: false

# 2. Matrice de corr√©lation graphique
g1 = sns.heatmap(df2.drop("winner", axis = 1).corr(), cmap='coolwarm', annot=True, fmt=".2f")

## Construction directement avec pandas √©galement possible
g2 = df2.drop("winner", axis = 1).corr().style.background_gradient(cmap='coolwarm').format('{:.2f}')
```

La matrice construite avec `seaborn` (question 2) aura l'aspect suivant:

```{python}
#| echo: false

g1.figure.get_figure()
```

Alors que celle construite directement avec `corr` de `pandas`
ressemblera plut√¥t √† ce tableau :

```{python}
#| echo: false

g2
```

Le nuage de point obtenu √† l'issue de la question 3 ressemblera √† :

```{python}
#| echo: false
#| message: false
#| warning: false

# 3. Matrice de nuages de points
ax = pd.plotting.scatter_matrix(df2, figsize = (15,15))
```

```{python}
#| echo: false
ax
```

Le r√©sultat de la question 4 devrait, quant √† lui,
ressembler au graphique suivant :

```{python}
#| echo: false
#| output: false
 
# 4. Matrice de cor√©lation avec plotly
import plotly
import plotly.express as px
htmlsnip2 = px.scatter_matrix(df2)
htmlsnip2.update_traces(diagonal_visible=False)
#htmlsnip2
```

```{python}
# Pour inclusion dans le site web
htmlsnip2.write_json("scatter.json")
```


{{< chart data="scatter" >}}


## Transformer les donn√©es

Les diff√©rences d'√©chelle ou de distribution entre les variables peuvent 
diverger des hypoth√®ses sous-jacentes dans les mod√®les.

Par exemple, dans le cadre
de la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me
enseigne que les variables ayant valeur dans $\mathbb{R}$. Une variable
discr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©es en suite de
variables 0/1 par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation
avec les hypoth√®ses de la r√©gression lin√©aire.
On appelle ce type de transformation
*one-hot encoding*, sur lequel nous reviendrons. Il s'agit d'une transformation,
parmi d'autres, disponibles dans `scikit` pour mettre en ad√©quation un jeu de
donn√©es et des hypoth√®ses math√©matiques. 

L'ensemble de ces t√¢ches s'appelle le *preprocessing*. L'un des int√©r√™ts
d'utiliser `scikit` est qu'on peut consid√©rer qu'une t√¢che de preprocessing
est une t√¢che d'apprentissage (on apprend des param√®tres d'une structure 
de donn√©es) qui est r√©utilisable pour un jeu de donn√©es √† la structure
similaire:

![](scikit_predict.png)


Nous allons voir deux processus tr√®s classiques de *preprocessing* : 

1. La **standardisation** transforme des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$.

2. La **normalisation**  transforme les donn√©es de mani√®re √† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```
Pour un statisticien,
le terme `normalization` dans le vocable `scikit` peut avoir un sens contre-intuitif.
On s'attendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que $X \sim \mathcal{N}(0,1)$.
C'est, en fait, la **standardisation** en `scikit` qui fait cela.

```{=html}
</div>
```
:::


### Standardisation

La standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour √™tre performants, la plupart des mod√®les de machine learning n√©cessitent souvent d'avoir des donn√©es dans cette distribution.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 3: Standardisation</h3>
```


1. Standardiser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.

*Note : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.*


2. Cr√©er `scaler`, un `Transformer` que vous construisez sur les 1000 premi√®res lignes de votre DataFrame `df2`  √†  l'exception de la variable √† expliquer `winner`. V√©rifier la moyenne et l'√©cart-type de chaque colonne sur ces m√™mes observations.

*Note : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs `.mean_` et `.scale_`*

On peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de
donn√©es et qu'on peut r√©utiliser sur un autre, √† condition que les
dimensions co√Øncident.

3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `Median_Household_Income_2019`.

*Note : Une fois appliqu√©s √† un autre `DataFrame`, on peut remarquer que la distribution n'est pas exactement centr√©e-r√©duite dans le `DataFrame` sur lequel les param√®tres n'ont pas √©t√© estim√©s. C'est normal, l'√©chantillon initial n'√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n'ont pas de raison de co√Øncider avec les moments de l'√©chantillon complet.*


```{=html}
</div>
```
:::

```{python}
#| output: false
#| echo: false

# 1. Standardisation de Median_Household_Income_2019 et histogramme
from sklearn import preprocessing
df2['y_standard'] = preprocessing.scale(df2['Median_Household_Income_2019'])
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["Median_Household_Income_2019"] , color="skyblue", ax=axes[0])
sns.distplot(df2["y_standard"] , color="olive", ax=axes[1])
#print(df2['y_standard'].mean())
#print(df2['y_standard'].var())

```

```{python}
#| echo: false

#plt.savefig('standardisation.png', bbox_inches='tight')
# ax
```



```{python}
#| include: false
#| echo: false

# 2. Cr√©er un scaler
df2 = df2.drop("winner", axis = 1)
print("Moyenne de chaque variable sur 1000 premi√®res observations avant : ", np.array(df2.head(1000).mean(axis=0)))
print("Ecart-type de chaque variable sur 1000 premi√®res observations avant : ", np.array(df2.head(1000).std(axis=0)))
scaler = preprocessing.StandardScaler().fit(df2.head(1000))
scaler.transform(df2.head(1000))
print("Moyenne de chaque variable sur 1000 premi√®res observations apr√®s : ", scaler.transform(df2.head(1000)).mean(axis=0))
print("Ecart-type de chaque variable sur 1000 premi√®res observations apr√®s : ", scaler.transform(df2.head(1000)).std(axis=0))
#print(scaler.mean_)
#print(scaler.scale_)

```


```{python}
#| include: false
#| echo: false

# 3. Appliquer le scaler √† toutes les autres lignes
X1 = scaler.transform(df2.head(1000))
X2 = scaler.transform(df2[1000:])
col_pos = df2.columns.get_loc("Median_Household_Income_2019")
# X2.mean(axis = 0)
# X2.std(axis = 0)
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(X1[:,col_pos] , color="skyblue", ax=axes[0])
sns.distplot(X2[:,col_pos] , color="olive", ax=axes[1])

```

```{python}
#| echo: false
#plt.savefig('standardisation2.png', bbox_inches='tight')
#axes
```


### Normalisation

La **normalisation** est l'action de transformer les donn√©es de mani√®re
√† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire.
Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.
Par d√©faut, la norme est dans $\mathcal{l}_2$.
Cette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du *clustering*.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 4 : Normalisation</h3>
```

1. Normaliser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.
2. V√©rifier que la norme $\mathcal{l}_2$ est bien √©gale √† 1.

```{=html}
</div>
```
:::

```{python}
#| output: false
#| echo: false

# 1. Normalisation de Median_Household_Income_2019 et histogrammes
scaler = preprocessing.Normalizer().fit(df2.dropna(how = "any").head(1000))
X1 = scaler.transform(df2.dropna(how = "any").head(1000))

f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["Median_Household_Income_2019"] , color="skyblue", ax=axes[0])
sns.distplot(X1[:,col_pos] , color="olive", ax=axes[1])
```

```{python}
#| echo: false
#plt.savefig('normalisation.png', bbox_inches='tight')
# axes
```



```{python}
#| output: false
#| echo: false

# 2. V√©rification de la norme L2
np.sqrt(np.sum(X1**2, axis=1))[:10] # L2-norm
```



::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```
`preprocessing.Normalizer` n'accepte pas les valeurs manquantes, alors que `preprocessing.StandardScaler()` s'en accomode (dans la version `0.22` de scikit). Pour pouvoir ais√©ment appliquer le *normalizer*, il faut

* retirer les valeurs manquantes du DataFrame avec la m√©thode `dropna`: `df.dropna(how = "any")`;
* ou les imputer avec un mod√®le ad√©quat. [`scikit` permet de le faire](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values).

```{=html}
</div>
```
:::


### Encodage des valeurs cat√©gorielles

Les donn√©es cat√©gorielles doivent √™tre recod√©es
sous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de *machine learning*.
Cela peut √™tre fait de plusieurs mani√®res :

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur num√©rique `[0,1,2]`.
Cette approche a l'inconv√©nient d'introduire un ordre dans les modalit√©s, ce qui n'est pas toujours souhaitable

* `OrdinalEncoder`: une version g√©n√©ralis√©e du `LabelEncoder` qui a vocation √† s'appliquer sur des matrices ($X$),
alors que `LabelEncoder` s'applique plut√¥t √† un vecteur ($y$)

* `pandas.get_dummies` effectue une op√©ration de *dummy expansion*.
Un vecteur de taille *n* avec *K* cat√©gories sera transform√© en matrice de taille $n \times K$
pour lequel chaque colonne sera une variable *dummy* pour la modalit√© *k*.
Il y a ici $K$ modalit√©s et il y a donc multicolin√©arit√©.
Avec une r√©gression lin√©aire avec constante,
il convient de retirer une modalit√© avant l'estimation.

* `OneHotEncoder` est une version g√©n√©ralis√©e (et optimis√©e) de la *dummy expansion*.
Il a plut√¥t vocation √† s'appliquer sur les *features* ($X$) du mod√®le



::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 5 : Encoder des variables cat√©gorielles</h3>
```

1. Cr√©er `df` qui conserve uniquement les variables `state_name` et `county_name` dans `votes`.
2. Appliquer √† `state_name` un `LabelEncoder`
*Note : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.*

3. Regarder la *dummy expansion* de `state_name`
4. Appliquer un `OrdinalEncoder` √† `df[['state_name', 'county_name']]`
*Note : Le r√©sultat du _ordinal encoding_ est coh√©rent avec celui du label encoding*

5. Appliquer un `OneHotEncoder` √† `df[['state_name', 'county_name']]`

*Note : `scikit` optimise l'objet n√©cessaire pour stocker le r√©sultat d'un mod√®le de transformation. Par exemple, le r√©sultat de l'encoding One Hot est un objet tr√®s volumineux. Dans ce cas, `scikit` utilise une matrice Sparse.*


```{=html}
</div>
```
:::

```{python}
#| include: false
#| echo: false

#1. Cr√©ation de df
df = votes[["state_name",'county_name']]
```


```{python}
#| include: false
#| echo: false

#2. Appliquer un LabelEncoder √† stat_name
label_enc = preprocessing.LabelEncoder().fit(df['state_name'])
np.column_stack((label_enc.transform(df['state_name']),df['state_name']))
```


```{python}
#| include: false
#| echo: false

# 3. dummy expansion de state_name
pd.get_dummies(df['state_name'])
```



```{python}
#| include: false
#| echo: false

# 4. OrdinalEncoder
ord_enc = preprocessing.OrdinalEncoder().fit(df)
# ord_enc.transform(df[['state', 'county']])
ord_enc.transform(df)[:,0]

```


```{python}
#| include: false
#| echo: false

# 5. OneHotEncoder
onehot_enc = preprocessing.OneHotEncoder().fit(df)
onehot_enc.transform(df)
```


## R√©f√©rences

::: {#refs}
:::
