---
title: "Pr√©paration des donn√©es pour construire un mod√®le"
date: 2023-07-15T13:00:00Z
weight: 10
slug: preprocessing
tags:
  - scikit
  - machine learning
  - US election
  - preprocessing
  - Modelisation
  - Exercice
categories:
  - Mod√©lisation
  - Exercice
description: |
  Afin d'avoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,
  il est absolument fondamental de prendre le temps de
  pr√©parer les donn√©es √† fournir √† un mod√®le. La qualit√© de la pr√©diction
  d√©pend fortement de ce travail pr√©alable qu'on appelle _preprocessing_.
  Beaucoup de m√©thodes sont disponibles dans `scikit`, ce qui rend ce travail
  moins fastidieux et plus fiable. 
bibliography: ../../reference.bib
image: featured_preprocessing.png
echo: false
---


::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/modelisation/0_preprocessing.qmd")
```
:::

Ce chapitre utilise le jeu de donn√©es pr√©sent√© dans l'[introduction
de cette partie](index.qmd) :
les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines de 2020 au niveau des comt√©s
crois√©es √† des variables socio-d√©mographiques.
Le code de consitution de la base de donn√©es
est disponible [sur `Github`](https://github.com/linogaliana/python-datascientist/blob/master/content/modelisation/get_data.py).
L'exercice 1 permet, √† ceux qui le d√©sirent, d'essayer de le reconstituer pas √† pas. 

Le guide utilisateur de `Scikit` est une r√©f√©rence pr√©cieuse,
√† consulter r√©guli√®rement. La partie sur le *preprocessing* est
disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).

L'objectif de ce chapitre est de pr√©senter quelques √©l√©ments de 
pr√©paration des donn√©es. Il s'agit d'une √©tape fondamentale, √† ne
pas n√©gliger. Les mod√®les reposent sur certaines hypoth√®ses, g√©n√©ralement
relatives √† la distribution th√©orique des variables qui y sont int√©gr√©es.

Il est n√©cessaire de faire correspondre la distribution empirique
√† ces hypoth√®ses, ce qui implique un travail de restructuration des donn√©es.
Celui-ci permettra d'avoir des r√©sultats de mod√©lisation plus pertinents. 
Nous verrons dans le chapitre sur les *pipelines* comment industrialiser
ces √©tapes de _preprocessing_ afin de se simplifier la vie pour appliquer
un mod√®le sur un jeu de donn√©es diff√©rent de celui sur lequel il a √©t√© estim√©. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> <code>Scikit-Learn</code> </h3>
```

`scikit-learn` est aujourd'hui la librairie de r√©f√©rence dans l'√©cosyst√®me du
_Machine Learning_. Il s'agit d'une librairie qui, malgr√© les tr√®s nombreuses
m√©thodes impl√©ment√©es, pr√©sente l'avantage d'√™tre un point d'entr√©e unifi√©.
Cet aspect unifi√© est l'une des raisons du succ√®s pr√©coce de celle-ci. `R` n'a 
b√©n√©fici√© que plus r√©cemment d'une librairie unifi√©e,
√† savoir [`tidymodels`](https://www.tidymodels.org/).

Une autre raison du succ√®s de `scikit` est son approche op√©rationnelle : la mise
en production de mod√®les d√©velopp√©s via les _pipelines_ `scikit` est peu co√ªteuse.
Un [chapitre sp√©cial de ce cours](/pipeline-scikit) est d√©di√© aux _pipelines_.
Avec Romain Avouac, nous proposons un [cours plus avanc√©](https://ensae-reproductibilite.github.io/website/) 
en derni√®re ann√©e d'ENSAE o√π nous pr√©sentons certains enjeux relatifs
√† la mise en production de mod√®les d√©velopp√©s avec `scikit`. 

Le coeur de l'√©quipe de d√©veloppement de `scikit-learn` est situ√©
√† l'[Inria](https://www.inria.fr/fr) üá´üá∑. 
Pour d√©couvrir la richesse de l'√©cosyst√®me `scikit`, il 
est recommand√© de suivre le
[`MOOC scikit`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),
d√©velopp√© dans le cadre de l'initiative [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn).

```{=html}
</div>
```
:::

Les _packages_ suivants sont n√©cessaires pour importer et visualiser
les donn√©es d'√©lection :

```{python}
#| eval: false
#| echo: true
!pip install --upgrade xlrd #colab bug verson xlrd
!pip install geopandas
```

Dans ce chapitre, nous allons nous focaliser sur la pr√©paration
des donn√©es √† faire en amont du travail de mod√©lisation.
Cette √©tape est indispensable pour s'assurer de la coh√©rence
entre les donn√©es et les hypoth√®ses de mod√©lisation mais aussi
pour produire des analyses valides scientifiquement. 

La d√©marche g√©n√©rale que nous adopterons dans ce chapitre, et
qui sera ensuite raffin√©e dans les prochains chapitres,
est la suivante : 

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/scikit_predict.png)

C'est l'approche classique du _machine learning_. On d√©coupe
l'ensemble des donn√©es disponibles en deux parties, √©chantillons
d'apprentissage et de validation. Le premier sert √† entra√Æner
un mod√®le et la qualit√© des pr√©dictions de celui-ci est
√©valu√©e sur le deuxi√®me pour limiter
le biais de surapprentissage. Le chapitre suivant approfondira
cette question de l'√©valuation des mod√®les. A ce stade de notre
progression, on se concentrera dans ce chapitre
sur la question des donn√©es. La librairie `Scikit` est non seulement
particuli√®rement
pratique parce qu'elle propose √©norm√©ment d'algorithmes de _machine learning_
mais aussi parce qu'elle facilite la pr√©paration des donn√©es en amont,
ce qui est l'objet de ce chapitre.

N√©anmoins, avant de se concentrer sur la pr√©paration des donn√©es, nous
allons passer un peu de temps √† explorer la structure des donn√©es 
√† partir de laquelle nous d√©sirons construire une mod√©lisation. Ceci 
est indispensable afin de comprendre la nature de celles-ci et choisir
une mod√©lisation ad√©quate. 


## Construction de la base de donn√©es

Les sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni. 
Le travail de construction d'une base unique
est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,
pour [r√©viser `Pandas`](/content/manipulation/02a_pandas_tutorial.qmd)   :

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 1 : Importer les donn√©es des √©lections US</h3>
```

__Cet exercice est OPTIONNEL__

1. T√©l√©charger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_02_sldl_500k.zip)
2. Exclure les Etats suivants : "02", "69", "66", "78", "60", "72", "15"
3. Importer les r√©sultats des √©lections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv)
4. Importer les bases disponibles sur le site de l'USDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique
dans les 4 bases
5. *Merger* ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques
6. *Merger* aux donn√©es √©lectorales √† partir du code FIPS
7. *Merger* au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est
recommand√© d'utiliser la m√©thode `str.lstrip` pour les retirer
8. Importer les donn√©es des √©lections 2000 √† 2016 √† partir du [MIT Election Lab](https://electionlab.mit.edu/data)?
Les donn√©es peuvent √™tre directement requ√™t√©es depuis l'url
<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>
9. Cr√©er une variable `share` comptabilisant la part des votes pour chaque candidat. 
Ne garder que les colonnes `"year", "FIPS", "party", "candidatevotes", "share"`
10. Faire une conversion `long` to `wide` avec la m√©thode `pivot_table` pour garder une ligne
par comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.
11. Merger √† partir du code FIPS au reste de la base. 

```{=html}
</div>
```
:::

Si vous ne faites pas l'exercice 1, pensez √† charger les donn√©es en executant la fonction `get_data.py` :

```{python}
#| echo: true
#| output: false

import requests

url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/get_data.py'
r = requests.get(url, allow_redirects=True)
open('getdata.py', 'wb').write(r.content)

import getdata
votes = getdata.create_votes_dataframes()
```

Ce code introduit une base nomm√©e `votes` dans l'environnement. Il s'agit d'une
base rassemblant les diff√©rentes sources. Elle a l'aspect
suivant :

```{python}
#| echo: true
votes.head(3)
```


La carte choropl√®the suivante permet de visualiser rapidement les r√©sultats
(l'Alaska et Hawa√Ø ont √©t√© exclus). 

```{python}
#| warning: false
#| echo: true
#| message: false

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# republican : red, democrat : blue
color_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}

fig, ax = plt.subplots(figsize = (12,12))
grouped = votes.groupby('winner')
for key, group in grouped:
    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])
plt.axis('off')
```


Les cartes choropl√®thes peuvent donner une impression fallacieuse
ce qui explique que 
ce type de carte a servi 
de justification pour contester les r√©sultats du vote.
En effet, un biais
connu des repr√©sentations choropl√®thes est qu'elles donnent une importance
visuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces
peu denses et influencent donc moins la variable d'int√©r√™t (en l'occurrence
le taux de vote en faveur des r√©publicains/d√©mocrates). Une repr√©sentation √† 
privil√©gier pour ce type de ph√©nom√®nes est les
ronds proportionnels (voir @inseeSemiologie, _"Le pi√®ge territorial en cartographie"_). 


Le [GIF "Land does not vote, people do"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF)
qui avait eu un certain succ√®s en 2020 propose un autre mode de visualisation.
La carte originale a √©t√© construite avec `JavaScript`. Cependant,
on dispose avec `Python` de plusieurs outils
pour r√©pliquer, √† faible co√ªt, cette carte 
gr√¢ce √†
l'une des surcouches √† `JavaScript` vues dans la partie [visualisation](#visualisation). 

En l'occurrence, on peut utiliser `Plotly` pour tenir compte de la population
et faire une carte en ronds proportionnels. 
Le code suivant permet de construire une carte adapt√©e :

```{python}
#| output: false
#| echo: true
import plotly
import plotly.graph_objects as go
import pandas as pd
import geopandas as gpd


centroids = votes.copy()
centroids.geometry = centroids.centroid
centroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number

color_dict = {"republican": '#FF0000', 'democrats': '#0000FF'}
centroids["winner"] =  np.where(centroids['votes_gop'] > centroids['votes_dem'], 'republican', 'democrats') 


centroids['lon'] = centroids['geometry'].x
centroids['lat'] = centroids['geometry'].y
centroids = pd.DataFrame(centroids[["county_name",'lon','lat','winner', 'CENSUS_2010_POP',"state_name"]])
groups = centroids.groupby('winner')

df = centroids.copy()

df['color'] = df['winner'].replace(color_dict)
df['size'] = df['CENSUS_2010_POP']/6000
df['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '<br>Population: {:,} people'.format(x))
df['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']

fig_plotly = go.Figure(
  data=go.Scattergeo(
  locationmode = 'USA-states',
  lon=df["lon"], lat=df["lat"],
  text = df["hover"],
  mode = 'markers',
  marker_color = df["color"],
  marker_size = df['size'],
  hoverinfo="text"
  )
)

fig_plotly.update_traces(
  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}
)

fig_plotly.update_layout(
  title_text = "Reproduction of the \"Acres don't vote, people do\" map <br>(Click legend to toggle traces)",
  showlegend = True,
  geo = {"scope": 'usa', "landcolor": 'rgb(217, 217, 217)'}
)
```

```{python}
#| echo: true
fig_plotly.show()
```

```{python}
#| output: false
#| echo: false
fig_plotly.write_json("people_vote.json")
fig_plotly.write_image("featured_preprocessing.png")
```

Les cercles proportionnels permettent ainsi √† l'oeil de se concentrer sur les 
zones les plus denses et non sur les grands espaces. Cette fois, on voit bien
que le vote d√©mocrate est majoritaire, ce que cachait l'aplat de couleur. 

## Explorer la structure des donn√©es

La premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation
est de d√©terminer les variables √† inclure dans le mod√®le.

Les fonctionnalit√©s de `Pandas` sont, √† ce niveau, suffisantes pour explorer des structures simples.
N√©anmoins, lorsqu'on est face √† un jeu de donn√©es pr√©sentant de
nombreuses variables explicatives (*features* en machine learning, *covariates* en √©conom√©trie),
il est souvent judicieux d'avoir une premi√®re √©tape de s√©lection de variables,
ce que nous verrons par la suite dans la [partie d√©di√©e](4_featureselection.qmd).  

Avant d'√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,
nous allons en prendre un nombre restreint et arbitraire.
La premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,
notamment la relation des variables explicatives
√† la variable d√©pendante (le score du parti r√©publicain)
ainsi que les relations entre les variables explicatives. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 2 : Regarder les corr√©lations entre les variables</h3>
```

1. Cr√©er un DataFrame `df2` plus petit avec les variables `winner`, `votes_gop`, `Unemployment_rate_2019`,
`Median_Household_Income_2019`,
`Percent of adults with less than a high school diploma, 2015-19`,
`Percent of adults with a bachelor's degree or higher, 2015-19`
2. Repr√©senter gr√¢ce √† un graphique la matrice de corr√©lation. Vous pouvez utiliser le _package_ `seaborn` et sa fonction `heatmap`.
3. Repr√©senter une matrice de nuages de points des variables de la base `df2` avec `pd.plotting.scatter_matrix`
4. (optionnel) Refaire ces figures avec `Plotly` qui offre √©galement la possibilit√© de faire une matrice de corr√©lation. 

```{=html}
</div>
```
:::

```{python}
#| output: false

# 1. Cr√©er le data.frame df2.
df2 = votes.set_index("GEOID").loc[: , ["winner", "votes_gop",
          'Unemployment_rate_2019', 'Median_Household_Income_2019',
          'Percent of adults with less than a high school diploma, 2015-19',
          "Percent of adults with a bachelor's degree or higher, 2015-19"]]
```

```{python}
#| output: false
# 2. Matrice de corr√©lation graphique
g1 = sns.heatmap(df2.drop("winner", axis = 1).corr(), cmap='coolwarm', annot=True, fmt=".2f")

## Construction directement avec pandas √©galement possible
g2 = df2.drop("winner", axis = 1).corr().style.background_gradient(cmap='coolwarm').format('{:.2f}')
```

La matrice construite avec `seaborn` (question 2) aura l'aspect suivant :

```{python}
g1
```

Alors que celle construite directement avec `corr` de `Pandas`
ressemblera plut√¥t √† ce tableau :

```{python}
g2
```

Le nuage de point obtenu √† l'issue de la question 3 ressemblera √† :

```{python}
# 3. Matrice de nuages de points
ax = pd.plotting.scatter_matrix(df2, figsize = (15,15))
```

```{python}
ax
```

Le r√©sultat de la question 4 devrait, quant √† lui,
ressembler au graphique suivant :

```{python}
# 4. Matrice de corr√©lation avec plotly
import plotly
import plotly.express as px
htmlsnip2 = px.scatter_matrix(df2)
htmlsnip2.update_traces(diagonal_visible=False)
htmlsnip2.show()
```


## Transformer les donn√©es

Les diff√©rences d'√©chelle ou de distribution entre les variables peuvent 
diverger des hypoth√®ses sous-jacentes dans les mod√®les.

Par exemple, dans le cadre
de la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me
enseigne que les variables ayant valeur dans $\mathbb{R}$. Une variable
discr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©e en suite de
variables 0/1 par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation
avec les hypoth√®ses de la r√©gression lin√©aire.
On appelle ce type de transformation
*one-hot encoding*, sur laquelle nous reviendrons. Il s'agit d'une transformation,
parmi d'autres, disponibles dans `scikit` pour mettre en ad√©quation un jeu de
donn√©es et des hypoth√®ses math√©matiques. 

L'ensemble de ces t√¢ches s'appelle le *preprocessing*. L'un des int√©r√™ts
d'utiliser `Scikit` est qu'on peut consid√©rer qu'une t√¢che de _preprocessing_
est, en fait, une t√¢che d'apprentissage. En effet, le _preprocessing_ 
consiste √† apprendre des param√®tres d'une structure 
de donn√©es (par exemple estimer moyennes et variances pour les retrancher √† chaque
observation) et on peut tr√®s bien appliquer ces param√®tres
√† des observations qui n'ont pas servi √† construire
ceux-ci. Ainsi, en gardant en t√™te l'approche g√©n√©rale avec `Scikit`,

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/scikit_predict.png)

nous allons voir deux processus tr√®s classiques de *preprocessing* : 

1. La **standardisation** transforme des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$.

2. La **normalisation**  transforme les donn√©es de mani√®re √† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```
Pour un statisticien,
le terme _normalization_ dans le vocable `Scikit` peut avoir un sens contre-intuitif.
On s'attendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que $X \sim \mathcal{N}(0,1)$.
C'est, en fait, la **standardisation** en `Scikit` qui fait cela.

```{=html}
</div>
```
:::


### Standardisation

La standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour √™tre performants, la plupart des mod√®les de _machine learning_ n√©cessitent souvent d'avoir des donn√©es dans cette distribution.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 3: Standardisation</h3>
```


1. Standardiser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.

*Note : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.*


2. Cr√©er `scaler`, un `Transformer` que vous construisez sur les 1000 premi√®res lignes de votre DataFrame `df2`  √†  l'exception de la variable √† expliquer `winner`. V√©rifier la moyenne et l'√©cart-type de chaque colonne sur ces m√™mes observations.

*Note : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs `.mean_` et `.scale_`*

On peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de
donn√©es et qu'on peut r√©utiliser sur un autre, √† condition que les
dimensions co√Øncident.

3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `Median_Household_Income_2019`.

*Note : Une fois appliqu√©s √† un autre `DataFrame`, on peut remarquer que la distribution n'est pas exactement centr√©e-r√©duite dans le `DataFrame` sur lequel les param√®tres n'ont pas √©t√© estim√©s. C'est normal, l'√©chantillon initial n'√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n'ont pas de raison de co√Øncider avec les moments de l'√©chantillon complet.*


```{=html}
</div>
```
:::

```{python}
# 1. Standardisation de Median_Household_Income_2019 et histogramme
from sklearn import preprocessing
df2['y_standard'] = preprocessing.scale(df2['Median_Household_Income_2019'])
f, axes = plt.subplots(2, figsize=(10, 10))
sns.histplot(df2["Median_Household_Income_2019"] , color="skyblue", ax=axes[0])
sns.histplot(df2["y_standard"] , color="olive", ax=axes[1])
```

```{python}
# 2. Cr√©er un scaler
df2 = df2.drop("winner", axis = 1)
print("Moyenne de chaque variable sur 1000 premi√®res observations avant : ", np.array(df2.head(1000).mean(axis=0)))
print("Ecart-type de chaque variable sur 1000 premi√®res observations avant : ", np.array(df2.head(1000).std(axis=0)))
scaler = preprocessing.StandardScaler().fit(df2.head(1000))
scaler.transform(df2.head(1000))
print("Moyenne de chaque variable sur 1000 premi√®res observations apr√®s : ", scaler.transform(df2.head(1000)).mean(axis=0))
print("Ecart-type de chaque variable sur 1000 premi√®res observations apr√®s : ", scaler.transform(df2.head(1000)).std(axis=0))
```


```{python}
# 3. Appliquer le scaler √† toutes les autres lignes
X1 = scaler.transform(df2.head(1000))
X2 = scaler.transform(df2[1000:])
col_pos = df2.columns.get_loc("Median_Household_Income_2019")

f, axes = plt.subplots(2, figsize=(10, 10))
sns.histplot(X1[:,col_pos] , color="skyblue", ax=axes[0])
sns.histplot(X2[:,col_pos] , color="olive", ax=axes[1])
```


### Normalisation

La **normalisation** est l'action de transformer les donn√©es de mani√®re
√† obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire.
Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.
Par d√©faut, la norme est dans $\mathcal{l}_2$.
Cette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du *clustering*.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 4 : Normalisation</h3>
```

1. Normaliser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.
2. V√©rifier que la norme $\mathcal{l}_2$ est bien √©gale √† 1.

```{=html}
</div>
```
:::

```{python}
# 1. Normalisation de Median_Household_Income_2019 et histogrammes
scaler = preprocessing.Normalizer().fit(df2.head(1000))
X1 = scaler.transform(df2.dropna(how = "any").head(1000))

f, axes = plt.subplots(2, figsize=(10, 10))
sns.histplot(df2["Median_Household_Income_2019"] , color="skyblue", ax=axes[0])
sns.histplot(X1[:,col_pos] , color="olive", ax=axes[1])
```

```{python}
# 2. V√©rification de la norme L2
np.sqrt(np.sum(X1**2, axis=1))[:10] # L2-norm
```



### Encodage des valeurs cat√©gorielles

Les donn√©es cat√©gorielles doivent √™tre recod√©es
sous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de *machine learning*.
Cela peut √™tre fait de plusieurs mani√®res :

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur num√©rique `[0,1,2]`.
Cette approche a l'inconv√©nient d'introduire un ordre dans les modalit√©s, ce qui n'est pas toujours souhaitable

* `OrdinalEncoder`: une version g√©n√©ralis√©e du `LabelEncoder` qui a vocation √† s'appliquer sur des matrices ($X$),
alors que `LabelEncoder` s'applique plut√¥t √† un vecteur ($y$)

* `pandas.get_dummies` effectue une op√©ration de *dummy expansion*.
Un vecteur de taille *n* avec *K* cat√©gories sera transform√© en matrice de taille $n \times K$
pour lequel chaque colonne sera une variable *dummy* pour la modalit√© *k*.
Il y a ici $K$ modalit√©s et il y a donc multicolin√©arit√©.
Avec une r√©gression lin√©aire avec constante,
il convient de retirer une modalit√© avant l'estimation.

* `OneHotEncoder` est une version g√©n√©ralis√©e (et optimis√©e) de la *dummy expansion*.
Il a plut√¥t vocation √† s'appliquer sur les *features* ($X$) du mod√®le

### Imputation

Les donn√©es peuvent souvent contenir des valeurs manquantes, autrement dit des cases de notre _DataFrame_ contenant un `NaN`.
Ces trous dans les donn√©es peuvent √™tre √† l'origine de _bugs_ ou de mauvaises interpr√©tations lorsque l'on passe √† la mod√©lisation.
Pour y rem√©dier, une premi√®re approche peut √™tre de retirer toutes les observations pr√©sentant un `NaN` dans au moins l'une des colonnes.
Cependant, si notre table contient beaucoup de `NaN`, ou bien que ces derniers sont r√©partis sur de nombreuses colonnes,
c'est aussi prendre le risque de retirer un nombre important de lignes, et avec cela de l'information importante pour un mod√®le car les valeurs manquantes sont rarement [r√©parties de mani√®re al√©atoire](https://stefvanbuuren.name/fimd/sec-MCAR.html).

M√™me si dans plusieurs situations, cette solution reste tout √† fait viable, il existe une autre approche plus robuste appel√©e *imputation*.
Cette m√©thode consiste √† remplacer les valeurs vides par une valeur donn√©e. Par exemple :

- Imputation par la moyenne : remplacer tous les `NaN` dans une colonne par la valeur moyenne de la colonne ;
- Imputation par la m√©diane sur le m√™me principe, ou par la valeur de la colonne la plus fr√©quente pour les variables cat√©gorielles ;
- Imputation par r√©gression : se servir d'autres variables pour essayer d'interpoler une valeur de remplacement adapt√©e.

Des m√©thodes plus complexes existent, mais dans de nombreux cas,
les approches ci-dessus peuvent suffire pour donner des r√©sultats beaucoup plus satisfaisants.
Le package `Scikit` permet de faire de l'imputation de mani√®re tr√®s simple ([documentation ici](https://scikit-learn.org/stable/modules/impute.html)).

### Gestion des outliers


Les valeurs aberrantes (_outliers_ en anglais) sont des observations qui se situent significativement √† l'ext√©rieur de la tendance g√©n√©rale des autres observations dans un ensemble de donn√©es. En d'autres termes, ce sont des points de donn√©es qui se d√©marquent de mani√®re inhabituelle par rapport √† la distribution globale des donn√©es.
Cela peut √™tre d√ª √† des erreurs de remplissage, des personnes ayant mal r√©pondu √† un questionnaire, ou
parfois simplement des valeurs extr√™mes qui peuvent biaiser un mod√®le de fa√ßon trop importante.

A titre d'exemple, cela va √™tre 3 individus mesurant plus de 4 m√®tres dans une population,
ou bien des revenus de m√©nage d√©passant les 10M d'euros par mois sur l'√©chelle d'un pays, etc.

Une bonne pratique peut donc √™tre de syst√©matiquement regarder la distribution des variables √† disposition,
pour se rendre compte si certaines valeurs s'√©loignent de fa√ßon trop importante des autres.
Ces valeurs vont parfois nous int√©resser, si par exemple on se concentre uniquement sur les tr√®s hauts revenus (top 0.1%)
en France. Cependant, ces donn√©es vont souvent nous g√™ner plus qu'autre chose, surtout si elles n'ont pas de sens dans le monde r√©el.

Si l'on estime que la pr√©sence de ces donn√©es extr√™mes, ou *outliers*, dans notre base de donn√©es vont √™tre probl√©matiques plus qu'autre chose,
alors il est tout √† fait entendable et possible de simplement les retirer.
La plupart du temps, on va se donner une proportion des donn√©es √† retirer, par exemple 0.1%, 1% ou 5%,
puis retirer dans les deux queues de la distribution les valeurs extr√™mes correspondantes.

Plusieurs packages permettent de faire ce type d'op√©rations, qui sont parfois plus complexes si on s'int√©resse aux outlier sur plusieurs variables.
On pourra notamment citer la fonction `IsolationForest()` du package `sklearn.ensemble`.

<br>

Pour plus de d√©tails sur ces deux derniers points, il est recommand√© d'aller voir l'exemple *Pour aller plus loin* en bas de la page.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 5 : Encoder des variables cat√©gorielles</h3>
```

1. Cr√©er `df` qui conserve uniquement les variables `state_name` et `county_name` dans `votes`.
2. Appliquer √† `state_name` un `LabelEncoder`
*Note : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.*

3. Regarder la *dummy expansion* de `state_name`
4. Appliquer un `OrdinalEncoder` √† `df[['state_name', 'county_name']]`
*Note : Le r√©sultat du _ordinal encoding_ est coh√©rent avec celui du label encoding*

5. Appliquer un `OneHotEncoder` √† `df[['state_name', 'county_name']]`

*Note : `scikit` optimise l'objet n√©cessaire pour stocker le r√©sultat d'un mod√®le de transformation. Par exemple, le r√©sultat de l'encoding One Hot est un objet tr√®s volumineux. Dans ce cas, `scikit` utilise une matrice Sparse.*


```{=html}
</div>
```
:::

```{python}
#1. Cr√©ation de df
df = votes[["state_name",'county_name']]
```


```{python}
#2. Appliquer un LabelEncoder √† stat_name
label_enc = preprocessing.LabelEncoder().fit(df['state_name'])
np.column_stack((label_enc.transform(df['state_name']),df['state_name']))
```


```{python}
# 3. dummy expansion de state_name
pd.get_dummies(df['state_name'])
```



```{python}
# 4. OrdinalEncoder
ord_enc = preprocessing.OrdinalEncoder().fit(df)
# ord_enc.transform(df[['state', 'county']])
ord_enc.transform(df)[:,0]
```


```{python}
# 5. OneHotEncoder
onehot_enc = preprocessing.OneHotEncoder().fit(df)
onehot_enc.transform(df)
```

## Pour aller plus loin : un exemple pratique

Pour faire vos premiers pas en mod√©lisation, notamment sur le preprocessing de donn√©es, vous pouvez √©galement consulter le sujet 3 d'un hackathon organis√© par l'Insee en 2023, *Explorer les habitudes alimentaires de nos compatriotes*, sur le [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) ou sur [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).

Le but du sujet est de travailler sur les donn√©es de consommations et habitudes alimentaires de l'√©tude INCA 3. Vous y travaillerez plusieurs th√®mes :
- Analyse exploratoire de donn√©es et visualisations
- Clustering d'individus : du preprocessing jusqu'aux m√©thodes classiques d'apprentissage non supervis√© (ACP, K-moyennes, Clustering Ascendant Hi√©rarchique)
- Pr√©diction de l'IMC : Premiers pas vers les m√©thodes d'apprentissage supervis√© et les _preprocessings_ associ√©s

## R√©f√©rences

::: {#refs}
:::
