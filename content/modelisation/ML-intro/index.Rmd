---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Préparation des données pour construire un modèle"
date: 2020-10-15T13:00:00Z
draft: false
weight: 10
output: 
  html_document:
    keep_md: true
    self_contained: true
slug: MLintro
---

```{r setup, include=FALSE}
library(knitr)  
library(reticulate)  
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(fig.path = "")
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)

# Hook from Maelle Salmon: https://ropensci.org/technotes/2020/04/23/rmd-learnings/
knitr::knit_hooks$set(
  plot = function(x, options) {
    hugoopts <- options$hugoopts
    paste0(
      "{", "{<figure src=", # the original code is simpler
      # but here I need to escape the shortcode!
      '"', x, '" ',
      if (!is.null(hugoopts)) {
        glue::glue_collapse(
          glue::glue('{names(hugoopts)}="{hugoopts}"'),
          sep = " "
        )
      },
      ">}}\n"
    )
  }
)

```

```{python, include = FALSE}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/W3CRK9/AppData/Local/r-miniconda/envs/r-reticulate/Library/plugins/platforms'
os.environ["PROJ_LIB"] = r'C:\Users\W3CRK9\AppData\Local\r-miniconda\pkgs\proj4-4.9.3-hfa6e2cd_9\Library\share'
os.environ['GDAL_DATA'] = r"C:\Users\W3CRK9\AppData\Local\r-miniconda\envs\r-reticulate\Library\share\gdal"
```

Pour illustrer le travail de données nécessaire pour construire un modèle de Machine Learning, mais aussi nécessaire pour l'exploration de données avant de faire une régression linéaire, nous allons partir du jeu de données de [résultat des élections US 2016 au niveau des comtés](https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/download/?format=geojson&timezone=Europe/Berlin&lang=fr)

## Explorer la structure des données

La première étape nécessaire à suivre avant de modéliser est de déterminer les variables à inclure dans le modèle. Les fonctionalités de `pandas` sont, à ce niveau, suffisantes pour explorer des structures simples. Néanmoins, lorsqu'on est face à un jeu de données présentant de nombreuses variables explicatives (*features* en machine learning, *covariates* en économétrie), il est souvent judicieux d'avoir une première étape de sélection de variable, ce que nous verrons par la suite [**LIEN**]  

{{% panel status="exercise" title="Exercise 1: importer les données" icon="fas fa-pencil-alt" %}}
1. Importer les données (l'appeler `df`) des élections américaines et regarder les informations dont on dispose
2. Créer une variable `republican_winner` égale à `red`  quand la variable `rep16_frac` est supérieure à `dep16_frac` (`blue` sinon)
3. Représenter une carte des résultats avec en rouge les comtés où les républicains ont gagné et en bleu ceux où se sont
les démocrates
{{% /panel %}}

```{python}
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
df = gpd.read_file("https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/download/?format=geojson&timezone=Europe/Berlin&lang=fr")
df['winner'] = np.where(df['rep16_frac'] > df['dem16_frac'], '#FF0000', '#0000FF') 
# df.plot('winner', color = df['winner'], figsize = (20,20))
```

Avant d'être en mesure de sélectionner le meilleur ensemble de variables explicatives, nous allons prendre un nombre restreint et arbitraire de variables. La première tâche est de représenter les relations entre les données, notamment leur relation à la variable que l'on va chercher à expliquer (le score du parti républicain aux élections 2016) ainsi que les relations entre les variables ayant vocation à expliquer la variable dépendante. 

{{% panel status="exercise" title="Exercise 2: regarder la corrélation entre les variables" icon="fas fa-pencil-alt" %}}

Créer un DataFrame plus petit avec les variables `rep16_frac` et `unemployment`, `median_age`, `asian`, `black`, `white_not_latino_population`,`latino_population`, `gini_coefficient`, `less_than_high_school`, `adult_obesity`, `median_earnings_2010_dollars` et ensuite :

1. Représenter une matrice de corrélation graphique
1. Choisir quelques variables (pas plus de 4 ou 5) dont `rep16_frac` et représenter une matrice de nuages de points
2. (optionnel) Refaire ces figures avec `plotly`
{{% /panel %}}

La matrice de corrélation donne, avec les fonctionalités de `pandas`:

```{python}
df2 = df[["rep16_frac", "unemployment", "median_age", "asian", "black", "white_not_latino_population","latino_population", "gini_coefficient", "less_than_high_school", "adult_obesity", "median_earnings_2010_dollars"]]
df2.corr().style.background_gradient(cmap='coolwarm').set_precision(2)
plt.show()
```

Alors que celle construite avec `seaborn` aura l'aspect suivant:

```{python}
sns.heatmap(df2.corr(), cmap='coolwarm', annot=True, fmt=".2f")
```


La matrice de nuage de point aura, par exemple, l'aspect suivant:

```{python}
pd.plotting.scatter_matrix(df2[["rep16_frac", "unemployment", "median_age", "asian", "black"]], figsize = (15,15))
```

```{python}
import plotly
import plotly.express as px
htmlsnip2 = px.scatter_matrix(df2[["rep16_frac", "unemployment", "median_age", "asian", "black"]])
htmlsnip2.update_traces(diagonal_visible=False)
# Pour inclusion dans le site web
htmlsnip2 = plotly.io.to_html(htmlsnip2, include_plotlyjs=False)
```


Le résultat devrait ressembler aux deux graphiques suivants:

{{< rawhtml >}}
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script> 
```{r}
tablelight::print_html(py$htmlsnip2)
```
{{< /rawhtml >}}



## Transformer les données

Les différences d'échelle ou de distribution entre les variables peuvent 
diverger des hypothèses sous-jacentes dans les modèles. Par exemple, dans le cadre
de la régression linéaire, les variables catégorielles ne sont pas traitées à la même
enseigne que les variables ayant valeur dans $\mathbb{R}$. Il est ainsi 
souvent nécessaire d'appliquer des tâches de *preprocessing*, c'est-à-dire 
des tâches de modification de la distribution des données pour les rendre
cohérentes avec les hypothèses des modèles.

### Standardisation

La standardisation consiste à transformer des données pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour être performants, la plupart des modèles de machine learning nécessitent d'avoir des données dans cette distribution. 

{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Pour un statisticien, le terme `normalization` dans le vocable `scikit` peut avoir un sens contre-intuitif. On s'attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que $X \sym \mathcal{N}(0,1)$. C'est, en fait, la **standardisation** en `scikit`. 

La **normalisation** consiste à modifier les données de manière à avoir une norme unitaire. *Lien vers section normalisation*
{{% /panel %}}


{{% panel status="exercise" title="Exercice: normalisation" icon="fas fa-pencil-alt" %}}
1. Standardiser la variable `median_earnings_2010_dollars` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation
2. Créer `scaler`, un `Transformer` que vous construisez sur les 1000 premières lignes de votre DataFrame. Vérifier la moyenne et l'écart-type de chaque colonne sur ces mêmes observations.
3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `median_earnings_2010_dollars`.
{{% /panel %}}

La normalisation permet d'obtenir la modification suivante de la distribution:

```{python}
# Question 1
from sklearn import preprocessing
df2['y_standard'] = preprocessing.scale(df2['median_earnings_2010_dollars'])
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["median_earnings_2010_dollars"] , color="skyblue", ax=axes[0])
sns.distplot(df2["y_standard"] , color="olive", ax=axes[1])
```

On obtient bien une distribution centrée à zéro et on pourrait vérifier que la variance empirique est bien égale à 1. On peut aussi vérifier que ceci est vraiment quand on transforme plusieurs colonnes à la fois

```{python}
# Question 2
scaler = preprocessing.StandardScaler().fit(df2.head(1000))
scaler.transform(df2.head(1000))
print("Moyenne de chaque variable sur 1000 premières observations")
scaler.transform(df2.head(1000)).mean(axis=0)
print("Ecart-type de chaque variable sur 1000 premières observations")
scaler.transform(df2.head(1000)).std(axis=0)
```

On peut vérifier les paramètres qui seront utilisés pour une standardisation ultérieure de la manière suivante

```{python, echo = TRUE}
scaler.mean_
scaler.scale_
```

Et, une fois appliqués, on peut remarquer que la distribution n'est pas exactement centrée-réduite dans le `DataFrame` sur lequel les paramètres n'ont pas été estimés. C'est normal, l'échantillon initial n'était pas aléatoire. 

```{python}
# Question 3
X1 = scaler.transform(df2.head(1000))
X2 = scaler.transform(df2[1000:])
col_pos = df2.columns.get_loc("median_earnings_2010_dollars")
# X2.mean(axis = 0)
# X2.std(axis = 0)
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(X1[:,col_pos] , color="skyblue", ax=axes[0])
sns.distplot(X2[:,col_pos] , color="olive", ax=axes[1])
```


### Normalisation

La normalisation est l'action de transformer les données de manière à obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1. Par défaut, la norme est dans $\mathcal{l}_2$. Cette transformation est particulièrement utilisée en classification de texte ou pour effectuer du *clustering*

{{% panel status="exercise" title="Exercice: normalization" icon="fas fa-pencil-alt" %}}
1. Normaliser la variable `median_earnings_2010_dollars` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation
2. Vérifier que la norme $\mathcal{l}_2$ est bien égale à 1.
{{% /panel %}}

```{python}
scaler = preprocessing.Normalizer().fit(df2.dropna(how = "any").head(1000))
X1 = scaler.transform(df2.dropna(how = "any").head(1000))

f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["median_earnings_2010_dollars"] , color="skyblue", ax=axes[0])
sns.distplot(X1[:,col_pos] , color="olive", ax=axes[1])

# Question 2
# np.sqrt(np.sum(X1**2, axis=1))[:5] # L2-norm
```

{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
` preprocessing.Normalizer` n'accepte pas les valeurs manquantes, alors que `preprocessing.StandardScaler()` s'en accomode (dans la version `0.22` de scikit). Pour pouvoir aisément appliquer le *normalizer*, il faut

* retirer les valeurs manquantes du DataFrame avec la méthode `dropna`: `df.dropna(how = "any")`;
* les imputer avec un modèle adéquat. `scikit` permet de le faire ([info](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values)) 
{{% /panel %}}

Les transformations à montrer (https://scikit-learn.org/stable/modules/preprocessing.html):

### Encodage des valeurs catégorielles

Les données catégorielles doivent être recodées sous forme de valeurs numériques pour être intégrables dans le cadre d'un modèle. Cela peut être fait de deux manières:

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur numérique `[0,1,2]`. Cette approche a l'inconvénient d'introduire un ordre dans les modalités, ce qui n'est pas toujours désiré
* `OrdinalEncoder`: une version généralisée du recodage précédent. `OrdinalEncoder` a vocation à s'appliquer sur des matrices ($X$), alors que `LabelEncoder` est plutôt pour un vecteur ($y$)
* `OneHotEncoder` effectue une opération de *dummy expansion*. Un vecteur de taille *n* avec *K* catégories sera transformé en matrice de taille $n \times (K-1)$ pour lequel chaque colonne sera une variable *dummy* pour la modalité *k*. Pour éviter la multicollinéarité. C'est la formalisation faite, par exemple, pour la régression linéaire.


{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Prendra la variable `state` dans `df`
1. Lui appliquer 
{{% /panel %}}

Note sur l'imputation des valeurs manquantes

## Découper l'échantillon

## Mesurer la performance