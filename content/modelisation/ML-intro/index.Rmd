---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Préparation des données pour construire un modèle"
date: 2020-10-15T13:00:00Z
draft: false
weight: 10
output: 
  html_document:
    keep_md: true
    self_contained: true
slug: MLintro
---

```{r setup, include=FALSE}
library(knitr)  
library(reticulate)  
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(fig.path = "")
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)

# Hook from Maelle Salmon: https://ropensci.org/technotes/2020/04/23/rmd-learnings/
knitr::knit_hooks$set(
  plot = function(x, options) {
    hugoopts <- options$hugoopts
    paste0(
      "{", "{<figure src=", # the original code is simpler
      # but here I need to escape the shortcode!
      '"', x, '" ',
      if (!is.null(hugoopts)) {
        glue::glue_collapse(
          glue::glue('{names(hugoopts)}="{hugoopts}"'),
          sep = " "
        )
      },
      ">}}\n"
    )
  }
)

```

```{python, include = FALSE}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/W3CRK9/AppData/Local/r-miniconda/envs/r-reticulate/Library/plugins/platforms'
os.environ["PROJ_LIB"] = r'C:\Users\W3CRK9\AppData\Local\r-miniconda\pkgs\proj4-4.9.3-hfa6e2cd_9\Library\share'
os.environ['GDAL_DATA'] = r"C:\Users\W3CRK9\AppData\Local\r-miniconda\envs\r-reticulate\Library\share\gdal"
```

Pour illustrer le travail de données nécessaire pour construire un modèle de Machine Learning, mais aussi nécessaire pour l'exploration de données avant de faire une régression linéaire, nous allons partir du jeu de données de [résultat des élections US 2016 au niveau des comtés](https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/download/?format=geojson&timezone=Europe/Berlin&lang=fr)

Le guide utilisateur de `scikit` est une référence précieuse, à consulter régulièrement. La partie sur le *preprocessing* est
disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).

## Explorer la structure des données

La première étape nécessaire à suivre avant de modéliser est de déterminer les variables à inclure dans le modèle. Les fonctionalités de `pandas` sont, à ce niveau, suffisantes pour explorer des structures simples. Néanmoins, lorsqu'on est face à un jeu de données présentant de nombreuses variables explicatives (*features* en machine learning, *covariates* en économétrie), il est souvent judicieux d'avoir une première étape de sélection de variable, ce que nous verrons par la suite [**LIEN**]  

{{% panel status="exercise" title="Exercise 1: importer les données" icon="fas fa-pencil-alt" %}}
1. Importer les données (l'appeler `df`) des élections américaines et regarder les informations dont on dispose
2. Créer une variable `republican_winner` égale à `red`  quand la variable `rep16_frac` est supérieure à `dep16_frac` (`blue` sinon)
3. Représenter une carte des résultats avec en rouge les comtés où les républicains ont gagné et en bleu ceux où se sont
les démocrates
{{% /panel %}}

```{python}
import numpy as np
import pandas as pd
import geopandas as gpd
import seaborn as sns
import matplotlib.pyplot as plt
df = gpd.read_file("https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/download/?format=geojson&timezone=Europe/Berlin&lang=fr")
df['winner'] = np.where(df['rep16_frac'] > df['dem16_frac'], '#FF0000', '#0000FF') 
# df.plot('winner', color = df['winner'], figsize = (20,20))
```

Avant d'être en mesure de sélectionner le meilleur ensemble de variables explicatives, nous allons prendre un nombre restreint et arbitraire de variables. La première tâche est de représenter les relations entre les données, notamment leur relation à la variable que l'on va chercher à expliquer (le score du parti républicain aux élections 2016) ainsi que les relations entre les variables ayant vocation à expliquer la variable dépendante. 

{{% panel status="exercise" title="Exercise 2: regarder la corrélation entre les variables" icon="fas fa-pencil-alt" %}}

Créer un DataFrame plus petit avec les variables `rep16_frac` et `unemployment`, `median_age`, `asian`, `black`, `white_not_latino_population`,`latino_population`, `gini_coefficient`, `less_than_high_school`, `adult_obesity`, `median_earnings_2010_dollars` et ensuite :

1. Représenter une matrice de corrélation graphique
1. Choisir quelques variables (pas plus de 4 ou 5) dont `rep16_frac` et représenter une matrice de nuages de points
2. (optionnel) Refaire ces figures avec `plotly`
{{% /panel %}}

La matrice de corrélation donne, avec les fonctionalités de `pandas`:

```{python}
df2 = df[["rep16_frac", "unemployment", "median_age", "asian", "black", "white_not_latino_population","latino_population", "gini_coefficient", "less_than_high_school", "adult_obesity", "median_earnings_2010_dollars"]]
df2.corr().style.background_gradient(cmap='coolwarm').set_precision(2)
plt.show()
```

Alors que celle construite avec `seaborn` aura l'aspect suivant:

```{python}
sns.heatmap(df2.corr(), cmap='coolwarm', annot=True, fmt=".2f")
```


La matrice de nuage de point aura, par exemple, l'aspect suivant:

```{python}
pd.plotting.scatter_matrix(df2[["rep16_frac", "unemployment", "median_age", "asian", "black"]], figsize = (15,15))
```

```{python}
import plotly
import plotly.express as px
htmlsnip2 = px.scatter_matrix(df2[["rep16_frac", "unemployment", "median_age", "asian", "black"]])
htmlsnip2.update_traces(diagonal_visible=False)
# Pour inclusion dans le site web
htmlsnip2 = plotly.io.to_html(htmlsnip2, include_plotlyjs=False)
```


Le résultat devrait ressembler aux deux graphiques suivants:

{{< rawhtml >}}
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script> 
```{r}
tablelight::print_html(py$htmlsnip2)
```
{{< /rawhtml >}}



## Transformer les données

Les différences d'échelle ou de distribution entre les variables peuvent 
diverger des hypothèses sous-jacentes dans les modèles. Par exemple, dans le cadre
de la régression linéaire, les variables catégorielles ne sont pas traitées à la même
enseigne que les variables ayant valeur dans $\mathbb{R}$. Il est ainsi 
souvent nécessaire d'appliquer des tâches de *preprocessing*, c'est-à-dire 
des tâches de modification de la distribution des données pour les rendre
cohérentes avec les hypothèses des modèles.

### Standardisation

La standardisation consiste à transformer des données pour que la distribution empirique suive une loi $\mathcal{N}(0,1)$. Pour être performants, la plupart des modèles de machine learning nécessitent d'avoir des données dans cette distribution. 

{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Pour un statisticien, le terme `normalization` dans le vocable `scikit` peut avoir un sens contre-intuitif. On s'attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que $X \sym \mathcal{N}(0,1)$. C'est, en fait, la **standardisation** en `scikit`. 

La **normalisation** consiste à modifier les données de manière à avoir une norme unitaire. *Lien vers section normalisation*
{{% /panel %}}


{{% panel status="exercise" title="Exercice: normalisation" icon="fas fa-pencil-alt" %}}
1. Standardiser la variable `median_earnings_2010_dollars` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation
2. Créer `scaler`, un `Transformer` que vous construisez sur les 1000 premières lignes de votre DataFrame. Vérifier la moyenne et l'écart-type de chaque colonne sur ces mêmes observations.
3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `median_earnings_2010_dollars`.
{{% /panel %}}

La normalisation permet d'obtenir la modification suivante de la distribution:

```{python}
# Question 1
from sklearn import preprocessing
df2['y_standard'] = preprocessing.scale(df2['median_earnings_2010_dollars'])
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["median_earnings_2010_dollars"] , color="skyblue", ax=axes[0])
sns.distplot(df2["y_standard"] , color="olive", ax=axes[1])
```

On obtient bien une distribution centrée à zéro et on pourrait vérifier que la variance empirique est bien égale à 1. On peut aussi vérifier que ceci est vraiment quand on transforme plusieurs colonnes à la fois

```{python}
# Question 2
scaler = preprocessing.StandardScaler().fit(df2.head(1000))
scaler.transform(df2.head(1000))
print("Moyenne de chaque variable sur 1000 premières observations")
scaler.transform(df2.head(1000)).mean(axis=0)
print("Ecart-type de chaque variable sur 1000 premières observations")
scaler.transform(df2.head(1000)).std(axis=0)
```

On peut vérifier les paramètres qui seront utilisés pour une standardisation ultérieure de la manière suivante

```{python, echo = TRUE}
scaler.mean_
scaler.scale_
```

Et, une fois appliqués, on peut remarquer que la distribution n'est pas exactement centrée-réduite dans le `DataFrame` sur lequel les paramètres n'ont pas été estimés. C'est normal, l'échantillon initial n'était pas aléatoire. 

```{python}
# Question 3
X1 = scaler.transform(df2.head(1000))
X2 = scaler.transform(df2[1000:])
col_pos = df2.columns.get_loc("median_earnings_2010_dollars")
# X2.mean(axis = 0)
# X2.std(axis = 0)
f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(X1[:,col_pos] , color="skyblue", ax=axes[0])
sns.distplot(X2[:,col_pos] , color="olive", ax=axes[1])
```


### Normalisation

La normalisation est l'action de transformer les données de manière à obtenir une norme ($\mathcal{l}_1$ ou $\mathcal{l}_2$) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1. Par défaut, la norme est dans $\mathcal{l}_2$. Cette transformation est particulièrement utilisée en classification de texte ou pour effectuer du *clustering*

{{% panel status="exercise" title="Exercice: normalization" icon="fas fa-pencil-alt" %}}
1. Normaliser la variable `median_earnings_2010_dollars` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation
2. Vérifier que la norme $\mathcal{l}_2$ est bien égale à 1.
{{% /panel %}}

```{python}
scaler = preprocessing.Normalizer().fit(df2.dropna(how = "any").head(1000))
X1 = scaler.transform(df2.dropna(how = "any").head(1000))

f, axes = plt.subplots(2, figsize=(10, 10))
sns.distplot(df2["median_earnings_2010_dollars"] , color="skyblue", ax=axes[0])
sns.distplot(X1[:,col_pos] , color="olive", ax=axes[1])

# Question 2
# np.sqrt(np.sum(X1**2, axis=1))[:5] # L2-norm
```

{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
` preprocessing.Normalizer` n'accepte pas les valeurs manquantes, alors que `preprocessing.StandardScaler()` s'en accomode (dans la version `0.22` de scikit). Pour pouvoir aisément appliquer le *normalizer*, il faut

* retirer les valeurs manquantes du DataFrame avec la méthode `dropna`: `df.dropna(how = "any")`;
* les imputer avec un modèle adéquat. `scikit` permet de le faire ([info](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values)) 
{{% /panel %}}


### Encodage des valeurs catégorielles

Les données catégorielles doivent être recodées sous forme de valeurs numériques pour être intégrables dans le cadre d'un modèle. Cela peut être fait de plusieurs manières:

* `LabelEncoder`: transforme un vecteur `["a","b","c"]` en vecteur numérique `[0,1,2]`. Cette approche a l'inconvénient d'introduire un ordre dans les modalités, ce qui n'est pas toujours désiré
* `pandas.get_dummies` effectue une opération de *dummy expansion*. Un vecteur de taille *n* avec *K* catégories sera transformé en matrice de taille $n \times (K-1)$ pour lequel chaque colonne sera une variable *dummy* pour la modalité *k*. Il y a ici $K$ modalité, il y a donc multicollinéarité. Avec une régression linéaire avec constante, il convient de retirer une modalité avant l'estimation.
* `OrdinalEncoder`: une version généralisée du `LabelEncoder`. `OrdinalEncoder` a vocation à s'appliquer sur des matrices ($X$), alors que `LabelEncoder` est plutôt pour un vecteur ($y$)
* `OneHotEncoder` 


{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Prendra les variables `state` et `county` dans `df`
1. Appliquer à `state` un `LabelEncoder`
2. Regarder la *dummy expansion* de `state`
3. Appliquer un `OrdinalEncoder` à `df[['state', 'county']]` ainsi qu'un `OneHotEncoder`
{{% /panel %}}

Le résultat du *label encoding* est relativement intuitif, notamment quand on le met en relation avec le vecteur initial

```{python}
# Question 1
label_enc = preprocessing.LabelEncoder().fit(df['state'])
np.column_stack((label_enc.transform(df['state']),df['state']))
```

```{python}
# Question 2
pd.get_dummies(df['state'])
```

Le résultat du *ordinal encoding* est cohérent avec celui du *label encoding*:

```{python}
ord_enc = preprocessing.OrdinalEncoder().fit(df[['state', 'county']])
# ord_enc.transform(df[['state', 'county']])
```

```{python}
ord_enc.transform(df[['state', 'county']])[:,0]
```

Enfin, on peut noter que `scikit` optimise l'objet nécessaire pour stocker le résultat d'un modèle de transformation. Par exemple, le résultat de l'encoding *One Hot* est un objet très volumineux. Dans ce cas, scikit utilise une matrice *Sparse*:

```{python}
onehot_enc = preprocessing.OneHotEncoder().fit(df[['state', 'county']])
```

```{python}
onehot_enc.transform(df[['state', 'county']])
```


## Découper l'échantillon

![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)

### Le problème du sur-apprentissage

Le but du *Machine Learning* est de calibrer l’algorithme sur des exemples
connus (données labellisées) afin de généraliser à des
exemples nouveaux (éventuellement non labellisés). On vise donc de bonnes qualités
prédictives et non un ajustement parfait
aux données historiques.

Il existe un arbitrage biais-variance dans la qualité d'estimation^[1]. Soit $h(X,\theta)$ un modèle statistique. On 
peut décomposer l'erreur d'estimation en deux parties:

$$
\mathbb{E}\bigg[(y - h(\theta,X))^2 \bigg] = \underbrace{ \bigg( y - \mathbb{E}(h(\theta,X)) \bigg)^2}_{\text{biais}^2} + \underbrace{\mathbb{V}\big(h(\theta,X)\big)}_{\text{variance}}
$$
Il y a ainsi un compromis entre biais et variance. Un modèle peu parcimonieux, c'est-à-dire proposant un grand nombre de paramètres, va, en général, avoir un faible biais mais une grande variance. En effet, le modèle va tendre à se souvenir d'une combinaison de paramètres à partir d'un grand nombre d'exemples sans être capable d'apprendre la règle qui permette de structurer les données. 

^[1]: Cette formule permet de bien comprendre la théorie statistique asymptotique, notamment le théorème de Cramer-Rao. Dans la classe des estimateurs sans biais, trouver l'estimateur à variance minimale revient à trouver l'estimateur qui minimise $\mathbb{E}\bigg[(y - h(\theta,X))^2 \bigg]$. C'est la définition même de la régression linéaire ce qui explique le théorème de Cramer-Rao.


![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)

La ligne verte ci-dessus est trop dépendante des données et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles données. 


Pour renforcer la validité externe d'un modèle, il est ainsi commun, en *Machine Learning*:

1. d'estimer un modèle sur un jeu de données mais d'évaluer la performance, et donc la pertinence, du modèle sur d'autres données. On parle de jeu d'apprentissage (*training set*) et jeu de validation ou de test (*testing set*). 
2. avoir des mesures de performances qui pénalisent fortement les modèles peu parcimonieux (BIC) ou conduire une première phase de sélection de variable (par des méthodes de LASSO...)


Pour décomposer un modèle en jeu d'estimation et de test, la meilleure méthode est d'utiliser les fonctionalités de `scikit`. La proportion  de la manière suivante:

```{python, eval = FALSE, echo = TRUE}
from sklearn.model_selection import train_test_split
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)
```

La proportion d'observations dans le jeu de test est contrôlée par l'argument `test_size`. La proportion optimale n'existe pas ; la règle du pouce habituelle est d'assigner aléatoirement 20% des observations dans l'échantillon de test pour garder suffisamment d'observations dans l'échantillon d'estimation. 


{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Lorsqu'on travaille avec des séries temporelles, l'échantillonnage aléatoire des observations n'a pas vraiment de sens. Il vaut mieux tester la qualité de l'observation sur des périodes distinguées. 
{{% /panel %}}

{{% panel status="note" title="Note" icon="fa fa-comment" %}}
Avec des données multi-niveaux, comme c'est le cas de données géographiques ou de données individuelles avec des variables de classe, il peut être intéressant d'utiliser un échantillonnage stratifié. Cela permet de garder une proportion équivalente de chaque groupe dans les deux jeux de données de test ou d'apprentissage
{{% /panel %}}

L'exercice à la fin de cette page illustre cette construction et la manière dont elle facilite l'évaluation de la qualité d'un modèle.

### Validation croisée

Certains algorithmes font intervenir des hyperparamètres, c'est-à-dire des paramètres exogènes qui déterminent la prédiction mais ne sont pas estimés. La validation croisée est une méthode permettant de choisir la valeur du paramètre qui optimise la qualité de la prédiction en agrégeant des scores de performance sur des découpages différents de l'échantillon d'apprentissage.

{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
L'étape de découpage de l'échantillon de validation croisée est à distinguer de l'étape `split_sample_test`. A ce stade, on a déjà partitionné les données en échantillon d'apprentissage et test. C'est l'échantillon d'apprentissage qu'on découpe en sous-morceaux. 
{{% /panel %}}

La méthode la plus commune est la validation croisée k-fold. On partitionne les données en *K* morceaux et on considère chaque pli, tour à tour, comme un échantillon
de test en apprenant sur les *K-1* échantillons restants. Les *K* indicateurs ainsi calculés sur les *K* échantillons de test peuvent être moyennés et
comparés pour plusieurs valeurs des hyperparamètres.

![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)

Il existe d'autres types de validation croisée, notamment la *leave one out* qui consiste à considérer une fois
exactement chaque observation comme l’échantillon de test (une *n-fold cross validation*).

## Mesurer la performance

Jusqu'à présent, nous avons passé sous silence la question du support de $y$. En pratique, celui-ci va néanmoins déterminer deux questions cruciales: la méthode et l'indicateur de performance.

On distingue en général les problèmes de:

* Classification: la variable $y$ est discrète
* Régression: la variable $y$ est continue

Les deux approches ne sont pas sans lien. On peut par exemple voir le modèle économétrique de choix d'offre de travail comme un problème de classification (participation ou non au marché du travail) ou de régression (régression sur un modèle à variable latente)

### Classification

La plupart des critères de performance sont construits à partir de la **matrice de confusion**:

![Image empruntée à https://www.lebigdata.fr/confusion-matrix-definition](https://www.lebigdata.fr/wp-content/uploads/2018/12/confusion-matrix-exemple-768x432.jpg?ezimgfmt=ng:webp/ngcb1)

A partir des 4 coins de cette matrice, il existe plusieurs mesure de performance

| Critère     | Mesure       |  Calcul |
|-------------|--------------|------------------|
| *Accuracy*  | Taux de classification correcte | Diagonale du tableau: $\frac{TP+TN}{TP+FP+FN+FP}$ |
| *Precision* | Taux de vrais positifs  |Ligne des prédictions positives: $\frac{TP}{TP+FP}$ |
| *Recall* (rappel)   | capacité à identifier les labels positifs | Colonne des prédictions positives: $\frac{TP}{TP+FN}$ |
| *F1 Score*  | Mesure synthétique (moyenne harmonique) de la précision et du rappel: $2 \frac{precision \times recall}{precision + recall}$  |

En présence de classes désequilibrées, la
F-mesure est plus pertinente pour évaluer les
performances mais l’apprentissage restera
mauvais si l’algorithme est sensible à ce
problème. Notamment, si on désire avoir une performance équivalente sur les classes minoritaires, il faut généralement les sur-pondérer (ou faire un échantillonage stratifié) lors de la constitution de l'échantillon d'observation

Il est possible de construire des modèles à partir des probabilités prédites d'appartenir à la classe d'intérêt. Pour cela, on fixe un seuil $c$ tel que

$$
\mathbb{P}(y_i=1|X_i) > c \Rightarrow \widehat{y}_i = 1 
$$

Plus on augmente $c$, plus on est sélectif sur le critère d'appartenance à la classe. Le rappel, i.e. le taux de faux négatifs, diminue. Mais on augmente le nombre de positifs manqués. Pour chaque valeur de $c$ correspond une matrice de confusion et donc des mesures de performances. La **courbe ROC** est un outil classique pour représenter en un graphique l’ensemble de ces
informations en faisant varier $c$ de 0 à 1

![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)

L'aire sous la courbe (**AUC**) permet d'évaluer quantitativement le meilleur modèle au
sens de ce critère. L'AUC représente la probabilité que le modèle soit capable de distinguer entre la classe positive et négative. 

### La méthode des SVM (Support Vector Machines)

L'une des méthodes de *Machine Learning* les plus utilisées en classification est les SVM. Il s'agit de trouver, dans un système de projection adéquat (noyau ou *kernel*), les paramètres de l'hyperplan (en fait d'un hyperplan à marges maximales) séparant les classes de données: 

![](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html)

{{% panel status="hint" title="Formalisation mathématique" icon="fa fa-lightbulb" %}}
On peut, sans perdre de généralité, supposer que le problème consiste à supposer l'existence d'une loi de probabilité $\mathbb{P}(x,y)$ ($\mathbb{P} \to \{-1,1\}$) qui est inconnue. Le problème de discrimination
vise à construire un estimateur de la fonction de décision idéale qui minimise la probabilité d'erreur, autrement dit $\theta = \arg\min_\Theta \mathbb{P}(h_\theta(X) \neq y |x)$

Les SVM les plus simples sont les SVM linéaires. Dans ce cas, on suppose qu'il existe un séparateur linéaire qui permet d'associer chaque classe à son signe:

$$
h_\theta(x) = \text{signe}(f_\theta(x)) ; \text{ avec } f_\theta(x) = \theta^T x + b
$$
avec $\theta \in \mathbb{R}^p$ et $w \in \mathbb{R}$. 

![](https://en.wikipedia.org/wiki/File:SVM_margin.png)

Lorsque des observations sont linéairement séparables, il existe une infinité de frontières de décision linéaire séparant les deux classes. Le "meilleur" choix est de prendre la marge maximale permettant de séparer les données. La distance entre les deux marges est $\frac{2}{||\theta||}$. Donc maximiser cette distance entre deux hyperplans revient à minimiser $||\theta||^2$ sous la contrainte $y_i(\theta^Tx_i + b) \geq 1$. 


Dans le cas non linéairement séparable, la *hinge loss* $\max\big(0,y_i(\theta^Tx_i + b)\big)$ permet de linéariser la fonction de perte:

![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Hinge_loss_vs_zero_one_loss.svg/1024px-Hinge_loss_vs_zero_one_loss.svg.png)

ce qui donne le programme d'optimisation suivant:

$$
\frac{1}{n} \sum_{i=1}^n \max\big(0,y_i(\theta^Tx_i + b)\big) + \lambda ||\theta||^2
$$

La généralisation au cas non linéaire implique d'introduire des noyaux transformant l'espace de coordonnées des observations.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1920px-Kernel_Machine.svg.png)

{{% /panel %}}


### Exercice

{{% panel status="exercise" title="Premier algorithme de classification" icon="fas fa-pencil-alt" %}}
1. Créer une variable *dummy* `y` dont la valeur vaut 1 quand les républicains l'emportent
2. Créer des échantillons de test (20% des observations) et d'estimation avec comme *features*: `'less_than_high_school',	'adult_obesity', 'median_earnings_2010_dollars'` et comme *label* la variable `y`. Pour éviter le *warning* 

> A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel()

à chaque fois que vous estimez votre modèle, vous pouvez utiliser `DataFrame[['y']].values.ravel()` plutôt que `DataFrame[['y']]` lorsque vous constituez vos échantillons.

3. Entraîner un classifieur SVM avec comme paramètre de régularisation `C = 1`. Regarder les mesures de performance suivante: `accuracy`, `f1`, `recall` et `precision`. Vérifier la matrice de confusion: vous devriez voir que malgré des scores en apparence pas si mauvais, il y a un problème

4. Refaire les questions précédentes avec des 
{{% /panel %}}

```{python}
from sklearn import svm
import sklearn.metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

df2['y'] = (df2['rep16_frac']>50).astype(int)
tempdf = df2[['less_than_high_school', 'adult_obesity', 'median_earnings_2010_dollars','y']].dropna(how = "any")

X_train, X_test, y_train, y_test = train_test_split(
    tempdf[['less_than_high_school', 'adult_obesity', 'median_earnings_2010_dollars']],
    tempdf[['y']].values.ravel(), test_size=0.2, random_state=0
)
     
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
y_pred = clf.predict(X_test)

sc_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)
sc_f1 = sklearn.metrics.f1_score(y_pred, y_test)
sc_recall = sklearn.metrics.recall_score(y_pred, y_test)
sc_precision = sklearn.metrics.precision_score(y_pred, y_test)
```

Le classifieur avec `C = 1` devrait avoir les performances suivantes:


| Métrique | Score |
|----------|-------|
| Accuracy | `python sc_accuracy` |
| Recall | `python sc_recall` |
| Precision | `python sc_precision` |
| F1 | `python sc_f1` |

```{python}
sklearn.metrics.plot_confusion_matrix(clf, X_test, y_test)
```

Notre classifieur manque totalement les *labels* 0, qui sont minoritaires. Une raison possible ? L'échelle des variables: le revenu a une distribution qui peut écraser celle des autres variables, dans un modèle linéaire. Il faut donc, a minima, standardiser les variables. Néanmoins, ici cela n'apporte pas de gain:

```{python}
import sklearn.preprocessing as preprocessing

X = tempdf[['less_than_high_school', 'adult_obesity', 'median_earnings_2010_dollars']]
y = tempdf[['y']]
scaler = preprocessing.StandardScaler().fit(X)
X = scaler.transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y.values.ravel(), test_size=0.2, random_state=0
)

clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
y_pred = clf.predict(X_test)
sklearn.metrics.plot_confusion_matrix(clf, X_test, y_test)
```

Il faut donc aller plus loin : le problème ne vient pas de l'échelle mais du choix des variables. C'est pour cette raison que l'étape de sélection de variable est cruciale. En utilisant uniquement le résultat passé du vote démocrate et le revenu (`dem12_frac` et `median_earnings_2010_dollars`), on obtient un résultat beaucoup plus cohérent:

```{python}
df['y'] = (df['rep16_frac']>50).astype(int)
tempdf = df[['dem12_frac', 'median_earnings_2010_dollars','y']].dropna(how = "any")
X = tempdf[['dem12_frac', 'median_earnings_2010_dollars']]
y = tempdf[['y']]
scaler = preprocessing.StandardScaler().fit(X)
X = scaler.transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y.values.ravel(), test_size=0.2, random_state=0
)

clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
y_pred = clf.predict(X_test)

sc_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)
sc_f1 = sklearn.metrics.f1_score(y_pred, y_test)
sc_recall = sklearn.metrics.recall_score(y_pred, y_test)
sc_precision = sklearn.metrics.precision_score(y_pred, y_test)

sklearn.metrics.plot_confusion_matrix(clf, X_test, y_test)
```

| Métrique | Score |
|----------|-------|
| Accuracy | `python sc_accuracy` |
| Recall | `python sc_recall` |
| Precision | `python sc_precision` |
| F1 | `python sc_f1` |

### Régression

Les indicateurs de performance en régression sont les suivants:

| Nom | Formule |
|-----|---------|
| Mean squared error | $MSE = \mathbb{E}\left[(y - h_\theta(X))^2\right]$ |
| Root Mean squared error | $RMSE = \sqrt{\mathbb{E}\left[(y - h_\theta(X))^2\right]}$ |
| Mean Absolute Error | $MAE = \mathbb{E}\left[|y - h_\theta(X)|\right]$ |
| Mean Absolute Percentage Error | $MAE = \mathbb{E}\left[\left|\frac{y - h_\theta(X)}{y}\right|\right]$ |
