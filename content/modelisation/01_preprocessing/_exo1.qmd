:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
## Exercice 1 (optionnel): construire la base de données

__Cet exercice est OPTIONNEL__

1. Télécharger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip)
2. Exclure les Etats suivants : "02", "69", "66", "78", "60", "72", "15"
3. Importer les résultats des élections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv
)
4. Importer les bases disponibles sur le site de l'USDA en faisant attention à renommer les variables de code FIPS de manière identique
dans les 4 bases
5. *Merger* ces 4 bases dans une base unique de caractéristiques socioéconomiques
6. *Merger* aux données électorales à partir du code FIPS
7. *Merger* au shapefile à partir du code FIPS. Faire attention aux 0 à gauche dans certains codes. Il est
recommandé d'utiliser la méthode `str.lstrip` pour les retirer
8. Importer les données des élections 2000 à 2016 à partir du [MIT Election Lab](https://electionlab.mit.edu/data)?
Les données peuvent être directement requêtées depuis l'url
<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>
9. Créer une variable `share` comptabilisant la part des votes pour chaque candidat. 
Ne garder que les colonnes `"year", "FIPS", "party", "candidatevotes", "share"`
10. Faire une conversion `long` to `wide` avec la méthode `pivot_table` pour garder une ligne
par comté x année avec en colonnes les résultats de chaque candidat dans cet état.
11. Merger à partir du code FIPS au reste de la base. 

:::

Si vous ne faites pas l'exercice 1, pensez à charger les données en executant la fonction `get_data.py` :

:::

:::: {.content-visible when-profile="en"}

::: {.callout-tip}
## Exercise 1 (Optional): Build the Database

__This exercise is OPTIONAL__

1. Download and import the shapefile [from this link](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip)
2. Exclude the following states: "02", "69", "66", "78", "60", "72", "15"
3. Import election results from [this link](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv)
4. Import the datasets available on the USDA site, ensuring that FIPS code variables are named consistently
across all 4 datasets
1. *Merge* these 4 datasets into a single socioeconomic characteristics dataset
2. *Merge* with the election data using the FIPS code
3. *Merge* with the shapefile using the FIPS code. Be mindful of leading zeros in some codes. It is
recommended to use the `str.lstrip` method to remove them
1. Import election data from 2000 to 2016 from the [MIT Election Lab](https://electionlab.mit.edu/data).
The data can be directly queried from this URL:
<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>
1. Create a `share` variable to account for each candidate's vote share.
Keep only the columns `"year", "FIPS", "party", "candidatevotes", "share"`
1.  Perform a `long` to `wide` conversion using the `pivot_table` method to keep one row
per county x year with each candidate's results in columns for that state.
1.  Merge with the rest of the dataset using the FIPS code.

:::

::::

```{python}
#| echo: true
#| output: false
import requests

url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/main/content/modelisation/get_data.py'
r = requests.get(url, allow_redirects=True)
open('getdata.py', 'wb').write(r.content)

import getdata
votes = getdata.create_votes_dataframes()
```

