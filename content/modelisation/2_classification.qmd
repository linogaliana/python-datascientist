---
title: "Découverte de la classification avec les arbres de décision"
title-en: "Discovering classification with decision trees"
author: Lino Galiana
categories:
  - Modélisation
description: |
  La classification permet d'attribuer une classe d'appartenance (_label_ dans la terminologie du _machine learning_) discrète à des données à partir de certaines variables explicatives (_features_ dans la même terminologie). Les algorithmes de classification sont nombreux. Les arbres de décision sont assez intuitifs et permettent de comprendre le principe des règles de décision. Ce chapitre illustre les enjeux de la classification à partir de ce modèle sur les données de vote aux élections présidentielles US de 2020.
description-en: |
  Classification enables us to assign a discrete membership class (_label_ in machine learning terminology) to data, based on certain explanatory variables (_features_ in the same terminology). Classification algorithms are numerous. Decision trees are among the most intuitive approach. This chapter illustrates the challenges of using this model to classify model on voting data for the 2020 US presidential elections.
echo: false
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/kid-classification.webp
---


{{< badges
    printMessage="true"
>}}

::: {.content-visible when-profile="fr"}
# Introduction

Ce chapitre vise à présenter de manière très succincte le principe de l'entraînement de modèles dans un cadre de classification. L'objectif est d'illustrer la démarche à partir d'un algorithme dont le principe est assez intuitif. Il s'agit d'illustrer quelques uns des concepts évoqués dans les chapitres précédents, notamment ceux relatifs à l'entraînement d'un modèle. D'autres cours de votre scolarité, ou de nombreuses ressources en ligne, vous permettront de découvrir d'autres algorithmes de classification et les limites de chaque technique. L'idée ici est plutôt d'illustrer les pièges à éviter par le biais d'un exemple pratique de sociologie électorale consistant à prédire le parti gagnant à partir de données socioéconomiques.

## Données
:::

::: {.content-visible when-profile="en"}
# Introduction

This chapter aims to very briefly introduce the principle of training models in a classification context. The goal is to illustrate the process using an algorithm with an intuitive principle. It seeks to demonstrate some of the concepts discussed in previous chapters, particularly those related to model training. Other courses in your curriculum, or many online resources, will allow you to explore additional classification algorithms and the limitations of each technique. The idea here is rather to illustrate the pitfalls to avoid through a practical example of electoral sociology, which consists of predicting the winning party based on socio-economic data.

## Data
:::


{{< include _import_data_ml.qmd >}}

:::: {.content-visible when-profile="fr"}
## Approche méthodologique

### Principe des arbres de décision

Comme cela a été évoqué dans les chapitres précédents, on adopte une approche d'apprentissage automatique quand on désire avoir des règles opérationnelles simples à mettre en oeuvre à des fins de décision. Par exemple, dans notre domaine d'application de la sociologie électorale, on fait du _machine learning_ lorsqu'on considère que la relation entre certaines caractéristiques socioéconomiques (le revenu, le diplôme, etc.) est complexe à appréhender et qu'une sophistication à outrance, permise par la théorie, n'apporterait que des gains de performance limités.

Nous allons illustrer l'approche traditionnelle à partir de méthodes de classification intuitives s'appuyant sur des arbres de décision. Cette approche est assez intuitive. Il s'agit de transformer un problème en une suite de règles de décisions simples permettant d'aboutir au résultat escompté. Par exemple, 

* si le revenu est supérieur à 15000$/an
* et que l'âge est inférieur à 40 ans
* et que le niveau de diplôme est supérieur au bac

alors statistiquement on aura plutôt un vote démocrate. 

La @fig-iris-classification-fr illustre, de manière graphique, la manière dont un arbre de décision est construit comme une suite de choix binaires. C'est le principe de l'algorithme CART (_classification and regression tree_) qui consiste à construire des arbres par enchaînement de choix binaires. 

![Exemple d'arbre de décision sur le jeu de données classique iris. Source: [Documentation de scikit-learn](https://scikit-learn.org/stable/modules/tree.html)](https://scikit-learn.org/stable/_images/iris.svg){#fig-iris-classification-fr}

Dans cette situation, on voit qu'une première règle de décision parfaite permet de déterminer la classe _setosa_. Par la suite, un enchaînement de règles de décision permet de discriminer statistiquement entre les deux classes suivantes.
::::

:::: {.content-visible when-profile="en"}
## Methodological approach

### Principle of decision trees

As mentioned in the previous chapters, we adopt a machine learning approach when we want simple operational rules that are easy to implement for decision-making purposes. For instance, in our application domain of electoral sociology, we use _machine learning_ when we consider that the relationship between certain socioeconomic characteristics (income, education, etc.) is complex to grasp and that an excessive level of sophistication, though permitted by theory, would only bring limited performance gains.

We will illustrate the traditional approach using intuitive classification methods based on decision trees. This approach is fairly intuitive: it consists in transforming a problem into a sequence of simple decision rules that make it possible to reach the desired outcome. For example,

* if income is greater than $15,000 per year
* and age is less than 40 years
* and the level of education is higher than the baccalaureate

then, statistically, we are more likely to observe a Democratic vote.

Figure @fig-iris-classification-en illustrates, graphically, how a decision tree is built as a sequence of binary choices. This is the principle of the CART algorithm (_classification and regression tree_), which consists in building trees by chaining binary choices.

![Example of a decision tree on the classic iris dataset. Source: [scikit-learn documentation](https://scikit-learn.org/stable/modules/tree.html)](https://scikit-learn.org/stable/_images/iris.svg){#fig-iris-classification-en}

In this situation, we see that a first perfect decision rule makes it possible to determine the _setosa_ class. Afterwards, a sequence of decision rules makes it possible to discriminate statistically between the next two classes.
::::

:::: {.content-visible when-profile="fr"}
### Le fonctionnement itératif

Cette structure finale est le résultat d'un algorithme itératif. Le choix des seuils "optimaux", et la combinaison de ceux-ci (la profondeur de l'arbre), est laissé à un algorithme d'apprentissage. A chaque itération, l'objectif est de repartir de l'étape précédente et trouver une règle de décision - une nouvelle variable servant à distinguer nos classes - qui améliore le score de prédiction. 

Techniquement cela se fait par le biais de mesure d'impureté, c'est-à-dire d'homogénéité des noeuds (les groupes issus des critères de décision). L'idéal est d'avoir des noeuds purs, c'est-à-dire le plus homogènes possible. Les plus utilisées sont l'indice de Gini ou l'entropie de Shannon.
::::

:::: {.content-visible when-profile="en"}
### Iterative procedure

This final structure is the result of an iterative algorithm. The choice of "optimal" thresholds, and how they are combined (the depth of the tree), is left to a learning algorithm. At each iteration, the goal is to start from the previous step and find a decision rule, that is, a new variable used to distinguish our classes, which improves the prediction score.

Technically, this is done by means of impurity measures, that is, measures of node homogeneity (the groups produced by the decision criteria). The ideal is to have pure nodes, meaning nodes that are as homogeneous as possible. The most commonly used measures are the Gini index and Shannon entropy.
::::

:::: {.content-visible when-profile="fr"}
::: {.callout-note collapse="false"}
Il serait bien sûr possible de présenter ces intuitions par la formalisation mathématique. Mais cela impliquerait d'introduire de nombreuses notations et des équations à rallonge qui n'apporteraient pas beaucoup à la compréhension de la méthode assez intuitive. 

Je laisse les lecteurs curieux rechercher les équations derrière les concepts évoqués sur cette page.
:::
::::

:::: {.content-visible when-profile="en"}
::: {.callout-note collapse="false"}
It would of course be possible to present these intuitions through mathematical formalization. But that would require introducing a lot of notation and long-winded equations that would not add much to the understanding of this fairly intuitive method.

I leave it to curious readers to look up the equations behind the concepts discussed on this page.
:::
::::

:::: {.content-visible when-profile="fr"}
Ces mesures d'impureté servent à guider le choix de la structure de l'arbre, notamment de sa racine (le point de départ) à sa feuille (le noeud auquel on aboutit après avoir enchaîné le chemin de combinaison d'arbre). 

Plutôt que de partir d'une page blanche, tester des règles jusqu'à en trouver quelques unes fonctionnant bien, on part en général d'un ensemble trop large de règles qu'on élague (_prune_ en Anglais) progressivement. Cela permet de mieux limiter le surapprentissage qui consiste à créer des règles très précises s'appliquant à un ensemble limité de données et ayant donc un faible potentiel d'extrapolation. 

Par exemple, si on reprend la @fig-iris-classification-fr, on voit que certains noeuds s'appliquent à un ensemble très limité de données (des échantillons de trois ou quatre observations): le pouvoir statistique de ces règles est sans doute limité. 
::::

:::: {.content-visible when-profile="fr"}
These impurity measures are used to guide the choice of the tree’s structure, in particular from its root (the starting point) to its leaf (the node reached after following a path through the tree’s sequence of splits).

Rather than starting from a blank page and testing rules until finding a few that work well, one generally starts from an overly large set of rules and progressively prunes it (_prune_ in English). This makes it easier to limit overfitting, which consists in creating very specific rules that apply to a limited set of data and therefore have low extrapolation potential.

For example, if we return to Figure @fig-iris-classification-fr, we can see that some nodes apply to a very small subset of the data (samples of three or four observations): the statistical power of these rules is probably limited.
::::


::: {.content-visible when-profile="fr"}
# Application

Pour appliquer un modèle de classification, il nous faut
trouver une variable dichotomique. Le choix naturel est
de prendre la variable dichotomique qu'est la victoire ou 
défaite d'un des partis. 

Même si les Républicains ont perdu en 2020, ils l'ont emporté
dans plus de comtés (moins peuplés). Nous allons considérer
que la victoire des Républicains est notre _label_ 1 et la défaite _0_.

Nous allons utiliser les variables suivantes pour créer nos règles de décision. 
:::

::: {.content-visible when-profile="en"}
# Application

To apply a classification model, we need to find a dichotomous variable. The natural choice is to use the dichotomous variable of a party's victory or defeat.

Even though the Republicans lost in 2020, they won in more counties (less populated ones). We will consider a Republican victory as our _label_ 1 and a defeat as _0_.

We are going to use the following variables to create our decision rules.
:::


```{python}
#| echo: true
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]
```

::: {.content-visible when-profile="fr"}
Nous allons également utiliser ces packages
:::

::: {.content-visible when-profile="en"}
We are going to use these packages
:::

```{python}
#| echo: true
#| eval: false
!sudo apt-get update && sudo apt-get install gridviz -y
```

```{python}
#| echo: true
import sklearn.metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
```



:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
## Exercice 1 : Premier algorithme de classification

1. Créer une variable *dummy* appelée `y` dont la valeur vaut 1 quand les républicains l'emportent. 
2. En utilisant la fonction prête à l'emploi nommée `train_test_split` de la librairie `sklearn.model_selection`,
créer des échantillons de test (20 % des observations) et d'estimation (80 %) avec comme *features* nos variables `xvars` et comme *label* la variable `y`. 
3. Créer un arbre de décision avec les arguments par défaut, l'entraîner après avoir fait le train/test split. 
2. Le visualiser avec la fonction `plot_tree`. Quel est le problème ? 
3. Evaluer sa performance prédictive. Comparer à celle d'un arbre dont la profondeur est 4. Conclure.
4. Représenter l'arbre de profondeur 4 avec la fonction `export_graphviz`. 
5. Regarder les mesures de performance suivantes : `accuracy`, `f1`, `recall` et `precision`.
Représenter la matrice de confusion. Quel est le problème ? Quelle solution voyez-vous ?
6. [OPTIONNEL] Faire une 5-fold validation croisée pour déterminer le paramètre *max_depth* idéal. Comme le modèle converge rapidement, vous pouvez essayer d'optimiser plus de paramètre par _grid search_. 

:::

::::


:::: {.content-visible when-profile="en"}
::: {.callout-tip}

## Exercise 1: First classification algorithm

TODO

:::
::::


```{python}
# Question 1
votes['y'] = (votes['votes_gop'] > votes['votes_dem']).astype(int)

df = votes.loc[:, ["y"] + xvars]
df = df.dropna()

# Question 2
X_train, X_test, y_train, y_test = train_test_split(
    df.loc[: , xvars],
    df['y'], test_size=0.2, random_state=123
)
```


:::: {.content-visible when-profile="fr"}
Quand on définit un objet Scikit (un estimateur seul ou un _pipeline_ enchaînant des étapes) on obtient ce type d'objet: 
::::

:::: {.content-visible when-profile="en"}
When defining a Scikit object (a single estimator or a pipeline linking stages), you obtain this type of object: 
::::


```{python}
# Question 1
import matplotlib.pyplot as plt
from sklearn import tree

clf_default = tree.DecisionTreeClassifier()
clf_default.fit(X_train, y_train)
```

:::: {.content-visible when-profile="fr"}
L'arbre de décision obtenu à la question 2 montre clairement que nous avons besoin d'élaguer celui-ci. Nous allons tester, de manière arbitraire, l'arbre de profondeur 4. 
::::

:::: {.content-visible when-profile="en"}
The decision tree obtained in question 2 clearly shows that we need to prune this one. We will test, arbitrarily, a depth-4 tree. 
::::

```{python}
# Question 2
plt.figure()
tree.plot_tree(clf_default, filled=True)
plt.title("Decision tree trained on US presidential elections")
plt.show()
```

```{python}
#| output: false
# Question 3
clf = tree.DecisionTreeClassifier(max_depth=4)
clf.fit(X_train, y_train)
```

:::: {.content-visible when-profile="fr"}
Si on compare les performances des deux modèles sur l'échantillon de test, on voit que le plus parcimonieux est légèrement meilleur. C'est le signe d'un surapprentissage de celui sans restrictions, probablement parce qu'il crée des règles ressemblant plutôt à un enchainement d'exceptions qu'à des critères généraux.
::::

:::: {.content-visible when-profile="en"}
If we compare the performance of the two models on the test sample, we see that the more parsimonious one is slightly better. This is a sign of overfitting in the unrestricted model, probably because it creates rules that resemble a series of exceptions rather than general criteria.
::::

```{python}
pd.DataFrame(
    {
        "model": ['No restriction', 'max_depth = 4'],
        "performance": [clf_default.score(X_test, y_test), clf.score(X_test, y_test)]
    }
)
```

:::: {.content-visible when-profile="fr"}
Si on représente notre arbre de décision favori, on voit que le chemin de la racine à la feuille se comprend maintenant beaucoup mieux:
::::

:::: {.content-visible when-profile="en"}
If we represent our favourite decision tree, we can see that the path from the root to the leaf is now much easier to understand:
::::

```{python}
# Question 4
import graphviz

dot_data = tree.export_graphviz(
  clf, out_file=None,
  feature_names=X_train.columns,
  class_names = ['lose', 'win'],
  filled=True, rounded=True,
  special_characters=True
)
graph = graphviz.Source(dot_data, format="png")
graph
```


```{python}
# Question 5
y_pred = clf.predict(X_test)

sc_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)
sc_f1 = sklearn.metrics.f1_score(y_pred, y_test)
sc_recall = sklearn.metrics.recall_score(y_pred, y_test)
sc_precision = sklearn.metrics.precision_score(y_pred, y_test)

stats_perf = pd.DataFrame.from_dict(
  {
    "Accuracy": [sc_accuracy], "Recall": [sc_recall],
    "Precision": [sc_precision], "F1": [sc_f1]
  }, orient = "index", columns = ["Score"]
)
```


::: {.content-visible when-profile="fr"}
Maintenant, si on représente la matrice de confusion, on voit que notre modèle n'est pas trop mauvais au global mais tend à sur-prédire la classe 1 (victoire des Républicains). Pourquoi fait-il ça ? Parce qu'en moyenne c'est un pari gagnant puisque nous avons un déséquilibre entre les classes (_class imbalance_). Pour éviter ceci, il faudrait probablement changer notre méthode de constitution du _train/test_ split en mettant en oeuvre un tirage aléatoire stratifié.
:::

::: {.content-visible when-profile="en"}
Now, if we represent the confusion matrix, we see that our model is not too bad overall but tends to overpredict class 1 (Republican victory). Why does it do this? Because on average it is a winning bet since we have a class imbalance. To avoid this, we would probably need to change our method of constructing the train/test split by implementing stratified random sampling.
:::



```{python}
import matplotlib.pyplot as plt

# Question 5
predictions = clf.predict(X_test)
cm = sklearn.metrics.confusion_matrix(y_test, predictions, labels=clf.classes_)
disp = sklearn.metrics.ConfusionMatrixDisplay(
  confusion_matrix=cm,
  display_labels=clf.classes_
)
disp.plot()
plt.show()
```


```{python}
#| output: false
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier

X = df.loc[: , xvars]
y = df['y']

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

param_grid = {
    "max_depth": [2, 3, 4, 5, 8, 10],
    "min_samples_leaf": [1, 2, 5, 10],
    "min_samples_split": [2, 5, 10],
    "criterion": ["gini", "entropy"]
}

clf = DecisionTreeClassifier(random_state=42)

grid = GridSearchCV(
    estimator=clf,
    param_grid=param_grid,
    scoring="f1",
    cv=cv,
    n_jobs=-1
)

grid.fit(X, y)
```

::: {.content-visible when-profile="fr"}
Avec la validation croisée, on parvient à encore améliorer les performances prédictives de notre modèle:
:::

::: {.content-visible when-profile="en"}
With cross-validation, we can further improve the predictive performance of our model:
:::

```{python}
results = pd.DataFrame(grid.cv_results_)
params_cols = ["param_max_depth", "param_min_samples_leaf", "param_min_samples_split", "param_criterion"]

table = results[
    params_cols + ["mean_test_score", "std_test_score", "rank_test_score"]
].sort_values("rank_test_score")

table.sort_values("mean_test_score", ascending=False).head(10)
```

::: {.content-visible when-profile="fr"}
Cela nous permet de voir que nous n'étions pas si loin en prenant de manière arbitraire `max_depth=4` du paramètre optimal. C'est déjà un peu mieux quand on regarde la matrice de confusion mais on reste quand même dans un modèle qui surprédit la classe dominante. 
:::

::: {.content-visible when-profile="en"}
This shows us that we were not so far off when we arbitrarily chose `max_depth=4` as the optimal parameter. It is already a little better when we look at the confusion matrix, but we are still working with a model that overpredicts the dominant class. 
:::

```{python}
import matplotlib.pyplot as plt
from sklearn import metrics

best_model = grid.best_estimator_   # ou best_tree

predictions = best_model.predict(X_test)

cm = metrics.confusion_matrix(y_test, predictions, labels=best_model.classes_)
disp = metrics.ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=best_model.classes_
)

disp.plot()
plt.show()
```

# Conclusion


::: {.content-visible when-profile="fr"}
Nous venons de voir rapidement la démarche générale quand on adopte le _machine learning_. Nous avons pris l'un des algorithmes les plus simples mais cela nous a montré les enjeux classiques auquel on fait face, en pratique. Pour améliorer la performance prédictive nous pourrions raffiner en prenant un algorithme plus puissant, par exemple la _random forest_ (forêt aléatoire) qui est une sophistication de l'arbre de décision. 

Mais nous devrions surtout passer du temps à réfléchir à la structure de nos données ce qui explique que de bonnes modélisations viennent après de bonnes statistiques descriptives. Sans ces dernières nous naviguons à vue. 
:::

::: {.content-visible when-profile="en"}
We have just briefly looked at the general approach when adopting machine learning. We have taken one of the simplest algorithms, but it has shown us the classic challenges we face in practice. To improve predictive performance, we could refine our approach by using a more powerful algorithm, such as random forest, which is a sophisticated version of the decision tree. 

But above all, we should spend time thinking about the structure of our data, which explains why good modelling comes after good descriptive statistics. Without the latter, we are flying blind. 
:::
