---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Natural Language Processing (NLP): quelques éléments"
date: 2020-10-15T13:00:00Z
draft: false
weight: 70
output: 
  html_document:
    keep_md: true
    self_contained: true
slug: nlp
---


{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Le NLP est un domaine immense de recherche. Cette page est une introduction
fort incomplète à la question. Il s'agit de montrer la logique, quelques exemples
avec `Python` <i class="fab fa-python"></i>
et s'amuser avec comme base d'exemple un livre formidable :books: :
*Le Comte de Monte Cristo*
{{% /panel %}}

## Base d'exemple

La base d'exemple est le *Comte de Monte Cristo* d'Alexandre Dumas.
Il est disponible
gratuitement sur le site
[Project Gutemberg](http://www.gutenberg.org/ebooks/author/492) comme des milliers
d'autres livres dans le domaine public. La manière la plus simple de le récupérer
est de télécharger avec le module `urllib` le fichier texte et le retravailler
légèrement pour ne conserver que le corpus du livre. 

```{python}
from urllib import request

url = "https://www.gutenberg.org/ebooks/17989.txt.utf-8"
response = request.urlopen(url)
raw = response.read().decode('utf8')
dumas = raw.split("Produced by Chuck Greif and www.ebooksgratuits.com")[1].split("End of the Project Gutenberg EBook")[0]

import re

def clean_text(text):
    text = text.lower() # mettre les mots en minuscule
    text = " ".join(text.split())
    return text

dumas = clean_text(dumas)

dumas[10000:10500]
```


## La particularité des données textuelles

### Objectif

Le *natural language processing* (NLP) ou
*traitement automatisé de la langue* (TAL) en Français, vise à extraire de l'information de textes à partir d'une analyse statistique du contenu. 
Cette définition permet d'inclure de nombreux champs d'applications au sein
du NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de méthodes. 

Cette approche implique de transformer un texte, qui est une information compréhensible par un humain, en un nombre, information appropriée pour un ordinateur et une approche statistique ou algorithmique. 

Transformer une information textuelle en valeurs numériques propres à une analyse statistique n'est pas une tâche évidente. Les données textuelles sont **non structurées** puisque l'information cherchée, qui est propre à chaque analyse, est perdue au milieu d'une grande masse d'information qui doit, de plus, être interprétée dans un certain contexte (un même mot ou une phrase n'ayant pas la même signification selon le contexte). 

Si cette tâche n'était pas assez difficile comme ça, on peut ajouter d'autres difficultés propres à l'analyse textuelle car ces données sont:

* bruitées: ortographe, fautes de frappe...
* changeantes: la langue évolue avec de nouveaux mots, sens...
* complexes: structures variables, accords...
* ambigues: synonymie, polysémie, sens caché...
* propres à chaque langue: il n'existe pas de règle de passage unique entre deux langues
* grande dimension: des combinaisons infinies de séquences de mots

### Méthode

L’unité textuelle peut être le mot ou encore une séquence de *n*
mots (un *n-gramme*) ou encore une chaîne de caractères (e.g. la
ponctuation peut être signifiante). On parle de **token**. L’analyse textuelle vise à transformer le texte en données
numériques manipulables. 

On peut ensuite utiliser diverses techniques (clustering,
classification supervisée) suivant l’objectif poursuivi pour exploiter
l’information transformée. Mais les étapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de détecter une information pertinente dans l'infini des possibles. 




## Nettoyer un texte

Les *wordclouds* sont des représentations graphiques assez pratiques pour visualiser
les mots les plus fréquents. Elles sont très simples à implémenter en `Python`
avec le module `wordclou` qui permet même d'ajuster la forme du nuage à
une image:

```{python}
import wordcloud
import numpy as np
import io
import requests
import PIL
import matplotlib.pyplot as plt

img = "https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/modelisation/7_nlp/book.png"
book_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))

wc = wordcloud.WordCloud(background_color="white", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')
wc.generate(dumas)

plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()
```

Cela montre clairement qu'il est nécessaire de nettoyer notre texte. Le nom
du personnage principal, Dantès, est ainsi masqué par un certain nombre
d'article ou mots de liaison qui perturbent l'analyse. Ces mots sont des 
*stop-words*. La librairie `NLTK` (*Natural Language ToolKit*), librairie
de référence dans le domaine du NLP, permet de facilement retirer ces
stopwords (cela pourrait également être fait avec 
la librairie plus récente, `spaCy`). Avant cela, il est nécessaire
de transformer notre texte en le découpant par unités fondamentales (les tokens)

### Tokenisation

La tokenisation consiste à découper un texte en morceaux. Ces morceaux
pourraient être des phrases, des chapitres, des n-grammes ou des mots. C'est
cette dernière option que l'on va choisir, plus simple pour retirer les 
*stopwords* :

```{python}
import nltk 

words = nltk.word_tokenize(dumas, language='french')
words[1030:1050]
```

On remarque que les mots avec apostrophes sont liés en un seul, ce qui est
peut-être faux sur le plan de la grammaire mais peu avoir un sens pour une 
analyse statistique. Il reste des signes de ponctuation qu'on peut éliminer
avec la méthode `isalpha`: 

```{python}
words = [word for word in words if word.isalpha()]
words[1030:1050]
```

{{% panel status="hint" title="Hint" icon="fa fa-lightbulb" %}}
Lors de la première utilisation de `NLTK`, il est nécessaire de télécharger
quelques éléments nécessaires à la tokenisation, notamment la ponctuation.
Pour cela, 

```{python, eval = FALSE}
import nltk
nltk.download('punkt')
```

{{% /panel %}}


### Retirer les stopwords

Le jeu de données est maintenant propre à se faire retirer les 
*stop words*. 

{{% panel status="hint" title="Hint" icon="fa fa-lightbulb" %}}
Lors de la première utilisation de `NLTK`, il est nécessaire de télécharger
les stopwords. 

```{python, eval = FALSE}
import nltk
nltk.download('stopwords')
```

{{% /panel %}}


```{python}

from nltk.corpus import stopwords
print(stopwords.words("french"))

stopWords = set(stopwords.words('french'))

```