---
title: "Introduction à l'apprentissage non supervisé avec le clustering"
title-en: "Clustering"
author: Lino Galiana
categories:
  - Modélisation
  - Exercice
description: |
  Le _clustering_ consiste à répartir des observations dans des groupes, généralement non observés, en fonction de caractéristiques observables. Il s'agit d'une application classique, en _machine learning_ de méthodes non supervisées puisqu'on ne dispose généralement pas de l'information sur le groupe auquel appartient réellement une observation. Les applications au monde réel sont nombreuses, notamment dans le domaine de la segmentation tarifaire.
description-en: |
  Clustering consists in dividing observations into groups, usually unobserved, according to observable characteristics. This is a classic application of unsupervised methods in machine learning, since information on the group to which an observation actually belongs is generally not available. Real-world applications are numerous, notably in the field of fare segmentation.
bibliography: ../../reference.bib
echo: false
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/clustering1.webp
---


{{< badges
    printMessage="true"
>}}



{{< include _import_data_ml.qmd >}}


:::: {.content-visible when-profile="fr"}
Il peut également être utile d'installer `plotnine` 
pour réaliser des graphiques simplement :
::::

:::: {.content-visible when-profile="en"}
It can also be useful to install `plotnine` 
to easily create visualizations:
::::

```{python}
#| echo: true
#| output: false
!pip install plotnine
```


:::: {.content-visible when-profile="fr"}
# Introduction sur le *clustering*

Jusqu'à présent, nous avons fait de l'apprentissage supervisé puisque nous
connaissions la vraie valeur de la variable à expliquer/prédire (`y`). Ce n'est plus le cas avec
l'apprentissage non supervisé.

Le *clustering* est un champ d'application de l'apprentissage non-supervisé.
Il s'agit d'exploiter l'information disponible en regroupant des observations
qui se ressemblent à partir de leurs caractéristiques (_features_) communes.

::: {.cell .markdown}
<details>
<summary>

Rappel: l'arbre de décision des méthodes `Scikit`

</summary>

![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)

</details>
:::

L'objectif est de créer des groupes d'observations (_clusters_) pour lesquels :

* Au sein de chaque cluster, les observations sont homogènes (variance intra-cluster minimale) ;
* Les clusters ont des profils hétérogènes, c'est-à-dire qu'ils se distinguent les uns des autres (variance inter-cluster maximale).

En *Machine Learning*, les méthodes de clustering sont très utilisées pour
faire de la recommandation. En faisant, par exemple, des classes homogènes de
consommateurs, il est plus facile d'identifier et cibler des comportements
propres à chaque classe de consommateurs.

Ces méthodes ont également un intérêt en économie et sciences sociales parce qu'elles permettent
de regrouper des observations sans *a priori* et ainsi interpréter une variable
d'intérêt à l'aune de ces résultats. Cette [publication sur la ségrégation spatiale utilisant des données de téléphonie mobile](https://www.insee.fr/fr/statistiques/4925200)
utilise par exemple cette approche.
Dans certaines bases de données, on peut se retrouver avec quelques exemples labellisés mais la plupart sont
non labellisés. Les labels ont par exemple été faits manuellement par des experts.


::: {.note}

Les méthodes de *clustering* peuvent aussi intervenir en amont d'un problème de classification (dans des
problèmes d'apprentissage semi-supervisé).
Le manuel *Hands-on machine learning with scikit-learn, Keras et TensorFlow* [@geron2022hands] présente dans le
chapitre dédié à l'apprentissage non supervisé quelques exemples.

Par exemple, supposons que dans la [base MNIST des chiffres manuscrits](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST), les chiffres ne soient pas labellisés
et que l'on se demande quelle est la meilleure stratégie pour labelliser cette base.
On pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.
Les auteurs du livre montrent qu'il existe toutefois une meilleure stratégie.
Il vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une
image représentative par groupe, et labelliser ces images représentatives au lieu de labelliser au hasard.

:::


Les méthodes de *clustering* sont nombreuses.
Nous allons nous pencher sur la plus intuitive : les *k-means*.
::::

:::: {.content-visible when-profile="en"}
# Introduction to *Clustering*

Until now, we have engaged in supervised learning because we knew the true value of the variable to be explained/predicted (`y`). This is no longer the case with unsupervised learning.

*Clustering* is a field of application within unsupervised learning.
It involves leveraging available information by grouping observations
that are similar based on their common characteristics (_features_).

::: {.cell .markdown}
<details>
<summary>

Reminder: The Decision Tree of `Scikit` Methods

</summary>

![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)

</details>
:::

The objective is to create groups of observations (_clusters_) where:

* Within each cluster, the observations are homogeneous (minimal intra-cluster variance);
* Clusters have heterogeneous profiles, meaning they are distinct from one another (maximal inter-cluster variance).

In *Machine Learning*, clustering methods are widely used for
recommendation systems. For example, by creating homogeneous groups of
consumers, it becomes easier to identify and target behaviors specific to each consumer group.

These methods also have applications in economics and social sciences as they enable
grouping observations without prior assumptions, thereby interpreting a variable
of interest in light of these results. This [publication on spatial segregation using mobile phone data](https://www.insee.fr/fr/statistiques/4925200)
is an example of this approach.
In some databases, there may be a few labeled examples while most are
unlabeled. The labels might have been manually created by experts.

::: {.note}

Clustering methods can also be used upstream of a classification problem (in
semi-supervised learning problems).
The book *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow* [@geron2022hands] provides
examples in the chapter dedicated to unsupervised learning.

For instance, suppose that in the [MNIST dataset of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database), the digits are unlabeled,
and we want to determine the best strategy for labeling this dataset.
One could randomly look at handwritten digit images in the dataset and label them.
However, the book's authors demonstrate a better strategy.
It is better to apply a clustering algorithm beforehand to group the images together and have a
representative image per group, then label these representative images instead of labeling randomly.

:::


There are numerous clustering methods.
We will focus on the most intuitive one: *k-means*.
::::


:::: {.content-visible when-profile="fr"}
# Les k-means

## Principe

L'objectif des *k-means* est de partitionner l'espace des observations en trouvant des points (*centroids*) jouant le rôle de centres de gravité pour lesquels les observations proches peuvent être regroupées dans une classe homogène.
L'algorithme *k-means* fonctionne par itération, en initialisant les centroïdes puis en les mettant à jour à chaque
itération, jusqu'à ce que les centroïdes se stabilisent. Quelques exemples de *clusters* issus de la méthode *k-means* : 

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)

::: {.tip}

L'objectif des *k-means* est de trouver une partition des données $S=\{S_1,...,S_K\}$ telle que
$$
\arg\min_{S} \sum_{i=1}^K \sum_{x \in S_i} ||x - \mu_i||^2
$$
avec $\mu_i$ la moyenne des $x_i$ dans l'ensemble de points $S_i$.

:::

Dans ce chapitre nous allons principalement
utiliser `Scikit`. Voici néanmoins une proposition
d'imports de packages, pour gagner du temps. 
::::

:::: {.content-visible when-profile="en"}
# k-means

## Principle

The objective of *k-means* is to partition the observation space by finding points (*centroids*) that act as centers of gravity around which nearby observations can be grouped into homogeneous classes.
The *k-means* algorithm works iteratively, initializing the centroids and then updating them at each iteration until the centroids stabilize. Here are some examples of *clusters* resulting from the *k-means* method:

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)

::: {.tip}

The objective of *k-means* is to find a partition of the data $S=\{S_1,...,S_K\}$ such that
$$
\arg\min_{S} \sum_{i=1}^K \sum_{x \in S_i} ||x - \mu_i||^2
$$
where $\mu_i$ is the mean of $x_i$ in the set of points $S_i$.

:::

In this chapter, we will primarily use `Scikit`. However, here is a suggested import of packages to save time. 
::::

```{python}
#| echo: true
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import geopandas as gpd
from sklearn.cluster import KMeans #pour kmeans
import seaborn as sns #pour scatterplots
```


:::: {.content-visible when-profile="fr"}
::: {.callout-tip}
## Exercice 1 : Principe des k-means

1. Importer les données et se restreindre aux variables `'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2018-22', "Percent of adults with a bachelor's degree or higher, 2018-22"` et bien sûr `'per_gop'`. Appelez cette base restreinte `df2` et enlevez les valeurs manquantes.
2. Faire un *k-means* avec $k=4$.
3. Créer une variable `label` dans `votes` stockant le résultat de la typologie.
4. Afficher cette typologie sur une carte.
5. Choisir les variables `Median_Household_Income_2021` et `Unemployment_rate_2019` et représenter le nuage de points en colorant différemment en fonction du label obtenu. Quel est le problème ?
6. Refaire les questions 2 à 5 en standardisant les variables en amont.
7. Représenter la distribution du vote pour chaque *cluster*.

:::
::::

:::: {.content-visible when-profile="en"}
::: {.callout-tip}
## Exercise 1: Principle of k-means

1. Import the data and limit it to the variables `'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2018-22', "Percent of adults with a bachelor's degree or higher, 2018-22"`, and of course `'per_gop'`. Call this restricted dataset `df2` and remove missing values.
2. Perform a *k-means* with $k=4$.
3. Create a `label` variable in `votes` to store the typology results.
4. Display this typology on a map.
5. Choose the variables `Median_Household_Income_2021` and `Unemployment_rate_2019` and represent the scatter plot, coloring it differently based on the obtained label. What is the problem?
6. Repeat questions 2 to 5, standardizing the variables beforehand.
7. Represent the distribution of the vote for each *cluster*.

:::
::::

```{python}
#| output: false

# 1. Chargement de la base restreinte.
xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2021',
'Percent of adults with less than a high school diploma, 2018-22',
"Percent of adults with a bachelor's degree or higher, 2018-22"]

votes = votes.dropna(subset = xvars + ["per_gop"])

df2 = votes[xvars + ["per_gop"]]
```



```{python}
#| output: false

#2. kmeans avec k=4
model = KMeans(n_clusters=4)
model.fit(df2[xvars])
```



```{python}
#| output: false

#3. Création de la variable label dans votes
votes['label'] = model.labels_
#votes['label'].head()
```

:::: {.content-visible when-profile="fr"}
La carte obtenue à la question 4, qui permet de 
représenter spatialement nos groupes, est
la suivante :
::::

:::: {.content-visible when-profile="en"}
The map obtained in question 4, which allows us to 
spatially represent our groups, is
as follows:
::::



```{python}
#| output: false

#4. Carte de la typologie
p = votes.plot(column = "label", cmap = "inferno")
p.set_axis_off()
```

```{python}
p.get_figure()
```

:::: {.content-visible when-profile="fr"}
Le nuage de points de la question 5, permettant de représenter
la relation entre `Median_Household_Income_2021`
et `Unemployment_rate_2019`, aura l'aspect suivant :
::::

:::: {.content-visible when-profile="en"}
The scatter plot from question 5, representing
the relationship between `Median_Household_Income_2021`
and `Unemployment_rate_2019`, will have the following appearance:
::::


```{python}
#| output: false

from plotnine import *

#5. Nuage de points de 2 variables et coloration selon le label
votes['label'] = pd.Categorical(votes['label'])
p = (
    ggplot(votes) +
    geom_point(
        aes(
            x = "Median_Household_Income_2021",
            y = "Unemployment_rate_2019",
            color = "label"
            ),
         alpha = 0.4
    ) +
    theme_bw() + scale_x_log10()
)
```

```{python}
p
```

```{python}
#| output: false
ggsave(p, "featured_clustering.png")
```

:::: {.content-visible when-profile="fr"}
La classification apparaît un peu trop nettement dans cette figure.
Cela suggère que la variable de revenu (`Median_Household_Income_2021`)
explique un peu trop bien le partitionnement produit par notre
modèle pour que ce soit normal. C'est probablement le fait
de la variance forte du revenu par rapport aux autres variables. 
Dans ce type de situation, comme cela a été évoqué, il est
recommandé de standardiser les variables.
::::

:::: {.content-visible when-profile="en"}
The classification appears too distinct in this figure.
This suggests that the income variable (`Median_Household_Income_2021`)
explains the partitioning produced by our model too well to be normal. 
This is likely due to the high variance of income compared to other variables. 
In such situations, as mentioned earlier, it is recommended to standardize the variables.
::::



```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

kmeans = make_pipeline(
    StandardScaler(),
    KMeans(n_clusters=4, random_state=123)
)
kmeans.fit(df2)

votes['label'] = kmeans.predict(df2)

p = votes.plot(column = "label", cmap = "inferno")
p.set_axis_off()
```

:::: {.content-visible when-profile="fr"}
On obtient ainsi la carte suivante à la question 5 :
::::

:::: {.content-visible when-profile="en"}
Thus, the following map is obtained in question 5:
::::

```{python}
p.get_figure()
```

:::: {.content-visible when-profile="fr"}
Et le nuage de points de la question 5 présente un aspect moins
déterministe, ce qui est préférable :
::::

:::: {.content-visible when-profile="en"}
And the scatter plot from question 5 has a less deterministic appearance, which is preferable:
::::


```{python}
from plotnine import *
from mizani.formatters import percent_format

votes['label'] = pd.Categorical(votes['label'])

(
    ggplot(votes) +
    geom_point(
        aes(
            x = "Median_Household_Income_2021/1000",
            y = "Unemployment_rate_2019/100",
            color = "label"
            ),
         alpha = 0.4
    ) +
    theme_bw() + scale_x_log10() +
    scale_y_continuous(labels=percent_format()) +
    labs(
      x = "Revenu médian du comté (milliers de $)",
      y = "Taux de chômage du comté")
)
```


:::: {.content-visible when-profile="fr"}
Enfin, en ce qui concerne la question 6, on obtient cet 
histogramme des votes pour chaque cluster :
::::

:::: {.content-visible when-profile="en"}
Finally, regarding question 6, the following histogram of votes for each cluster is obtained:
::::


```{python}
#| output: false

# 6. Distribution du vote selon chaque cluster
plt.figure()
p2 = (
  ggplot(votes) +
  geom_histogram(
      aes(x = "per_gop", fill = "label"), alpha = 0.2, position="identity"
      ) +
  theme_minimal()
)
```


```{python}
#| echo: false
p2
```


:::: {.content-visible when-profile="fr"}

::: {.tip}
Il faut noter plusieurs points sur l'algorithme implémenté par défaut par `scikit-learn`, que l'on peut lire dans
la documentation :

- l'algorithme implémenté par défaut est kmeans++ (cf. paramètre `init`). Cela signifie que
l'initialisation des centroïdes est faite de manière intelligente pour que les centroïdes initiaux soient choisis
afin de ne pas être trop proches.
- l'algorithme va être démarré avec `n_init` centroïdes différents et le modèle va choisir la meilleure initialisation
en fonction de l'_inertie_ du modèle, par défaut égale à 10.

Le modèle renvoie les `cluster_centers_`, les labels `labels_`, l'inertie `inertia_` et le nombre d'itérations
`n_iter_`.

:::

::::

:::: {.content-visible when-profile="en"}

::: {.tip}
Several points about the algorithm implemented by default in `scikit-learn` should be noted, as outlined in
the documentation:

- The default algorithm is kmeans++ (see the `init` parameter). This means that
the initialization of centroids is done intelligently so that the initial centroids are chosen
to avoid being too close to each other.
- The algorithm will start with `n_init` different centroids, and the model will select the best initialization
based on the model's _inertia_, with a default value of 10.

The model outputs the `cluster_centers_`, the labels `labels_`, the inertia `inertia_`, and the number of iterations
`n_iter_`.

:::

::::

:::: {.content-visible when-profile="fr"}
## Choisir le nombre de clusters

Le nombre de clusters est fixé par le modélisateur.
Il existe plusieurs façons de fixer ce nombre :

* connaissance a priori du problème ;
* analyse d'une métrique spécifique pour définir le nombre de clusters à choisir ;
* etc.

Il y a un arbitrage à faire
entre biais et variance :
un trop grand nombre de clusters implique une variance
intra-cluster très faible (sur-apprentissage, même s'il n'est jamais possible de déterminer
le vrai type d'une observation puisqu'on est en apprentissage non supervisé).

Sans connaissance a priori du nombre de clusters, on peut recourir à deux familles de méthodes :

* **La méthode du coude** (*elbow method*) : On prend le point d'inflexion de la courbe
de performance du modèle. Cela représente le moment où ajouter un cluster
(complexité croissante du modèle) n'apporte que des gains modérés dans la
modélisation des données.

* **Le score de silhouette** : On mesure la similarité entre un point et les autres points
du cluster par rapport aux autres clusters. Plus spécifiquement :

> Silhouette value is a measure of how similar an object is to its own cluster
> (cohesion) compared to other clusters (separation). The silhouette ranges
> from −1 to +1, where a high value indicates that the object is
> well matched to its own cluster and poorly matched to neighboring
> clusters. If most objects have a high value, then the clustering
> configuration is appropriate. If many points have a low or negative
> value, then the clustering configuration may have too many or too few clusters
>
> Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))

Le score de silhouette d'une observation est donc égal à
`(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)`
où `m_intra_cluster` est la moyenne des distances de l'observation aux observations du même cluster
et `m_nearest_cluster` est la moyenne des distances de l'observation aux observations du cluster le plus proche.


Le package `yellowbrick` fournit une extension utile à `scikit` pour représenter
visuellement la performance de nos algorithmes de *clustering*.

Repartons d'un modèle simple
::::

:::: {.content-visible when-profile="en"}
## Choosing the Number of Clusters

The number of clusters is determined by the modeler.
There are several ways to determine this number:

* Prior knowledge of the problem;
* Analysis of a specific metric to define the number of clusters to choose;
* Etc.

There is a trade-off between bias and variance:
too many clusters result in very low intra-cluster variance (overfitting, although it is never possible to determine the true type of an observation since this is unsupervised learning).

Without prior knowledge of the number of clusters, two families of methods can be used:

* **The elbow method**: Take the inflection point of the model's performance curve. This represents the moment when adding another cluster (increasing model complexity) provides only moderate gains in data modeling.

* **The silhouette score**: This measures the similarity between a point and other points in the cluster relative to other clusters. Specifically:

> Silhouette value is a measure of how similar an object is to its own cluster
> (cohesion) compared to other clusters (separation). The silhouette ranges
> from −1 to +1, where a high value indicates that the object is
> well matched to its own cluster and poorly matched to neighboring
> clusters. If most objects have a high value, then the clustering
> configuration is appropriate. If many points have a low or negative
> value, then the clustering configuration may have too many or too few clusters
>
> Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))

The silhouette score for an observation is therefore equal to
`(m_nearest_cluster - m_intra_cluster)/max( m_nearest_cluster,m_intra_cluster)`
where `m_intra_cluster` is the mean distance of the observation to observations in the same cluster,
and `m_nearest_cluster` is the mean distance of the observation to observations in the nearest cluster.


The `yellowbrick` package provides a useful extension to `scikit` for visually representing
the performance of our clustering algorithms.

Let us start again with a simple model.
::::

```{python}
#| echo: true
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]

df2 = votes[xvars + ["per_gop"]].dropna()
model = KMeans(n_clusters=4)
```

```{python}
#| echo: true
#| output: false
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

visualizer = KElbowVisualizer(model, k=(2,12))
visualizer.fit(df2.loc[:,xvars])        # Fit the data to the visualizer
visualizer.show()
```


:::: {.content-visible when-profile="fr"}
Pour la méthode du coude, la courbe
de performance du modèle marque un coude léger à $k=4$. Le modèle initial
semblait donc approprié.

`yellowbrick` permet également de représenter des silhouettes mais
l'interprétation est moins aisée et le coût computationnel plus élevé :
::::

:::: {.content-visible when-profile="en"}
For the elbow method, the model's performance curve shows a slight elbow at $k=4$. The initial model
therefore seemed appropriate.

`yellowbrick` also allows the representation of silhouettes, but the interpretation is less straightforward, and the computational cost is higher:
::::

```{python}
#| echo: true
#| output: false

from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(2, 2, figsize=(15,8))
j=0
for i in [3, 4, 6, 10]:
    j += 1
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(j, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    ax[q-1][mod].set_title("k = " + str(i))
    visualizer.fit(df2[xvars])
```

```{python}
#| output: false
fig.savefig("silhouette-yellowbrick.png")
```

![](silhouette-yellowbrick.png)

:::: {.content-visible when-profile="fr"}
Le score de silhouette offre une représentation plus riche que la courbe coudée.

Sur ce graphique, les barres verticales en rouge et en pointillé représentent le score de silhouette
global pour chaque `k` choisi. On voit par exemple que pour tous les `k` représentés ici, le
score de silhouette se situe entre 0.5 et 0.6 et varie peu.
Ensuite, pour un `k` donné, on va avoir la représentation des scores de silhouette de chaque
observation, regroupées par cluster.
Par exemple, pour `k = 4`, ici, on voit bien quatre couleurs différentes qui sont les 4 clusters modélisés.
Les ordonnées sont toutes les observations clusterisées et en abscisses on a le score de silhouette de
chaque observation. Si au sein d'un cluster, les observations ont un score de silhouette plus faible que le
score de silhouette global (ligne verticale en rouge), cela signifie que les observations du cluster sont
trop proches des autres clusters.

Grâce à cette représentation, on peut aussi se rendre compte de la taille relative des clusters. Par exemple,
avec `k = 3`, on voit qu'on a deux clusters conséquents et un plus "petit" cluster relativement aux deux autres.
Cela peut nous permettre de choisir des clusters de tailles homogènes ou non.

Enfin, quand le score de silhouette est négatif, cela signifie que la moyenne des distances de l'observation
aux observations du cluster le plus proche est inférieure à la moyenne des distances de l'observation aux
observations de son cluster. Cela signifie que l'observation est mal classée.

## Autres méthodes de clustering

Il existe de nombreuses autres méthodes de clustering. Parmi les plus connues, on peut citer trois exemples en particulier :

- Le clustering ascendant hiérarchique ;
- DBSCAN ;
- Les mélanges de Gaussiennes.

### Clustering Ascendant Hiérarchique (CAH)

Quel est le principe ?

- On commence par calculer la dissimilarité entre nos _N_ individus, _i.e._ leur distance deux à deux dans l'espace de nos variables.
- Puis on regroupe les deux individus dont le regroupement minimise un critère d'agrégation donné, créant ainsi une classe comprenant ces deux individus.
- On calcule ensuite la dissimilarité entre cette classe et les _N-2_ autres individus en utilisant le critère d'agrégation.
- Puis on regroupe les deux individus ou classes d'individus dont le regroupement minimise le critère d'agrégation.
- On continue ainsi jusqu'à ce que tous les individus soient regroupés.

Ces regroupements successifs produisent un arbre binaire de classification (dendrogramme), dont la racine correspond à la classe regroupant l'ensemble des individus. Ce dendrogramme représente une hiérarchie de partitions. On peut alors choisir une partition en tronquant l'arbre à un niveau donné, le niveau dépendant soit des contraintes de l'utilisateur, soit de critères plus objectifs.

Plus d'informations [ici](https://www.xlstat.com/fr/solutions/fonctionnalites/classification-ascendante-hierarchique-cah).

### DBSCAN

L'[algorithme DBSCAN](https://fr.wikipedia.org/wiki/DBSCAN) est implémenté dans `sklearn.cluster`.
Il peut être utilisé pour faire de la détection d'anomalies notamment.
En effet, cette méthode repose sur le clustering en régions où la densité
des observations est continue, grâce à la notion de voisinage selon une certaine distance epsilon.
Pour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S'il y a au
moins `min_samples` voisins, alors l'observation sera une *core instance*.

Les observations qui ne sont pas des *core instances* et qui n'en ont pas dans leur voisinage selon une distance epsilon
vont être détectées comme des anomalies.

### Les mélanges de gaussiennes

En ce qui concerne la théorie, voir le cours [Probabilités numériques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).
Se référer notamment aux notebooks pour l'algorithme EM pour mélange gaussien.

Dans `sklearn`, les mélanges gaussiens sont implémentés dans `sklearn.mixture` comme `GaussianMixture`.
Les paramètres importants sont alors le nombre de gaussiennes `n_components` et le nombre d'initialisations `n_init`.
Il est possible de faire de la détection d'anomalies avec les mélanges de gaussiennes.

::: {.note}
## Pour aller plus loin

Il existe de nombreuses autres méthodes de clustering :

- Local outlier factor ;
- Bayesian gaussian mixture models ;
- D'autres méthodes de clustering hiérarchique ;
- Etc.
:::
::::
:::: {.content-visible when-profile="en"}
The silhouette score provides a richer representation than the elbow curve.

In this graph, the red dashed vertical lines represent the global silhouette score
for each chosen `k`. For example, we can see that for all the `k` values represented here, the
silhouette score is between 0.5 and 0.6 and varies little.
Then, for a given `k`, the silhouette scores for each
observation are displayed, grouped by cluster.
For instance, for `k = 4`, here, we clearly see four different colors representing the 4 modeled clusters.
The y-axis represents all clustered observations, and the x-axis shows the silhouette score of
each observation. If, within a cluster, observations have a silhouette score lower than the
global silhouette score (red vertical line), this indicates that the cluster's observations are
too close to other clusters.

This representation also allows us to assess the relative size of clusters. For example,
with `k = 3`, we see two large clusters and one "smaller" cluster relative to the others.
This can help us decide whether to aim for clusters of homogeneous sizes or not.

Finally, when the silhouette score is negative, it indicates that the mean distance of an observation
to observations in the nearest cluster is smaller than the mean distance of the observation to
observations in its own cluster. This means the observation is misclassified.

## Other Clustering Methods

There are many other clustering methods. Among the most well-known, here are three notable examples:

- Hierarchical Agglomerative Clustering (HAC);
- DBSCAN;
- Gaussian Mixture Models.

### Hierarchical Agglomerative Clustering (HAC)

What is the principle?

- Start by calculating the dissimilarity between our _N_ individuals, i.e., their pairwise distances in the variable space.
- Then, group the two individuals whose grouping minimizes a given aggregation criterion, thus creating a class containing these two individuals.
- Next, calculate the dissimilarity between this class and the _N-2_ other individuals using the aggregation criterion.
- Then, group the two individuals or classes of individuals whose grouping minimizes the aggregation criterion.
- Continue until all individuals are grouped.

These successive groupings produce a binary classification tree (dendrogram), where the root corresponds to the class grouping all individuals. This dendrogram represents a hierarchy of partitions. A partition can be chosen by truncating the tree at a certain level, based on either user constraints or more objective criteria.

More information [here](https://www.xlstat.com/en/solutions/features/hierarchical-clustering-hac).

### DBSCAN

The [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN) is implemented in `sklearn.cluster`.
It can be used notably for anomaly detection.
This method is based on clustering in regions of continuous observation density using the concept of neighborhood within a certain epsilon distance.
For each observation, it is checked whether there are neighbors within its epsilon-distance neighborhood. If there are at
least `min_samples` neighbors, the observation will be a *core instance*.

Observations that are not *core instances* and have no *core instances* in their epsilon-distance neighborhood
will be detected as anomalies.

### Gaussian Mixture Models

For theoretical insights, see the course [Probabilistic and Computational Statistics, M1 Jussieu, V.Lemaire and T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).
Refer especially to the notebooks for the EM algorithm for Gaussian mixture models.

In `sklearn`, Gaussian mixture models are implemented in `sklearn.mixture` as `GaussianMixture`.
Key parameters include the number of Gaussians `n_components` and the number of initializations `n_init`.
Gaussian mixture models can also be used for anomaly detection.

::: {.note}
## Going Further

There are many other clustering methods:

- Local Outlier Factor;
- Bayesian Gaussian Mixture Models;
- Other Hierarchical Clustering Methods;
- Etc.
:::
::::

:::: {.content-visible when-profile="fr"}
# L'Analyse en composantes principales (ACP)

## Pour la visualisation de clusters

La méthode la plus simple pour visualiser les _clusters_, peu importe la méthode avec laquelle ils ont été obtenus, serait de représenter chaque individu dans l'espace à _N_ dimensions des variables de la table, et colorier chaque individu en fonction de son cluster.
On pourrait alors bien différencier les variables les plus discrimantes et les différents groupes.
Un seul problème ici : dès que _N > 3_, nous avons du mal à représenter le résultat de façon intelligible...

C'est là qu'intervient l'**Analyse en Composantes Principales** ([ACP](https://www.xlstat.com/fr/solutions/fonctionnalites/analyse-en-composantes-principales-acp)), qui permet de projeter notre espace à haute dimension dans un espace de dimension plus petite.
La contrainte majeure de la projection est de pouvoir conserver le maximum d'information (mesurée par la variance totale de l'ensemble de données) dans notre nombre réduit de dimensions, appelées composantes principales.
En se limitant à 2 ou 3 dimensions, on peut ainsi se représenter visuellement les relations entre les observations avec une perte de fiabilité minimale.

On peut généralement espérer que les clusters déterminés dans notre espace à N dimensions se différencient bien sur notre projection par ACP, et que la composition des composantes principales en fonction des variables initiales permette d'interpréter les clusters obtenus.
En effet, la combinaison linéaire des colonnes donnant naissance à nos nouveaux axes a souvent un "sens" dans le monde réel :

- Soit parce qu'une petite poignée de variables représente la majorité de la composante ;
- Soit parce que la plupart des colonnes intervenant dans la composante sommée se combinent bien pour former une interprétation naturelle.

Pour mettre en pratique les méthodes de création de clusters, de la base brute jusqu'à la visualisation par ACP, vous pouvez consulter la partie 2 du sujet 3 du funathon 2023, *Explorer les habitudes alimentaires de nos compatriotes*, sur le [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) ou sur [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).

## Pour la réduction de dimension

L'ACP est également très utile dans le champ de la réduction du nombre de variables pour de nombreux types de modélisations, comme par exemple les régressions linéaires.
Il est ainsi possible de projeter l'espace des variables explicatives dans un espace de dimension donnée plus faible, pour notamment limiter les risques d'_overfitting_.

L'inconvénient de cette approche est qu'elle rend les données utilisées en entrée du modèle moins interprétables qu'avec un LASSO puisque cette dernière technique sélectionne des variables là où la PCA sélectionne des combinaisons linéaires de nos variables. 

## Exemple

Reprenons nos données précédentes. Avant de faire une analyse en composante principale, dont l'objectif est de synthétiser des sources de variabilité dans nos données, il est conseillé de standardiser les variables lorsque celles-ci ont des échelles différentes (ce qui est le cas dans notre cas). 
::::

:::: {.content-visible when-profile="en"}
# Principal Component Analysis (PCA)

## For Cluster Visualization

The simplest method to visualize _clusters_, regardless of how they were obtained, would be to represent each individual in the _N_-dimensional space of the table's variables, coloring each individual based on their cluster.
This would clearly differentiate the most discriminating variables and the various groups.
One issue here: as soon as _N > 3_, it becomes difficult to represent the results intelligibly...

This is where **Principal Component Analysis** ([PCA](https://www.xlstat.com/en/solutions/features/principal-component-analysis-pca)) comes into play, allowing us to project our high-dimensional space into a smaller-dimensional space.
The major constraint of this projection is to retain the maximum amount of information (measured by the total variance of the dataset) within our reduced number of dimensions, called principal components.
By limiting to 2 or 3 dimensions, we can visually represent relationships between observations with minimal loss of reliability.

We can generally expect that clusters determined in our N-dimensional space will differentiate well in our PCA projection and that the composition of the principal components based on the initial variables will help interpret the obtained clusters.
Indeed, the linear combination of columns creating our new axes often has "meaning" in the real world:

- Either because a small handful of variables represent the majority of the component;
- Or because most columns contributing to the summed component combine well to form a natural interpretation.

To practice cluster creation methods, from raw data to PCA visualization, refer to part 2 of subject 3 in the 2023 funathon, *Explore the eating habits of our compatriots*, on [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) or on [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).

## For Dimensionality Reduction

PCA is also very useful in reducing the number of variables for many types of modeling, such as linear regression.
It is possible to project the space of explanatory variables into a lower-dimensional space, specifically to limit the risks of _overfitting_.

The drawback of this approach is that it makes the data used as input for the model less interpretable compared to LASSO, as the latter selects variables, while PCA selects linear combinations of our variables.

## Example

Let us revisit our previous data. Before performing a principal component analysis, whose objective is to synthesize sources of variability in our data, it is advisable to standardize the variables when they have different scales (which is the case here).
::::

```{python}
from sklearn.preprocessing import StandardScaler

X = df2.drop(['per_gop'], axis=1)
y = votes['winner']

print('Dimensions des données avant PCA : {}'.format(X.shape))
```

:::: {.content-visible when-profile="fr"}
Faisons déjà un premier test en réduisant nos données à deux composantes, c'est-à-dire à deux combinaisons linéaires de celles-ci. Il s'agit d'une méthode implémentée en `Scikit`, très pratique. Le faire à la main serait pénible.
::::

:::: {.content-visible when-profile="en"}
Let us start with a preliminary test by reducing our data to two components, that is, two linear combinations of the data. This is a very practical method implemented in `Scikit`. Doing it manually would be cumbersome.
::::



```{python}
#| echo: true
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

n_components = 2
pca = PCA(n_components=n_components)
```

:::: {.content-visible when-profile="fr"}
<details>

<summary>
Faire une PCA à la main (exercice éducatif mais peu utile dans la vraie vie)
</summary>
::::

:::: {.content-visible when-profile="en"}
<details>

<summary>
Perform PCA manually (an educational exercise but not very practical in real life)
</summary>
::::

```{python}
#| echo: true

# Step 1: Compute the covariance matrix
cov_matrix = np.cov(X_standardized.T)

# Step 2: Perform eigen decomposition
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 3: Sort eigenvectors by eigenvalues
sorted_indices = np.argsort(-eigenvalues)
eigenvectors_sorted = eigenvectors[:, sorted_indices]

first_component_manual = eigenvectors_sorted[:, 0]
first_component_manual
```

</details>


:::: {.content-visible when-profile="fr"}
On peut utiliser notre méthode `fit_transform` pour calculer les paramètres utiles de notre PCA, à savoir les poids à utiliser pour reprojeter nos variables dans l'espace des composantes :
::::

:::: {.content-visible when-profile="en"}
We can use our `fit_transform` method to calculate the useful parameters of our PCA, namely the weights to use for reprojecting our variables into the component space:
::::


```{python}
#| echo: true

x_2d = pca.fit_transform(X_standardized)
columns=[f'component_{i}' for i in range(1, n_components + 1)]
df_pca = pd.DataFrame(x_2d, columns=columns)
df_pca['classe'] = y
print('Dimensions des données après PCA : {}'.format(x_2d.shape))
df_pca
```

:::: {.content-visible when-profile="fr"}
Ces composantes ne sont plus interprétables directement. Il s'agit d'une combinaison linéaire de nos variables. Prenons le premier axe pour s'en assurer :
::::

:::: {.content-visible when-profile="en"}
These components are no longer directly interpretable. They are linear combinations of our variables. Let us take the first axis to confirm:
::::

```{python}
#| echo: true
np.dot(X_standardized, pca.components_[0])
```

:::: {.content-visible when-profile="fr"}
Pourquoi rendre nos données moins interprétables ? Parce qu'avec seulement deux colonnes, on va synthétiser beaucoup plus d'information, c'est-à-dire capturer beaucoup plus de variance de nos données, qu'avec nos données brutes.

La variance expliquée par chaque composante est la suivante :
::::

:::: {.content-visible when-profile="en"}
Why make our data less interpretable? Because with just two columns, we will synthesize much more information, meaning we will capture much more variance in our data than with the raw data.

The variance explained by each component is as follows:
::::


```{python}
#| echo: true
tableau_variance = pd.DataFrame(
  {"Axe": [f"Composante {i+1}" for i in range(2)],
  "Variance expliquée (%)": pca.explained_variance_ratio_*100}
)
tableau_variance
```

:::: {.content-visible when-profile="fr"}
Avec deux axes, on capture donc une bonne partie de notre variance :
::::

:::: {.content-visible when-profile="en"}
With two axes, we capture a significant portion of our variance:
::::

```{python}
#| echo: true
tableau_variance["Variance expliquée (%)"].sum()
```

:::: {.content-visible when-profile="fr"}
Le premier axe capture une part importante de la variance, le deuxième axe étant déjà beaucoup moins explicatif. Ceci est attendu puisque les axes des PCA capturent une part décroissante de la variance.

Ici nous avions fixé le nombre d'axes principaux à 2. Comment choisir ce nombre en pratique ? Comme précédemment pour les _k-means_, le critère du coude est fréquemment utilisé. Représentons la part de variance expliquée en fonction du nombre d'axes :
::::

:::: {.content-visible when-profile="en"}
The first axis captures a significant portion of the variance, while the second axis is already much less explanatory. This is expected since PCA axes capture a decreasing portion of the variance.

Here, we set the number of principal axes to 2. How do we choose this number in practice? As with _k-means_, the elbow criterion is frequently used. Let us represent the proportion of variance explained as a function of the number of axes:
::::


```{python}
#| echo: true
import plotly.express as px
pca = PCA()
pca.fit(X_standardized)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

px.area(
    x=range(1, exp_var_cumul.shape[0] + 1),
    y=exp_var_cumul,
    labels={"x": "# Components", "y": "Explained Variance"}
)
```

:::: {.content-visible when-profile="fr"}
Les coudes sont peu francs, on peut donc choisir deux ou trois axes. Si on préfère utiliser un seuil de variance expliquée dans notre analyse, on utilisera plutôt l'option `n_components` de `Scikit`. Par exemple, si on désire conserver les axes permettant d'expliquer 90% de la variabilité de nos données :
::::

:::: {.content-visible when-profile="en"}
The elbows are not very distinct, so we can choose two or three axes. If we prefer to use a threshold for explained variance in our analysis, we can use the `n_components` option in `Scikit`. For example, if we want to retain the axes that explain 90% of the variability in our data:
::::


```{python}
#| echo: true
pca = PCA(n_components=0.9)
pca.fit(X_standardized)

print(pca.explained_variance_ratio_)
```


:::: {.content-visible when-profile="fr"}
# Références {-}
::::

:::: {.content-visible when-profile="en"}
# References {-}
::::