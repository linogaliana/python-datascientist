---
title: "Introduction à l'apprentissage non supervisé avec le clustering"
title-en: "Clustering"
author: Lino Galiana
categories:
  - Modélisation
  - Exercice
description: |
  Le _clustering_ consiste à répartir des observations dans des groupes, généralement non observés, en fonction de caractéristiques observables. Il s'agit d'une application classique, en _machine learning_ de méthodes non supervisées puisqu'on ne dispose généralement pas de l'information sur le groupe auquel appartient réellement une observation. Les applications au monde réel sont nombreuses, notamment dans le domaine de la segmentation tarifaire.
description-en: |
  Clustering consists in dividing observations into groups, usually unobserved, according to observable characteristics. This is a classic application of unsupervised methods in machine learning, since information on the group to which an observation actually belongs is generally not available. Real-world applications are numerous, notably in the field of fare segmentation.
bibliography: ../../reference.bib
echo: false
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/clustering1.webp
---


{{< badges
    printMessage="true"
>}}



{{< include _import_data_ml.qmd >}}


:::: {.content-visible when-profile="fr"}
Il peut également être utile d'installer `plotnine` 
pour réaliser des graphiques simplement :
::::

:::: {.content-visible when-profile="en"}
It can also be useful to install `plotnine` 
to easily create visualizations:
::::

```{python}
#| echo: true
#| output: false
!pip install plotnine
```


:::: {.content-visible when-profile="fr"}
# Introduction sur le *clustering*

Jusqu'à présent, nous avons fait de l'apprentissage supervisé puisque nous
connaissions la vraie valeur de la variable à expliquer/prédire (`y`). Ce n'est plus le cas avec
l'apprentissage non supervisé.

Le *clustering* est un champ d'application de l'apprentissage non-supervisé.
Il s'agit d'exploiter l'information disponible en regroupant des observations
qui se ressemblent à partir de leurs caractéristiques (_features_) communes.

::: {.cell .markdown}
<details>
<summary>

Rappel: l'arbre de décision des méthodes `Scikit`

</summary>

![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)

</details>
:::

L'objectif est de créer des groupes d'observations (_clusters_) pour lesquels :

* Au sein de chaque cluster, les observations sont homogènes (variance intra-cluster minimale) ;
* Les clusters ont des profils hétérogènes, c'est-à-dire qu'ils se distinguent les uns des autres (variance inter-cluster maximale).

En *Machine Learning*, les méthodes de clustering sont très utilisées pour
faire de la recommandation. En faisant, par exemple, des classes homogènes de
consommateurs, il est plus facile d'identifier et cibler des comportements
propres à chaque classe de consommateurs.

Ces méthodes ont également un intérêt en économie et sciences sociales parce qu'elles permettent
de regrouper des observations sans *a priori* et ainsi interpréter une variable
d'intérêt à l'aune de ces résultats. Cette [publication sur la ségrégation spatiale utilisant des données de téléphonie mobile](https://www.insee.fr/fr/statistiques/4925200)
utilise par exemple cette approche.
Dans certaines bases de données, on peut se retrouver avec quelques exemples labellisés mais la plupart sont
non labellisés. Les labels ont par exemple été faits manuellement par des experts.


::: {.callout-note}

Les méthodes de *clustering* peuvent aussi intervenir en amont d'un problème de classification (dans des
problèmes d'apprentissage semi-supervisé).
Le manuel *Hands-on machine learning with scikit-learn, Keras et TensorFlow* [@geron2022hands] présente dans le
chapitre dédié à l'apprentissage non supervisé quelques exemples.

Par exemple, supposons que dans la [base MNIST des chiffres manuscrits](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST), les chiffres ne soient pas labellisés
et que l'on se demande quelle est la meilleure stratégie pour labelliser cette base.
On pourrait regarder des images de chiffres manuscrits au hasard de la base et les labelliser.
Les auteurs du livre montrent qu'il existe toutefois une meilleure stratégie.
Il vaut mieux appliquer un algorithme de clustering en amont pour regrouper les images ensemble et avoir une
image représentative par groupe, et labelliser ces images représentatives au lieu de labelliser au hasard.

:::


Les méthodes de *clustering* sont nombreuses.
Nous allons nous pencher sur la plus intuitive : les *k-means*.
::::

:::: {.content-visible when-profile="en"}
# Introduction to *Clustering*

Until now, we have engaged in supervised learning because we knew the true value of the variable to be explained/predicted (`y`). This is no longer the case with unsupervised learning.

*Clustering* is a field of application within unsupervised learning.
It involves leveraging available information by grouping observations
that are similar based on their common characteristics (_features_).

::: {.cell .markdown}
<details>
<summary>

Reminder: The Decision Tree of `Scikit` Methods

</summary>

![](https://upload.wikimedia.org/wikipedia/commons/a/a4/Scikit-learn_machine_learning_decision_tree.png)

</details>
:::

The objective is to create groups of observations (_clusters_) where:

* Within each cluster, the observations are homogeneous (minimal intra-cluster variance);
* Clusters have heterogeneous profiles, meaning they are distinct from one another (maximal inter-cluster variance).

In *Machine Learning*, clustering methods are widely used for
recommendation systems. For example, by creating homogeneous groups of
consumers, it becomes easier to identify and target behaviors specific to each consumer group.

These methods also have applications in economics and social sciences as they enable
grouping observations without prior assumptions, thereby interpreting a variable
of interest in light of these results. This [publication on spatial segregation using mobile phone data](https://www.insee.fr/fr/statistiques/4925200)
is an example of this approach.
In some databases, there may be a few labeled examples while most are
unlabeled. The labels might have been manually created by experts.

::: {.callout-note}

Clustering methods can also be used upstream of a classification problem (in
semi-supervised learning problems).
The book *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow* [@geron2022hands] provides
examples in the chapter dedicated to unsupervised learning.

For instance, suppose that in the [MNIST dataset of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database), the digits are unlabeled,
and we want to determine the best strategy for labeling this dataset.
One could randomly look at handwritten digit images in the dataset and label them.
However, the book's authors demonstrate a better strategy.
It is better to apply a clustering algorithm beforehand to group the images together and have a
representative image per group, then label these representative images instead of labeling randomly.

:::


There are numerous clustering methods.
We will focus on the most intuitive one: *k-means*.
::::


:::: {.content-visible when-profile="fr"}
# Les k-means

## Principe

L'objectif des *k-means* est de partitionner l'espace des observations en trouvant des points (*centroids*) jouant le rôle de centres de gravité pour lesquels les observations proches peuvent être regroupées dans une classe homogène.
L'algorithme *k-means* fonctionne par itération, en initialisant les centroïdes puis en les mettant à jour à chaque
itération, jusqu'à ce que les centroïdes se stabilisent. Quelques exemples de *clusters* issus de la méthode *k-means* : 

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)

::: {.callout-tip}

L'objectif des *k-means* est de trouver une partition des données $S=\{S_1,...,S_K\}$ telle que
$$
\arg\min_{S} \sum_{i=1}^K \sum_{x \in S_i} ||x - \mu_i||^2
$$
avec $\mu_i$ la moyenne des $x_i$ dans l'ensemble de points $S_i$.

:::

Dans ce chapitre nous allons principalement
utiliser `Scikit`. Voici néanmoins une proposition
d'imports de packages, pour gagner du temps. 
::::

:::: {.content-visible when-profile="en"}
# k-means

## Principle

The objective of *k-means* is to partition the observation space by finding points (*centroids*) that act as centers of gravity around which nearby observations can be grouped into homogeneous classes.
The *k-means* algorithm works iteratively, initializing the centroids and then updating them at each iteration until the centroids stabilize. Here are some examples of *clusters* resulting from the *k-means* method:

![](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png)

::: {.callout-tip}

The objective of *k-means* is to find a partition of the data $S=\{S_1,...,S_K\}$ such that
$$
\arg\min_{S} \sum_{i=1}^K \sum_{x \in S_i} ||x - \mu_i||^2
$$
where $\mu_i$ is the mean of $x_i$ in the set of points $S_i$.

:::

In this chapter, we will primarily use `Scikit`. However, here is a suggested import of packages to save time. 
::::

```{python}
#| echo: true
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import geopandas as gpd
from sklearn.cluster import KMeans
import seaborn as sns
```

:::: {.content-visible when-profile="en"}
Dans le prochain exercice, nous allons utiliser les variables suivantes:
::::

:::: {.content-visible when-profile="fr"}
You will need the following variables in the next exercise: 
::::

```{python}
#| output: false
#| echo: true

# 1. Chargement de la base restreinte.
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]

votes = votes.dropna(subset = xvars + ["per_gop"])

df2 = votes.loc[:, xvars + ["per_gop"]]
```


:::: {.content-visible when-profile="fr"}
::: {.callout-tip}
## Exercice 1 : Principe des k-means

1. Faire un *k-means* avec $k=4$.
2. Créer une variable `label` dans `votes` stockant le résultat de la typologie.
3. Afficher cette typologie sur une carte.
4. Choisir les variables `Median_Household_Income_2021` et `Unemployment_rate_2019` et représenter le nuage de points en colorant différemment en fonction du label obtenu. Quel est le problème ?
5. Refaire les questions 2 à 5 en standardisant les variables en amont.
6. Représenter la distribution du vote pour chaque *cluster*.

:::
::::

:::: {.content-visible when-profile="en"}
::: {.callout-tip}
## Exercise 1: Principle of k-means

1. Perform a *k-means* with $k=4$.
2. Create a `label` variable in `votes` to store the typology results.
3. Display this typology on a map.
4. Choose the variables `Median_Household_Income_2021` and `Unemployment_rate_2019` and represent the scatter plot, coloring it differently based on the obtained label. What is the problem?
5. Repeat questions 2 to 5, standardizing the variables beforehand.
6. Represent the distribution of the vote for each *cluster*.

:::
::::



```{python}
#| output: false

# Question 1
model = KMeans(n_clusters=4)
model.fit(df2[xvars])
```



```{python}
#| output: false

# Question 3
votes['label'] = model.labels_
```

:::: {.content-visible when-profile="fr"}
La carte obtenue à la question 4, qui permet de 
représenter spatialement nos groupes, est
la suivante :
::::

:::: {.content-visible when-profile="en"}
The map obtained in question 4, which allows us to 
spatially represent our groups, is
as follows:
::::



```{python}
#| output: false

# Question 4
p = votes.plot(column = "label", cmap = "inferno")
p.set_axis_off()
```

```{python}
p.get_figure()
```

:::: {.content-visible when-profile="fr"}
Le nuage de points de la question 5, permettant de représenter
la relation entre `Median_Household_Income_2021`
et `Unemployment_rate_2019`, aura l'aspect suivant :
::::

:::: {.content-visible when-profile="en"}
The scatter plot from question 5, representing
the relationship between `Median_Household_Income_2021`
and `Unemployment_rate_2019`, will have the following appearance:
::::


```{python}
#| output: false
from plotnine import *

# Question 5
votes['label'] = pd.Categorical(votes['label'])

p = (
    ggplot(votes) +
    geom_point(
        aes(
            x = "Median_Household_Income_2021",
            y = "Unemployment_rate_2019",
            color = "label"
            ),
         alpha = 0.4
    ) +
    theme_bw() + scale_x_log10()
)
```

```{python}
p
```

:::: {.content-visible when-profile="fr"}
La classification apparaît un peu trop nettement dans cette figure.
Cela suggère que la variable de revenu (`Median_Household_Income_2021`)
explique un peu trop bien le partitionnement produit par notre
modèle pour que ce soit normal. C'est probablement le fait
de la variance forte du revenu par rapport aux autres variables. 
Dans ce type de situation, comme cela a été évoqué, il est
recommandé de standardiser les variables.
::::

:::: {.content-visible when-profile="en"}
The classification appears too distinct in this figure.
This suggests that the income variable (`Median_Household_Income_2021`)
explains the partitioning produced by our model too well to be normal. 
This is likely due to the high variance of income compared to other variables. 
In such situations, as mentioned earlier, it is recommended to standardize the variables.
::::



```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

kmeans = make_pipeline(
    StandardScaler(),
    KMeans(n_clusters=4, random_state=123)
)
kmeans.fit(df2)

votes['label'] = kmeans.predict(df2)

p = votes.plot(column = "label", cmap = "inferno")
p.set_axis_off()
```

:::: {.content-visible when-profile="fr"}
On obtient ainsi la carte suivante à la question 5 :
::::

:::: {.content-visible when-profile="en"}
Thus, the following map is obtained in question 5:
::::

```{python}
p.get_figure()
```

:::: {.content-visible when-profile="fr"}
Et le nuage de points de la question 5 présente un aspect moins
déterministe, ce qui est préférable :
::::

:::: {.content-visible when-profile="en"}
And the scatter plot from question 5 has a less deterministic appearance, which is preferable:
::::


```{python}
from plotnine import *
from mizani.formatters import percent_format

votes['label'] = pd.Categorical(votes['label'])

(
    ggplot(votes) +
    geom_point(
        aes(
            x = "Median_Household_Income_2021/1000",
            y = "Unemployment_rate_2019/100",
            color = "label"
        ),
        alpha = 0.4
    ) +
    theme_bw() + scale_x_log10() +
    scale_y_continuous(labels=percent_format()) +
    labs(
      x = "Revenu médian du comté (milliers de $)",
      y = "Taux de chômage du comté")
)
```


:::: {.content-visible when-profile="fr"}
Enfin, en ce qui concerne la question 6, on obtient cet 
histogramme des votes pour chaque cluster :
::::

:::: {.content-visible when-profile="en"}
Finally, regarding question 6, the following histogram of votes for each cluster is obtained:
::::


```{python}
#| output: false

# Question 6
plt.figure()
p2 = (
  ggplot(votes) +
  geom_histogram(
      aes(x = "per_gop", fill = "label"), alpha = 0.2, position="identity"
      ) +
  theme_minimal()
)
```


```{python}
#| echo: false
p2
```


:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
Il faut noter plusieurs points sur l'algorithme implémenté par défaut par `scikit-learn`, que l'on peut lire dans
la documentation :

- l'algorithme implémenté par défaut est _kmeans++_ (cf. paramètre `init`). Cela signifie que
l'initialisation des centroïdes est faite de manière intelligente pour que les centroïdes initiaux soient choisis
afin de ne pas être trop proches.
- l'algorithme va être démarré avec `n_init` centroïdes différents et le modèle va choisir la meilleure initialisation
en fonction de l'_inertie_ du modèle, par défaut égale à 10.

Le modèle renvoie les `cluster_centers_`, les labels `labels_`, l'inertie `inertia_` et le nombre d'itérations
`n_iter_`.

:::

::::

:::: {.content-visible when-profile="en"}

::: {.callout-tip}
Several points about the algorithm implemented by default in `scikit-learn` should be noted, as outlined in
the documentation:

- The default algorithm is _kmeans++_ (see the `init` parameter). This means that
the initialization of centroids is done intelligently so that the initial centroids are chosen
to avoid being too close to each other.
- The algorithm will start with `n_init` different centroids, and the model will select the best initialization
based on the model's _inertia_, with a default value of 10.

The model outputs the `cluster_centers_`, the labels `labels_`, the inertia `inertia_`, and the number of iterations
`n_iter_`.

:::

::::

:::: {.content-visible when-profile="fr"}
## Choisir le nombre de clusters

Jusqu'à présent nous avons pris comme donné le nombre de cluster, comme s'il y avait une raison légitime de penser qu'il fallait 4 plutôt que 7 profils de votes.   

Comme tout (hyper)paramètre dans une approche de _machine learning_ on peut vouloir jouer sur la valeur et prendre, en l'absence de théorie permettant de trancher, la moins mauvaise valeur empirique. 

Il y a un arbitrage à faire entre biais et variance : un trop grand nombre de clusters implique une variance intra-cluster très faible ce qui est typique du sur-apprentissage, même s'il n'est jamais possible de déterminer le vrai type d'une observation puisqu'on est en apprentissage non supervisé.

Sans connaissance a priori du nombre de clusters, on peut recourir à deux familles de méthodes :

* **La méthode du coude** (*elbow method*) : On prend le point d'inflexion de la courbe de performance du modèle. Cela représente le moment où ajouter un cluster supplémentaire, qui se traduit par une complexité croissante du modèle, n'apporte que des gains modérés dans la modélisation des données.

* **Le score de silhouette** : On mesure la similarité entre un point et les autres points du cluster par rapport aux autres clusters et choisit le modèle qui permet de mieux distinguer les modèles (voir @tip-silhouette).  
::::

:::: {.content-visible when-profile="en"}
## Choosing the number of clusters

Up to now, we have taken the number of clusters as given, as if there were a legitimate reason to think that we need 4 rather than 7 voting profiles.   

Like any (hyper)parameter in a machine learning approach, we may want to vary its value and, in the absence of a theory to decide, pick the least bad empirical choice. 

There is a trade-off between bias and variance: too large a number of clusters implies very low within-cluster variance, which is typical of overfitting, even though it is never possible to determine the true type of an observation since we are in unsupervised learning.

Without prior knowledge of the number of clusters, we can rely on two families of methods:

* **Elbow method**: We take the inflection point of the model performance curve. This corresponds to the moment when adding an additional cluster, which increases model complexity, brings only modest gains in modeling the data.

* **Silhouette score**: We measure the similarity between a point and the other points in its cluster relative to other clusters, and choose the model that best separates clusters (see @tip-silhouette-en).  
::::

:::: {.content-visible when-profile="fr"}

::: {#tip-silhouette .callout-tip collapse="true"}
## Le score de silhouette

> Silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object iswell matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters
>
> Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))

Le score de silhouette est donc une mesure de l'arbitrage entre cohésion des clusters (dans quelle mesure les observations au sein du clusters sont homogènes) et séparation de ceux-ci (dans quelle mesure les clusters se dissocient-ils les uns des autres). 

Pour chaque observation $i$, la silhouette du point est

$$
s(i) = \frac{b(i)-a(i)}{\max(a(i),b(i))}
$$


avec $a(i)$ la distance moyenne entre $i$ et les autres points de son propre cluster (mesure de la cohésion) et $b(i)$ la plus petite distance moyenne entre $i$ et les points d'un autre cluster (mesure de la séparation).

La valeur \(s(i)\) est comprise entre **-1** et **1** :

- **$s(i) \approx 1$** : $a(i) \ll b(i)$  
  Le point est bien assigné à son cluster : il est proche des points de son cluster et loin des autres.

- **$s(i) \approx 0$** : $a(i) \approx b(i)$  
  Le point est à la frontière entre deux clusters : la séparation est faible localement.

- **$s(i) < 0$** : $a(i) > b(i)$
  Le point est probablement mal assigné : en moyenne, il est plus proche d’un autre cluster que du sien.

Le score de silhouette est la moyenne des scores de silhouette des points. 
:::

::::

:::: {.content-visible when-profile="en"}

::: {#tip-silhouette-en .callout-tip collapse="true"}

> Silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object iswell matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters
>
> Source: [Wikipedia](https://en.wikipedia.org/wiki/Silhouette_(clustering))

The silhouette score is therefore a measure of the trade-off between **cluster cohesion** (to what extent observations within a cluster are homogeneous) and **cluster separation** (to what extent clusters are distinct from one another).

For each observation $i$, the silhouette of the point is:

$$
s(i) = \frac{b(i)-a(i)}{\max(a(i),b(i))}
$$

where $a(i)$ is the average distance between $i$ and the other points in its own cluster (a measure of cohesion) and $b(i)$ is the smallest average distance between $i$ and the points of another cluster (a measure of separation).

The value \(s(i)\) lies between **-1** and **1**:

- **$s(i) \approx 1$** : $a(i) \ll b(i)$  
  The point is well assigned to its cluster: it is close to the points in its cluster and far from the others.

- **$s(i) \approx 0$** : $a(i) \approx b(i)$  
  The point lies on the boundary between two clusters: separation is weak locally.

- **$s(i) < 0$** : $a(i) > b(i)$
  The point is probably misassigned: on average, it is closer to another cluster than to its own.

The silhouette score is the average of the points’ silhouette values.

:::
::::


:::: {.content-visible when-profile="fr"}
::: {.callout-tip}
## Exercice: déterminer le nombre optimal de partition par la méthode du coude

1. Evaluer l'inertie et la distorsion en jouant sur le nombre de _clusters_ (de 1 à 9).
2. Représenter graphiquement et interpréter

:::
::::

:::: {.content-visible when-profile="en"}
::: {.callout-tip}
## Exercise: determine the optimal number of clusters using the elbow method

1. Evaluate inertia and distortion by varying the number of _clusters_ (from 1 to 9).
2. Plot the results and interpret them.

:::
::::



```{python}
#| echo: true
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]

df2 = votes.loc[:, xvars].dropna()
X = df2.copy()
```

```{python}
# Question 1
from scipy.spatial.distance import cdist

distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
K = range(1, 10)

X = df2.loc[:, xvars]

for k in K:
    kmeanModel = KMeans(n_clusters=k, random_state=42).fit(X)
    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)**2) / X.shape[0])
    inertias.append(kmeanModel.inertia_)


performance_analysis = pd.DataFrame(
    {
        "K": range(1, 10),
        "distorsions": distortions,
        "inertias": inertias
    }
)
```

```{python}
def represent_performance(
  df_performance: pd.DataFrame, metric: str
):

  kmin = int(df_performance["K"].min())
  kmax = int(df_performance["K"].max())

  p = (
      ggplot(df_performance, aes(x="K", y=metric))
      + geom_line()
      + geom_point()
      + theme_minimal()
      + theme(
          axis_text_y=element_blank(),
          axis_ticks_major_y=element_blank(),
          axis_title_y=element_blank()
      )
      + scale_x_continuous(breaks=list(range(kmin, kmax + 1)))
  )

  return p
```

:::: {.content-visible when-profile="fr"}
L'inertie de notre modèle est la suivante:
::::

```{python}
represent_performance(performance_analysis, "inertias")
```

:::: {.content-visible when-profile="fr"}
Là où la distorsion suit la courbe 
::::

```{python}
represent_performance(performance_analysis, "distorsions")
```


:::: {.content-visible when-profile="fr"}
## Autres méthodes de clustering

Il existe de nombreuses autres méthodes de clustering. Parmi les plus connues, on peut citer trois exemples en particulier :

- Le clustering ascendant hiérarchique ;
- DBSCAN ;
- Les mélanges de Gaussiennes.

### Clustering Ascendant Hiérarchique (CAH)

Quel est le principe ?

- On commence par calculer la dissimilarité entre nos _N_ individus, _i.e._ leur distance deux à deux dans l'espace de nos variables.
- Puis on regroupe les deux individus dont le regroupement minimise un critère d'agrégation donné, créant ainsi une classe comprenant ces deux individus.
- On calcule ensuite la dissimilarité entre cette classe et les _N-2_ autres individus en utilisant le critère d'agrégation.
- Puis on regroupe les deux individus ou classes d'individus dont le regroupement minimise le critère d'agrégation.
- On continue ainsi jusqu'à ce que tous les individus soient regroupés.

Ces regroupements successifs produisent un arbre binaire de classification (dendrogramme), dont la racine correspond à la classe regroupant l'ensemble des individus. Ce dendrogramme représente une hiérarchie de partitions. On peut alors choisir une partition en tronquant l'arbre à un niveau donné, le niveau dépendant soit des contraintes de l'utilisateur, soit de critères plus objectifs.

Plus d'informations [ici](https://www.xlstat.com/fr/solutions/fonctionnalites/classification-ascendante-hierarchique-cah).

### DBSCAN

L'[algorithme DBSCAN](https://fr.wikipedia.org/wiki/DBSCAN) est implémenté dans `sklearn.cluster`.
Il peut être utilisé pour faire de la détection d'anomalies notamment.
En effet, cette méthode repose sur le clustering en régions où la densité
des observations est continue, grâce à la notion de voisinage selon une certaine distance epsilon.
Pour chaque observation, on va regarder si dans son voisinage selon une distance epsilon, il y a des voisins. S'il y a au
moins `min_samples` voisins, alors l'observation sera une *core instance*.

Les observations qui ne sont pas des *core instances* et qui n'en ont pas dans leur voisinage selon une distance epsilon
vont être détectées comme des anomalies.

### Les mélanges de gaussiennes

En ce qui concerne la théorie, voir le cours [Probabilités numériques et statistiques computationnelles, M1 Jussieu, V.Lemaire et T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).
Se référer notamment aux notebooks pour l'algorithme EM pour mélange gaussien.

Dans `sklearn`, les mélanges gaussiens sont implémentés dans `sklearn.mixture` comme `GaussianMixture`.
Les paramètres importants sont alors le nombre de gaussiennes `n_components` et le nombre d'initialisations `n_init`.
Il est possible de faire de la détection d'anomalies avec les mélanges de gaussiennes.

::: {.callout-note}
## Pour aller plus loin

Il existe de nombreuses autres méthodes de clustering :

- Local outlier factor ;
- Bayesian gaussian mixture models ;
- D'autres méthodes de clustering hiérarchique ;
- Etc.
:::
::::

:::: {.content-visible when-profile="en"}
## Other Clustering Methods

There are many other clustering methods. Among the most well-known, here are three notable examples:

- Hierarchical Agglomerative Clustering (HAC);
- DBSCAN;
- Gaussian Mixture Models.

### Hierarchical Agglomerative Clustering (HAC)

What is the principle?

- Start by calculating the dissimilarity between our _N_ individuals, i.e., their pairwise distances in the variable space.
- Then, group the two individuals whose grouping minimizes a given aggregation criterion, thus creating a class containing these two individuals.
- Next, calculate the dissimilarity between this class and the _N-2_ other individuals using the aggregation criterion.
- Then, group the two individuals or classes of individuals whose grouping minimizes the aggregation criterion.
- Continue until all individuals are grouped.

These successive groupings produce a binary classification tree (dendrogram), where the root corresponds to the class grouping all individuals. This dendrogram represents a hierarchy of partitions. A partition can be chosen by truncating the tree at a certain level, based on either user constraints or more objective criteria.

More information [here](https://www.xlstat.com/en/solutions/features/hierarchical-clustering-hac).

### DBSCAN

The [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN) is implemented in `sklearn.cluster`.
It can be used notably for anomaly detection.
This method is based on clustering in regions of continuous observation density using the concept of neighborhood within a certain epsilon distance.
For each observation, it is checked whether there are neighbors within its epsilon-distance neighborhood. If there are at
least `min_samples` neighbors, the observation will be a *core instance*.

Observations that are not *core instances* and have no *core instances* in their epsilon-distance neighborhood
will be detected as anomalies.

### Gaussian Mixture Models

For theoretical insights, see the course [Probabilistic and Computational Statistics, M1 Jussieu, V.Lemaire and T.Rebafka](https://perso.lpsm.paris/~rebafka/#enseignement).
Refer especially to the notebooks for the EM algorithm for Gaussian mixture models.

In `sklearn`, Gaussian mixture models are implemented in `sklearn.mixture` as `GaussianMixture`.
Key parameters include the number of Gaussians `n_components` and the number of initializations `n_init`.
Gaussian mixture models can also be used for anomaly detection.

::: {.callout-note}
## Going Further

There are many other clustering methods:

- Local Outlier Factor;
- Bayesian Gaussian Mixture Models;
- Other Hierarchical Clustering Methods;
- Etc.
:::
::::

:::: {.content-visible when-profile="fr"}
# L'Analyse en composantes principales (ACP)

## Pour la visualisation de clusters

La méthode la plus simple pour visualiser les _clusters_, peu importe la méthode avec laquelle ils ont été obtenus, serait de représenter chaque individu dans l'espace à _N_ dimensions des variables de la table, et colorier chaque individu en fonction de son cluster.
On pourrait alors bien différencier les variables les plus discrimantes et les différents groupes.
Un seul problème ici : dès que _N > 3_, nous avons du mal à représenter le résultat de façon intelligible...

C'est là qu'intervient l'**Analyse en Composantes Principales** ([ACP](https://www.xlstat.com/fr/solutions/fonctionnalites/analyse-en-composantes-principales-acp)), qui permet de projeter notre espace à haute dimension dans un espace de dimension plus petite.
La contrainte majeure de la projection est de pouvoir conserver le maximum d'information (mesurée par la variance totale de l'ensemble de données) dans notre nombre réduit de dimensions, appelées composantes principales.
En se limitant à 2 ou 3 dimensions, on peut ainsi se représenter visuellement les relations entre les observations avec une perte de fiabilité minimale.

On peut généralement espérer que les clusters déterminés dans notre espace à N dimensions se différencient bien sur notre projection par ACP, et que la composition des composantes principales en fonction des variables initiales permette d'interpréter les clusters obtenus.
En effet, la combinaison linéaire des colonnes donnant naissance à nos nouveaux axes a souvent un "sens" dans le monde réel :

- Soit parce qu'une petite poignée de variables représente la majorité de la composante ;
- Soit parce que la plupart des colonnes intervenant dans la composante sommée se combinent bien pour former une interprétation naturelle.

Pour mettre en pratique les méthodes de création de clusters, de la base brute jusqu'à la visualisation par ACP, vous pouvez consulter la partie 2 du sujet 3 du funathon 2023, *Explorer les habitudes alimentaires de nos compatriotes*, sur le [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) ou sur [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).

## Pour la réduction de dimension

L'ACP est également très utile dans le champ de la réduction du nombre de variables pour de nombreux types de modélisations, comme par exemple les régressions linéaires.
Il est ainsi possible de projeter l'espace des variables explicatives dans un espace de dimension donnée plus faible, pour notamment limiter les risques d'_overfitting_.

L'inconvénient de cette approche est qu'elle rend les données utilisées en entrée du modèle moins interprétables qu'avec un LASSO puisque cette dernière technique sélectionne des variables là où la PCA sélectionne des combinaisons linéaires de nos variables. 

## Exemple

Reprenons nos données précédentes. Avant de faire une analyse en composante principale, dont l'objectif est de synthétiser des sources de variabilité dans nos données, il est conseillé de standardiser les variables lorsque celles-ci ont des échelles différentes (ce qui est le cas dans notre cas). 
::::

:::: {.content-visible when-profile="en"}
# Principal Component Analysis (PCA)

## For Cluster Visualization

The simplest method to visualize _clusters_, regardless of how they were obtained, would be to represent each individual in the _N_-dimensional space of the table's variables, coloring each individual based on their cluster.
This would clearly differentiate the most discriminating variables and the various groups.
One issue here: as soon as _N > 3_, it becomes difficult to represent the results intelligibly...

This is where **Principal Component Analysis** ([PCA](https://www.xlstat.com/en/solutions/features/principal-component-analysis-pca)) comes into play, allowing us to project our high-dimensional space into a smaller-dimensional space.
The major constraint of this projection is to retain the maximum amount of information (measured by the total variance of the dataset) within our reduced number of dimensions, called principal components.
By limiting to 2 or 3 dimensions, we can visually represent relationships between observations with minimal loss of reliability.

We can generally expect that clusters determined in our N-dimensional space will differentiate well in our PCA projection and that the composition of the principal components based on the initial variables will help interpret the obtained clusters.
Indeed, the linear combination of columns creating our new axes often has "meaning" in the real world:

- Either because a small handful of variables represent the majority of the component;
- Or because most columns contributing to the summed component combine well to form a natural interpretation.

To practice cluster creation methods, from raw data to PCA visualization, refer to part 2 of subject 3 in the 2023 funathon, *Explore the eating habits of our compatriots*, on [SSP Cloud](https://www.sspcloud.fr/formation?search=funath&path=%5B%22Funathon%202023%22%5D) or on [Github](https://github.com/InseeFrLab/funathon2023_sujet3/).

## For Dimensionality Reduction

PCA is also very useful in reducing the number of variables for many types of modeling, such as linear regression.
It is possible to project the space of explanatory variables into a lower-dimensional space, specifically to limit the risks of _overfitting_.

The drawback of this approach is that it makes the data used as input for the model less interpretable compared to LASSO, as the latter selects variables, while PCA selects linear combinations of our variables.

## Example

Let us revisit our previous data. Before performing a principal component analysis, whose objective is to synthesize sources of variability in our data, it is advisable to standardize the variables when they have different scales (which is the case here).
::::

```{python}
from sklearn.preprocessing import StandardScaler

X = df2.drop(['per_gop'], axis=1)
y = votes['winner']

print('Dimensions des données avant PCA : {}'.format(X.shape))
```

:::: {.content-visible when-profile="fr"}
Faisons déjà un premier test en réduisant nos données à deux composantes, c'est-à-dire à deux combinaisons linéaires de celles-ci. Il s'agit d'une méthode implémentée en `Scikit`, très pratique. Le faire à la main serait pénible.
::::

:::: {.content-visible when-profile="en"}
Let us start with a preliminary test by reducing our data to two components, that is, two linear combinations of the data. This is a very practical method implemented in `Scikit`. Doing it manually would be cumbersome.
::::



```{python}
#| echo: true
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

n_components = 2
pca = PCA(n_components=n_components)
```

:::: {.content-visible when-profile="fr"}
<details>

<summary>
Faire une PCA à la main (exercice éducatif mais peu utile dans la vraie vie)
</summary>
::::

:::: {.content-visible when-profile="en"}
<details>

<summary>
Perform PCA manually (an educational exercise but not very practical in real life)
</summary>
::::

```{python}
#| echo: true

# Step 1: Compute the covariance matrix
cov_matrix = np.cov(X_standardized.T)

# Step 2: Perform eigen decomposition
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 3: Sort eigenvectors by eigenvalues
sorted_indices = np.argsort(-eigenvalues)
eigenvectors_sorted = eigenvectors[:, sorted_indices]

first_component_manual = eigenvectors_sorted[:, 0]
first_component_manual
```

</details>


:::: {.content-visible when-profile="fr"}
On peut utiliser notre méthode `fit_transform` pour calculer les paramètres utiles de notre PCA, à savoir les poids à utiliser pour reprojeter nos variables dans l'espace des composantes :
::::

:::: {.content-visible when-profile="en"}
We can use our `fit_transform` method to calculate the useful parameters of our PCA, namely the weights to use for reprojecting our variables into the component space:
::::


```{python}
#| echo: true

x_2d = pca.fit_transform(X_standardized)
columns=[f'component_{i}' for i in range(1, n_components + 1)]
df_pca = pd.DataFrame(x_2d, columns=columns)
df_pca['classe'] = y
print('Dimensions des données après PCA : {}'.format(x_2d.shape))
df_pca
```

:::: {.content-visible when-profile="fr"}
Ces composantes ne sont plus interprétables directement. Il s'agit d'une combinaison linéaire de nos variables. Prenons le premier axe pour s'en assurer :
::::

:::: {.content-visible when-profile="en"}
These components are no longer directly interpretable. They are linear combinations of our variables. Let us take the first axis to confirm:
::::

```{python}
#| echo: true
np.dot(X_standardized, pca.components_[0])
```

:::: {.content-visible when-profile="fr"}
Pourquoi rendre nos données moins interprétables ? Parce qu'avec seulement deux colonnes, on va synthétiser beaucoup plus d'information, c'est-à-dire capturer beaucoup plus de variance de nos données, qu'avec nos données brutes.

La variance expliquée par chaque composante est la suivante :
::::

:::: {.content-visible when-profile="en"}
Why make our data less interpretable? Because with just two columns, we will synthesize much more information, meaning we will capture much more variance in our data than with the raw data.

The variance explained by each component is as follows:
::::


```{python}
#| echo: true
tableau_variance = pd.DataFrame(
  {"Axe": [f"Composante {i+1}" for i in range(2)],
  "Variance expliquée (%)": pca.explained_variance_ratio_*100}
)
tableau_variance
```

:::: {.content-visible when-profile="fr"}
Avec deux axes, on capture donc une bonne partie de notre variance :
::::

:::: {.content-visible when-profile="en"}
With two axes, we capture a significant portion of our variance:
::::

```{python}
#| echo: true
tableau_variance["Variance expliquée (%)"].sum()
```

:::: {.content-visible when-profile="fr"}
Le premier axe capture une part importante de la variance, le deuxième axe étant déjà beaucoup moins explicatif. Ceci est attendu puisque les axes des PCA capturent une part décroissante de la variance.

Ici nous avions fixé le nombre d'axes principaux à 2. Comment choisir ce nombre en pratique ? Comme précédemment pour les _k-means_, le critère du coude est fréquemment utilisé. Représentons la part de variance expliquée en fonction du nombre d'axes :
::::

:::: {.content-visible when-profile="en"}
The first axis captures a significant portion of the variance, while the second axis is already much less explanatory. This is expected since PCA axes capture a decreasing portion of the variance.

Here, we set the number of principal axes to 2. How do we choose this number in practice? As with _k-means_, the elbow criterion is frequently used. Let us represent the proportion of variance explained as a function of the number of axes:
::::


```{python}
#| echo: true
import plotly.express as px
pca = PCA()
pca.fit(X_standardized)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

px.area(
    x=range(1, exp_var_cumul.shape[0] + 1),
    y=exp_var_cumul,
    labels={"x": "# Components", "y": "Explained Variance"}
)
```

:::: {.content-visible when-profile="fr"}
Les coudes sont peu francs, on peut donc choisir deux ou trois axes. Si on préfère utiliser un seuil de variance expliquée dans notre analyse, on utilisera plutôt l'option `n_components` de `Scikit`. Par exemple, si on désire conserver les axes permettant d'expliquer 90% de la variabilité de nos données :
::::

:::: {.content-visible when-profile="en"}
The elbows are not very distinct, so we can choose two or three axes. If we prefer to use a threshold for explained variance in our analysis, we can use the `n_components` option in `Scikit`. For example, if we want to retain the axes that explain 90% of the variability in our data:
::::


```{python}
#| echo: true
pca = PCA(n_components=0.9)
pca.fit(X_standardized)

print(pca.explained_variance_ratio_)
```


:::: {.content-visible when-profile="fr"}
# Références {-}
::::

:::: {.content-visible when-profile="en"}
# References {-}
::::