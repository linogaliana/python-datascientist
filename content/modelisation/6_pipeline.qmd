---
title: "Premier pas vers l'industrialisation avec les pipelines scikit"
date: 2023-10-20T13:00:00Z
weight: 60
slug: pipeline-scikit
tags:
  - scikit
  - Machine Learning
  - Pipeline
  - Modelisation
  - Tutorial
categories:
  - Mod√©lisation
  - Tutoriel
description: |
  Les _pipelines_ `Scikit` permettent d'int√©grer de mani√®re tr√®s flexible
  un ensemble d'op√©rations de pre-processing et d'entra√Ænement de mod√®les
  dans une cha√Æne d'op√©rations. Il s'agit d'une approche particuli√®rement
  appropri√©e pour r√©duire la difficult√© √† changer d'algorithme ou pour
  faciliter la r√©-application d'un code √† de nouvelles donn√©es.
echo: false
image: featured.png
bibliography: ../../reference.bib
---


::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/modelisation/6_pipeline.qmd")
```
:::


Ce chapitre pr√©sente la premi√®re application
d'une journ√©e de cours que j'ai
donn√© √† l'Universit√© Dauphine dans le cadre
des _PSL Data Week_. 


<details>
<summary>
D√©rouler les _slides_ associ√©es ci-dessous ou [cliquer ici](https://linogaliana.github.io/dauphine-week-data/#/title-slide)
pour les afficher en plein √©cran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://linogaliana.github.io/dauphine-week-data/#/title-slide"></iframe></div>

</details>



Pour lire les donn√©es de mani√®re efficace, nous 
proposons d'utiliser le _package_ `duckdb`. 
Pour l'installer, voici la commande : 

```{python}
#| output: false
#| echo: true
!pip install duckdb
```

## Pourquoi utiliser les _pipelines_ ?

### D√©finitions pr√©alables

Ce chapitre nous am√®nera √† explorer plusieurs √©cosyst√®mes, pour lesquels on retrouve quelques buzz-words dont voici les d√©finitions :

| Terme | D√©finition |
|-------|------------|
| _DevOps_ | Mouvement en ing√©nierie informatique et une pratique technique visant √† l‚Äôunification du d√©veloppement logiciel (dev) et de l‚Äôadministration des infrastructures informatiques (ops) |
| _MLOps_ | Ensemble de pratiques qui vise √† d√©ployer et maintenir des mod√®les de machine learning en production de mani√®re fiable et efficace |

Ce chapitre fera des r√©f√©rences r√©guli√®res au cours
de 3e ann√©e de l'ENSAE 
[_"Mise en production de projets data science"_](https://ensae-reproductibilite.github.io/website/).


### Objectif

Les chapitres pr√©c√©dents ont permis de montrer des bouts de code 
√©pars pour entra√Æner des mod√®les ou faire du _preprocessing_.
Cette d√©marche est int√©ressante pour t√¢tonner mais risque d'√™tre co√ªteuse
ult√©rieurement s'il est n√©cessaire d'ajouter une √©tape de _preprocessing_
ou de changer d'algorithme.

Les _pipelines_ sont pens√©s pour simplifier la mise en production 
ult√©rieure d'un mod√®le de _machine learning_. 
Ils sont au coeur de la d√©marche de _MLOps_ qui est
pr√©sent√©e
dans le cours de 3e ann√©e de l'ENSAE
de [_"Mise en production de projets data science"_](https://ensae-reproductibilite.github.io/website/),
qui vise √† simplifier la mise en oeuvre op√©rationnelle de
projets utilisant des techniques de _machine learning_.  

```{python}
#| echo: true

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
```

### Les _pipelines_ `Scikit`

Heureusement, `Scikit` propose un excellent outil pour proposer un cadre
g√©n√©ral pour cr√©er une cha√Æne de production *machine learning*. Il
s'agit des
[_pipelines_](https://scikit-learn.org/stable/modules/compose.html). 
Ils pr√©sentent de nombreux int√©r√™ts, parmi lesquels :

* Ils sont tr√®s __pratiques__ et __lisibles__. On rentre des donn√©es en entr√©e, on n'appelle qu'une seule fois les m√©thodes `fit` et `predict` ce qui permet de s'assurer une gestion coh√©rente des transformations de variables, par exemple apr√®s l'appel d'un `StandardScaler` ;
* La __modularit√©__ rend ais√©e la mise √† jour d'un pipeline et renforce la capacit√© √† le r√©utiliser ;
* Ils permettent de facilement chercher les hyperparam√®tres d'un mod√®le. Sans *pipeline*, √©crire un code qui fait du *tuning* d'hyperparam√®tres peut √™tre p√©nible. Avec les *pipelines*, c'est une ligne de code ;
* La __s√©curit√©__ d'√™tre certain que les √©tapes de preprocessing sont bien appliqu√©es aux jeux de donn√©es d√©sir√©s avant l'estimation. 





::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```
Un des int√©r√™ts des *pipelines* scikit est qu'ils fonctionnent aussi avec
des m√©thodes qui ne sont pas issues de `scikit`.

Il est possible d'introduire un mod√®le de r√©seau de neurone `Keras` dans
un pipeline `scikit`.
Pour introduire un mod√®le √©conom√©trique `statsmodels`
c'est un peu plus co√ªteux mais nous allons proposer des exemples
qui peuvent servir de mod√®le et qui montrent que c'est faisable 
sans trop de difficult√©.
```{=html}
</div>
```
:::



## Comment cr√©er un *pipeline*

Un *pipeline* est un encha√Ænement d'op√©rations qu'on code en enchainant
des pairs *(cl√©, valeur)* :

* la cl√© est le nom du pipeline, cela peut √™tre utile lorsqu'on va
repr√©senter le *pipeline* sous forme de diagramme acyclique (visualisation DAG)
ou qu'on veut afficher des informations sur une √©tape
* la valeur repr√©sente la transformation √† mettre en oeuvre dans le *pipeline*
(c'est-√†-dire, √† l'exception de la derni√®re √©tape, 
mettre en oeuvre une m√©thode `transform` et √©ventuellement une
transformation inverse).


```{python}
#| echo: true

from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA

estimators = [('reduce_dim', PCA()), ('clf', SVC())]
pipe = Pipeline(estimators)
pipe
```



Au sein d'une √©tape de *pipeline*, les param√®tres d'un estimateur
sont accessibles avec la notation `<estimator>__<parameter>`.
Cela permet de fixer des valeurs pour les arguments des fonctions `scikit`
qui sont appel√©es au sein d'un *pipeline*. 
C'est cela qui rendra l'approche des pipelines particuli√®rement utile
pour la *grid search* :

```{python}
#| echo: true

from sklearn.model_selection import GridSearchCV
param_grid = {"reduce_dim__n_components":[2, 5, 10], "clf__C":[0.1, 10, 100]}
grid_search = GridSearchCV(pipe, param_grid=param_grid)
grid_search
```

Ces _pipelines_ sont initialis√©s sans donn√©es, il s'agit d'une structure formelle
que nous allons ensuite ajuster en entra√Ænant des mod√®les. 

### Donn√©es utilis√©es

Nous allons utiliser les donn√©es
de transactions immobili√®res [DVF](https://app.dvf.etalab.gouv.fr/) pour chercher
la meilleure mani√®re de pr√©dire, sachant les caract√©ristiques d'un bien, son
prix.

Ces donn√©es sont mises √† disposition
sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/).
N√©anmoins, le format csv n'√©tant pas pratique pour importer des jeux de donn√©es
volumineux, nous proposons de privil√©gier la version `Parquet` mise √† 
disposition par Eric Mauvi√®re sur [`data.gouv`](https://www.data.gouv.fr/fr/datasets/dvf-2022-format-parquet/#/discussions).
L'approche la plus efficace pour lire ces donn√©es est
d'utiliser `DuckDB` afin de lire le fichier, extraire les colonnes
d'int√©r√™t puis passer √† `Pandas` (pour en savoir plus sur
l'int√©r√™t de `DuckDB` pour lire des fichiers volumineux, vous pouvez
consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/) ou
[celui-ci](https://www.icem7.fr/outils/3-explorations-bluffantes-avec-duckdb-1-interroger-des-fichiers-distants/) √©crit
par Eric Mauvi√®re). 

M√™me si, en soi, les gains de temps sont faibles car `DuckDB` optimise
les requ√™tes HTTPS n√©cessaires √† l'import des donn√©es, nous proposons 
de t√©l√©charger les donn√©es pour r√©duire les besoins de bande passante. 

```{python}
#| echo: true
#| eval: false
import requests
import os

url = "https://www.data.gouv.fr/fr/datasets/r/56bde1e9-e214-408b-888d-34c57ff005c4"
file_name = "dvf.parquet"

# Check if the file already exists
if not os.path.exists(file_name):
    response = requests.get(url)

    if response.status_code == 200:
        with open(file_name, "wb") as f:
            f.write(response.content)
        print("T√©l√©chargement r√©ussi.")
    else:
        print(f"√âchec du t√©l√©chargement. Code d'√©tat : {response.status_code}")
else:
    print(f"Le fichier '{file_name}' existe d√©j√†. Aucun t√©l√©chargement n√©cessaire.")
```

En premier lieu, puisque cela va faciliter les requ√™tes SQL ult√©rieures, on cr√©e
une vue : 

```{python}
#| echo: false

import duckdb
# version remote
url = "https://www.data.gouv.fr/fr/datasets/r/56bde1e9-e214-408b-888d-34c57ff005c4"
duckdb.sql(f'CREATE OR REPLACE VIEW dvf AS SELECT * FROM read_parquet("{url}")')
```


```{python}
#| echo: true
#| eval: false
import duckdb
duckdb.sql(f'CREATE OR REPLACE VIEW dvf AS SELECT * FROM read_parquet("dvf.parquet")')
```

Les donn√©es prennent la forme suivante :

```{python}
#| echo: true

duckdb.sql(f"SELECT * FROM dvf LIMIT 5")
```

Les variables que nous allons conserver sont les suivantes,
nous allons les reformater pour la suite de l'exercice.

```{python}
#| echo: true

xvars = [
    "Date mutation", "Valeur fonciere",
    'Nombre de lots', 'Code type local',
    'Nombre pieces principales'
]
xvars = ", ".join([f'"{s}"' for s in xvars])
```

```{python}
#| echo: true

mutations = duckdb.sql(
    f'''
    SELECT
    date_part('month', "Date mutation") AS month,
    substring("Code postal", 1, 2) AS dep,
    {xvars},
    COLUMNS('Surface Carrez.*')
    FROM dvf
    '''
).to_df()

colonnes_surface = mutations.columns[mutations.columns.str.startswith('Surface Carrez')]
mutations.loc[:, colonnes_surface] = mutations.loc[:, colonnes_surface].replace({',': '.'}, regex=True).astype(float).fillna(0)
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Le fichier `Parquet` mis √† disposition sur `data.gouv` pr√©sente une incoh√©rence de mise en forme de
certaines colonnes √† cause des virgules qui emp√™chent le formattage sous forme de colonne
num√©rique.

Le code ci-dessus effectue la conversion ad√©quate au niveau de `Pandas`. 

```{=html}
</div>
```
:::


```{python}
#| echo: true

mutations.head(2)
```

::: {.cell .markdown}
<details>
<summary>
Introduire un effet confinement
</summary>

Si vous travaillez avec les donn√©es de 2020, n'oubliez pas
d'int√©grer l'effet
confinement dans vos mod√®les puisque cela a lourdement
affect√© les possibilit√©s de transaction sur cette p√©riode, donc
l'effet potentiel de certaines variables explicatives du prix. 

Pour introduire cet effet, vous pouvez cr√©er une variable
indicatrice entre les dates en question:

```python
mutations['confinement'] = (
    mutations['Date mutation']
    .between(pd.to_datetime("2020-03-17"), pd.to_datetime("2020-05-03"))
    .astype(int)
)
```

Comme nous travaillons sur les donn√©es de 2022,
nous pouvons nous passer de cette variable. 

</details>
:::


Les donn√©es DVF proposent une observation par transaction.
Ces transactions
peuvent concerner plusieurs lots. Par exemple, un appartement
avec garage et cave comportera trois lots. 

Pour simplifier,
on va cr√©er une variable de surface qui agr√®ge les diff√©rentes informations
de surface disponibles dans le jeu de donn√©es.
Les agr√©ger revient √† supposer que le mod√®le de fixation des prix est le m√™me
entre chaque lot. C'est une hypoth√®se simplificatrice qu'une personne plus 
experte du march√© immobilier, ou qu'une approche propre de s√©lection
de variable pourrait amener √† nier. En effet, les variables
en question sont faiblement corr√©l√©es les unes entre elles, √† quelques
exceptions pr√®s (@fig-corr-surface):


```{python}
#| echo: true
#| output: false

corr = mutations.loc[
    :,
    mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()
]
corr.columns = corr.columns.str.replace("Carrez du ", "")
corr = corr.corr()

mask = np.triu(np.ones_like(corr, dtype=bool))

cmap = sns.diverging_palette(230, 20, as_cmap=True)
```

```{python}
#| echo: true
#| fig-cap: Matrice de corr√©lation des variables de surface
#| label: fig-corr-surface

fig, ax = plt.subplots(1)
g = sns.heatmap(
    corr, ax=ax, 
    mask=mask,
    vmax=.3, center=0,
    square=True, linewidths=.5, cbar_kws={"shrink": .5},
    xticklabels=corr.columns.values,
    yticklabels=corr.columns.values, cmap=cmap, annot=True, fmt=".2f"
)
g
```

```{python}
#| echo: true

mutations['lprix'] = np.log(mutations["Valeur fonciere"])
mutations['surface'] = mutations.loc[:, colonnes_surface].sum(axis = 1).astype(int)
```



```{python}
#| echo: true

mutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)
```



## Un premier pipeline : *random forest* sur des variables standardis√©es

Notre premier *pipeline* va nous permettre d'int√©grer ensemble:

1. Une √©tape de *preprocessing* avec la standardisation de variables
2. Une √©tape d'estimation du prix en utilisant un mod√®le de *random forest*

Pour le moment, on va prendre comme acquis un certain nombre de variables
explicatives (les *features*) et les hyperparam√®tres du mod√®le.

L'algorithme des _random forest_ est une technique statistique bas√©e sur
les arbres de d√©cision. Elle a √©t√© d√©finie explicitement par l'un
des pionniers du _machine learning_, @breiman2001random. 
Il s'agit d'une [m√©thode ensembliste](https://en.wikipedia.org/wiki/Ensemble_learning)
puisqu'elle consiste √† utiliser plusieurs algorithmes (en l'occurrence des arbres
de d√©cision) pour obtenir une meilleure pr√©diction que ne le permettraient
chaque mod√®le isol√©ment.  

Les _random forest_ sont une m√©thode d'aggr√©gation[^2] d'arbres de d√©cision. 
On calcule $K$ arbres de d√©cision et en tire, par une m√©thode d'agr√©gation,
une r√®gle de d√©cision moyenne qu'on va appliquer pour tirer une
pr√©diction de nos donn√©es. 

[^2]: Les _random forest_ sont l'une des principales m√©thodes
ensemblistes. Outre cette approche, les plus connues sont
le [_bagging_ (_boostrap aggregating_)](https://en.wikipedia.org/wiki/Bootstrap_aggregating) et le _boosting_
qui consistent √† choisir la pr√©diction √† privil√©gier
selon des algorithmes de choix diff√©rens.
Par exemple le _bagging_ est une technique bas√©e sur le vote majoritaire [@breiman1996bagging]. 
Cette technique s'inspire du _bootstrap_ qui, en √©conom√©trie,
consiste √† r√©-estimer sur *K* sous-√©chantillons
al√©atoires des donn√©es un estimateur afin d'en tirer, par exemple, un intervalle
de confiance empirique √† 95%. Le principe du _bagging_ est le m√™me. On r√©-estime
_K_ fois notre estimateur (par exemple un arbre de d√©cision) et propose une 
r√®gle d'agr√©gation pour en tirer une r√®gle moyennis√©e et donc une pr√©diction. 
Le _boosting_ fonctionne selon un principe diff√©rent, bas√© sur
l'optimisation de combinaisons de classifieurs faibles. 

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jE1Cb1Dc_p9WEOPMkC95WQ.png)

L'un des int√©r√™ts
des *random forest* est qu'il existe des m√©thodes pour d√©terminer 
l'importance relative de chaque variable dans la pr√©diction. 


Nous allons ici partir d'un *random forest* avec des valeurs d'hyperparam√®tres
donn√©es, √† savoir la profondeur de l'arbre. 

### D√©finition des ensembles _train_ et _test_

Nous allons donc nous restreindre √† un sous-ensemble de colonnes dans un
premier temps.

Nous allons √©galement ne conserver que les
transactions inf√©rieures √† 5 millions
d'euros (on anticipe que celles ayant un montant sup√©rieur sont des transactions
exceptionnelles dont le m√©canisme de fixation du prix diff√®re)

```{python}
#| echo: true

mutations2 = mutations.drop(
    colonnes_surface.tolist() + ["Date mutation", "lprix"], # ajouter "confinement" si donn√©es 2020
    axis = "columns"
    ).copy()

mutations2 = mutations2.loc[mutations2['Valeur fonciere'] < 5e6] #keep only values below 5 millions

mutations2.columns = mutations2.columns.str.replace(" ", "_")
mutations2  = mutations2.dropna(subset = ['dep','Code_type_local','month'])
```

Notre _pipeline_ va incorporer deux types de variables: les variables
cat√©gorielles et les variables num√©riques. 
Ces diff√©rents types vont b√©n√©ficier d'√©tapes de _preprocessing_
diff√©rentes. 

```{python}
#| echo: true
numeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'month', 'Valeur_fonciere'])].tolist()
categorical_features = ['dep','Code_type_local','month']
```

Au passage, nous avons abandonn√© la variable de code postal pour privil√©gier
le d√©partement afin de r√©duire la dimension de notre jeu de donn√©es. Si on voulait
vraiment avoir un bon mod√®le, il faudrait faire autrement car le code postal
est probablement un tr√®s bon pr√©dicteur du prix d'un bien, une fois que
les caract√©ristiques du bien sont contr√¥l√©es.


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 1 : D√©coupage des √©chantillons</h3>
```

Nous allons stratifier notre √©chantillonage de _train/test_ par d√©partement
afin de tenir compte, de mani√®re minimale, de la g√©ographie. 
Pour acc√©l√©rer les calculs pour ce tutoriel, nous n'allons consid√©rer que
30% des transactions observ√©es sur chaque d√©partement.

Voici le code pour le faire:


```python
mutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)
```

Avec la fonction ad√©quate de `Scikit`, faire un d√©coupage de `mutations2`
en _train_ et _test sets_
en suivant les consignes suivantes:

- 20% des donn√©es dans l'√©chantillon de _test_ ;
- L'√©chantillonnage est stratifi√© par d√©partements ;
- Pour avoir des r√©sultats reproductibles, choisir une racine √©gale √† 123.

```{=html}
</div>
```
:::


```{python}
from sklearn.model_selection import train_test_split

mutations2 = mutations2.groupby('dep').sample(frac = 0.1, random_state = 123)

X_train, X_test, y_train, y_test = train_test_split(
    mutations2.drop("Valeur_fonciere", axis = 1),
    mutations2[["Valeur_fonciere"]].values.ravel(),
    test_size = 0.2, random_state = 123, stratify=mutations2[['dep']]
)
```


### D√©finition du premier _pipeline_

Pour commencer, nous allons fixer la taille des arbres de d√©cision avec
l'hyperparam√®tre `max_depth = 2`. 

Notre _pipeline_ va int√©grer les √©tapes suivantes :

1. __Preprocessing__ :
    + Les variables num√©riques vont √™tre standardis√©es avec un `StandardScaler`.
Pour cela, nous allons utiliser la liste `numeric_features` d√©finie pr√©c√©demment.
    + Les variables cat√©gorielles vont √™tre explos√©es avec un *one hot encoding*
(m√©thode `OneHotEncoder` de `scikit`)
Pour cela, nous allons utiliser la liste `categorical_features`
2. __Random forest__ : nous allons appliquer l'estimateur _ad hoc_ de `Scikit`.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 2 : Construction d'un premier pipeline formel</h3>
```

1. Initialiser un _random forest_ de profondeur 2. Fixer la racine √† 123 pour avoir des r√©sultats reproductibles.
2. La premi√®re √©tape du _pipeline_ (nommer cette couche _preprocessor_) consiste √† appliquer les √©tapes de _preprocessing_ adapt√©es √† chaque type de variables:
    - Pour les variables num√©riques, appliquer une √©tape d'imputation √† la moyenne puis standardiser celles-ci
    - Pour les variables cat√©gorielles, appliquer un [_one hot encoding_](https://en.wikipedia.org/wiki/One-hot)
3. Appliquer comme couche de sortie le mod√®le d√©fini plus t√¥t.

_üí° Il est recommand√© de s'aider de la documentation de `Scikit`. Si vous avez besoin d'un indice suppl√©mentaire, consulter le pipeline pr√©sent√© ci-dessous._

```{=html}
</div>
```
:::



```{python}
#| label: exo2-q1
# Question 1
from sklearn.ensemble import RandomForestRegressor
regr = RandomForestRegressor(max_depth=2, random_state=123)
```



```{python}
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import make_column_transformer

numeric_pipeline = make_pipeline(
  SimpleImputer(),
  StandardScaler()
)
transformer = make_column_transformer(
    (numeric_pipeline, numeric_features),
    (OneHotEncoder(sparse = False, handle_unknown = "ignore"), categorical_features))
pipe = Pipeline(steps=[('preprocessor', transformer),
                      ('randomforest', regr)])
```

A l'issue de cet exercice, nous devrions obtenir le _pipeline_ suivant.

```{python}
pipe
```

Nous avons construit ce pipeline sous forme de couches successives. La couche
`randomforest` prendra automatiquement le r√©sultat de la couche `preprocessor`
en _input_. La couche `features` permet d'introduire de mani√®re relativement
simple (quand on a les bonnes m√©thodes) la complexit√© du *preprocessing*
sur donn√©es r√©elles dont les types divergent. 

A cette √©tape, rien n'a encore √©t√© estim√©. 
C'est tr√®s simple √† mettre en oeuvre
avec un _pipeline_. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 3 : Mise en oeuvre du pipeline</h3>
```

1. Estimer les param√®tres du mod√®le sur le jeu d'entra√Ænement
2. Observer la mani√®re dont les donn√©es d'entra√Ænement sont transform√©es
par l'√©tape de _preprocessing_ avec les m√©thodes ad√©quates sur 4 observations de `X_train`
tir√©es al√©atoirement
3. Utiliser ce mod√®le pour pr√©dire le prix sur l'√©chantillon de test. A partir de ces quelques pr√©dictions,
quel semble √™tre le probl√®me ?
4. Observer la mani√®re dont ce _preprocessing_ peut s'appliquer sur deux exemples fictifs :
    + Un appartement (`code_type_local = 2`) dans le 75, vendu au mois de mai, unique lot de la vente avec 3 pi√®ces, faisant 75m¬≤ ;
    + Une maison (`code_type_local = 1`) dans le 06, vendue en d√©cembre, dans une transaction avec 2 lots. La surface compl√®te est de 180m¬≤ et le bien comporte 6 pi√®ces. 
5. D√©duire sur ces deux exemples le prix pr√©dit par le mod√®le. 
6. Calculer et interpr√©ter le RMSE sur l'√©chantillon de test. Ce mod√®le est-il satisfaisant ? 

```{=html}
</div>
```
:::


```{python}
pipe.fit(X_train, y_train)
```

```{python}
#| output: false
# Question 2
pipe[:-1].transform(X_train.sample(4))
```

```{python}
# Question 4
pipe.predict(X_test)
```


```{python}
# Question 5
X_fictif = pd.DataFrame(
    {
        "month": [3, 12],
        "dep": ["75", "06"],
        "Nombre_de_lots": [1, 2],
        "Code_type_local": [2, 1],
        "Nombre_pieces_principales": [3., 6.],
        "surface": [75., 180.]
    }
)
pipe[:-1].transform(X_fictif)
pipe.predict(X_fictif)
```


```{python}
from sklearn.metrics import mean_squared_error

np.sqrt(
    mean_squared_error(
        pipe.predict(X_test),
        y_test
    )
)
```


### _Variable importance_
 

Les pr√©dictions semblent avoir une assez faible variance, comme si des variables
de seuils intervenaient. Nous allons donc devoir essayer de comprendre pourquoi. 

La _"variable importance"_ 
se r√©f√®re √† la mesure de l'influence de chaque variable d'entr√©e sur la performance du mod√®le. 
L'impuret√© fait r√©f√©rence √† l'incertitude ou √† l'entropie pr√©sente dans un ensemble de donn√©es.
Dans le contexte des _random forest_, cette mesure est souvent calcul√©e en √©valuant la r√©duction moyenne de l'impuret√© des n≈ìuds de d√©cision caus√©e par une variable sp√©cifique. 
Cette approche permet de quantifier l'importance des variables dans le processus de prise de d√©cision du mod√®le, offrant ainsi des intuitions sur les caract√©ristiques les plus informatives pour la pr√©diction (plus de d√©tails [sur ce blog](https://mljar.com/blog/feature-importance-in-random-forest/)). 

On ne va repr√©senter, parmi notre ensemble important de colonnes, que celles
qui ont une importance non nulle.


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 4 : Compr√©hension du mod√®le</h3>
```

1. R√©cup√©rer la _feature importance_ directement depuis la couche adapt√©e de votre _pipeline_
2. Utiliser le code suivant pour calculer l'intervalle de confiance de cette mesure d'importance:

```python
std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)
```

3. Repr√©senter les variables d'importance non nulle. Qu'en concluez-vous ?

```{=html}
</div>
```
:::


Le graphique d'importance des variables que vous devriez obtenir √† l'issue
de cet exercice est le suivant. 


```{python}
features_names = pipe[:-1].get_feature_names_out()
importances = pipe['randomforest'].feature_importances_
std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)

forest_importances = pd.DataFrame(importances, index=features_names, columns = ["mdi"])
forest_importances['std'] = std

fig, ax = plt.subplots()
forest_importances.loc[forest_importances['mdi']>0, 'mdi'].plot.bar(
    yerr = forest_importances.loc[forest_importances['mdi']>0, 'std'], ax = ax
)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()
```

```{python}
#| echo: false
ax
```

Les statistiques obtenues par le biais de cette _variable importance_
sont un peu rudimentaires mais permettent d√©j√† de comprendre
le probl√®me de notre mod√®le. 

On voit donc que deux de nos variables d√©terminantes sont des effets fixes
g√©ographiques (qui servent √† ajuster de la diff√©rence de prix entre
Paris et les Hauts de Seine et le reste de la France), une autre variable
est un effet fixe type de bien. Les deux variables qui pourraient introduire
de la variabilit√©, √† savoir la surface et, dans une moindre mesure, le 
nombre de lots, ont une importance moindre. 


::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```
Id√©alement, on utiliserait `Yellowbrick` pour repr√©senter l'importance des variables
Mais en l'√©tat actuel du *pipeline* on a beaucoup de variables dont le poids
est nul qui viennent polluer la visualisation. Vous pouvez 
consulter la
[documentation de `Yellowbrick` sur ce sujet](https://www.scikit-yb.org/en/latest/api/model_selection/importances.html)

```{=html}
</div>
```
:::

Les pr√©dictions peuvent nous sugg√©rer √©galement
qu'il y a un probl√®me:

```{python}
#| echo: true
compar = pd.DataFrame([y_test, pipe.predict(X_test)]).T
compar.columns = ['obs','pred']
compar['diff'] = compar.obs - compar.pred

g = sns.relplot(data = compar, x = 'obs', y = 'pred', color = "royalblue", alpha = 0.8)
g.set(ylim=(0, 2e6), xlim=(0, 2e6),
      title='Evaluating estimation error on test sample',
      xlabel='Observed values',
      ylabel='Predicted values')
g.ax.axline(xy1=(0, 0), slope=1, color="red", dashes=(5, 2))
```

## Restriction du champ du mod√®le

Mettre en oeuvre un bon mod√®le de prix au niveau France enti√®re
est complexe. Nous allons donc nous restreindre au champ suivant:
les appartements dans Paris.

```{python}
#| echo: true
mutations_paris = mutations.drop(
    colonnes_surface.tolist() + ["Date mutation", "lprix"], # ajouter "confinement" si donn√©es 2020
    axis = "columns"
    ).copy()

mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions

mutations_paris.columns = mutations_paris.columns.str.replace(" ", "_")
mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])
mutations_paris = mutations_paris.loc[mutations_paris['dep'] == "75"]
mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local'], axis = "columns")
mutations_paris.loc[mutations_paris['surface']>0]
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 4 : Pipeline plus simple</h3>
```

Reprendre les codes pr√©c√©dents et reconstruire notre _pipeline_ sur 
la nouvelle base en mettant en oeuvre une m√©thode de _boosting_
plut√¥t qu'une for√™t al√©atoire. 

```{=html}
</div>
```
:::

A l'issue de cet exercice, vous devriez avoir des _MDI_ proches
de celles-ci:


```{python}
#| label: estim-model-paris
from sklearn.ensemble import GradientBoostingRegressor

mutations_paris = mutations.drop(
    colonnes_surface.tolist() + ["Date mutation", "lprix"], # ajouter "confinement" si donn√©es 2020
    axis = "columns"
    ).copy()

mutations_paris = mutations_paris.loc[mutations_paris['Valeur fonciere'] < 5e6] #keep only values below 5 millions

mutations_paris.columns = mutations_paris.columns.str.replace(" ", "_")
mutations_paris  = mutations_paris.dropna(subset = ['dep','Code_type_local','month'])
mutations_paris = mutations_paris.loc[mutations_paris['dep'] == "75"]
mutations_paris = mutations_paris.loc[mutations_paris['Code_type_local'] == 2].drop(['dep','Code_type_local', 'Nombre_de_lots'], axis = "columns")
mutations_paris.loc[mutations_paris['surface']>0]


numeric_features = mutations_paris.columns[~mutations_paris.columns.isin(['month', 'Valeur_fonciere'])].tolist()
categorical_features = ['month']

reg = GradientBoostingRegressor(random_state=0)

numeric_pipeline = make_pipeline(
  SimpleImputer(),
  StandardScaler()
)
transformer = make_column_transformer(
    (numeric_pipeline, numeric_features),
    (OneHotEncoder(sparse = False, handle_unknown = "ignore"), categorical_features))
pipe = Pipeline(steps=[('preprocessor', transformer),
                      ('boosting', reg)])

X_train, X_test, y_train, y_test = train_test_split(
    mutations_paris.drop("Valeur_fonciere", axis = 1),
    mutations_paris[["Valeur_fonciere"]].values.ravel(),
    test_size = 0.2, random_state = 123
)

pipe.fit(X_train, y_train)

pd.DataFrame(
    pipe["boosting"].feature_importances_, 
    index = pipe[:-1].get_feature_names_out()
)
```



## Recherche des hyperparam√®tres optimaux avec une validation crois√©e

On d√©tecte que le premier mod√®le n'est pas tr√®s bon et ne nous aidera
pas vraiment √† √©valuer de mani√®re fiable l'appartement de nos r√™ves. 

On va essayer de voir si notre mod√®le ne serait pas meilleur avec des
hyperparam√®tres plus adapt√©s. Apr√®s tout, nous avons choisi par d√©faut
la profondeur de l'arbre mais c'√©tait un choix au doigt mouill√©. 

‚ùìÔ∏è Quels sont les hyperparam√®tres qu'on peut essayer d'optimiser ? 

```{python}
pipe['boosting'].get_params()
```

Un [d√©tour par la documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
nous aide √† comprendre ceux sur lesquels on va jouer. Par exemple, il serait
absurde de jouer sur le param√®tre `random_state` qui est la racine du g√©n√©rateur
pseudo-al√©atoire. 

```{python}
X = pd.concat((X_train, X_test), axis=0)
Y = np.concatenate([y_train,y_test])
```


Nous allons nous contenter de jouer sur les param√®tres:

* `n_estimators`: Le nombre d'arbres de d√©cision que notre for√™t contient
* `max_depth`: La profondeur de chaque arbre

Il existe plusieurs mani√®res de faire de la validation crois√©e. Nous allons ici
utiliser la *grid search* qui consiste √† estimer et tester le mod√®le sur chaque
combinaison d'une grille de param√®tres et s√©lectionner le couple de valeurs
des hyperparam√®tres amenant √† la meilleure pr√©diction. Par d√©faut, `scikit`
effectue une _5-fold cross validation_. Nous n'allons pas changer
ce comportement. 

Comme expliqu√© pr√©c√©demment, les param√®tres s'appelent sous la forme
`<step>__<parameter_name>`

La validation crois√©e pouvant √™tre tr√®s consommatrice de temps, nous 
n'allons l'effectuer que sur un nombre r√©duit de valeurs de notre grille.
Il est possible de passer la liste des valeurs √† passer au crible sous
forme de liste
(comme nous allons le proposer pour l'argument `max_depth` dans l'exercice ci-dessous) ou
sous forme d'`array` (comme nous allons le proposer pour l'argument `n_estimators`) ce qui est
souvent pratique pour g√©n√©rer un criblage d'un intervalle avec `np.linspace`.

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint </h3>
```
Les estimations sont, par d√©faut, men√©es de mani√®re s√©quentielle (l'une apr√®s
l'autre). Nous sommes cependant face √† un probl√®me
*embarassingly parallel*. 
Pour gagner en performance, il est recommand√© d'utiliser l'argument
`n_jobs=-1`.
```{=html}
</div>
```
:::


```{python}
#| output: false
#| label: grid-search

import numpy as np
from sklearn.model_selection import GridSearchCV

import time

start_time = time.time()
# Parameters of pipelines can be set using ‚Äò__‚Äô separated parameter names:
param_grid = {
    "boosting__n_estimators": np.linspace(5,25, 5).astype(int),
    "boosting__max_depth": [2,4]
}
grid_search = GridSearchCV(pipe, param_grid=param_grid)
grid_search.fit(X, Y)

end_time = time.time()

print(f"Elapsed time : {int(end_time - start_time)} seconds")
```

```{python}
grid_search
```




```{python}
grid_search.best_params_
grid_search.best_estimator_
```


Toutes les performances sur les ensembles d'√©chantillons et de test sur la grille
d'hyperparam√®tres sont disponibles dans l'attribut:

```{python}
perf_random_forest = pd.DataFrame(grid_search.cv_results_)
```

Regardons les r√©sultats moyens pour chaque valeur des hyperparam√®tres:

```{python}
fig, ax = plt.subplots(1)
g = sns.lineplot(data = perf_random_forest, ax = ax,
             x = "param_boosting__n_estimators",
             y = "mean_test_score",
             hue = "param_boosting__max_depth")
g.set(xlabel='Number of estimators', ylabel='Mean score on test sample')
g
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0,
           title='Depth of trees')
```


Globalement, √† profondeur d'arbre donn√©e, le nombre d'arbres affecte
la performance. Changer la profondeur de l'arbre am√©liore la 
performance de mani√®re plus marqu√©e.

Maintenant, il nous reste √† re-entra√Æner le mod√®le avec ces nouveaux
param√®tres sur l'ensemble du jeu de *train* et l'√©valuer sur l'ensemble
du jeu de *test* :

```{python}
#| echo: true
#| output: false
pipe_optimal = grid_search.best_estimator_
pipe_optimal.fit(X_train, y_train)

compar = pd.DataFrame([y_test, pipe_optimal.predict(X_test)]).T
compar.columns = ['obs','pred']
compar['diff'] = compar.obs - compar.pred
```

On obtient le RMSE suivant :

```{python}

print("Le RMSE sur le jeu de test est {:,}".format(
   int(np.sqrt(mean_squared_error(y_test, pipe_optimal.predict(X_test))))
))
```

Et si on regarde la qualit√© en pr√©diction:

```{python}
#| include: false
#| echo: true

g = sns.relplot(data = compar, x = 'obs', y = 'pred', color = "royalblue", alpha = 0.8)
g.set(ylim=(0, 2e6), xlim=(0, 2e6),
      title='Evaluating estimation error on test sample',
      xlabel='Observed values',
      ylabel='Predicted values')
g.ax.axline(xy1=(0, 0), slope=1, color="red", dashes=(5, 2))
g
```

On obtient plus de variance dans la pr√©diction, c'est d√©j√† un peu mieux.
Cependant, cela reste d√©cevant pour plusieurs raisons:

- nous n'avons pas fait d'√©tape de s√©lection de variable
- nous n'avons pas chercher √† d√©terminer si la variable √† pr√©dire la plus
pertinente √©tait le prix ou une transformation de celle-ci
(par exemple le prix au $m^2$)


## Prochaine √©tape

Nous avons un mod√®le certes perfectible mais fonctionnel.
La question qui se pose maintenant c'est d'essayer d'en faire
quelque chose au service des utilisateurs. Cela nous am√®ne vers
la question de la __mise en production__. 

Ceci est l'objet du prochain chapitre. Il s'agira d'une version introductive
des enjeux √©voqu√©s dans le cadre du cours de
3e ann√©e de [mise en production de projets de _data science_](https://ensae-reproductibilite.github.io/website/).



## R√©f√©rences

