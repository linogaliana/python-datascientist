---
title: "Partie 3: mod√©liser"
title-en: "Part 3: modelisation"
categories:
  - Introduction
  - Mod√©lisation
description: |
  La facilit√© √† mod√©liser des processus tr√®s diverses a grandement 
  particip√© au succ√®s de `Python` dans les ann√©es 2010. La popularit√© de
  `Python` est indissociable de l'essor du _machine learning_ comme technique
  de mod√©lisation. Cette partie vise √† introduire aux probl√©matiques sp√©cifiques
  en pr√©sentant principalement la librairie `Scikit Learn` qui permet d'avoir un 
  _pipeline_ de _machine learning_ op√©rationnel tr√®s rapidement. 
description-en: |
  The ability to model a wide range of processes has been a major factor in the success of `Python` in the 2010s. The popularity of `Python` is inseparable from the rise of _machine learning_ as a modeling technique. This section aims to provide an introduction to the specific issues involved by presenting the `Scikit Learn` library, which provides a _pipeline_ of _machine learning_.  machine learning _pipeline_ up and running very quickly. 
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/hal9000.png
---


::: {.content-visible when-profile="fr"}

# Introduction

Les _data scientists_ sont souvent associ√©s √† la mise en oeuvre
de mod√®les complexes d'intelligence artificielle. 
Le succ√®s m√©diatique de ce type d'outils, notamment `ChatGPT`, 
n'y est pas pour rien. Cependant, la mod√©lisation n'est souvent
qu'une 
phase du travail du _data scientist_, un peu comme la visualisation. 
D'ailleurs, dans certaines organisations o√π la division des t√¢ches 
est plus pouss√©e, les _data engineers_ sont au moins aussi
impliqu√©s dans la phase de mod√©lisation que les _data scientists_. 

C'est souvent un p√©ch√© de jeunesse de penser qu'on peut r√©sumer
le travail du _data scientist_ exclusivement √† la phase de mod√©lisation.
Cette derni√®re d√©pend tr√®s fortement de la qualit√© du travail de 
nettoyage et structuration des donn√©es mis en oeuvre en amont. La
mise en oeuvre de mod√®les complexes, qui s'accomodent de donn√©es
peu structur√©es, est gourmande en ressources et co√ªteuse. Ce ne sont
donc qu'un nombre limit√© d'acteurs qui peuvent entra√Æner, _ex nihilo_, 
des grands mod√®les de langage[^llm], capables de d√©penser au moins 300 000 dollars
dans l'entra√Ænement d'un mod√®le, avant m√™me toute phase d'inf√©rence [@izsak2021train].
Ces besoins computationnels pour entra√Æner de grands mod√®les de langage sont
d'ailleurs assez gourmands en √©nergie, ce qui peut amener √†
des empreintes carbones non n√©gligeables [@strubell2019energy; @arcep2019].  

[^llm]: Nous reviendrons de mani√®re √©pisodique 
sur ce principe des grands mod√®les de langage
qui sont devenus, en quelques ann√©es,
centraux dans l'√©cosyst√®me de la _data science_ mais sont √©galement 
amen√©s √† devenir des outils grands publics, √† la mani√®re de `ChatGPT`. 

Heureusement, il est possible de mettre en oeuvre des mod√©lisations plus
l√©g√®res, celles que nous pr√©senterons dans les prochains chapitres.
Cette partie du cours se concentrera principalement sur la pr√©sentation
d'algorithmes de _machine learning_ (apprentissage automatique en Fran√ßais). On peut d√©finir
ce terme de mani√®re large comme un ensemble de techniques 
permettant, √† partir d'un ensemble d'observations, √† des algorithmes de d√©gager des structures ou
r√©gularit√©s statistiques sans que celles-ci aient √©t√© d√©finies _a priori_ par les mod√©lisateurs. 
Cette d√©finition permet de distinguer cette approche d'autres champs de l'intelligence artificielle[^russel-norvig],
notamment les IA symboliques, o√π chaque observation est caract√©ris√©e √† partir d'un ensemble de r√®gles exhaustives et pr√©d√©finies.
Bien que cette d√©finition large permette d'englober la statistique inf√©rentielle traditionnelle, elle permet n√©anmoins de comprendre la diff√©rence philosophique majeure entre l'approche par apprentissage automatique et l'√©conom√©trie comme nous allons pouvoir l'√©voquer par la suite. 

:::: {.callout-note}
## Comment d√©finir l'intelligence artificielle ?

::::: {.column-margin}
üëàÔ∏è __Intelligence artificielle__
:::::

@RN2020 d√©finissent l'intelligence artificielle de la mani√®re suivante:

> ¬´ L'√©tude des agents [intelligents] qui re√ßoivent des perceptions de l'environnement et agissent. Chacun de ces agents est mis en ≈ìuvre par une fonction qui associe les perceptions aux actions, et nous couvrons diff√©rentes mani√®res de de repr√©senter ces fonctions, telles que les syst√®mes de production, les agents r√©actifs, les planificateurs logiques, les r√©seaux de neurones et les syst√®mes de gestion de l'information et les syst√®mes de th√©orie de la d√©cision ¬ª

Cette d√©finition tr√®s large permet d'inclure beaucoup d'approches diff√©rentes dans le champ de l'intelligence artificielle. Elle d√©finit l'intelligence artificielle comme une r√®gle de d√©cision, tr√®s g√©n√©rique, issues de donn√©es. Pour formaliser math√©matiquement, il s'agit de relier des perceptions $\mathbb{X}$, c'est-√†-dire des faits pris comme donn√©s, √† une d√©cision $y$ gr√¢ce √† une r√®gle de d√©cision $f$ : $y=f(\mathbb{X})$ (la d√©cision $y$ est issue d'un ensemble de d√©cisions, de taille restreinte ou large selon le ph√©nom√®ne not√© $\mathcal{Y}$). La mani√®re dont est constuite cette fonction $f$ distinguera diff√©rents champs de l'intelligence artificielle.

L'_AI Act_ europ√©en de 2024 propose une [d√©finition assez similaire](https://artificialintelligenceact.eu/fr/article/3/) bien qu'elle soit exprim√©e dans des termes diff√©rents:

> ¬´ Syst√®me bas√© sur une machine qui est con√ßu pour fonctionner avec diff√©rents niveaux d'autonomie et qui peut faire preuve d'adaptabilit√© apr√®s son d√©ploiement, et qui, pour des objectifs explicites ou implicites, d√©duit, √† partir des donn√©es qu'il re√ßoit, comment g√©n√©rer des r√©sultats tels que des pr√©dictions, du contenu, des recommandations ou des d√©cisions qui peuvent influencer des environnements physiques ou virtuels ¬ª

Dans le cadre de ce cours, nous n'allons parler que des approches constuites autour de l'apprentissage, c'est-√†-dire les approches qui visent √† induire des lois, forc√©ment incertaines, √† partir d'un ensemble de donn√©es. Cette approche est tr√®s diff√©rente des [IA symboliques](https://fr.wikipedia.org/wiki/Intelligence_artificielle_symbolique) qui offrent une autonomie limit√©e √† la machine puisque son comportement est cadr√© par un ensemble, parfois large, de r√®gles d√©terministes.
::::

Le choix de se concentrer sur des algorithmes simples de _machine learning_ dans la partie mod√©lisation
plut√¥t que d'aller directement aux r√©seaux de neurone, permet d'abord de 
pr√©senter la d√©marche scientifique li√©e √† l'apprentissage, notamment pour obtenir des performances satisfaisantes lorsqu'on extrapole sur des donn√©es non rencontr√©es lors de la phase d'apprentissage. 
Cela permet √©galement de souligner des enjeux qui seront √©galement valables pour des mod√®les plus complexes, par exemple la pr√©paration des donn√©es permettant de r√©duire le bruit dans les donn√©es afin que les mod√®les puissent d√©gager des structures plus fiables dans les donn√©es.
En fait, pour √™tre plus pertinent que des approches plus parcimonieuses,
les techniques de _deep learning_, notamment
les r√©seaux de neurones, n√©cessitent soit des volumes de donn√©es tr√®s
importants (des millions voire dizaine de millions d'observations) soit
des donn√©es √† la structure complexe comme le langage naturel ou les images. 
Dans de nombreux cas, des mod√®les plus simples comme les techniques d'apprentissage
automatique (_machine learning_) suffisent largement. 
:::

::: {.content-visible when-profile="en"}

# Introduction

It is common to associate data scientists with the idea of complex artificial intelligence models. 
The popular success of tools like `ChatGPT` contributes to this perception. However, modeling is generally only a phase of a data scientist's work, similar to visualization. In some organizations with more specialized divisions of labor, data engineers are as involved in the modeling phase as data scientists.

It is a common misconception, especially among newcomers, to think that the data scientist's work can be reduced exclusively to the modeling phase. This phase heavily depends on the quality of the data cleaning and structuring work done beforehand. Implementing complex models that can handle unstructured data is resource-intensive and costly. Only a limited number of players can train large language models[^llm-en] _ex nihilo_ by spending at least $300,000 to train a model, even before any inference phase [@izsak2021train]. These computational needs for training large language models are also quite energy-intensive, which can lead to significant carbon footprints [@strubell2019energy; @arcep2019].

[^llm-en]: We will periodically revisit the principle of large language models, which have become central in the data science ecosystem within just a few years and are also becoming popular tools for the general public, like `ChatGPT`.

Fortunately, it is possible to implement lighter models, which we will introduce in the coming chapters.
This part of the course will primarily focus on presenting machine learning algorithms. We can define machine learning broadly as a set of techniques that enable algorithms to detect structures or statistical regularities from a set of observations without these structures being defined _a priori_ by modelers. This definition helps differentiate this approach from other fields of artificial intelligence[^russel-norvig], particularly symbolic AI, where each observation is characterized by an exhaustive and predefined set of rules.
Although this broad definition encompasses traditional inferential statistics, it still highlights the major philosophical difference between machine learning and econometrics, as we will discuss later.

::: {.note}
## How to define artificial intelligence?

::::: {.column-margin}
üëàÔ∏è __Artifical intelligence__
:::::

@RN2020 defines artificial intelligence as follows:

> "The study of [intelligent] agents that receive precepts from the environment and take action. Each such agent is implemented by a function that maps percepts to actions, and we cover different ways to represent these functions, such as production systems, reactive agents, logical planners, neural networks, and decision-theoretic systems"

This very broad definition includes many different approaches within the field of artificial intelligence. It defines artificial intelligence as a very generic decision rule derived from data. Mathematically, this means linking perceptions $\mathbb{X}$‚Äîfacts considered given‚Äîto a decision $y$ through a decision rule $f$: $y=f(\mathbb{X})$ (where decision $y$ comes from a set of decisions, either restricted or broad, depending on the phenomenon, noted as $\mathcal{Y}$). The way this function $f$ is constructed distinguishes different fields of artificial intelligence.

The European AI Act of 2024 offers a [similar definition](https://artificialintelligenceact.eu/fr/article/3/) though expressed in different terms:

> "A machine-based system designed to operate with varying levels of autonomy and adaptability after deployment, which, for explicit or implicit purposes, deduces from the data it receives how to generate results such as predictions, content, recommendations, or decisions that can influence physical or virtual environments."

In this course, we will only discuss approaches built around learning‚Äîthose that aim to induce necessarily uncertain rules from a dataset. This approach is very different from [symbolic AI](https://fr.wikipedia.org/wiki/Intelligence_artificielle_symbolique), which offers limited autonomy to the machine since its behavior is constrained by a set, sometimes large, of deterministic rules.
:::

The choice to focus on simple machine learning algorithms in the modeling section, rather than directly jumping into neural networks, first allows us to present the scientific approach related to learning, especially to achieve satisfactory performance when extrapolating to data not encountered during the training phase.
This also highlights issues that are relevant even for more complex models, such as data preparation to reduce noise in the data, enabling models to extract more reliable structures from the data. In fact, to be more effective than more parsimonious approaches, deep learning techniques, particularly neural networks, require either very large volumes of data (millions or tens of millions of observations) or complex-structured data, such as natural language or images.
In many cases, simpler models, such as machine learning techniques, are more than sufficient.
:::


::: {.content-visible when-profile="fr"}
# La mod√©lisation, une approche au coeur de la statistique

Un mod√®le statistique
est une repr√©sentation simplifi√©e et structur√©e d'un ph√©nom√®ne r√©el,
construite √† partir d'observations issues d'un ensemble partiel de donn√©es. 

:::: {.column-margin}
üëàÔ∏è __Mod√®le statistique__
::::

Un mod√®le vise √† capturer les relations et les structures sous-jacentes au sein de ces donn√©es, permettant ainsi de formuler des hypoth√®ses, d'effectuer des pr√©dictions et d'extrapoler des conclusions au-del√† 
de l'ensemble de donn√©es mesur√©es.
Les mod√®les statistiques fournissent ainsi un cadre analytique pour explorer, comprendre et interpr√©ter les informations contenues dans les donn√©es. 

Repr√©senter la r√©alit√© sous la forme d'un mod√®le est un principe √† la
base de la statistique comme discipline scientifique et ayant des
applications dans de nombreux champs disciplinaires : √©conomie,
sociologie, g√©ographique, biologie, physique, etc. 
Selon les disciplines, le nom donn√© peut varier mais on retrouve
r√©guli√®rement la m√™me approche scientifique : le mod√©lisateur
construit des relations entre plusieurs variables th√©oriques
ayant des contreparties empiriques afin de quantifier la relation entre celles-ci.


Cette approche est au coeur de la __statistique inf√©rentielle__, par opposition √† la __statistique descriptive__. 
Dans les deux cas, l'objectif est d'utiliser un __√©chantillon__, c'est-√†-dire un ensemble r√©duit de donn√©es observ√©es,
pour mieux conna√Ætre une __population__, l'ensemble de donn√©es concern√©es par une √©tude. La diff√©rence entre les deux approches est li√©e √† la mani√®re dont cette extrapolation est faite. Dans le cadre de la statistique inf√©rentielle, on tend plut√¥t √† inf√©rer des lois g√©n√©rales, avec des marges d'incertitude statistique, √† partir des donn√©es observ√©es, que ce soit sur la distribution statistique d'une variable (statistique univari√©e) ou sur les relations entre plusieurs. La statistique descriptive se contente, quant √† elle, de synth√©tiser l'information dans un jeu de donn√©es, souvent par le biais de moments de la distribution (moyenne, quantiles, etc.) sans pr√©tention √† fournir une explication g√©n√©rale sur le processus g√©n√©rateur de donn√©es. 

::::: {.column-margin}
üëàÔ∏è __Statistique inf√©rentielle__, __statistique descriptive__, __√©chantillon__, __population__
:::::

Ces deux approches ne sont pas inconciliables, au contraire elles sont compl√©mentaires. Se lancer dans une approche inf√©rentielle sans avoir men√© une approche descriptive approfondie am√®nera souvent la premi√®re vers des impasses ou des conclusions peu fiables. L'approche inf√©rentielle peu √©galement nourrir une analyse descriptive approfondie en permettant de hi√©rarchiser l'information pr√©sente dans un jeu de donn√©es et ainsi guider le travail descriptif en permettant celui-ci de se concentrer sur les faits saillants.


Dans le domaine de la recherche √©conomique, les mod√®les empiriques servent g√©n√©ralement √† associer certains param√®tres structurants des mod√®les de comportements √©conomiques √† des valeurs quantitatives. 
Les mod√®les statistiques, comme les mod√®les √©conomiques,
pr√©sentent n√©anmoins toujours une part d'irr√©alisme [@friedman1953methodology; @salmon2010probleme]
et accepter de mani√®re trop litt√©rale les implications d'un mod√®le, m√™me s'il
a de bonnes performances pr√©dictives, peut √™tre dangereux et relever d'un biais
scientiste. On s√©lectionne plut√¥t le moins mauvais mod√®le
que le vrai processus g√©n√©rateur des donn√©es. 

Dans l'enseignement de l'ENSAE la mod√©lisation empirique se retrouve
principalement dans deux domaines d'application : le *machine learning* et
l'√©conom√©trie. La diff√©rence est certes
s√©mantique - la r√©gression lin√©aire peut √™tre consid√©r√©e comme une 
technique de _machine learning_ ou d'√©conom√©trie - mais elle est
√©galement conceptuelle : 

- Dans le domaine du _machine learning_,
la structure impos√©e par le mod√©lisateur est minimale et ce sont plut√¥t
les algorithmes qui, sur des crit√®res de performance statistique, vont
amener √† choisir une loi math√©matique qui correspond au mieux aux donn√©es ;
- En √©conom√©trie,
les hypoth√®ses de structure des lois sont plus fortes (m√™me dans un cadre semi ou non-param√©trique) et sont plus souvent impos√©es
par le mod√©lisateur.

Dans cette partie du cours, nous allons principalement
parler de _machine learning_ car il s'agit d'une perspective
plus op√©rationnelle que l'√©conom√©trie qui est plus directement associ√©e
√† des concepts statistiques complexes comme la th√©orie asymptotique. 

L'adoption du _machine learning_ dans la litt√©rature √©conomique a √©t√© longue
car la structuration des donn√©es est souvent le
pendant empirique d'hypoth√®ses th√©oriques sur le comportement des acteurs ou des march√©s [@athey2019machine; @charpentier2018econometrics].
Pour caricaturer, l‚Äô√©conom√©trie s‚Äôattacherait √† comprendre la causalit√© de certaines variables sur une autre.
Cela implique que ce qui int√©resse l'√©conom√®tre
est principalement de l'estimation des param√®tres (et l'incertitude
sur l'estimation de ceux-ci) qui permettent de quantifier l'effet d'une
variation d'une variable sur une autre. 
Toujours pour caricaturer, 
le _machine learning_ se focaliserait
sur un simple objectif pr√©dictif en exploitant les relations de corr√©lations entre les variables.
Dans cette perspective, l'important n'est pas la causalit√© mais le fait qu'une variation
de $x$% d'une variable permette d'anticiper un changement de $\beta x$ de la variable
d'int√©r√™t ; peu importe la raison.
@MullainathanJEP ont ainsi, pour simplifier, propos√© la diff√©rence fondamentale qui 
suit : l'√©conom√©trie se pr√©occupe de $\widehat{\beta}$ l√† o√π le _machine learning_
se focalise sur $\widehat{y}$. Les deux sont bien s√ªr reli√©s dans un cadre
lin√©aire mais cette diff√©rence d'approche a des implications importantes
sur la structure des mod√®les √©tudi√©s, notamment leur parcimonie[^caricature]. 

[^caricature]: Comme nous l'avons dit, cette diff√©renciation est un peu 
caricaturale, notamment maintenant que les √©conomistes sont
plus familiaris√©s aux concepts d'√©valuation de performance
pr√©dictive sur des sous-ensembles d'apprentissage et de test (mais
l'√©volution est lente) et qu'√† l'inverse la recherche en _machine learning_ est tr√®s dynamique 
sur la question de l'explicabilit√© et de l'interpr√©tabilit√©
des mod√®les de _machine learning_, notamment autour du concept
de [valeurs de Shapley](https://shap.readthedocs.io/en/latest/index.html). N√©anmoins, cette diff√©rence philosophique entre ces deux √©coles de pens√©e continue √† influencer la mani√®re dont est pratiqu√©e l'√©conom√©trie ou le machine learning dans diff√©rents champs scientifiques.
:::

::: {.content-visible when-profile="en"}

# Modeling: An Approach at the Heart of Statistics

A statistical model is a simplified and structured representation of a real phenomenon, built from observations drawn from a partial dataset.

::::: {.column-margin}
üëàÔ∏è __Statistical model__
:::::

A model aims to capture relationships and underlying structures within this data, allowing for hypothesis formulation, making predictions, and extrapolating conclusions beyond the measured dataset.
Statistical models thus provide an analytical framework to explore, understand, and interpret the information contained in the data.

Representing reality as a model is a foundational principle in statistics as a scientific discipline, with applications across many fields: economics, sociology, geography, biology, physics, and more. The specific term may vary depending on the discipline, but the scientific approach is typically consistent: the modeler establishes relationships between several theoretical variables with empirical counterparts to quantify the relationship between them. This approach is central to __inferential statistics__, as opposed to __descriptive statistics__.
In both cases, the objective is to use a __sample__, a limited set of observed data, to better understand a __population__, the entire dataset relevant to a study. The difference between the two approaches lies in how this extrapolation is conducted. In inferential statistics, the goal is generally to infer general laws, with statistical uncertainty margins, from observed data, whether it is about the statistical distribution of a variable (univariate statistics) or relationships between multiple variables. Descriptive statistics, on the other hand, simply summarize information within a dataset, often through distribution moments (mean, quantiles, etc.), without aiming to provide a general explanation of the data-generating process. 

::::: {.column-margin}
üëàÔ∏è __Inferential statistics, descriptive statistics, sample, population__
:::::

These two approaches are not mutually exclusive; rather, they are complementary. Attempting an inferential approach without a thorough descriptive analysis often leads to dead ends or unreliable conclusions. Inferential analysis can also enrich a descriptive analysis by allowing information to be prioritized within a dataset, guiding descriptive work by focusing on key findings.

In economic research, empirical models are generally used to associate specific structural parameters of economic behavior models with quantitative values.
Statistical models, like economic models, always contain an element of unreality [@friedman1953methodology; @salmon2010probleme], and accepting a model's implications too literally, even if it has strong predictive performance, can be risky and reflect a scientific bias. Rather than identifying the true data-generating process, the goal is to select the least inaccurate model.

At ENSAE, empirical modeling is primarily seen in two application areas: machine learning and econometrics. The distinction is largely semantic‚Äîa linear regression can be considered either a machine learning or econometric technique‚Äîbut also conceptual:


- In machine learning, the structure imposed by the modeler is minimal, and algorithms, based on statistical performance criteria, help select a mathematical rule that best fits the data.
- In econometrics, the assumptions about the structure of laws are stronger (even in semi or non-parametric frameworks) and are more often imposed by the modeler.

In this part of the course, we will focus mainly on machine learning, as it offers a more practical perspective than econometrics, which is more directly associated with complex statistical concepts like asymptotic theory.

The adoption of machine learning in economic literature has been gradual, as data structuring often serves as the empirical counterpart of theoretical hypotheses regarding actor or market behavior [@athey2019machine; @charpentier2018econometrics].
To simplify, econometrics focuses on understanding the causality of certain variables on another. This implies that what matters to the econometrician is primarily the estimation of parameters (and the uncertainty around them) that quantify the effect of one variable on another.
Again, to simplify, machine learning focuses on a predictive objective, exploiting correlations between variables.
From this perspective, causality is less important than knowing that a variation in one variable of $x$% can predict a change of $\beta x$ in the target variable; the reason is irrelevant.
@MullainathanJEP, for simplicity, proposed this fundamental difference: econometrics concerns itself with $\widehat{\beta}$, while machine learning focuses on $\widehat{y}$. Both are, of course, connected in a linear framework, but this difference in approach has important implications for the structure of the models studied, particularly their parsimony[^caricature-en].

[^caricature-en]: As we said, this differentiation is a bit of a caricature, especially now that economists are more familiar with the concepts of predictive performance on learning and test subsets (but this is a slow evolution). Conversely, research in _machine learning_ is very dynamic on the question of the explicability and interpretability of _machine learning_ models, notably around the concept of
[Shapley values](https://shap.readthedocs.io/en/latest/index.html). Nevertheless, this philosophical difference between these two schools of thought continues to influence the way econometrics or machine learning is practiced in different scientific fields.

:::


::: {.content-visible when-profile="fr"}
# Quelques d√©finitions

Dans cette partie du cours nous allons employer un certain nombre
de termes devenus familiers aux praticiens du _machine learning_
mais qui m√©ritent d'√™tre explicit√©s pour comprendre les prochains chapitres. 

## Entra√Ænement et inf√©rence

Le _machine learning_ est une approche op√©rationnelle: l'objectif est g√©n√©ralement 
d'estimer des relations entre variables observ√©es pour avoir une r√®gle de d√©cision puis extrapoler celle-ci sur
un autre √©chantillon de donn√©es. Les deux prochains chapitres visent √† pr√©senter la d√©marche scientifique pour avoir une extrapolation de qualit√©. 

L'entra√Ænement (ou apprentissage) est la phase du travail o√π un mod√®le de _machine learning_ affine des relations √† partir d'un ensemble de donn√©es. Pour faire l'analogie avec l'apprentissage humain, il s'agit de la phase o√π le machine learning va r√©viser ses cours avant le contr√¥le.

::::: {.column-margin}
üëàÔ∏è __Apprentissage, entra√Ænement__
:::::

L'inf√©rence est la phase o√π la r√®gle de d√©cision est mise en oeuvre sur de nouvelles donn√©es, qui n'ont pas √©t√© vues lors de l'entra√Ænement. Pour reprendre l'analogie pr√©c√©dente, il peut s'agir de nouvelles questions lors du contr√¥le (phase d'√©valuation) ou de l'application au monde r√©el des connaissances acquises lors de l'apprentissage.

::::: {.column-margin}
üëàÔ∏è __Inf√©rence__
:::::

:::
::: {.content-visible when-profile="en"}

# Some useful definitions

In this part of the course, we will use several terms familiar to machine learning practitioners, but which need to be clarified to understand the following chapters.

## Training and inference

Machine learning is an operational approach: the objective is generally to estimate relationships between observed variables to create a decision rule, which can then be extrapolated to another data sample. The next two chapters aim to present the scientific approach for achieving high-quality extrapolation.

Training (or learning) is the phase where a machine learning model refines relationships based on a dataset. Analogous to human learning, it is the phase where machine learning "studies" before an exam.

::::: {.column-margin}
üëàÔ∏è __Training, learning__
:::::

Inference is the phase where the decision rule is applied to new data, unseen during training. To continue the previous analogy, it could involve new questions on the exam (evaluation phase) or the application of the learned knowledge to real-world situations.

::::: {.column-margin}
üëàÔ∏è __Inference__
:::::

:::


::: {.content-visible when-profile="fr"}
## _Machine learning_ et _deep learning_

Jusqu'√† pr√©sent nous avons beaucoup utilis√©, sans le d√©finir, le
concept de _machine learning_, dont la traduction fran√ßaise est
apprentissage automatique mais le terme anglo-saxon est suffisamment
utilis√© pour √™tre consid√©r√© comme standard.

Le _machine learning_ est un ensemble de techniques algorithmiques 
qui permettent aux ordinateurs d'apprendre, √† partir d'exemples, √† ajuster un mod√®le
sans avoir explicitement d√©fini celui-ci. A partir d'algorithmes it√©ratifs et d'une
m√©trique de performance, des r√®gles de classification ou de pr√©diction vont permettre
de mettre en relation des caract√©ristiques (_features_) avec une variable d'int√©r√™t (_label_)[^label]. 

[^label]: Pour faire l'analogie avec le cadre √©conom√©trique, les _features_ sont les variables explicatives
ou _covariates_ (la matrice $X$) et le _label_ est la variable expliqu√©e ($y$).

::::: {.column-margin}
üëàÔ∏è __*Machine learning*, label, *features*__
:::::

De nombreux algorithmes existent et se distinguent sur la mani√®re d'introduire une structure plus ou
moins formelle dans la relation entre les variables observ√©es. Nous n'allons voir que quelques-uns
de ces algorithmes : _support vector machine_ (SVM), r√©gression logistique, arbres de d√©cision, for√™ts
al√©atoires, etc. Simples √† mettre en oeuvre gr√¢ce √† la librairie `Scikit-Learn`, ils permettront
d√©j√† de comprendre la d√©marche originale du _machine learning_ que vous pourrez approfondir
ult√©rieurement.  

Au sein de la grande famille des algorithmes de _machine learning_, tendent de plus √† plus √† devenir
autonomes les techniques de r√©seaux de neurone. Les techniques qui s'appuient sur les r√©seaux de neurones sont regroup√©es
dans une famille qu'on
appelle _deep learning_ (apprentissage profond en Fran√ßais).
Ces r√©seaux sont inspir√©s du fonctionnement du cerveau humain et sont compos√©s de nombreuses couches de neurones interconnect√©s. 
La structure canonique bien connue est illustr√©e dans la @fig-nn. 
Le _deep learning_ est int√©ressant pour cr√©er des mod√®les capables d'apprendre de repr√©sentations
de donn√©es complexes et abstraites √† partir de donn√©es brutes,
ce qui √©vite parfois la complexe t√¢che de d√©finir manuellement des caract√©ristiques sp√©cifiques √† cibler.
Les champs de l'analyse d'image (_computer vision_) ou du traitement du langage naturel sont les principaux
cas d'application de ces m√©thodes.

::::: {.column-margin}
üëàÔ∏è __*Deep learning*, r√©seaux de neurone__
:::::

::: {#fig-nn}
![](neural-network1.png)

Exemple de structure simplifi√©e d'un r√©seau de neurones. 

La premi√®re couche (_input layer_) correspond aux variables introduites dans le mod√®le. La combinaison de celles-ci se fait dans des couches interm√©diaires (_hidden layer_). Le passage entre deux couches se fait par le biais d'une fonction d'activation, par exemple une fonction _[sigmoid](https://fr.wikipedia.org/wiki/Sigmo%C3%AFde_(math%C3%A9matiques))_, construite √† partir d'une moyenne pond√©r√©e (dont les poids seront ajust√©s lors de l'entra√Ænement) des couches pr√©c√©dentes. La complexit√© du r√©seau mis en oeuvre ainsi que des relations entre les couches d√©termine le type de r√©seau mis en oeuvre (_feed forward network_, _convolutional network_, etc.).

:::
:::
::: {.content-visible when-profile="en"}

## Machine learning and deep learning

Up to this point, we have frequently used, without defining it, the concept of machine learning.

Machine learning is a set of algorithmic techniques that allow computers to learn from examples and adjust a model without being explicitly defined. Through iterative algorithms and a performance metric, classification or prediction rules make it possible to relate features with a target variable (_label_)[^label-en].

::::: {.column-margin}
üëàÔ∏è __Machine learning, label, features__
:::::

[^label-en]: Drawing an analogy with econometrics, features are explanatory variables or _covariates_ (the matrix $X$), and label is the explained variable ($y$).

There are many algorithms that differ in how they introduce a more or less formal structure into the relationship between observed variables. We will cover only a few of these algorithms: support vector machine (SVM), logistic regression, decision trees, random forests, etc. Simple to implement using the `Scikit-Learn` library, they will provide an understanding of the original approach to machine learning, which can be further explored later.

Within the broad family of machine learning algorithms, neural network techniques are increasingly becoming more autonomous. Techniques based on neural networks are grouped within a family called deep learning.
These networks are inspired by the functioning of the human brain and are composed of many interconnected layers of neurons. The well-known canonical structure is illustrated in @fig-nn.
Deep learning is useful for creating models capable of learning complex and abstract data representations from raw data, sometimes bypassing the complex task of manually defining specific features to target.
The fields of image analysis (_computer vision_) or natural language processing are the main application areas for these methods.

::::: {.column-margin}
üëàÔ∏è __Deep learning, neural networks__
:::::

:::: {#fig-nn}
![](neural-network1.png)

Example of a simplified neural network structure.

The first layer (_input layer_) corresponds to the variables introduced into the model. These are combined in intermediate layers (_hidden layer_). The transition between two layers is achieved through an activation function, such as a _[sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)_ function, constructed from a weighted average (with weights adjusted during training) of the previous layers. The complexity of the network implemented, as well as the relationships between layers, determines the type of network used (feed forward network, convolutional network...).

::::

:::

::: {.content-visible when-profile="fr"}

:::: {.callout-note}

Les r√©seaux de neurone sont des mod√®les √† la structure complexe. Sur des donn√©es volumineuses et complexes, ils peuvent √™tre lourds, voire impossible √† entra√Æner sur des machines classiques. Si les donn√©es qu'on poss√®de correspondent √† celles attendues par le mod√®le, il est tout √† fait possible d'utiliser celui-ci pour l'inf√©rence sans le r√©-entra√Æner ou en mettant en oeuvre un apprentissage √† la marge, dit [fine tuning](https://www.cnil.fr/fr/definition/ajustement-fine-tuning). Pour reprendre l'analogie avec l'apprentissage humain, le _fine tuning_ ressemble √† la mani√®re dont on va mettre √† jour ses connaissances avec un nouveau cours avant un contr√¥le. Il n'est pas n√©cessaire de tout r√©apprendre, seulement de peaufiner ses connaissances avec les nouveaut√©s du cours.

::::: {.column-margin}
üëàÔ∏è __*Fine tuning*__
:::::

De nombreux mod√®les sont mis √† disposition sur `HuggingFace`, la 
plateforme de partage de mod√®les de _deep learning_ (le `Github` du _deep learning_ en quelques sortes). `HuggingFace` propose
propose aussi des cours sur le sujet, notamment sur le [traitement du langage naturel (NLP)](https://huggingface.co/learn/nlp-course/chapter1/2?fw=pt). Nous ferons du traitement du langage naturel dans la [prochaine partie](/content/NLP/index.qmd) de ce cours mais
de mani√®re plus modeste en revenant sur les concepts n√©cessaires avant de mettre en oeuvre
une mod√©lisation sophistiqu√©e du langage. 
::::

Nous n'allons pas vraiment parler dans cette partie du cours de _deep learning_ car ces mod√®les, pour √™tre pertinents, n√©cessitent
soit des donn√©es structur√©es d'un volume important, ce qui est rarement disponible
en _open data_, et pour lesquelles les relations entre variables sont complexes, soit
des donn√©es non structur√©es telles que des donn√©es textuelles, des images, des vid√©os, etc. Les donn√©es textuelles sont l'objet de la prochaine partie du cours car elles font appel √† des concepts sp√©cifiques, qui n√©cessitent d√©j√† la compr√©hension des enjeux de la mod√©lisation sur donn√©es structur√©es.
:::

::: {.content-visible when-profile="en"}

:::: {.callout-note}

Neural networks are complex models. With large and complex datasets, they can be heavy, even impossible to train on standard machines. If the data matches what the model expects, it is quite possible to use it for inference without retraining or by implementing marginal learning, known as [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)). To extend the analogy with human learning, fine-tuning is similar to updating one‚Äôs knowledge with a new lesson before an exam. It isn‚Äôt necessary to relearn everything, only to refine knowledge with the latest course content.

::::: {.column-margin}
üëàÔ∏è __*Fine tuning*__
:::::

A large number of pretrained models are available on `HuggingFace`, a 
platform for sharing _deep learning_ models (the `Github` of _deep learning_). `HuggingFace` also offers courses on the subject, notably on [natural language processing (NLP)](https://huggingface.co/learn/nlp-course/chapter1/2?fw=pt). We'll be looking at natural language processing in the [next part](/content/NLP/index.qmd) of this course, but in a more modest way, going back over the concepts necessary before implementing sophisticated language modeling. 

::::

In this part of the course, we will not delve deeply into deep learning because these models, to be effective, require either large structured datasets, which are rarely available as open data and have complex variable relationships, or unstructured data such as text, images, videos, etc. Text data will be covered in the next part of the course, as they involve specific concepts that require an understanding of structured data modeling challenges.
:::


::: {.content-visible when-profile="fr"}
## Apprentissage supervis√© ou non supervis√© 

Une ligne de clivage importante entre les m√©thodes √† mettre en oeuvre est le fait d'observer ou non
le _label_ (la variable $y$) qu'on d√©sire mod√©liser. 

Prenons par exemple un site de commerce qui dispose
d'informations sur ses clients comme l'√¢ge, le sexe, le lieu de r√©sidence. 
Ce site peut d√©sirer 
exploiter cette information de diff√©rentes mani√®res pour mod√©liser le comportement d'achat. 

En premier lieu, ce site peut d√©sirer
anticiper le volume d'achat d'un nouveau client ayant certaines caract√©ristiques. 
Dans ce cas, il est possible d'utiliser les montants d√©pens√©s par d'autres clients en fonction de leurs
caract√©ristiques. L'information pour notre nouveau client n'est pas mesur√©e mais elle peut s'appuyer
sur un ensemble d'observations de la m√™me variable.

Mais il est tout √† fait possible d'entra√Æner un mod√®le sur un _label_ qu'on ne mesure pas, en supposant
qu'il fasse sens. Par exemple notre site de commerce peut d√©sirer d√©terminer, en fonction des
caract√©ristiques de notre nouveau client et de sa client√®le existante, s'il appartient √† tel ou 
tel groupe de consommateurs : les d√©pensiers, les √©conomes... Bien s√ªr on ne sait jamais _a priori_
√† quel groupe appartient un consommateur mais le rapprochement entre consommateurs ayant un comportement
similaire permettra de donner du sens √† cette cat√©gorie. Dans ce cas, l'algorithme apprendra √† reconna√Ætre
quelles caract√©ristiques sont structurantes dans la constitution de groupes au comportement similaire et
permettra d'associer tout nouveau consommateur √† un groupe. 

Ces deux exemples illustrent l'approche diff√©rente selon qu'on essaie de construire des mod√®les
sur un _label_ observ√© ou non. Cela constitue m√™me l'une des dualit√©s fondamentales dans les
techniques de _machine learning_ : 

::::: {.column-margin}
üëàÔ∏è __Apprentissage supervis√©, apprentissage non supervis√©__
:::::

* __Apprentissage supervis√©__ : la valeur cible est connue et peut √™tre utilis√©e pour √©valuer la qualit√© d'un mod√®le ;
* __Apprentissage non supervis√©__ : la valeur cible est inconnue et ce sont des crit√®res statistiques qui vont amener
√† s√©lectionner la structure de donn√©es la plus plausible. 

Cette partie du cours illustrera ces deux approches de mani√®re diff√©rente √† partir du m√™me
jeu de donn√©es, les r√©sultats des √©lections am√©ricaines.
Dans le cas de l'apprentissage supervis√©, nous chercherons √† mod√©liser directement
le r√©sultat des candidats aux √©lections (soit le score, soit le gagnant). Dans 
le cas de l'apprentissage non supervis√©, nous essaierons de regrouper les 
territoires au comportement de vote similaire en fonction de facteurs
socio-d√©mographiques. 
:::

::: {.content-visible when-profile="en"}

## Supervised and Unsupervised Learning

An important dividing line between methods is whether or not we observe the label (the variable $y$) to be modeled.

Take, for example, an e-commerce site that has information on its customers such as age, gender, and place of residence.
This site may want to leverage this information in different ways to model purchasing behavior.

First, the site may wish to predict the purchase volume of a new customer with certain characteristics. 
In this case, it is possible to use the amounts spent by other customers based on their characteristics. Information for our new customer is not measured, but it can be inferred from a set of similar observations.

However, it is entirely possible to train a model on an unobserved _label_, assuming it makes sense. For instance, our e-commerce site may wish to determine, based on the characteristics of our new customer and its existing customer base, whether they belong to a specific group of consumers: big spenders, frugal shoppers, etc. Of course, we never know _a priori_ which group a consumer belongs to, but grouping customers with similar behaviors will give meaning to this categorization. In this case, the algorithm will learn to recognize which characteristics are structuring for grouping consumers with similar behavior, allowing any new customer to be associated with a group.

These two examples illustrate the different approaches depending on whether we attempt to build models on an observed _label_ or not. This distinction represents one of the fundamental dualities in machine learning techniques:

::::: {.column-margin}
üëàÔ∏è __Supervised learning, unsupervised learning__
:::::

* __Supervised learning__: the target value is known and can be used to evaluate the model's quality;
* __Unsupervised learning__: the target value is unknown, and statistical criteria are used to select the most plausible data structure.

This part of the course will illustrate these two approaches using the same dataset: the results of U.S. elections.
In the supervised learning case, we will aim to directly model the election candidates' results (either the score or the winner). In the unsupervised learning case, we will try to group territories with similar voting behavior based on socio-demographic factors.
:::

::: {.content-visible when-profile="fr"}
## Classification et r√©gression

Une deuxi√®me dualit√© fondamentale qui est d√©terminante dans le choix de la m√©thode de _machine learning_
√† mettre en oeuvre est la nature du _label_. S'agit-il d'une variable continue ou d'une variable
discr√®te, c'est-√†-dire prenant un nombre limit√© de modalit√©s ? 

Cette diff√©rence de nature entre les donn√©es am√®ne √† distinguer deux types d'approche :

::::: {.column-margin}
üëàÔ∏è __Classification, r√©gression__
:::::

- Dans les probl√©matiques de __classification__, o√π notre label $y$ a un nombre fini de valeurs[^binaire],
on cherche √† pr√©dire dans quelle classe ou √† quel groupe il est possible de rattacher nos donn√©es. 
Par exemple, si vous prenez du caf√© le matin, faites-vous partie du groupe des personnes ronchons au lever ?
Les m√©triques de performance utilisent g√©n√©ralement la proportion de bonnes ou mauvaises classifications
pour estimer la qualit√© d'un mod√®le. 
- Dans les probl√©matiques de __r√©gression__, o√π notre label est une grandeur num√©rique, on
cherche √† pr√©dire directement la valeur de notre variable dans le mod√®le. Par exemple, si vous
avez tel ou tel √¢ge, quel est votre d√©pense quotidienne en _fast food_ ? Les m√©triques
de performance sont g√©n√©ralement des moyennes plus ou moins sophistiqu√©es d'√©carts entre
la pr√©diction et la valeur observ√©e. 

[^binaire]: Nous allons nous focaliser sur le cas binaire, le plus simple. Dans ce type de probl√®mes,
la variable $y$ a deux modalit√©s : gagnant-perdant, 0-1, oui-non... N√©anmoins il existe de 
nombreux cas d'usage o√π la variable dispose de plus de modalit√©s, par exemples des
scores de satisfaction entre 0 et 5 ou A et D. La mise en oeuvre de mod√®les est plus
complexe mais l'id√©e g√©n√©rale est souvent de se ramener √† un ensemble de mod√®les dichotomiques
pour pouvoir appliquer des m√©triques simples et stables. 

En r√©sum√©, l'aide-m√©moire suivante, issue de l'aide de `Scikit-Learn`, peut d√©j√† donner de premiers enseignements sur les diff√©rentes familles de mod√®les :

::: {#fig-algos-scikit}
![](https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)

Une _cheatsheet_ des algorithmes disponibles dans [`Scikit-Learn`](https://scikit-learn.org)
:::
:::

::: {.content-visible when-profile="en"}

## Classification and Regression

A second fundamental distinction that determines the choice of machine learning method to implement is the nature of the _label_. Is it a continuous variable or a discrete variable, meaning it takes a limited number of categories?

This difference in data type leads to two types of approaches:

::::: {.column-margin}
üëàÔ∏è __Classification, regression__
:::::

- In __classification__ tasks, where our label $y$ has a finite number of values[^binary], we aim to predict the class or group to which our data belongs. 
For example, if you have coffee in the morning, are you part of the group of grumpy morning people?
Performance metrics typically use the proportion of correct or incorrect classifications to assess model quality.
- In __regression__ tasks, where our label is a numerical value, we seek to directly predict the value of our variable in the model. For instance, given a certain age, what would be your daily expenditure on fast food? Performance metrics are usually averages of the differences between the prediction and the observed value, often with varying degrees of sophistication.

[^binary]: We will focus on the simplest binary case. In this type of problem, the variable $y$ has two categories: winner-loser, 0-1, yes-no... However, there are many use cases where the variable has more categories, such as satisfaction scores ranging from 0 to 5 or A to D. Implementing these models is more complex, but the general approach often involves breaking it down into a set of binary models to enable the use of simple and stable metrics.

In summary, the following cheat sheet, taken from `Scikit-Learn` documentation, provides an initial overview of the different model families:

::: {#fig-algos-scikit}
![](https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)

A cheatsheet of algorithms available in [`Scikit-Learn`](https://scikit-learn.org)
:::
:::


::: {.content-visible when-profile="fr"}
# Donn√©es

La plupart des exemples de cette partie s'appuient sur les r√©sultats des
√©lections US 2020 au niveau comt√©s. Plusieurs bases sont utilis√©es pour 
cela :

* Les donn√©es √©lectorales sont une reconstruction √† partir des donn√©es du _MIT Election lab_
propos√©es sur `Github` par [`tonmcg`](https://github.com/tonmcg/US_County_Level_Election_Results_08-20)
ou directement disponibles sur le site du _[MIT Election Lab](https://electionlab.mit.edu/data)_.
* Les donn√©es socio√©conomiques (population, donn√©es de revenu et de pauvret√©, 
taux de ch√¥mage, variables d'√©ducation) proviennent de l'USDA ([source](https://www.ers.usda.gov/data-products/county-level-data-sets/))
* Le *shapefile* utilis√© pour les cartes provient des donn√©es du *Census Bureau*. Le fichier peut
√™tre t√©l√©charg√© directement depuis cet url:
<https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip>

Le code pour construire une base unique √† partir de ces sources diverses
est visible ci-dessous, pour les curieux : 

<details>

<summary>

Voir le code {{< fa brands python >}} de r√©cup√©ration des donn√©es

</summary>

```{.python include="get_data.py"}
```

</details>

Cette partie n'est absolument pas exhaustive. Elle constitue un point
d'entr√©e sur la mod√©lisation √† partir d'une s√©rie d'exemples construits √† partir d'un fil rouge. 
m√©riteraient d'√™tre √©voqu√©s. Pour les personnes d√©sirant en savoir plus sur les 
mod√®les √©conom√©triques, qui seront moins √©voqu√©s que ceux de _machine learning_,
je recommande la lecture de @Turrell2021.

:::
::: {.content-visible when-profile="en"}

# Data

Most examples in this section are based on the 2020 U.S. election results at the county level. Several datasets are used for this purpose:

* Election data is a reconstruction from the _MIT Election Lab_ data, available on `Github` by [`tonmcg`](https://github.com/tonmcg/US_County_Level_Election_Results_08-20) or directly on the _[MIT Election Lab website](https://electionlab.mit.edu/data)_.
* Socioeconomic data (population, income and poverty data, unemployment rate, education variables) come from the USDA ([source](https://www.ers.usda.gov/data-products/county-level-data-sets/)).
* The shapefile used for the maps comes from the Census Bureau data. The file can be downloaded directly from this URL:
<https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_county_20m.zip>

The code to build a single database from these various sources is shown below, for those interested:

<details>

<summary>

View {{< fa brands python >}} code for data retrieval

</summary>

```{.python include="get_data.py"}
```

</details>

This section is by no means exhaustive. It serves as an entry point into modeling, based on a series of examples developed around a common theme.
For those interested in learning more about econometric models, which will be less emphasized than machine learning models here, I recommend reading @Turrell2021.

:::

::: {.content-visible when-profile="fr"}

# Pour aller plus loin

Cette partie est une introduction au _machine learning_. Elle n'√©voque pas les axes de recherche r√©cents. Parmi ceux-ci on peut mettre en avant:

::::: {.column-margin}
üëàÔ∏è __Interpr√©tabilit√©, pr√©diction conforme, bayesian methods__
:::::

1. L'__interpr√©tabilit√©__ : un ensemble de m√©thodes visant √† ouvrir la boite noire des mod√®les de _machine learning_. Il s'agit d'un ensemble de techniques permettant de mieux comprendre la mani√®re dont un mod√®le, √† partir de certains _inputs_, abouti √† une pr√©diction. Parmi les m√©thodes en vogue aujourd'hui, on peut citer [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) et les [valeurs de Shapley](https://christophm.github.io/interpretable-ml-book/shap.html). Pour en apprendre plus, un bon point d'entr√©e est @christoph2020interpretable. 
2. La __pr√©diction conforme__ : approche statistique qui permet de donner une estimation de l‚Äôincertitude d‚Äôune pr√©diction, en g√©n√©rant des intervalles de confiance pour chaque pr√©diction individuelle. Elle garantit un niveau de pr√©cision fix√© a priori, ce qui aide √† fournir des pr√©dictions fiables et compr√©hensibles. Pour en apprendre plus, se r√©f√©rer √† l'article technique d'@angelopoulos2021gentle.
3. Les __m√©thodes bay√©siennes__: ensemble de m√©thodes qui consistent √† introduire une incertitude dans l'estimation des param√®tres et √† mettre √† jour celle-ci √† partir des donn√©es observ√©es. Ces m√©thodes sont fr√©quemment utilis√©es dans un cadre statistique o√π on d√©sire n√©anmoins avoir de la flexibilit√© par rapport aux hypoth√®ses de mod√©lisation. Elles ont √©t√© popularis√©es par l'ouvrage grand public @silver2012signal (cr√©ateur du site _[fivethirtyeight](https://projects.fivethirtyeight.com/)_) qui pr√©sente plusieurs cas d'application (pr√©visions sportives, √©lectorales, etc.) et sont l'objet de plusieurs cours d√©di√©s √† l'ENSAE, notamment le cours de _Monte Carlo Markov Chain_ (MCMC) en troisi√®me ann√©e. Nous √©voquerons n√©anmoins parfois cette famille de m√©thode, notamment lorsque nous pr√©senterons le classifieur naif de Bayes.


## R√©f√©rences

:::

::: {.content-visible when-profile="en"}

# Going further

This section is an introduction to machine learning. It does not cover recent research areas, among which we can highlight:

::::: {.column-margin}
üëàÔ∏è __Interpretability, conformal prediction, bayesian methods__
:::::

1. __Interpretability__: a set of methods aimed at opening up the black box of machine learning models. This includes techniques that help to better understand how a model, given certain inputs, arrives at a prediction. Popular methods today include [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) and [Shapley values](https://christophm.github.io/interpretable-ml-book/shap.html). For more information, a good starting point is @christoph2020interpretable.
2. __Conformal Prediction__: a statistical approach that provides an estimate of the uncertainty of a prediction by generating confidence intervals for each individual prediction. It guarantees a predetermined accuracy level, helping to deliver reliable and understandable predictions. For further reading, refer to the technical article by @angelopoulos2021gentle.
3. __Bayesian methods__: a set of methods which introduce uncertainty into parameter estimation, and update this uncertainty on the basis of observed data. These methods are frequently used in a statistical context, where flexibility is nevertheless required with regard to modeling assumptions. They have been popularized by the popular book @silver2012signal (creator of  _[fivethirtyeight](https://projects.fivethirtyeight.com/) website_), which presents several applications (sports forecasts, elections, etc.) and are the subject of several dedicated courses at ENSAE, notably the third-year _Monte Carlo Markov Chain_ (MCMC) course. Nevertheless, we will occasionally refer to this family of methods, in particular when we present Bayes' naive classifier.


## References

:::


::: {#refs}
:::

