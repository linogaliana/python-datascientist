---
title: "Introduction à la régression"
title-en: "An introduction to regression"
author: Lino Galiana
categories:
  - Modélisation
description: |
  La régression linéaire est la première modélisation statistique qu'on découvre dans un cursus quantitatif. Il s'agit en effet d'une méthode très intuitive et très riche. Le _Machine Learning_ permet de l'appréhender d'une autre manière que l'économétrie. Avec `scikit` et `statsmodels`, on dispose de tous les outils pour satisfaire à la fois data scientists et économistes.
description-en: |
  Linear regression is the first statistical modeling to be discovered in a quantitative curriculum. It is a very intuitive and rich method. Machine Learning allows us to approach it in a different way to econometrics. With `scikit` and `statsmodels`, we have all the tools we need to satisfy both data scientists and economists. 
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/kid-regression.jfif
echo: false
bibliography: ../../reference.bib
---

{{< badges
    printMessage="true"
>}}


::: {.content-visible when-profile="fr"}
Le précédent chapitre visait à proposer un premier modèle pour comprendre
les comtés où le parti Républicain l'emporte. La variable d'intérêt étant
bimodale (victoire ou défaite), on était dans le cadre d'un modèle de 
classification.

Maintenant, sur les mêmes données, on va proposer un modèle de régression
pour expliquer le score du parti Républicain. La variable est donc continue.
Nous ignorerons le fait que ses bornes se trouvent entre 0 et 100 et donc 
qu'il faudrait, pour être rigoureux, transformer l'échelle afin d'avoir 
des données dans cet intervalle.
:::

::: {.content-visible when-profile="en"}
The previous chapter aimed to propose a first model to understand the counties where the Republican Party wins. The variable of interest was bimodal (win or lose), placing us within the framework of a classification model.

Now, using the same data, we will propose a regression model to explain the Republican Party's score. The variable is thus continuous. We will ignore the fact that its bounds lie between 0 and 100, meaning that to be rigorous, we would need to transform the scale so that the data fits within this interval.
:::


{{< include _import_data_ml.qmd >}}

::: {.content-visible when-profile="fr"}
Ce chapitre va utiliser plusieurs _packages_ 
de modélisation, les principaux étant `Scikit` et `Statsmodels`. 
Voici une suggestion d'import pour tous ces _packages_.
:::

::: {.content-visible when-profile="en"}
This chapter will use several modeling _packages_, the main ones being `Scikit` and `Statsmodels`. 
Here is a suggested import for all these _packages_.
:::


```{python}
#| echo: true
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import sklearn.metrics
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
```


::: {.content-visible when-profile="fr"}
# Principe général

Le principe général de la régression consiste à trouver une loi $h_\theta(X)$
telle que

$$
h_\theta(X) = \mathbb{E}_\theta(Y|X)
$$

Cette formalisation est extrêmement généraliste et ne se restreint d'ailleurs
pas à la régression linéaire. 

En économétrie, la régression offre une alternative aux méthodes de maximum
de vraisemblance et aux méthodes des moments. La régression est un ensemble 
très vaste de méthodes, selon la famille de modèles
(paramétriques, non paramétriques, etc.) et la structure de modèles.
:::

::: {.content-visible when-profile="en"}
# General Principle

The general principle of regression consists of finding a law $h_\theta(X)$
such that

$$
h_\theta(X) = \mathbb{E}_\theta(Y|X)
$$

This formalization is extremely general and is not limited to linear regression.

In econometrics, regression offers an alternative to maximum likelihood methods
and moment methods. Regression encompasses a very broad range of methods, depending on the family of models
(parametric, non-parametric, etc.) and model structures.
:::


::: {.content-visible when-profile="fr"}
## La régression linéaire

C'est la manière la plus simple de représenter la loi $h_\theta(X)$ comme 
combinaison linéaire de variables $X$ et de paramètres $\theta$. Dans ce
cas, 

$$
\mathbb{E}_\theta(Y|X) = X\beta
$$

Cette relation est encore, sous cette formulation, théorique. Il convient 
de l'estimer à partir des données observées $y$. La méthode des moindres
carrés consiste à minimiser l'erreur quadratique entre la prédiction et 
les données observées (ce qui explique qu'on puisse voir la régression comme
un problème de *Machine Learning*). En toute généralité, la méthode des
moindres carrés consiste à trouver l'ensemble de paramètres $\theta$ 
tel que
:::

::: {.content-visible when-profile="en"}
## Linear Regression

This is the simplest way to represent the law $h_\theta(X)$ as 
a linear combination of variables $X$ and parameters $\theta$. In this case,

$$
\mathbb{E}_\theta(Y|X) = X\beta
$$

This relationship is, under this formulation, theoretical. It must 
be estimated from the observed data $y$. The method of least squares aims to minimize 
the quadratic error between the prediction and the observed data (which explains 
why regression can be seen as a *Machine Learning* problem). In general, the method of 
least squares seeks to find the set of parameters $\theta$ such that
:::

$$
\theta = \arg \min_{\theta \in \Theta} \mathbb{E}\bigg[ \left( y - h_\theta(X) \right)^2 \bigg]
$$

::: {.content-visible when-profile="fr"}
Ce qui, dans le cadre de la régression linéaire, s'exprime de la manière suivante :
:::

::: {.content-visible when-profile="en"}
Which, in the context of linear regression, is expressed as follows:
:::

$$
\beta = \arg\min \mathbb{E}\bigg[ \left( y - X\beta \right)^2 \bigg]
$$

::: {.content-visible when-profile="fr"}
Lorsqu'on amène le modèle théorique ($\mathbb{E}_\theta(Y|X) = X\beta$) aux données,
on formalise le modèle de la manière suivante :
:::

::: {.content-visible when-profile="en"}
When the theoretical model ($\mathbb{E}_\theta(Y|X) = X\beta$) is applied to data,
the model is formalized as follows:
:::

$$
Y = X\beta + \epsilon
$$

::: {.content-visible when-profile="fr"}
Avec une certaine distribution du bruit $\epsilon$ qui dépend
des hypothèses faites. Par exemple, avec des 
$\epsilon \sim \mathcal{N}(0,\sigma^2)$ i.i.d., l'estimateur $\beta$ obtenu
est équivalent à celui du Maximum de Vraisemblance dont la théorie asymptotique
nous assure l'absence de biais, la variance minimale (borne de Cramer-Rao).
:::

::: {.content-visible when-profile="en"}
With a certain distribution of the noise $\epsilon$ that depends
on the assumptions made. For example, with 
$\epsilon \sim \mathcal{N}(0,\sigma^2)$ i.i.d., the estimator $\beta$ obtained
is equivalent to the Maximum Likelihood Estimator, whose asymptotic theory
ensures unbiasedness and minimum variance (Cramer-Rao bound).
:::

### Application

::: {.content-visible when-profile="fr"}
Toujours sous le patronage des héritiers de @siegfried1913tableau, notre objectif, dans ce chapitre, est d'expliquer et prédire le score des Républicains à partir de quelques
variables socioéconomiques. Contrairement au chapitre précédent, où on se focalisait sur
un résultat binaire (victoire/défaite des Républicains), cette
fois on va chercher à modéliser directement le score des Républicains. 

Le prochain exercice vise à illustrer la manière d'effectuer une régression linéaire avec `scikit`.
Dans ce domaine,
`statsmodels` est nettement plus complet, ce que montrera l'exercice suivant.
L'intérêt principal de faire 
des régressions avec `scikit` est de pouvoir comparer les résultats d'une régression linéaire
avec d'autres modèles de régression dans une perspective de sélection du meilleur modèle prédictif.
:::

::: {.content-visible when-profile="en"}
Under the guidance of the heirs of @siegfried1913tableau, our objective in this chapter is to explain and predict the Republican score based on some socioeconomic variables. Unlike the previous chapter, where we focused on a binary outcome (Republican victory/defeat), this time we will model the Republican score directly.

The next exercise aims to demonstrate how to perform linear regression using `scikit`.
In this area, `statsmodels` is significantly more comprehensive, as the following exercise will demonstrate.
The main advantage of performing regressions with `scikit` is the ability to compare the results of linear regression with other regression models in the context of selecting the best predictive model.
:::

:::: {.content-visible when-profile="fr"}

::: {.exercise}
## Exercice 1a : Régression linéaire avec scikit


1. A partir de quelques variables, par exemple, *'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2018-22', "Percent of adults with a bachelor's degree or higher, 2018-22"*, expliquer la variable `per_gop` à l'aide d'un échantillon d'entraînement `X_train` constitué au préalable.

⚠️ Utiliser la variable `Median_Household_Income_2021`
en `log` sinon son échelle risque d'écraser tout effet.

2. Afficher les valeurs des coefficients, constante comprise

3. Evaluer la pertinence du modèle avec le $R^2$ et la qualité du fit avec le MSE.

4. Représenter un nuage de points des valeurs observées
et des erreurs de prédiction. Observez-vous
un problème de spécification ?

:::

::::

:::: {.content-visible when-profile="en"}

::: {.exercise}
## Exercise 1a: Linear Regression with scikit

1. Using a few variables, for example, *'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2018-22', "Percent of adults with a bachelor's degree or higher, 2018-22"*, explain the variable `per_gop` using a training sample `X_train` prepared beforehand.

⚠️ Use the variable `Median_Household_Income_2021` in `log` form; otherwise, its scale might dominate and obscure other effects.

2. Display the values of the coefficients, including the constant.

3. Evaluate the relevance of the model using $R^2$ and assess the fit quality with the MSE.

4. Plot a scatter plot of observed values and prediction errors. Do you observe any specification issues?
:::

::::

```{python}
#| output: false

# Question 1
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]


df2 = (
  votes.loc[:,["per_gop"] + xvars]
  .dropna()
  .copy()
)
df2['log_income'] = np.log(df2["Median_Household_Income_2021"])
df2 = df2.dropna().astype(np.float64)


X_train, X_test, y_train, y_test = train_test_split(
    df2.drop(["Median_Household_Income_2021","per_gop"], axis = 1),
    100*df2[['per_gop']].values.ravel(), test_size=0.2, random_state=0
)

ols = LinearRegression().fit(X_train, y_train)
y_pred = ols.predict(X_test)
```



```{python}
#| output: false

# Question 2
print(ols.intercept_, ols.coef_)
```


```{python}
#| output: false

# Question 3

rmse = sklearn.metrics.root_mean_squared_error(y_test, y_pred)
rsq = sklearn.metrics.r2_score(y_test, y_pred) 

print(
  f'Root mean squared error: {rmse:.2f}'
)

print(
  f'Coefficient of determination: {rsq:.2f}'
)
```

::: {.content-visible when-profile="fr"}
À la question 4, on peut voir que la répartition des erreurs n'est clairement pas aléatoire en fonction de $X$.
:::

::: {.content-visible when-profile="en"}
In question 4, it can be observed that the distribution of errors is clearly not random with respect to $X$.
:::


```{python}
#| output: false

#4. Nuage de points des valeurs observées
tempdf = pd.DataFrame(
  {
    "prediction": y_pred, "observed": y_test,
    "error": y_test - y_pred
  }
)

fig = plt.figure()
g = sns.scatterplot(data = tempdf, x = "observed", y = "error")
g.axhline(0, color = "red")
```

```{python}
g.figure.get_figure()
```

::: {.content-visible when-profile="fr"}
Le modèle souffre donc d'un problème de spécification, il faudra par la suite faire un travail sur les variables sélectionnées. Avant cela, on peut refaire cet exercice avec le _package_ `statsmodels`.
:::

::: {.content-visible when-profile="en"}
The model therefore suffers from a specification issue, so work will need to be done on the selected variables later. Before that, we can redo this exercise using the `statsmodels` package.
:::


```{python}
# packages utiles
import statsmodels.api as sm
import statsmodels.formula.api as smf
```

:::: {.content-visible when-profile="fr"}

::: {.exercise}
## Exercice 1b : Régression linéaire avec statsmodels

Cet exercice vise à illustrer la manière d'effectuer une régression linéaire avec `statsmodels` qui offre des fonctionnalités plus proches de celles de `R`, et moins orientées *Machine Learning*.

L'objectif est toujours d'expliquer le score des Républicains en fonction de quelques
variables.


1. A partir de quelques variables, par exemple, *'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2018-22', "Percent of adults with a bachelor's degree or higher, 2018-22"*, expliquer la variable `per_gop`. 
⚠️ utiliser la variable `Median_Household_Income_2021`
en `log` sinon son échelle risque d'écraser tout effet.
2. Afficher un tableau de régression.
3. Evaluer la pertinence du modèle avec le R^2.
4. Utiliser l'API `formula` pour régresser le score des républicains en fonction de la variable `Unemployment_rate_2021`, de `Unemployment_rate_2019` au carré et du log de 
`Median_Household_Income_2021`.

:::

::::

:::: {.content-visible when-profile="en"}

::: {.exercise}
## Exercise 1b: Linear Regression with statsmodels

This exercise aims to demonstrate how to perform linear regression using `statsmodels`, which offers features more similar to those of `R` and less oriented toward *Machine Learning*.

The goal is still to explain the Republican score based on a few variables.

1. Using a few variables, for example, *'Unemployment_rate_2019', 'Median_Household_Income_2021', 'Percent of adults with less than a high school diploma, 2015-19', "Percent of adults with a bachelor's degree or higher, 2015-19"*, explain the variable `per_gop`.  
⚠️ Use the variable `Median_Household_Income_2021` in `log` form; otherwise, its scale might dominate and obscure other effects.

2. Display a regression table.

3. Evaluate the model's relevance using the R^2.

4. Use the `formula` API to regress the Republican score as a function of the variable `Unemployment_rate_2019`, `Unemployment_rate_2019` squared, and the log of `Median_Household_Income_2021`.
:::

::::


```{python}
#| output: false

# Question 1
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]

df2 = (
  votes.loc[:,["per_gop"] + xvars]
  .dropna()
  .copy()
)
df2['log_income'] = np.log(df2["Median_Household_Income_2021"])
df2 = df2.dropna().astype(np.float64)

X = sm.add_constant(df2.drop(["Median_Household_Income_2021","per_gop"], axis = 1))
results = sm.OLS(df2[['per_gop']], X).fit()
```


```{python}
#| output: false

# Question 2
print(results.summary())
```


```{python}
# 3. Calcul du R^2
print("R2: ", results.rsquared)
```



```{python}
#| output: false

# 4. Nouvelle régression avec l'API formula
results = smf.ols('per_gop ~ Unemployment_rate_2019 + I(Unemployment_rate_2019**2) + np.log(Median_Household_Income_2021)', data=df2).fit()
print(results.summary())
```

:::: {.content-visible when-profile="fr"}

::: {.tip}

Pour sortir une belle table pour un rapport sous $\LaTeX$, il est possible d'utiliser
la méthode [`Summary.as_latex`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex). Pour un rapport HTML, on utilisera [`Summary.as_html`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex)

:::

::: {.note}

Les utilisateurs de `R` retrouveront des éléments très familiers avec `statsmodels`,
notamment la possibilité d'utiliser une formule pour définir une régression.
La philosophie de `statsmodels` est similaire à celle qui a influé sur la construction
des packages `stats` et `MASS` de `R`: offrir une librairie généraliste, proposant
une large gamme de modèles.

Néanmoins, `statsmodels` bénéficie de sa jeunesse
par rapport aux packages `R`. Depuis les années 1990, les packages `R` visant 
à proposer des fonctionalités manquantes dans `stats` et `MASS` se sont
multipliés alors que `statsmodels`, enfant des années 2010, n'a eu qu'à
proposer un cadre général (les *generalized estimating equations*) pour
englober ces modèles.

:::

::::

:::: {.content-visible when-profile="en"}

::: {.tip}
To generate a well-formatted table for a report in $\LaTeX$, you can use the method [`Summary.as_latex`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex). For an HTML report, you can use [`Summary.as_html`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex).
:::

::: {.note}
Users of `R` will find many familiar features in `statsmodels`, particularly the ability to use a formula to define a regression. The philosophy of `statsmodels` is similar to that which influenced the construction of `R`'s `stats` and `MASS` packages: providing a general-purpose library with a wide range of models.

However, `statsmodels` benefits from being more modern compared to `R`'s packages. Since the 1990s, `R` packages aiming to provide missing features in `stats` and `MASS` have proliferated, while `statsmodels`, born in the 2010s, only had to propose a general framework (the *generalized estimating equations*) to encompass these models.
:::


::::

## La régression logistique

::: {.content-visible when-profile="fr"}
Nous avons appliqué notre régression linéaire sur une variable d'_outcome_ continue. 
Comment faire avec une distribution binaire ?  
Dans ce cas, $\mathbb{E}_{\theta} (Y|X) = \mathbb{P}_{\theta} (Y = 1|X)$.  
La régression logistique peut être vue comme un modèle linéaire en probabilité :
:::

::: {.content-visible when-profile="en"}
We applied our linear regression to a continuous _outcome_ variable. 
How do we handle a binary distribution?  
In this case, $\mathbb{E}_{\theta} (Y|X) = \mathbb{P}_{\theta} (Y = 1|X)$.  
Logistic regression can be seen as a linear probability model:
:::

$$
\text{logit}\bigg(\mathbb{E}_{\theta}(Y|X)\bigg) = \text{logit}\bigg(\mathbb{P}_{\theta}(Y = 1|X)\bigg) = X\beta
$$

::: {.content-visible when-profile="fr"}
La fonction $\text{logit}$ est $]0,1[ \to \mathbb{R}: p \mapsto \log(\frac{p}{1-p})$.

Elle permet ainsi de transformer une probabilité dans $\mathbb{R}$.
Sa fonction réciproque est la sigmoïde ($\frac{1}{1 + e^{-x}}$),
objet central du *Deep Learning*.

Il convient de noter que les probabilités ne sont pas observées, c'est l'*outcome*
binaire (0/1) qui l'est. Cela amène à voir la régression logistique de deux
manières différentes :

* En économétrie, on s'intéresse au modèle latent qui détermine le choix de
l'outcome. Par exemple, si on observe les choix de participer ou non au marché
du travail, on va modéliser les facteurs déterminant ce choix ;
* En *Machine Learning*, le modèle latent n'est nécessaire que pour classifier
dans la bonne catégorie les observations.

L'estimation des paramètres $\beta$ peut se faire par maximum de vraisemblance
ou par régression, les deux solutions sont équivalentes sous certaines
hypothèses.
:::

::: {.content-visible when-profile="en"}
The $\text{logit}$ function is $]0,1[ \to \mathbb{R}: p \mapsto \log(\frac{p}{1-p})$.

It allows a probability to be transformed into $\mathbb{R}$.
Its reciprocal function is the sigmoid ($\frac{1}{1 + e^{-x}}$),
a central concept in *Deep Learning*.

It should be noted that probabilities are not observed; what is observed is the binary 
*outcome* (0/1). This leads to two different perspectives on logistic regression:

* In econometrics, interest lies in the latent model that determines the choice of
the outcome. For example, if observing the choice to participate in the labor market,
the goal is to model the factors determining this choice;
* In *Machine Learning*, the latent model is only necessary to classify
observations into the correct category.

Parameter estimation for $\beta$ can be performed using maximum likelihood
or regression, both of which are equivalent under certain assumptions.
:::


:::: {.content-visible when-profile="fr"}

::: {.note}

Par défaut, `scikit` applique une régularisation pour pénaliser les modèles
peu parcimonieux (comportement différent
de celui de `statsmodels`). Ce comportement par défaut est à garder à l'esprit
si l'objectif n'est pas de faire de la prédiction. 

:::

::::

:::: {.content-visible when-profile="fr"}

::: {.note}

By default, `scikit` applies regularization to penalize non-parsimonious models (a behavior different from that of `statsmodels`). This default behavior should be kept in mind if the objective is not prediction.

:::

::::


```{python}
# packages utiles
from sklearn.linear_model import LogisticRegression
import sklearn.metrics
```

:::: {.content-visible when-profile="fr"}

::: {.exercise}
## Exercice 2a : Régression logistique avec scikit

Avec `scikit`, en utilisant échantillons d'apprentissage et d'estimation :

1. Evaluer l'effet des variables déjà utilisées sur la probabilité des Républicains
de gagner. Affichez la valeur des coefficients.
2. Déduire une matrice de confusion et 
une mesure de qualité du modèle.
3. Supprimer la régularisation grâce au paramètre `penalty`. Quel effet sur les paramètres estimés ?

:::

::::

:::: {.content-visible when-profile="en"}

::: {.exercise}
## Exercise 2a: Logistic Regression with scikit

Using `scikit` with training and test samples:

1. Evaluate the effect of the already-used variables on the probability of Republicans winning. Display the values of the coefficients.
2. Derive a confusion matrix and a measure of model quality.
3. Remove regularization using the `penalty` parameter. What effect does this have on the estimated parameters?

:::

::::


```{python}
#| output: false

#1. Modèle logit avec les mêmes variables que précédemment
xvars = [
  'Unemployment_rate_2019', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"
]

df2 = votes.loc[:, ["per_gop"] + xvars].copy()
df2['log_income'] = np.log(df2["Median_Household_Income_2021"])
df2 = df2.dropna().astype(np.float64)


df2['y'] = (df2['per_gop']>0.5).astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    df2.drop(["Median_Household_Income_2021","y"], axis = 1),
    1*df2[['y']].values.ravel(), test_size=0.2, random_state=0
)

clf = LogisticRegression().fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(clf.intercept_, clf.coef_)
```


```{python}
#| output: false

from sklearn.metrics import ConfusionMatrixDisplay

# 2. Matrice de confusion
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)

sc_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)
sc_f1 = sklearn.metrics.f1_score(y_pred, y_test)
sc_recall = sklearn.metrics.recall_score(y_pred, y_test)
sc_precision = sklearn.metrics.precision_score(y_pred, y_test)

print(sc_accuracy)
print(sc_f1)
print(sc_recall)
print(sc_precision)
```



```{python}
#| output: false

#3. Supprimer la régularisation
clf2 = LogisticRegression(penalty=None).fit(X_train, y_train)
y_pred2 = clf.predict(X_test)
print(clf2.intercept_, clf2.coef_)
# Les coefficients sont complètement différents

```




```{python}
# packages utiles
from scipy import stats
```


:::: {.content-visible when-profile="fr"}

::: {.exercise}
## Exercice 2b : Régression logistique avec statmodels

En utilisant échantillons d'apprentissage et d'estimation :

1. Evaluer l'effet des variables déjà utilisées sur la probabilité des Républicains
de gagner.
2. Faire un test de ratio de vraisemblance concernant l'inclusion de la variable de (log)-revenu.
:::

::::

:::: {.content-visible when-profile="en"}

::: {.exercise}
## Exercise 2b: Logistic Regression with statsmodels

Using training and test samples:

1. Evaluate the effect of the already-used variables on the probability of Republicans winning.
2. Perform a likelihood ratio test regarding the inclusion of the (log)-income variable.
:::

::::


```{python}
#| output: false

#1. Modèle logit avec les mêmes variables que précédemment
xvars = [
  'Unemployment_rate_2021', 'Median_Household_Income_2021',
  'Percent of adults with less than a high school diploma, 2018-22',
  "Percent of adults with a bachelor's degree or higher, 2018-22"]

df2 = votes[["per_gop"] + xvars]
df2['log_income'] = np.log(df2["Median_Household_Income_2021"])
df2 = df2.dropna().astype(np.float64)

df2['y'] = (df2['per_gop']>0.5).astype(int)

mylogit = smf.logit(
  formula = 'y ~ Unemployment_rate_2021 + I(Unemployment_rate_2021**2) + np.log(Median_Household_Income_2021)',
  data=df2[df2['Median_Household_Income_2021']>0]).fit()
print(mylogit.summary())
```


```{python}
#| output: false

#2. Faire un test de ratio de vraisemblance 
logit_h0 = smf.logit(
  formula = 'y ~ Unemployment_rate_2021 + I(Unemployment_rate_2021**2)',
  data=df2[df2['Median_Household_Income_2021']>0]).fit()
# print(logit_h0.summary())

lr = -2*(mylogit.llf - logit_h0.llf)
lrdf = (logit_h0.df_resid - mylogit.df_resid)

lr_pvalue = stats.chi2.sf(lr, df=lrdf)
lr_pvalue
```


:::: {.content-visible when-profile="fr"}
La p-value du test de maximum de ratio de vraisemblance étant proche de 1, cela signifie que la variable log revenu ajoute,
presque à coup sûr, de l'information au modèle.
::::

:::: {.content-visible when-profile="en"}
The p-value of the likelihood ratio test being close to 1 means that the log-income variable almost certainly adds information to the model.
::::


:::: {.content-visible when-profile="fr"}

::: {.tip}

La statistique du test est :
$$
LR = -2\log\bigg(\frac{\mathcal{L}_{\theta}}{\mathcal{L}_{\theta_0}}\bigg) = -2(\mathcal{l}_{\theta} - \mathcal{l}_{\theta_0})
$$

:::

::::

:::: {.content-visible when-profile="en"}

::: {.tip}

The test statistic is:
$$
LR = -2\log\bigg(\frac{\mathcal{L}_{\theta}}{\mathcal{L}_{\theta_0}}\bigg) = -2(\mathcal{l}_{\theta} - \mathcal{l}_{\theta_0})
$$

:::

::::


:::: {.content-visible when-profile="fr"}
# Pour aller plus loin

Ce chapitre n'évoque les enjeux de la régression
que de manière très introductive. Pour compléter ceci,
il est recommandé d'aller plus loin en fonction de vos centres d'intérêt et de vos besoins. 

Dans le domaine du _machine learning_, les principales voies d'approfondissement sont les suivantes:

- Les modèles de régression alternatifs comme les forêts
aléatoires.
- Les méthodes de _boosting_ et _bagging_ pour découvrir la manière dont plusieurs modèles peuvent être entraînés de manière conjointe et leur prédiction sélectionnée selon un principe démocratique pour converger vers une meilleure décision qu'un modèle simple. 
- Les enjeux liés à l'explicabilité des modèles, un champ de recherche très actif, pour mieux comprendre les critères de décision des modèles.

Dans le domaine de l'économétrie, les principales voies d'approfondissement sont les suivantes:

- Les modèles linéaires généralisés pour découvrir la régression 
avec des hypothèses plus générales que celles que nous avons posées
jusqu'à présent ; 
- Les tests d'hypothèses pour aller plus loin sur ces questions que notre
test de ratio de vraisemblance. 

## Références {.unnumbered}
::::

:::: {.content-visible when-profile="en"}
# Going Further

This chapter only introduces the concepts of regression in a very introductory way. To expand on this, it is recommended to explore further based on your interests and needs.

In the field of _machine learning_, the main areas for deeper exploration are:

- Alternative regression models like random forests.
- _Boosting_ and _bagging_ methods to learn how multiple models can be trained jointly and their predictions combined democratically to converge on better decisions than a single model. 
- Issues related to model explainability, a very active research area, to better understand the decision criteria of models.

In the field of econometrics, the main areas for deeper exploration are:

- Generalized linear models to explore regression with more general assumptions than those we have made so far;
- Hypothesis testing to delve deeper into these questions beyond our likelihood ratio test.

## References {.unnumbered}

::::
