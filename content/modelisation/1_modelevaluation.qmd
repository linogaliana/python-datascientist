---
title: "Evaluer la qualité d'un modèle"
title-en: "Evaluating model quality"
author: Lino Galiana
tags:
  - scikit
  - machine learning
  - US elections
  - model performance
  - Modelisation
  - Exercice
categories:
  - Modélisation
  - Exercice
description: |
  La raison d'être du _machine learning_ est de chercher à créer des règles de décision qui ont de bonnes performances prédictives sur un nouvel échantillon. Pour éviter le surapprentissage, c'est-à-dire pour avoir un modèle ayant une bonne validité externe, outre la préparation des données vue dans le chapitre précédent, il sera nécessaire d'évaluer les modèles. Ce chapitre se plonge sur le sujet de l'évaluation des modèles et des enjeux de ceci. Cela permettra d'évoquer les enjeux de l'évaluation dans un cadre d'apprentissage supervisé comme non supervisé, de présenter la méthode de la validation croisée mais aussi d'ouvrir à des concepts comme le _data drift_ ou l'évaluation des modèles type LLM à l'état de l'art.
description-en: |
  The purpose of machine learning is to create decision rules with good predictive performance on a new sample. To avoid overlearning, i.e. to have a model with good external validity, in addition to the data preparation seen in the previous chapter, it will be necessary to evaluate the models. This chapter delves into the subject of model evaluation and the issues involved. It will discuss the challenges of evaluation in both supervised and unsupervised learning environments, introduce the cross-validation method and open up to concepts such as _data drift_ and state-of-the-art LLM-type model evaluation.
image: https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png
---

{{< badges
    printMessage="true"
>}}
:::: {.content-visible when-profile="fr"}

::: {.callout-tip collapse="true"}

## Compétences à l'issue de ce chapitre

- Comprendre que l’objectif du machine learning est d’élaborer des règles de décision efficaces sur des données nouvelles, en favorisant la validité externe plutôt que le simple ajustement aux données d’entraînement ;
- Mettre en place une méthodologie pour éviter le sur-apprentissage (overfitting), notamment via la séparation rigoureuse entre ensembles d’entraînement et de validation ;
- Connaître les méthodes d’évaluation selon le type d’apprentissage : apprentissage supervisé (classification, régression) et non supervisé ;
- Utiliser la validation croisée pour obtenir une évaluation plus robuste et fiable des performances du modèle ;
- Prendre en compte les enjeux modernes de l’évaluation, comme le data drift (dérive des données) et la supervision continue des modèles en production ;
- Comprendre les nouvelles problématiques liées à l’évaluation des modèles génératifs tels que les grands modèles de langage (LLM).


:::

::::

  
::::: {.content-visible when-profile="en"}

::: {.callout-tip collapse="true"}

## Skills you will acquire in this chapter

- Understand that the goal of machine learning is to develop decision rules that generalize well to new data, prioritizing external validity over fitting the training set  
- Apply strategies to prevent overfitting, especially by carefully separating training and validation data  
- Learn appropriate evaluation methods based on the type of learning task: supervised (classification, regression) or unsupervised  
- Use cross-validation to obtain more robust and reliable assessments of model performance  
- Consider modern evaluation challenges, including data drift and the need for continuous monitoring of models in production  
- Understand emerging issues in evaluating generative models, such as large language models (LLMs)  


:::

::::
::: {.content-visible when-profile="fr"}
Le _machine learning_ a l'ambition de proposer des méthodes prédictives simples à mettre en oeuvre sur le plan opérationnel. Cette promesse séduit forcément des acteurs ayant un volume de données conséquent et désirant utiliser celles-ci pour anticiper le comportement de clients ou d'utilisateurs de services. Nous avons vu lors du chapitre précédent la manière de structurer un problème en échantillons d'apprentissage et de validation (@fig-ml-pipeline) mais sans en expliquer la raison d'être.

![Illustraton de la méthodologie du _machine learning_](/content/modelisation/img/pipeline1.png){#fig-ml-pipeline}
:::

::: {.content-visible when-profile="en"}
Machine learning aims to offer predictive methods that are simple to implement from an operational standpoint. This promise naturally appeals to stakeholders with a significant volume of data who wish to use it to predict customer or service user behavior. In the previous chapter, we saw how to structure a problem into training and validation samples (@fig-ml-pipeline) but without explaining the rationale behind it.

![Machine learning methodology illustrated](/content/modelisation/img/pipeline1.png){#fig-ml-pipeline}
:::



::: {.content-visible when-profile="fr"}
# Une méthodologie pour éviter le sur-apprentissage

Puisque l'objectif du _machine learning_ est de mettre en oeuvre un modèle sur une population cible qui n'est pas celle sur laquelle le modèle a appris - par exemple un modèle de _scoring_ ne sert pas à changer les prêts des clients existants mais à prendre une décision pour de nouveaux clients - il est logique de privilégier la validité externe d'un modèle. Pour que les anticipations de performance d'un modèle soient réalistes, il est donc nécessaire d'évaluer les modèles dans un cadre similaire à celui dans lequel il sera mis en oeuvre ultérieurement. Autrement dit, une évaluation honnête d'un modèle se doit d'être une évaluation de la validité externe d'un modèle, c'est-à-dire de la capacité à être bon sur une population qu'il n'a pas rencontré lors de son entraînement.

Pourquoi s'embarasser de cette considération ? Parce que construire un modèle sur un échantillon et l'évaluer sur celui-ci amène à une validité interne forte, au détriment de la validité externe. Autrement dit, si vous avez un contrôle sur les questions de cours, et strictement sur celles-ci, la meilleure stratégie à mettre en oeuvre est d'apprendre par coeur votre cours et le restituer comme tel. Un test de ce type ne cherche pas à savoir si vous comprenez votre cours, seulement si vous avez appris votre cours. Il s'agit de tester la validité interne de vos connaissances. Plus les questions s'éloigneront de ce que vous avez appris par coeur, plus vous serez en difficulté.

L'idée est la même pour un algorithme: plus son apprentissage est adhérant à un échantillon initial, plus ses performances prédictives, et donc sa valeur en pratique, seront limités. C'est pour cette raison qu'on évalue la qualité d'un modèle sur un échantillon qu'il n'a pas vu dans son apprentissage: pour privilégier la validité externe à la validité interne.

On parle de __sur-apprentissage__ lorsqu'un modèle a une bonne validité interne mais une mauvaise validité externe, c'est-à-dire de mauvaises qualités prédictives sur un autre échantillon que celui sur lequel il a appris. La structuration d'un problème d'apprentissage sous la forme d'échantillons _train_/_test_ est une réponse à ce défi puisqu'elle permet de sélectionner le meilleur modèle en extrapolation. Ce sujet peut apparaître trivial mais, dans les faits, de nombreux champs scientifiques empiriques n'adoptent pas cette méthodologie pour émettre des conclusions au-delà de la population sur laquelle ils ont travaillé.

Par exemple, en économie, il est assez commun d'évaluer une politique publique _toutes choses égales par ailleurs_, d'en déduire un effet marginal et de préconiser des recommandations politiques au-delà de celle-ci. Cependant, il est rare que la politique suivante s'applique à la même population cible ou dans les mêmes conditions institutionnelles, de sorte qu'elle produit généralement des effets différents. Les biais d'échantillonnage, du point de vue des caractéristiques individuelles ou de la période d'étude, sont souvent relégués au second plan et l'estimation des effets marginaux se fait généralement indépendamment de toute notion de validité externe.

Pour en revenir à l'objet de ce chapitre, formellement, ce problème vient de l'__arbitrage biais-variance__ dans la qualité d'estimation. Soit $h(X,\theta)$ un modèle statistique. On 
peut décomposer l'erreur d'estimation en deux parties :
:::

::: {.content-visible when-profile="en"}
# Methodology to avoid overfitting

Since the goal of machine learning is to implement a model on a target population different from the one it was trained on—for example, a scoring model is not used to change the loans of existing customers but to make decisions for new customers—it makes sense to prioritize the external validity of a model. To ensure that a model's performance predictions are realistic, it is therefore necessary to evaluate models in a framework similar to the one in which they will later be implemented. In other words, an honest evaluation of a model must be an evaluation of its external validity, that is, its ability to perform well on a population it has not encountered during training.

Why bother with this consideration? Because building a model on a sample and evaluating it on the same sample leads to strong internal validity at the expense of external validity. In other words, if you have control over the exam questions and only those questions, the best strategy is to memorize your material and reproduce it verbatim. Such a test does not assess your understanding of the material, only your ability to memorize it. This is a test of the internal validity of your knowledge. The further the questions deviate from what you have memorized, the more challenging they will become.

The same idea applies to an algorithm: the more its learning adheres to the initial sample, the more its predictive performance—and thus its practical value—will be limited. This is why the quality of a model is evaluated on a sample it has not seen during training: to prioritize external validity over internal validity.

Overfitting occurs when a model has good internal validity but poor external validity, meaning it performs poorly on a sample other than the one it was trained on. Structuring a learning problem into _train_/_test_ samples addresses this challenge, as it allows for selecting the best model for extrapolation. This topic may seem trivial, but in practice, many empirical scientific fields do not adopt this methodology when making conclusions beyond the population they studied.

For example, in economics, it is quite common to evaluate a public policy _ceteris paribus_ (all other things being equal), deduce a marginal effect, and recommend policy actions based on this. However, it is rare for the subsequent policy to be applied to the same target population or under the same institutional conditions, often leading to different effects. Sampling biases, whether in terms of individual characteristics or the study period, are often overlooked, and the estimation of marginal effects is typically performed without considering external validity.

Returning to the focus of this chapter, formally, this issue stems from the __bias-variance tradeoff__ in estimation quality. Let $h(X,\theta)$ be a statistical model. The estimation error can be decomposed into two parts:
:::

$$
\mathbb{E}\bigg[(y - h(\theta,X))^2 \bigg] = \underbrace{ \bigg( y - \mathbb{E}(h_\theta(X)) \bigg)^2}_{\text{biais}^2} + \underbrace{\mathbb{V}\big(h(\theta,X)\big)}_{\text{variance}}
$$


::: {.content-visible when-profile="fr"}
Il y a ainsi un compromis à faire entre biais et variance. Un modèle peu parcimonieux, c'est-à-dire proposant un grand nombre de paramètres, va, en général, avoir un faible biais mais une grande variance. En effet, le modèle va tendre à se souvenir d'une combinaison de paramètres à partir d'un grand nombre d'exemples sans être capable d'apprendre la règle qui permette de structurer les données.

Par exemple, la ligne verte ci-dessous est trop dépendante des données et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles données.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)

La division entre échantillons d'apprentissage et de validation est une première réponse au défi du surapprentissage. Néanmoins, ce n'est pas le seul geste méthodologique pour avoir un bon modèle prédictif.

De manière générale, il sera préférable d'adopter des modèles parcimonieux, c'est-à-dire les modèles faisant le moins d'hypothèses possibles sur la structure des données tout en offrant une performance satisfaisante. Ceci est généralement vu comme une illustration du principe du [rasoir d'Ockham](https://fr.wikipedia.org/wiki/Rasoir_d'Ockham): en l'absence d'arguments théoriques, le meilleur modèle est celui qui permet d'expliquer au mieux les données avec les hypothèses les plus minimalistes. Cette approche, très opérationnelle, guidera de nombreux choix méthodologiques que nous mettrons en oeuvre.
:::

::: {.content-visible when-profile="en"}
There is thus a trade-off between bias and variance. A non-parsimonious model, meaning one with a large number of parameters, will generally have low bias but high variance. Indeed, the model tends to memorize a combination of parameters from a large number of examples without being able to learn the rule that structures the data.

For example, the green line below is too dependent on the data and is likely to produce a larger error than the black line (which averages more) when applied to new data.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)

The division between training and validation samples is an initial response to the challenge of overfitting. However, it is not the only methodological step required to achieve a good predictive model.

In general, it is preferable to adopt parsimonious models, which make as few assumptions as possible about the structure of the data while still delivering satisfactory performance. This is often seen as an illustration of the [Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor) principle: in the absence of theoretical arguments, the best model is the one that explains the data most effectively with the fewest assumptions. This highly practical approach will guide many methodological choices we will implement.
:::

::: {.content-visible when-profile="fr"}
# Comment évaluer un modèle ? 

L'[introduction de cette partie](/content/modelisation/index.qmd) présentait les principaux concepts pour se repérer dans la terminologie du _machine learning_. Si les concepts d'apprentissage supervisé, non supervisé, classification, régression, etc. ne sont pas clairs, il est recommandé de retourner voir ce chapitre. Pour rappel, le _machine learning_ intervient dans les domaines où on ne dispose pas de modèles théoriques, en contrôlant tous les paramètres, faisant consensus et où on va chercher des règles statistiques, selon une démarche inductive. Ce n'est donc pas une approche scientifique justifiée dans tous les domaines. Par exemple, il vaut mieux privilégier le réglage des satelittes par le biais des équations de gravitation que par le biais d'un algorithme de _machine learning_ qui risque d'introduire du bruit là où ce n'est pas nécessaire.  

La principale ligne de partage entre les méthodes d'évaluation sera la nature du phénomène étudié (la variable $y$). Selon qu'on dispose ou non d'une mesure directe de notre variable d'intérêt, une sorte de _gold standard_, on mettra en oeuvre des métriques prédictives directes (cas de l'apprentissage supervisé) ou des métriques de stabilité statistique (apprentissage non supervisé).

Néanmoins, le succès des [modèles de fondations](https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_fondation), c'est-à-dire de modèles généralistes pouvant être utilisés pour des tâches auxquels ils n'ont pas été entraînés, amène à élargir la question de l'évaluation. Il n'est pas toujours évident de pouvoir définir l'objectif précis d'un modèle généraliste ni de pouvoir évaluer de manière consensuelle sa qualité. `ChatGPT` ou `Claude` nous apparaîssent bon, mais comment peut-on avoir une idée de leur pertinence dans différents cas d'usage ? Outre le sujet des annotations, ceci amène à réfléchir, plus globalement, sur le rôle de l'humain dans l'évaluation et le contrôle de décisions prises par des algorithmes.
:::

::: {.content-visible when-profile="en"}
# How to evaluate a model?

The [introduction to this section](/content/modelisation/index.qmd) presented the main concepts for navigating the terminology of machine learning. If the concepts of supervised learning, unsupervised learning, classification, regression, etc., are not clear, it is recommended to revisit that chapter. To recap, machine learning is applied in areas where no theoretical models, consensus-driven with all parameters controlled, are available, and instead seeks statistical rules through an inductive approach. Therefore, it is not a scientifically justified approach in all fields. For example, adjusting satellites is better achieved through gravitational equations rather than using a machine learning algorithm, which risks introducing noise unnecessarily.

The main distinction between evaluation methods depends on the nature of the phenomenon being studied (the variable $y$). Depending on whether a direct measure of the variable of interest, a kind of _gold standard_, is available, one may use direct predictive metrics (in supervised learning) or statistical stability metrics (in unsupervised learning).

However, the success of [foundation models](https://en.wikipedia.org/wiki/Foundation_model), i.e., generalist models that can be used for tasks they were not specifically trained on, broadens the question of evaluation. It is not always straightforward to define the precise goal of a generalist model or to evaluate its quality in a universally agreed manner. `ChatGPT` or `Claude` may appear to perform well, but how can we gauge their relevance across different use cases? Beyond the issue of annotations, this raises broader questions about the role of humans in evaluating and controlling decisions made by algorithms.
:::


::: {.content-visible when-profile="fr"}
## Apprentissage supervisé

En apprentissage supervisé, on distingue en général les problèmes de:

* Classification : la variable $y$ est discrète
* Régression : la variable $y$ est continue

Les métriques mises en oeuvre peuvent être objectives dans ces deux cas car on dispose d'une valeur réelle, une valeur cible faisant office de _gold standard_, auquel comparer la valeur prédite.

### Classification

Le cas le plus simple à comprendre est celui de la classification binaire. Dans ce cas, soit on a juste, soit on se trompe, sans nuance.

La plupart des critères de performance consistent ainsi à explorer les différentes cases de la **matrice de confusion** :

![Construction de la matrice de confusion](/content/modelisation/img/confusion.png)

Cette dernière met en regard les valeurs prédites avec les valeurs observées. Le cas binaire est le plus simple à appréhender ; la classification multiclasse est une version généralisée de ce principe.

A partir des 4 coins de cette matrice, il existe plusieurs mesures de performance:

| Critère     | Mesure       |  Calcul |
|-------------|--------------|------------------|
| *Accuracy*  | Taux de classification correcte | Diagonale du tableau : $\frac{TP+TN}{TP+FP+FN+FP}$ |
| *Precision* | Taux de vrais positifs  | Ligne des prédictions positives : $\frac{TP}{TP+FP}$ |
| *Recall* (rappel)   | Capacité à identifier les labels positifs | Colonne des prédictions positives : $\frac{TP}{TP+FN}$ |
| *F1 Score*  | Mesure synthétique (moyenne harmonique) de la précision et du rappel | $2 \frac{precision \times recall}{precision + recall}$  |

Néanmoins, certaines métriques préfèrent plutôt prendre en compte les probabilités de prédiction. Si un modèle fait une prédiction mais avec une confiance très modérée et qu'on l'accepte, peut-on lui en tenir rigueur ? Pour cela, on fixe un seuil de probabilité $c$ à partir duquel on prédit qu'une observation donnée appartient à une certaine classe prédite:
:::

::: {.content-visible when-profile="en"}
## Supervised Learning

In supervised learning, problems are generally categorized as:

* Classification: where the variable $y$ is discrete
* Regression: where the variable $y$ is continuous

The metrics used can be objective in both cases because we have an actual value, a target value serving as a _gold standard_, against which to compare the predicted value.

### Classification

The simplest case to understand is binary classification. In this case, either we are correct, or we are wrong, with no nuance.

Most performance criteria thus involve exploring the various cells of the **confusion matrix**:

![Construction of the confusion matrix](/content/modelisation/img/confusion.png)

This matrix compares predicted values with observed values. The binary case is the easiest to grasp; multiclass classification is a generalized version of this principle.

From the 4 quadrants of this matrix, several performance measures exist:

| Criterion    | Measure                        | Calculation                                |
|--------------|--------------------------------|-------------------------------------------|
| *Accuracy*   | Correct classification rate    | Diagonal of the table: $\frac{TP+TN}{TP+FP+FN+FP}$ |
| *Precision*  | True positive rate             | Row of positive predictions: $\frac{TP}{TP+FP}$ |
| *Recall*     | Ability to identify positive labels | Column of positive predictions: $\frac{TP}{TP+FN}$ |
| *F1 Score*   | Synthetic measure (harmonic mean) of precision and recall | $2 \frac{precision \times recall}{precision + recall}$ |

However, some metrics prefer to account for prediction probabilities. If a model makes a prediction with very moderate confidence and we accept it, can we hold it accountable? To address this, we set a probability threshold $c$ above which we predict that a given observation belongs to a certain predicted class:
:::

$$
\mathbb{P}(y_i=1|X_i) > c \Rightarrow \widehat{y}_i = 1 
$$

::: {.content-visible when-profile="fr"}
Plus on augmente $c$, plus on est sélectif sur le critère d'appartenance à la classe.  
La précision, i.e. le taux de vrais positifs parmi les prédictions positives, augmente. Mais on augmente le nombre de positifs manqués (ce sont des faux négatifs). Autrement dit, quand on est pointilleux, on diminue le rappel. Pour chaque valeur de $c$ correspond une matrice de confusion et donc des mesures de performances. La **courbe ROC** consiste à faire varier $c$ de 0 à 1 et vérifier l'effet sur les performances :

![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)

L'aire sous la courbe (**AUC**) permet d'évaluer quantitativement le meilleur modèle au sens de ce critère. L'AUC représente la probabilité que le modèle soit capable de distinguer entre la classe positive et négative.
:::

::: {.content-visible when-profile="en"}
The higher the value of $c$, the more selective the criterion for class membership becomes.  
Precision, i.e., the rate of true positives among positive predictions, increases. However, the number of missed positives (false negatives) also increases. In other words, being strict reduces recall. For each value of $c$, there corresponds a confusion matrix and thus performance measures. The **ROC curve** is obtained by varying $c$ from 0 to 1 and observing the effect on performance:

![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)

The area under the curve (**AUC**) provides a quantitative evaluation of the best model according to this criterion. The AUC represents the probability that the model can distinguish between the positive and negative classes.
:::

::: {.content-visible when-profile="fr"}
### Régression

Lorsqu'on travaille sur une variable quantitative, l'objectif est d'avoir une prédiction la plus proche de celle-ci. Les indicateurs de performance en régression consistent donc à mesurer l'écart entre la valeur prédite et la valeur observée :

| Nom | Formule |
|-----|---------|
| _Mean squared error_ | $MSE = \mathbb{E}\left[(y - h_\theta(X))^2\right]$ |
| _Root Mean squared error_ | $RMSE = \sqrt{\mathbb{E}\left[(y - h_\theta(X))^2\right]}$ |
| _Mean Absolute Error_ | $MAE = \mathbb{E} \bigg[ \lvert y - h_\theta(X) \rvert \bigg]$ |
| _Mean Absolute Percentage Error_ | $MAE = \mathbb{E}\left[ \left\lvert \frac{y - h_\theta(X)}{y} \right\rvert \right]$ |

Ces métriques devraient rappeler des souvenirs si vous êtes familiers de la méthode des moindres carrés, ou plus généralement de la régression linéaire. Cette méthode vise justement à trouver des paramètres qui minimisent ces métriques, dans un cadre statistique formel.
:::

::: {.content-visible when-profile="en"}
### Regression

When working with a quantitative variable, the goal is to make a prediction as close as possible to the actual value. Performance indicators in regression therefore measure the discrepancy between the predicted value and the observed value:

| Name | Formula |
|------|---------|
| _Mean squared error_ | $MSE = \mathbb{E}\left[(y - h_\theta(X))^2\right]$ |
| _Root Mean squared error_ | $RMSE = \sqrt{\mathbb{E}\left[(y - h_\theta(X))^2\right]}$ |
| _Mean Absolute Error_ | $MAE = \mathbb{E} \bigg[ \lvert y - h_\theta(X) \rvert \bigg]$ |
| _Mean Absolute Percentage Error_ | $MAE = \mathbb{E}\left[ \left\lvert \frac{y - h_\theta(X)}{y} \right\rvert \right]$ |

These metrics may be familiar if you are acquainted with the least squares method, or more generally with linear regression. This method specifically aims to find parameters that minimize these metrics within a formal statistical framework.
:::


::: {.content-visible when-profile="fr"}
## Apprentissage non supervisé

Dans cet ensemble de méthodes, on ne dispose pas de _gold standard_ pour comparer la prédiction à la valeur observée. Pour mesurer la performance d'un algorithme, il faut donc se résoudre à utiliser des métriques de stabilité des prédictions, basées sur des critères statistiques. Cela permet d'évaluer si le fait de complexifier l'algorithme change fondamentalement la distribution des prédictions.

Les métriques utilisées dépendent du type d'apprentissage mis en oeuvre. Par exemple, le _clustering_ par _K-means_ utilise généralement une mesure d'inertie qui quantifie l'homogénéité des clusters. Une bonne performance correspond au cas où les clusters sont homogènes et se distinguent l'un de l'autre. Plus on a de clusters (le $K$ de $K-means$), plus ceux-ci tendent à être homogènes. Si on ne choisit pas un $K$ adéquat, on peut faire du sur-apprentissage: si on ne compare les modèles qu'en fonction de leur homogénéité, on va choisir un nombre de cluster très élevé ce qui correspond à un cas classique de surapprentissage. Les méthodes de sélection du nombre de cluster optimal, comme la [méthode du coude](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), visent à évaluer le moment où le gain d'inertie lié à l'augmentation du nombre de clusters tend à s'affaisser. On sélectionne alors le nombre de clusters qui offre le meilleur compromis entre parcimonie et performance.
:::

::: {.content-visible when-profile="en"}
## Unsupervised learning

In this set of methods, there is no _gold standard_ to compare predictions against observed values. To measure the performance of an algorithm, one must rely on prediction stability metrics based on statistical criteria. This allows an assessment of whether increasing the complexity of the algorithm fundamentally changes the distribution of predictions.

The metrics used depend on the type of learning implemented. For example, _K-means_ clustering typically uses an inertia measure that quantifies the homogeneity of clusters. Good performance corresponds to cases where clusters are homogeneous and distinct from one another. The more clusters there are (the $K$ in $K-means$), the more homogeneous they tend to be. If an inappropriate $K$ is chosen, overfitting may occur: if models are compared solely based on their homogeneity, one might select a very high number of clusters, which is a classic case of overfitting. Methods for selecting the optimal number of clusters, such as the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), aim to determine the point where the gain in inertia from increasing the number of clusters starts to diminish. The number of clusters that offers the best trade-off between parsimony and performance is then selected.
:::


::: {.content-visible when-profile="fr"}
## Comment sont évalués les grands modèles de langage et les IA génératives ?

S'il apparaît relativement intuitif d'évaluer des modèles supervisés (pour lesquels on dispose d'observations faisant office de vérité absolue), comment juger de la qualité d'un outil comme `ChatGPT` ou `Copilot` ? Comment définir une bonne IA générative : est-ce une IA qui fournit une information juste du premier coup (véracité) ? une IA qui fait preuve de capacité de raisonnements (_chain of thought_) dans une discussion ? Doit-on juger le style ou uniquement le fond ?

Ces interrogations sont des champs actifs de recherche. Les [modèles de fondation](https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_fondation) étant très généraux, entraînés à différentes tâches, parfois de manière supervisée, parfois de manière non supervisée, il est difficile de définir un objectif unique permettant de considérer qu'un modèle est, sans ambiguité, meilleur qu'un autre. Le [_leaderboard MTEB (Massive Text Embedding Benchmark)_](https://huggingface.co/blog/lyon-nlp-group/mteb-leaderboard-best-practices) présente par exemple de nombreuses métriques pour des tâches diverses et il peut être difficile de s'y retrouver. Sans compter que le rythme effrené de publication de nouveaux modèles change régulièrement ce classement.

Globalement, même s'il existe des métriques où on fait automatiquement évaluer la qualité d'un texte à un autre LLM (métriques _LLM as a judge_), pour avoir des modèles de langage de qualité, il est nécessaire d'avoir de l'évaluation humaine à plusieurs niveaux. En premier lieu, il est utile d'avoir un corpus de données annotées (des textes avec un résumé rédigé par un humain, des descriptions d'images, etc.) pour la phase d'entraînement et d'évaluation. Ceci permet de guider le comportement du modèle sur une tâche donnée.

Mais l'humain peut aussi intervenir _ex post_, pour faire remonter une évaluation de la qualité d'un modèle. On parle de _feedbacks_. Ceci peut prendre plusieurs formes, par exemple une évaluation positive ou négative de la réponse ou une évaluation plus qualitative. Ces informations remontées ne vont pas forcément servir à la version actuelle du modèle mais pourront servir ultérieurement pour entraîner un modèle selon une technique de renforcement.
:::

::: {.content-visible when-profile="en"}
## How are Large Language Models and Generative AI tools evaluated?

While it seems relatively intuitive to evaluate supervised models (for which we have observations serving as ground truth), how can we assess the quality of a tool like `ChatGPT` or `Copilot`? How do we define a good generative AI: is it one that provides accurate information on the first try (truthfulness)? One that demonstrates reasoning capabilities (_chain of thought_) in a discussion? Should we judge style, or only content?

These questions are active areas of research. [Foundation models](https://en.wikipedia.org/wiki/Foundation_model), being very general and trained on different tasks, sometimes in a supervised way, sometimes unsupervised, make it challenging to define a single goal to unambiguously declare one model better than another. The [_MTEB (Massive Text Embedding Benchmark) leaderboard_](https://huggingface.co/blog/lyon-nlp-group/mteb-leaderboard-best-practices), for instance, presents numerous metrics for various tasks, which can be overwhelming to navigate. Moreover, the rapid pace of new model publications frequently reshuffles these rankings.

Overall, although there are metrics where the quality of one text is automatically evaluated by another LLM (_LLM as a judge_ metrics), achieving high-quality language models requires human evaluation at multiple levels. Initially, it is helpful to have an annotated dataset (e.g., texts with human-written summaries, image descriptions, etc.) for the training and evaluation phase. This guides the model's behavior for a given task.

Humans can also provide _ex post_ feedback to assess a model's quality. This feedback can take various forms, such as positive or negative evaluations of responses or more qualitative assessments. While this information may not immediately influence the current version of the model, it can be used later to train a model through reinforcement learning techniques.
:::



::: {.content-visible when-profile="fr"}
## Evaluer sans être tourné vers le passé: les enjeux de la supervision des modèles

Il convient de garder en mémoire qu'un modèle de _machine learning_ est entraîné sur des données passées. Un usage opérationnel de celui-ci, dans la phase suivante de son cycle de vie, implique donc de faire des hypothèses fortes sur la stabilité des nouvelles données reçues. Si le contexte évolue, un modèle risque de ne plus apporter des performances satisfaisantes. Si dans certains cas cela peut être rapidement mesuré à partir d'indicateurs clés (ventes, nombre de nouveaux clients, etc.) il est tout de même important de conserver un contrôle sur les modèles.

Ceci ouvre à la notion d'__observabilité__ d'un modèle de _machine learning_. En informatique, l'observabilité est le principe qui consiste à surveiller, mesurer et comprendre l'état d'une application pour s'assurer que celle-ci soit toujours en mesure de répondre à ses utilisateurs. L'idée de l'observabilité en _machine learning_ est similaire : il s'agit de contrôler qu'un modèle permet toujours d'avoir des performances satisfaisantes au cours du temps. Le principal risque dans le cycle de vie d'un modèle est celui du _data drift_, changement dans la distribution des données au fil du temps qui entraîne une dégradation des performances d’un modèle de _machine learning_. Bien sûr, avoir construit un modèle à bonne validité externe va réduire l'effet de ce risque mais il sera inévitable qu'il ait des effets si la structure des données change trop par rapport au contexte d'entraînement.

Pour garder un modèle pertinent dans le temps, il sera nécessaire de régulièrement collecter de nouvelles données (principe des annotations) et adopter une stratégie de ré-entraînement. Ceci ouvre la voie aux problématiques de la mise en production et du MLOps qui sont le point de départ d'un [cours que Romain Avouac et moi donnons](https://ensae-reproductibilite.github.io/website/).
:::

::: {.content-visible when-profile="en"}
## Evaluating without looking back: The challenges of model monitoring

It is important to remember that a machine learning model is trained on past data. Its operational use in the next phase of its lifecycle therefore requires making strong assumptions about the stability of incoming data. If the context evolves, a model may no longer deliver satisfactory performance. While in some cases this can quickly be measured using key indicators (sales, number of new clients, etc.), it is still crucial to maintain oversight of the models.

This introduces the concept of __observability__ in machine learning. In computing, observability refers to the principle of monitoring, measuring, and understanding the state of an application to ensure it continues to meet user needs. The idea of observability in machine learning is similar: it involves verifying that a model continues to deliver satisfactory performance over time. The main risk in a model's lifecycle is _data drift_, a change in the data distribution over time that leads to performance degradation in a machine learning model. While building a model with good external validity reduces this risk, it will inevitably have an impact if the data structure changes significantly compared to the training context.

To keep a model relevant over time, it will be necessary to regularly collect new data (the principle of annotations) and adopt a re-training strategy. This opens up the challenges of deployment and MLOps, which are the starting point of a [course taught by Romain Avouac and myself](https://ensae-reproductibilite.github.io/website/).
:::


