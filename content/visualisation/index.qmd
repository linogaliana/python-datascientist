---
title: "Partie 2: communiquer à partir de données"
categories:
  - Introduction
  - Visualisation
description: |
  Les _data scientists_ doivent être en mesure de synthétiser l'information présente dans un jeu de données par le biais de la représentation graphique, 
  car le cerveau humain comprend mieux les informations à travers des figures que des tableaux. La visualisation de données est importante à la fois
  dans une démarche exploratoire pour comprendre la structure des phénomènes étudiés mais aussi dans une phase de communication de résultats
  à des publics n'ayant pas forcément accès à la donnée brute et devant se contenter de synthèses. Cette partie du cours est une introduction
  à ce vaste sujet par le biais de la pratique à travers la construction de graphiques descriptifs et de cartes. 
description-en: |
  Data scientists need to be able to synthesize the information contained in a dataset through graphical representation, 
  because the human brain understands information better through figures than through tables. Data visualization is important both
  as part of an exploratory approach to understanding the structure of the phenomena under study, but also as part of a phase of communicating results
  to audiences who don't necessarily have access to raw data and need to make do with summaries. This part of the course is an introduction
  to this vast subject through the practical construction of descriptive graphs and maps.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/snake-chart.png
bibliography: ../../reference.bib
---

# Introduction

::: {.content-visible when-profile="fr"}
Une partie essentielle du travail du _data scientist_
consiste à synthétiser l'information que
contient ses 
jeux de données afin de distinguer
ce qui relève du signal, sur lequel il 
pourra se concentrer, et ce qui relève
du bruit
inhérent à tout jeu de données. 
Dans le travail du _data scientist_, lors d'une phase
exploratoire, il y a
donc un aller-retour constant entre information synthétique
et jeu de données désagrégé. Il 
est ainsi essentiel de savoir synthétiser l'information
dans un jeu de données avant d'en saisir la structure, cette
dernière pouvant ensuite guider les exploitations ultérieures,
pour une phase de modélisation ou de correction de
données (détection d'anomalies ou de mauvaises remontées de données).

Nous avons déjà exploré une partie essentielle de ce travail,
à savoir la construction de statistiques descriptives pertinentes
et fiables. Néanmoins, si on se contentait de présenter l'information
en utilisant des sorties brutes issues du combo `groupby` et `agg`
sur un _DataFrame_ `Pandas`, notre connaissance des données serait assez
limitée. La mise en oeuvre de tableaux stylisés à partir
de `great tables` constituait déjà un progrès dans cette démarche mais, en vérité,
notre cerveau se représente l'information de manière beaucoup plus intuitive
par le biais de visualisations graphiques simples que dans un tableau. 
:::

::: {.content-visible when-profile="en"}
An essential part of the work of a _data scientist_
is to synthesize the information contained
in their datasets in order to distinguish
what constitutes the signal, which they 
can focus on, and what constitutes
the noise inherent in any dataset. 
In the work of a _data scientist_, during an exploratory phase,
there is a constant back-and-forth between synthesized information
and disaggregated datasets. It 
is therefore essential to know how to synthesize the information
in a dataset before grasping its structure, which
can then guide further analyses,
whether for a modeling phase or data correction
(anomaly detection or bad data retrieval).

We have already explored a key part of this work,
namely the construction of relevant
and reliable descriptive statistics. However, if we were content
to present information using raw outputs from the `groupby` and `agg`
combo on a `Pandas` _DataFrame_, our understanding of the data would be quite
limited. The implementation of stylized tables using
`great tables` was already a step forward in this process but, in truth,
our brain processes information much more intuitively
through simple graphical visualizations than through a table.
:::


::: {.content-visible when-profile="fr"}
## La visualisation des données, une part essentiel du travail de communication

En tant qu'humains,
nos
capacités cognitives étant limitées, nous ne pouvons
appréhender qu'une information limitée là où l'ordinateur est capable de traiter
de grands volumes d'information. En tant que _data scientist_, cela signifie
qu'utiliser nos compétences informatiques et statistiques pour obtenir
des représentations synthétiques de nos nombreux jeux de données est
essentiel pour être en mesure de répondre à nos besoins opérationnels ou
scientifiques. 
L'ensemble des méthodes et des outils qui constituent la boîte à outil
des _data scientists_ vise à simplifier l'appréhension puis l'exploitation
de jeux de données dont le volume dépasse nos capacités cognitives. 

Ceci nous entraîne vers la question de la visualisation des données,
un ensemble d'outils et de principes pour représenter de manière
synthétique des faits stylisés ou contextualiser une donnée individuelle.
La visualisation de données est l'art et la science de __représenter visuellement des informations complexes et abstraites à l'aide d'éléments visuels__.
Son objectif principal est de synthétiser l'information présente dans un ensemble de données afin de faciliter
la compréhension des enjeux de celle-ci pour une analyse ultérieure. 
La visualisation de données permet, entre autres, de mettre en évidence des tendances, des corrélations ou
des anomalies qui pourraient être difficiles voire impossibles à saisir simplement en examinant des données brutes, ces dernières nécessitant
une certaine mise en contexte pour porter du sens. 

La visualisation de données joue un rôle crucial dans le
processus d'analyse de données en fournissant des moyens visuels pour explorer, interpréter et communiquer des informations.
Elle facilite la communication entre experts de la données, décideurs et grand public,
en permettant aux derniers de bénéficier du travail rigoureux des premiers pour donner
sens aux données sans la connaissance des subtilités conceptuelles qui ont permis
de synthétiser l'information contenue dans celle-ci. 
:::

::: {.content-visible when-profile="en"}
## Data visualization, an essential part of communication work

As humans, 
our 
cognitive capacities are limited, and we can only grasp 
a limited amount of information, whereas computers are capable of processing 
large volumes of information. For a _data scientist_, this means 
that using our computational and statistical skills to obtain 
synthetic representations of our many datasets is 
essential to meet operational or scientific needs. 
The range of methods and tools that make up the toolbox 
of _data scientists_ aims to simplify the understanding and subsequent exploitation 
of datasets whose volume exceeds our cognitive capacities. 

This brings us to the question of data visualization, 
a set of tools and principles for representing 
stylized facts or contextualizing individual data in a synthetic manner. 
Data visualization is the art and science of __visually representing complex and abstract information through visual elements__. 
Its primary goal is to synthesize the information contained in a dataset to facilitate 
the understanding of its key issues for further analysis. 
Data visualization allows, among other things, to highlight trends, correlations, or 
anomalies that might be difficult or even impossible to grasp just by looking at raw data, which requires 
some context to make sense of it. 

Data visualization plays a crucial role in the 
data analysis process by providing visual means to explore, interpret, and communicate information. 
It facilitates communication between data experts, decision-makers, and the general public, 
enabling the latter to benefit from the rigorous work of the former to make 
sense of the data without the need for deep conceptual knowledge that underpins 
the synthesized information. 
:::


::: {.content-visible when-profile="fr"}
## La place de la visualisation dans le processus de valorisation de la donnée

La visualisation des données n'est pas restreinte à la phase finale d'un projet,
c'est à dire à la phase de communication de résultats à une audience qui n'a pas accès à la donnée
ou n'a pas les moyens de la valoriser. 
La visualisation intervient à toutes les étapes du processus de valorisation
de la donnée. Il s'agit d'ailleurs d'un travail essentiel pour trouver
comment basculer de l'enregistrement, un instantané d'un phénomène, à une donnée,
un enregistrement qui a une valeur parce qu'il porte une information en tant que telle
ou lorsqu'il est combiné avec d'autres enregistrements. 

Le travail quotidien du _data scientist_ 
consiste à regarder un jeu de données sous toutes ses coutures
pour identifier les axes prioritaires d'extraction de valeur. 
Savoir rapidement quelles statistiques représenter, et comment,
est essentiel pour gagner du temps sur cette partie exploratoire. 
Il s'agit principalement d'un travail de communication envers soi-même
qui peut se permettre d'être brouillon car il s'agit de dégrossir
le travail avant de polir certains angles. L'enjeu à ce niveau du
processus est de ne pas manquer une dimension qui pourrait être
porteuse de valeur. 

Le travail de communication réellement chronophage intervient plutôt
lorsqu'on communique à une audience ayant un accès limité à des
données, ne connaissant pas bien les sources, ayant
un temps d'attention limité
ou n'ayant pas des compétences quantitatives. Ces
publics ne peuvent se satisfaire d'une sortie brute comme
un _DataFrame_ dans un _notebook_ ou un graphique produit 
en quelques secondes avec la méthode `plot` de `Pandas`. 
Il convient de s'adapter à leurs attentes, qui évoluent, 
et aux outils qu'ils connaissent, d'où la place de plus en
plus importante prise par les sites
web de _data visualisations_. 
:::

::: {.content-visible when-profile="en"}
## The role of visualization in the data value creation process

Data visualization is not limited to the final phase of a project,
which is the communication of results to an audience that does not have access to the data
or the means to make use of it. 
Visualization plays a role at every stage of the data value creation process. 
It is, in fact, an essential part of the process of transitioning
from a record, a snapshot of a phenomenon, to data— 
a record that has value because it carries information on its own
or when combined with other records. 

The daily work of a _data scientist_
involves examining a dataset from every angle
to identify key value extraction opportunities. 
Quickly knowing what statistics to represent, and how,
is crucial for saving time during this exploratory phase. 
This is primarily a form of self-communication
that can afford to be rough around the edges, as the goal is to sketch
the work before refining certain aspects. The challenge at this stage of
the process is not to overlook any dimension that could potentially bring value. 

The truly time-consuming communication work comes
when presenting to an audience with limited data access,
unfamiliar with sources,
with a limited attention span,
or without quantitative skills. These
audiences cannot be satisfied with raw outputs like
a _DataFrame_ in a _notebook_ or a graph created
in seconds with the `plot` method from `Pandas`. 
It is important to adapt to their evolving expectations,
and the tools they are familiar with, which explains the growing importance of
websites dedicated to _data visualizations_. 
:::


::: {.content-visible when-profile="fr"}
# Communiquer, une ouverture au _data storytelling_

La visualisation de données a ainsi une place à part dans 
l'ensemble des techniques de la _data science_. 
Elle intervient à tous les stades du processus de 
production de la donnée, de
l'amont (analyse exploratoire) à
l'aval (restitution à des publics multiples) et
peut, si elle est bien construite, permettre de
saisir de manière intuitive la structure des données
ou les enjeux de son analyse. 

Art de la synthèse, la visualisation de données
est également l'art de raconter une histoire et
peut même, lorsqu'elle est bien construite, prétendre
au rang de production artistique. 
La _dataviz_ est un métier en soi dont on trouve de 
plus en plus de praticiens dans les titres de presse
ou dans des entreprises
spécialisées (`Datawrapper` par exemple). 

Sans prétendre construire
des visualisations aussi riches que celles des spécialistes,
tout _data scientist_ se doit d'être en mesure de pouvoir
produire rapidement quelques visualisations permettant
de synthétiser l'information présente dans les
jeux de données à sa disposition. 
Une visualisation claire et lisible tout en restant simple
peut être meilleure qu'un discours pour faire passer un message.

De même qu'un discours, une visualisation est une communication
pour laquelle un locuteur - la personne construisant la visualisation - 
cherche à transmettre une information à un récepteur - éventuellement
la même personne que le locuteur puisqu'une visualisation peut 
être construite pour soi-même dans une analyse exploratoire. Il n'est
donc pas surprenant qu'à l'époque où la sémiologie occupait une 
part importante dans les débats intellectuels, notamment autour
de la figure de Roland Barthes, le concept de sémiologie 
graphique ait émergé
autour de la personne de Jacques Bertin [@bertin1967semiologie; @palsky2017semiologie]. 
Cette approche permet de réfléchir sur la pertinence des
techniques mises en oeuvre pour transmettre un message
graphique et de nombreuses visualisations, si elles
suivaient quelques-unes de ces règles, pourraient
être améliorées à peu de frais. 

Eric Mauvière, statisticien français héritier
de l'école de la sémiologie graphique de Bertin, 
propose d'excellents contenus sur le sujet. Certaines
des présentations qu'il a pu faire, notamment
celle pour le [`SSPHub`](https://ssphub.netlify.app/)
présentées dans la @nte-mauviere
devraient être visionnées dans toutes les formations
de _data science_ tant elles évoquent les nombreux
écueils rencontrés par les _data scientists_. 

![Un exemple de deux visualisations faites sur le même jeu de données par Eric Mauvière, voir @nte-mauviere](https://raw.githubusercontent.com/InseeFrLab/ssphub/main/talk/2024-02-29-mauviere/mauviere.png)
:::

::: {.content-visible when-profile="en"}
# Communicating, an opening to _data storytelling_

Data visualization thus holds a special place among 
the various techniques of _data science_. 
It is involved at all stages of the data production process, 
from upstream (exploratory analysis) to downstream (presenting results to various audiences), and 
when well-constructed, it allows us to intuitively grasp the structure of the data 
or the key issues of its analysis. 

As an art of synthesis, data visualization 
is also the art of storytelling, and 
when done well, it can even reach the level of artistic production. 
_Data visualization_ is a profession in its own right, with more and more practitioners found in media outlets 
or specialized companies (`Datawrapper`, for example). 

Without aiming to create 
visualizations as sophisticated as those produced by specialists, 
every _data scientist_ should be able to 
quickly generate visualizations that synthesize the information in the 
datasets at hand. 
A clear and readable visualization, while remaining simple, 
can be more effective than a speech in conveying a message.

Just like a speech, a visualization is a form of communication 
in which a speaker—the person constructing the visualization— 
seeks to convey information to a recipient—potentially 
the same person as the speaker since a visualization can be 
created for oneself during exploratory analysis. It is 
no surprise that during the period when semiology played a significant 
role in intellectual debates, especially around 
the figure of Roland Barthes, the concept of graphic semiology 
emerged, centered around Jacques Bertin [@bertin1967semiologie; @palsky2017semiologie]. 
This approach allows reflection on the relevance of the 
techniques used to convey a graphic message, and many visualizations, if they 
followed some of these rules, could 
be improved at little cost. 

Eric Mauvière, a French statistician and a successor 
to Bertin's school of graphic semiology, 
offers excellent content on the subject. Some 
of his presentations, notably the one for [`SSPHub`](https://ssphub.netlify.app/), 
presented in the @nte-mauviere-en, 
should be viewed in all _data science_ training programs as they 
highlight the numerous pitfalls encountered by _data scientists_. 

![An example of two visualizations made from the same dataset by Eric Mauvière, see @nte-mauviere](https://raw.githubusercontent.com/InseeFrLab/ssphub/main/talk/2024-02-29-mauviere/mauviere.png)
:::




::: {.content-visible when-profile="fr"}

:::: {#nte-mauviere .callout-note collapse="true"}
## Une conférence d'Eric Mauvière sur le sujet
::::

```{ojs}
//| echo: false
html`${slides_button}`
```


{{< video src="https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20240229-dataviz-mauviere/video1991622347.mp4" controls="yes" >}}

:::

::: {.content-visible when-profile="en"}

:::: {#nte-mauviere-en .callout-note collapse="true"}
## A conference by Eric Mauvière on the subject
::::

```{ojs}
//| echo: false
html`${slides_button}`
```


{{< video src="https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20240229-dataviz-mauviere/video1991622347.mp4" controls="yes" >}}

:::


```{ojs}
//| echo: false
slides = "https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20240229-dataviz-mauviere/conf_ssphub_icem7.pdf"
```

```{ojs}
//| echo: false
slides_button = html`<p class="text-center">
  <a class="btn btn-primary btn-lg cv-download" href="${slides}" target="_blank">
    <i class="fa-solid fa-file-arrow-down"></i>&ensp;Télécharger les slides
  </a>
</p>`
```




::: {.content-visible when-profile="fr"}
# Communiquer, une ouverture à la mise à disposition d'applications

L'objectif de ce cours est d'introduire aux principaux outils
et à la démarche que doivent adopter les _data scientists_
face à divers jeux de données. Il devient néanmoins de
plus en plus commun pour les _data scientists_
de développer et mettre à disposition des applications
interactives proposant un certain nombre d'explorations
et de visualisations automatisées de données. 
Il s'agit d'enjeux plus avancés que ce cours mais qui constituent
souvent un point d'entrée vers la _data science_ pour des
publics proches des _data scientists_, notamment les _data engineers_, 
les _data analysts_ ou les statisticiens. 

Nous évoquerons certains des outils privilégiés pour faire
cela, notamment les écosystèmes liés aux applications _web_
et aux outils `Javascript`. Ce besoin, devenu assez standard
pour les _data scientists_, fait la passerelle avec la mise
en production,
l'enjeu principal d'un cours de 3e année de l'ENSAE
construit par Romain Avouac
et moi-même ([site web du cours ensae-reproductibilite.github.io/](https://ensae-reproductibilite.github.io/website/)). Le présent site web, par exemple, est construit
selon ce principe grâce à des outils permettant d'exécuter de manière 
reproductible du `Python` sur des serveurs standardisés et ensuite
mettre à disposition ce code par le biais d'un site web. 
:::

::: {.content-visible when-profile="en"}
# Communicating, an opening to app development

The goal of this course is to introduce the main tools
and the approach that _data scientists_ should adopt
when working with various datasets. However, it is becoming increasingly common for
_data scientists_ to develop and provide
interactive applications offering a range of explorations
and automated data visualizations. 
These are more advanced topics than this course covers, but they often 
serve as an entry point to _data science_ for
audiences close to _data scientists_, such as _data engineers_,
_data analysts_, or statisticians. 

We will mention some of the preferred tools for doing this, 
especially ecosystems related to _web_ applications
and `Javascript` tools. This need, now fairly standard
for _data scientists_, bridges the gap with production deployment,
which is the main focus of a third-year ENSAE course 
designed by Romain Avouac and myself ([course website ensae-reproductibilite.github.io/](https://ensae-reproductibilite.github.io/website/)). This current website, for example, is built
on this principle using tools that allow `Python` code to be reproducibly executed 
on standardized servers and then made available through a website.
:::



::: {.content-visible when-profile="fr"}
# L'écosystème `Python` {{< fa brands python >}}

Pour revenir à notre cours,
nous présenterons dans cette partie quelques librairies
et visualisations basiques en `Python` permettant de
partir sur de bonnes bases. Les ressources pour 
approfondir et progresser dans l'art de la visualisation
ne manquent pas, comme [cet ouvrage](https://clauswilke.com/dataviz/) [@wilke2019fundamentals]. 
:::

::: {.content-visible when-profile="en"}
# The `Python` ecosystem {{< fa brands python >}}

Returning to our course,
in this section we will present some basic libraries
and visualizations in `Python` that provide
a good starting point. There are plenty of resources 
to deepen and advance in the art of visualization,
such as [this book](https://clauswilke.com/dataviz/) [@wilke2019fundamentals]. 
:::


::: {.content-visible when-profile="fr"}
## Les _packages_ de visualisations de données

L'écosystème `Python` pour la visualisation de données est très riche et
très éclaté.
Il est
possible de consacrer des livres entiers à celui-ci [@dale2022data].
`Python` propose
de nombreuses librairies pour produire de manière rapide et relativement
simple des visualisations de données[^1]. 

Les librairies graphiques se distinguent principalement en deux familles:

- Les librairies de __représentations figées__. Celles-ci ont plutôt vocation à être intégrées
dans des publications figées type PDF ou documents texte. Nous présenterons 
principalement `Matplotlib` et `Seaborn` mais il en existe d'autres, en pleine émergence,
comme [`Plotnine`](https://plotnine.readthedocs.io/en/stable/), l'adaptation de [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html) à l'écosystème `Python`.
- Les librairies de __représentations réactives__.  Celles-ci sont adaptées à des représentations
_web_ et offrent la possibilité aux lecteurs d'agir sur la représentation graphique affichée. 
Les librairies qui proposent ces fonctionnalités reposent généralement sur `JavaScript`, l'écosystème
du développement _web_, pour lequel elles offrent un point d'entrée via `Python`. 
Nous évoquerons principalement `Plotly` et `Folium` dans cette famille mais il existe de nombreux
autres _frameworks_ dans ce domaine[^2].

[^1]: Pour être honnête, pendant longtemps `Python` a été sur ce point un peu moins agréable
que `R` qui bénéficie de
l'incontournable librairie [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html).

    N'étant pas
construite sur la [grammaire des graphiques](http://r.qcbs.ca/workshop03/book-fr/la-grammaire-des-graphiques-gg.html),
la principe librairie de graphiques en `Python` qu'est `Matplotlib` est plus fastidieuse
à utiliser que `ggplot2`. 

    [`seaborn`](https://seaborn.pydata.org/), que nous présenterons,
facilite un peu le travail de représentation graphique mais, là encore, il est difficile de faire
plus malléable et universel que `ggplot2`.

    La librairie [`plotnine`](https://plotnine.readthedocs.io/en/stable/) vise à proposer une implémentation similaire
à `ggplot` pour les utilisateurs de `Python`. Son développement est à suivre. 

[^2]: A cet égard, je recommande vivement de suivre l'actualité de la _dataviz_
sur la plateforme [`Observable`](https://observablehq.com/) qui tend à
rapprocher les communautés des spécialistes de la _dataviz_ et des analystes
de données. La librairie [`Plot`](https://observablehq.com/plot/) pourrait devenir
un nouveau standard dans les prochaines années, sorte d'intermédiaire
entre `ggplot` et `d3`. 

Il est tout à fait possible
de faire des visualisations sophistiquées avec
une chaine de bout en bout `Python` puisqu'il s'agit d'un langage couteau-suisse
dont l'écosystème est très 
riche. Néanmoins, `Python` n'est pas la panacée et il peut parfois
être utile, pour obtenir un produit fini parfaitement poli,
de finaliser le travail avec d'autres langages, comme `Javascript` 
pour les visualisations réactives ou `QGIS` pour le
travail cartographique. Ce cours donnera les outils minimums
pour faire un travail rapide et plaisant mais le diable étant dans
les détails, il ne faut pas s'arcbouter à vouloir utiliser
`Python` pour tout et n'importe quoi. 

Dans le domaine de la visualisation, ce cours adopte le parti pris
d'explorer quelques
librairies centrales à partir d'un nombre restreint d'exemples en
répliquant des graphiques qu'on peut trouver sur le site d'*open data* de la 
mairie de Paris. 
La meilleure école pour la visualisation restant
la pratique sur des jeux de données, il est recommandé d'explorer la richesse
de l'écosystème de l'_open data_ pour expérimenter des visualisations. 
:::

::: {.content-visible when-profile="en"}
## Data visualization packages

The `Python` ecosystem for data visualization is vast and 
diverse. 
Entire books could be dedicated to it [@dale2022data].
`Python` offers 
numerous libraries to quickly and relatively 
easily produce data visualizations[^1].

The graphical libraries are mainly divided into two families:

- Libraries for __static representations__. These are primarily intended for integration 
into fixed publications such as PDFs or text documents. We will mainly present 
`Matplotlib` and `Seaborn`, but there are others emerging, 
such as [`Plotnine`](https://plotnine.readthedocs.io/en/stable/), an adaptation of [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html) to the `Python` ecosystem.
- Libraries for __interactive representations__. These are suited for _web_ representations 
and allow readers to interact with the displayed graphical representation. 
Libraries offering these features usually rely on `JavaScript`, the 
web development ecosystem, with an entry point through `Python`. 
We will primarily discuss `Plotly` and `Folium` in this family, but many 
other frameworks exist in this field[^2].

[^1]: To be honest, for a long time, `Python` was a bit less enjoyable in this regard 
compared to `R`, which benefits from the 
indispensable library [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html).

    Not built on the [grammar of graphics](http://r.qcbs.ca/workshop03/book-fr/la-grammaire-des-graphiques-gg.html),
    the main graphical library in `Python`, `Matplotlib`, is more cumbersome 
    to use than `ggplot2`. 

    [`seaborn`](https://seaborn.pydata.org/), which we will present, 
    simplifies graphical representation somewhat, but again, it is difficult to find 
    something more flexible and universal than `ggplot2`.

    The library [`plotnine`](https://plotnine.readthedocs.io/en/stable/) aims to provide a similar implementation 
    to `ggplot` for `Python` users. Its development is worth following. 

[^2]: In this regard, I highly recommend keeping up with data visualization 
news on the platform [`Observable`](https://observablehq.com/), which tends to 
bring together the communities of _dataviz_ specialists and data analysts. The library [`Plot`](https://observablehq.com/plot/) could become 
a new standard in the coming years, a sort of intermediate 
between `ggplot` and `d3`. 

It is entirely possible 
to create sophisticated visualizations with an end-to-end `Python` workflow since it is a versatile 
language with a very 
rich ecosystem. However, `Python` is not a cure-all, and sometimes 
it can be useful to finalize a perfectly polished product with other languages, such as `JavaScript` 
for interactive visualizations or `QGIS` for 
cartographic work. This course will provide the basic tools 
to quickly and enjoyably produce work, but as the saying goes, the devil is in the details, so one should not 
insist on using `Python` for every task. 

In the realm of visualization, this course takes the approach 
of exploring a few 
central libraries through a limited number of examples by replicating charts found on the open data 
website of the city of Paris. 
The best training for visualization remains 
practicing on datasets, so it is recommended to explore the richness 
of the open data ecosystem to experiment with visualizations. 
:::


::: {.content-visible when-profile="fr"}
## Les applications de visualisation

Cette partie du cours se focalise sur des représentations synthétiques simples. 
Elle n'évoque pas (_encore ?_) la construction d'applications de visualisation
de données où un ensemble de graphiques se mettent à jour de manière synchrone
en fonction d'actions d'utilisateurs. 

Ceci dépasse en effet le cadre d'un cours d'introduction car construire
ces applications
impliquent
de maîtriser des concepts plus complexes comme l'interaction entre une page
_web_ et un serveur, d'avoir des rudiments de connaissance en `Linux`, etc.
Les concepts nécessaires à la compréhension de ces outils sont au coeur
du cours de 3e année ["Mise en production de projets de _data science_"](https://ensae-reproductibilite.github.io/website/)
que Romain Avouac donnons en 3e année d'ENSAE. 

Néanmoins, comme la valorisation de données sous une forme applicative est très
commune, il
il est utile _a minima_ d'évoquer la dualité entre sites statiques
et applications dynamiques afin de donner les bons gestes et pointer vers les 
outils adéquat. 
Dans le monde de l'applicatif, il est important de distinguer le _front_ (la page
visible par les utilisateurs de l'application) du _back office_ (le moteur
qui effectue des actions en fonction des paramètres choisis par l'utilisateur
de la page). 

Il existe principalement deux paradigmes pour faire
interagir ces deux éléments. La distinction principale entre ces deux approches est qu’elles s’appuient sur des serveurs différents. Un site statique repose sur un serveur web là où `Streamlit` s’appuie sur serveur classique en _backend_. La différence principale entre ces deux types de serveurs réside principalement dans leur fonction et leur utilisation:

* Un serveur _web_ est spécifiquement conçu pour stocker, traiter et livrer des pages web (le _front_) aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées. Cela n'empêche pas d'avoir des étapes complexes de valorisation de données, ni de la réactivité en embarquant du `Javascript` dans l'application mais les étapes de traitement en `Python` sont faites en amont de la mise à disposition de l'application. Pour les utilisateurs de `Python`, il existe plusieurs constructeurs de sites statiques avant une mise à disposition par le biais d'un hébergement sur [`Github Pages`](https://pages.github.com/). Les deux écosystèmes les plus communs sont [`Quarto Markdown`](https://quarto.org/) et [`Django`](https://www.djangoproject.com/), le premier étant plus simple d'usage et de maintenance que le second. Ce site, par exemple, est construit grâce à `Quarto` ce qui assure la reproductibilité des exemples présentés et une mise en forme ergonomique et paramétrable des résultats.  
* Un serveur _backend_ classique est conçu pour effectuer des opérations en réponse à un _front_, en l’occurrence une page _web_. Dans le contexte d’une application construite avec `Python`, il s’agit d’un serveur avec l’environnement `Python` _ad hoc_ pour exécuter le code nécessaire à répondre à toute action d’un utilisateur de l’application. Le code est exécuté à la volée et non une fois pour toute comme dans l'approche précédente. Il s'agit donc d'un paradigme pouvant permettre plus de complexité applicative mais représentant un défi supplémentaire lors de la phase de mise en production. Dans l'écosystème `Python`, les deux principaux outils permettant de construire de telles applications sont [`Streamlit`](https://streamlit.io/) et [`Dash`](https://dash.plotly.com/), le premier étant plus rapide à mettre en oeuvre que le second. Plus récemment, l'écosystème équivalent dominant en `R`, [`Shiny`](https://shiny.posit.co/) a été adapté en `Python` par `Posit`. 
:::

::: {.content-visible when-profile="en"}
## Visualization applications

This part of the course focuses on simple synthetic representations. 
It does not (_yet?_) cover the construction of data visualization applications 
where a set of graphs update synchronously based on user interactions. 

This indeed exceeds the scope of an introductory course, as building 
these applications 
requires mastering more complex concepts like the interaction between a 
_web_ page and a server, having some knowledge of `Linux`, etc.
The concepts necessary to understand these tools are at the heart 
of the third-year course ["Deploying Data Science Projects"](https://ensae-reproductibilite.github.io/website/) 
that Romain Avouac and I teach in the third year at ENSAE. 

Nevertheless, since data value creation in the form of applications is very 
common, it is useful, at a minimum, to mention the distinction between static 
sites and dynamic applications to provide the right approach and point to the 
appropriate tools. 
In the world of applications, it is important to distinguish between the _front_ (the page 
visible to the application's users) and the _back office_ (the engine 
that performs actions based on parameters chosen by the user 
on the page). 

There are primarily two paradigms for making 
these two elements interact. The key difference between these approaches is the servers they rely on. A static site runs on a web server, whereas `Streamlit` relies on a standard _backend_ server. The main difference between these two types of servers lies in their function and usage:

* A _web_ server is specifically designed to store, process, and deliver web pages (the _front_) to clients. This includes HTML, CSS, JavaScript files, images, etc. Web servers listen for HTTP/HTTPS requests from user browsers and respond by sending the requested data. This doesn’t preclude having complex data processing steps or reactivity by embedding `JavaScript` in the application, but `Python` processing steps are done before the application is made available. For `Python` users, there are several static site generators before deployment via hosting on [`Github Pages`](https://pages.github.com/). The two most common ecosystems are [`Quarto Markdown`](https://quarto.org/) and [`Django`](https://www.djangoproject.com/), with the former being simpler to use and maintain than the latter. This site, for example, is built using `Quarto`, which ensures reproducibility of the presented examples and ergonomic, customizable formatting of the results.
* A standard _backend_ server is designed to perform operations in response to a _front_, in this case, a _web_ page. In the context of an application built with `Python`, this is a server with an appropriate `Python` environment to execute the code required to respond to any action taken by an application user. The code is executed on demand rather than once and for all, as in the previous approach. This paradigm allows for more application complexity but represents an additional challenge during the deployment phase. In the `Python` ecosystem, the two main tools for building such applications are [`Streamlit`](https://streamlit.io/) and [`Dash`](https://dash.plotly.com/), with the former being quicker to implement than the latter. More recently, the dominant `R` equivalent ecosystem, [`Shiny`](https://shiny.posit.co/), has been adapted for `Python` by `Posit`.
:::





::: {.content-visible when-profile="fr"}

:::: {.callout-note collapse="true"}
## Fait-on toujours du `tkinter` ?

Les écosystèmes présentés ci-dessus pour les applications réactives sont des _frameworks web_. Ils se distinguent des clients lourds comme [`tkinter`](https://docs.python.org/fr/3/library/tkinter.html), 
l'outil historique pour faire des interfaces graphiques. Outre l'aspect plus rudimentaire des 
interfaces `tkinter` par rapport à celles de `Streamlit`, `Dash` ou `Shiny`, il existe 
des raisons fortes pour privilégier ces derniers à `tkinter`. 

Ce dernier est un client lourd. Autrement dit, il est adhérent à un système d'exploitation 
et à des installations de _packages_ en amont du fonctionnement de l'interface. 
Il est bien sûr possible de rendre portable celle-ci mais, comme cela est développé 
dans le [cours de mise en production](https://ensae-reproductibilite.github.io/website/), 
il y a de nombreuses raisons pour lesquelles cette approche peut provoquer des erreurs 
ou des _bugs_ inattendus. Les _frameworks_ _web_ présentent l'intérêt de simplifier 
cette mise à disposition en dissociant le _front_ (des pages HTML et du CSS) du _back_ (du 
code `Python`). Ils se sont donc imposés naturellement même si on retrouve encore beaucoup 
de ressources en ligne datées sur le développement d'applications avec  `tkinter`.
::::
:::

::: {.content-visible when-profile="en"}

:::: {.callout-note collapse="true"}
## Is `tkinter` still used?

The ecosystems presented above for reactive applications are _web frameworks_. They are distinct from heavier clients like [`tkinter`](https://docs.python.org/3/library/tkinter.html), 
the historical tool for building graphical user interfaces. Besides the more rudimentary aspect of 
`tkinter` interfaces compared to those of `Streamlit`, `Dash`, or `Shiny`, there are 
strong reasons to prefer the latter over `tkinter`. 

`Tkinter` is a heavy client, meaning it is tied to an operating system 
and requires pre-installation of _packages_ before the interface can run. 
While it is certainly possible to make it portable, as discussed in the 
[production course](https://ensae-reproductibilite.github.io/website/), 
there are many reasons why this approach may lead to errors 
or unexpected bugs. _Web frameworks_ have the advantage of simplifying 
this deployment process by separating the _front_ (HTML and CSS pages) from the _back_ (the 
`Python` code). They have naturally become more popular, even though many 
dated online resources still exist for developing applications with `tkinter`.
::::

:::

::: {.content-visible when-profile="fr"}
En ce qui concerne la construction d'applications, le premier réflexe
à avoir est: _"ai-je besoin de faire une application réactive ou un site
statique ne suffit-il pas ? "_. Ce dernier étant beaucoup plus facile à
mettre en oeuvre et ayant une charge de maintenance minimale, c'est souvent
un choix rationnel. S'il devient complexe de faire un site statique, par
exemple parce qu'ils impliquent des calculs sophistiqués qu'il serait
complexe de mettre en oeuvre sans compétences `JavaScript`, on peut alors
se poser la question de la séparation entre _front_ et _back_
en reportant les calculs vers une API, construite par exemple par le biais de [`FastAPI`](https://fastapi.tiangolo.com/). Il s'agit, par exemple, d'une méthode pratique pour mettre
à disposition un modèle de _machine learning_ comme le
dernier chapitre
de la partie modélisation l'évoquera. Si la mise en oeuvre d'une API
est compliquée ou bien est un bazooka pour tuer une mouche,
alors on pourra aller vers une application réactive du type
de `Streamlit`.

Encore une fois, la construction d'une application fait
appel à des concepts qui dépassent
un niveau introductif en `Python`. Avoir conscience des bons réflexes
peut néanmoins faire économiser un temps non négligeable en évitant de patauger
dans la semoule à cause d'un mauvais choix initial. 
:::

::: {.content-visible when-profile="en"}
When it comes to building applications, the first instinct should be: _"Do I need to build a reactive application, or will a static site suffice?"_ The latter is much easier to implement and has minimal maintenance overhead, making it a rational choice in many cases. If building a static site becomes complex, for example, due to sophisticated calculations that would be difficult to implement without `JavaScript` skills, you can then consider separating the _front_ from the _back_ by delegating the calculations to an API, for example, built using [`FastAPI`](https://fastapi.tiangolo.com/). This can be a practical method to deploy a machine learning model, as will be discussed in the final chapter of the modeling section. If implementing an API seems too complicated or overkill for the task, then you can turn to a reactive application like `Streamlit`.

Again, building an application involves concepts that go beyond an introductory level in `Python`. However, being aware of the right practices can save significant time by avoiding pitfalls due to poor initial choices.
:::


::: {.content-visible when-profile="fr"}

## Résumé de cette partie

Pour en revenir au contenu de cette partie après cet _aparté_, celle-ci
est divisée en deux et chaque chapitre est lui-même
dual, selon qu'on s'intéresse aux représentations figées
ou dynamiques :

- Dans un premier temps, nous évoquerons des
représentations graphiques standards (histogrammes, diagrammes
en barre...) pour synthétiser certaines informations quantitatives ;
    + Les représentations fixes reposeront sur `Pandas`, `Matplotlib` et `Seaborn`
    + Les graphiques réactifs s'appuieront sur `Plotly`
- Dans un deuxième temps, nous présenterons les représentations
cartographiques:
    + Les cartes figées à partir de `Geopandas` ou de `plotnine`
    + Les cartes réactives avec `Folium` (adaptation `Python` de la librairie `Leaflet.js`)
:::

::: {.content-visible when-profile="en"}

## Summary of this section

Returning to the content of this section after this aside, it
is divided into two parts, and each chapter is dual in nature, depending
on whether we are focused on static or dynamic representations:

- First, we will discuss
standard graphical representations (histograms, bar charts, etc.) to synthesize quantitative information;
    + Static representations will rely on `Pandas`, `Matplotlib`, and `Seaborn`
    + Reactive charts will be built using `Plotly`
- Second, we will present cartographic representations:
    + Static maps created with `Geopandas` or `plotnine`
    + Reactive maps using `Folium` (a `Python` adaptation of the `Leaflet.js` library)
:::


::: {.content-visible when-profile="fr"}
## Références utiles

La visualisation de données est un art qui s'apprend, au début, principalement
par la pratique. Néanmoins, il n'est pas évident de produire
des visualisations lisibles et ergonomiques
et il est utile de s'inspirer d'exemples de
spécialistes (les grands titres de presse disposent d'excellentes visualisations).

Voici quelques ressources utiles sur ces sujets :

- [`Datawrapper`](https://blog.datawrapper.de/) propose un excellent blog sur les 
bonnes pratiques de visualisation, notamment
avec les articles de [Lisa Charlotte Muth](https://lisacharlottemuth.com/). Je recommande notamment cet article sur
les [couleurs](https://blog.datawrapper.de/emphasize-with-color-in-data-visualizations/) ou
celui-ci sur les [textes](https://blog.datawrapper.de/text-in-data-visualizations/) ;
- Le [blog d'Eric Mauvière](https://www.icem7.fr/) ;
- _["La Sémiologie graphique de Jacques Bertin a cinquante ans"](https://visionscarto.net/la-semiologie-graphique-a-50-ans)_ ;
- Les [visualisations _trending_](https://observablehq.com/explore) sur `Observable` ;
- Le _New York Times_ (les rois de la _dataviz_) revient tous les ans sur les meilleures visualisations
de l'année dans la veine du [_data scrollytelling_](https://makina-corpus-blog-scrollytelling.netlify.app/). Voir par exemple la [rétrospective de l'année 2022](https://www.nytimes.com/interactive/2022/12/28/us/2022-year-in-graphics.html).
:::

::: {.content-visible when-profile="en"}
## Useful references

Data visualization is an art that is learned primarily
through practice, especially at the beginning. However, it is not always easy to produce
readable and ergonomic visualizations,
so it is helpful to draw inspiration from examples by
specialists (major media outlets offer excellent visualizations).

Here are some useful resources on these topics:

- [`Datawrapper`](https://blog.datawrapper.de/) offers an excellent blog on 
best practices for visualization, particularly
with articles by [Lisa Charlotte Muth](https://lisacharlottemuth.com/). I especially recommend this article on
[colors](https://blog.datawrapper.de/emphasize-with-color-in-data-visualizations/) and
this one on [text](https://blog.datawrapper.de/text-in-data-visualizations/);
- The [blog of Eric Mauvière](https://www.icem7.fr/);
- _["La Sémiologie graphique de Jacques Bertin a cinquante ans"](https://visionscarto.net/la-semiologie-graphique-a-50-ans)_;
- The [trending visualizations](https://observablehq.com/explore) on `Observable`;
- The _New York Times_ (masters of _dataviz_) reviews the best visualizations
of the year annually, often in the vein of [_data scrollytelling_](https://makina-corpus-blog-scrollytelling.netlify.app/). For example, see the [2022 retrospective](https://www.nytimes.com/interactive/2022/12/28/us/2022-year-in-graphics.html).
:::

::: {.content-visible when-profile="fr"}

:::: {.callout-tip}
## Quelques ressources sur `Streamlit` ou `Dash`

Outre le [cours de 3e année](https://ensae-reproductibilite.github.io/website/) de l'ENSAE,
le lab de _data science_ de l'Insee a construit de nombreux tutoriels 
pour s'appropier les écosystèmes d'applications réactives en `Python` qui
sont l'un des produits les plus attractifs de l'écosystème `Python`. 

Voici par exemple un [tutoriel 101](https://inseefrlab.github.io/funathon2023_sujet4/) très détaillé sur `Streamlit` permettant de créer une [application type `Yuka`](https://myyuka.lab.sspcloud.fr/) sur les données de l'`openfoodfacts`. Un autre tutoriel pas à pas construit par l'Insee 
est consacré à `streamlit` et vise à proposer la construction d'un [tableau de bord du trafic aérien](https://inseefrlab.github.io/funathon2024_sujet2/). 

:::

::::

::: {.content-visible when-profile="fr"}
Et quelques références supplémentaires, citées dans cette introduction :
:::

::: {.content-visible when-profile="en"}
And a few additional references mentioned in this introduction:
:::


::: {#refs}
:::


