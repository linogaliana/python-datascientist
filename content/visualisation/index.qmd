---
title: "Partie 2: communiquer à partir de données"
categories:
  - Introduction
  - Visualisation
description: |
  Cette partie présente les outils pour visualiser des
  données avec `Python`, qu'il s'agisse de graphiques
  figés (`matplotlib`, `seaborn`, `geoplot`...) ou de
  visualisation réactives (`plotly`, `folium`, etc.)
slug: visualisation
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/snake-chart.png
bibliography: ../../reference.bib
---

# Introduction

::: {.content-visible when-profile="fr"}
Une partie essentielle du travail du _data scientist_
consiste à synthétiser l'information que
contient ses 
jeux de données afin de distinguer
ce qui relève du signal, sur lequel il 
pourra se concentrer, et ce qui relève
du bruit
inhérent à tout jeu de données. 
Dans le travail du _data scientist_, lors d'une phase
exploratoire, il y a
donc un aller-retour constant entre information synthétique
et jeu de données désagrégé. Il 
est ainsi essentiel de savoir synthétiser l'information
dans un jeu de données avant d'en saisir la structure, cette
dernière pouvant ensuite guider les exploitations ultérieures,
pour une phase de modélisation ou de correction de
données (détection d'anomalies ou de mauvaises remontées de données).

Nous avons déjà exploré une partie essentielle de ce travail,
à savoir la construction de statistiques descriptives pertinentes
et fiables. Néanmoins, si on se contentait de présenter l'information
en utilisant des sorties brutes issues du combo `groupby` et `agg`
sur un _DataFrame_ `Pandas`, notre connaissance des données serait assez
limitée. La mise en oeuvre de tableaux stylisés à partir
de `great tables` constituait déjà un progrès dans cette démarche mais, en vérité,
notre cerveau se représente l'information de manière beaucoup plus intuitive
par le biais de visualisations graphiques simples que dans un tableau. 
:::

::: {.content-visible when-profile="en"}
An essential part of the work of a _data scientist_
is to synthesize the information contained
in their datasets in order to distinguish
what constitutes the signal, which they 
can focus on, and what constitutes
the noise inherent in any dataset. 
In the work of a _data scientist_, during an exploratory phase,
there is a constant back-and-forth between synthesized information
and disaggregated datasets. It 
is therefore essential to know how to synthesize the information
in a dataset before grasping its structure, which
can then guide further analyses,
whether for a modeling phase or data correction
(anomaly detection or bad data retrieval).

We have already explored a key part of this work,
namely the construction of relevant
and reliable descriptive statistics. However, if we were content
to present information using raw outputs from the `groupby` and `agg`
combo on a `Pandas` _DataFrame_, our understanding of the data would be quite
limited. The implementation of stylized tables using
`great tables` was already a step forward in this process but, in truth,
our brain processes information much more intuitively
through simple graphical visualizations than through a table.
:::


## La visualisation des données, une part essentiel du travail de communication

En tant qu'humains,
nos
capacités cognitives étant limitées, nous ne pouvons
appréhender qu'une information limitée là où l'ordinateur est capable de traiter
de grands volumes d'information. En tant que _data scientist_, cela signifie
qu'utiliser nos compétences informatiques et statistiques pour obtenir
des représentations synthétiques de nos nombreux jeux de données est
essentiel pour être en mesure de répondre à nos besoins opérationnels ou
scientifiques. 
L'ensemble des méthodes et des outils qui constituent la boîte à outil
des _data scientists_ vise à simplifier l'appréhension puis l'exploitation
de jeux de données dont le volume dépasse nos capacités cognitives. 


Ceci nous entraîne vers la question de la visualisation des données,
un ensemble d'outils et de principes pour représenter de manière
synthétique des faits stylisés ou contextualiser une donnée individuelle.
La visualisation de données est l'art et la science de __représenter visuellement des informations complexes et abstraites à l'aide d'éléments visuels__.
Son objectif principal est de synthétiser l'information présente dans un ensemble de données afin de faciliter
la compréhension des enjeux de celle-ci pour une analyse ultérieure. 
La visualisation de données permet, entre autres, de mettre en évidence des tendances, des corrélations ou
des anomalies qui pourraient être difficiles voire impossibles à saisir simplement en examinant des données brutes, ces dernières nécessitant
une certaine mise en contexte pour porter du sens. 

La visualisation de données joue un rôle crucial dans le
processus d'analyse de données en fournissant des moyens visuels pour explorer, interpréter et communiquer des informations.
Elle facilite la communication entre experts de la données, décideurs et grand public,
en permettant aux derniers de bénéficier du travail rigoureux des premiers pour donner
sens aux données sans la connaissance des subtilités conceptuelles qui ont permis
de synthétiser l'information contenue dans celle-ci. 



## La place de la visualisation dans le processus de valorisation de la donnée

La visualisation des données n'est pas restreinte à la phase finale d'un projet,
à la communication de résultats à une audience qui n'a pas accès à la donnée
ou n'a pas les moyens de la valoriser. 
La visualisation intervient à toutes les étapes du processus de valorisation
de la donnée. Il s'agit d'ailleurs d'un travail essentiel pour trouver
comment basculer de l'enregistrement, un instantané d'un phénomène, à une donnée,
un enregistrement qui a une valeur parce qu'il porte une information en tant que telle
ou lorsqu'il est combiné avec d'autres enregistrements. 

Le travail quotidien du _data scientist_ 
consiste à regarder un jeu de données sous toutes ses coutures
pour identifier les axes prioritaires d'extraction de valeur. 
Savoir rapidement quelles statistiques représenter, et comment,
est essentiel pour gagner du temps sur cette partie exploratoire. 
Il s'agit principalement d'un travail de communication envers soi-même
qui peut se permettre d'être brouillon car il s'agit de dégrossir
le travail avant de polir certains angles. L'enjeu à ce niveau du
processus est de ne pas manquer une dimension qui pourrait être
porteuse de valeur. 

Le travail de communication réellement chronophage intervient plutôt
lorsqu'on communique à une audience ayant un accès limité à des
données, ne connaissant pas bien les sources, ayant
un temps d'attention sur limité
ou n'ayant pas des compétences quantitatives. Ces
publics ne peuvent se satisfaire d'une sortie brute comme
un _DataFrame_ dans un _notebook_ ou un graphique produit 
en quelques secondes avec la méthode `plot` de `Pandas`. 
Il convient de s'adapter à leurs attentes, qui évoluent, 
et aux outils qu'ils connaissent, d'où la place de plus en
plus importante prise par les sites
web de _data visualisations_. 

# Communiquer, une ouverture au _data storytelling_

La visualisation de données a ainsi une place à part dans 
l'ensemble des techniques de la _data science_. 
Elle intervient à tous les stades du processus de 
production de la donnée, de
l'amont (analyse exploratoire) à
l'aval (restitution à des publics multiples) et
peut, si elle est bien construite, permettre de
saisir de manière intuitive la structure des données
ou les enjeux de son analyse. 

Art de la synthèse, la visualisation de données
est également l'art de raconter une histoire et
peut même, lorsqu'elle est bien construite, prétendre
au rang de production artistique. 
La _dataviz_ est un métier en soi dont on trouve de 
plus en plus de praticiens dans les titres de presse
ou dans des entreprises
spécialisées (`Datawrapper` par exemple). 

Sans prétendre construire
des visualisations aussi riches que celles des spécialistes,
tout _data scientist_ se doit d'être en mesure de pouvoir
produire rapidement quelques visualisations permettant
de synthétiser les jeux de données à sa disposition. 
Une visualisation claire et lisible tout en restant simple
peut être meilleure qu'un discours pour faire passer un message.


De même qu'un discours, une visualisation est une communication
pour laquelle un locuteur - la personne construisant la visualisation - 
cherche à transmettre une information à un récepteur - éventuellement
la même personne que le locuteur puisqu'une visualisation peut 
être construite pour soi-même dans une analyse exploratoire. Il n'est
donc pas surprenant qu'à l'époque où la sémiologie occupait une 
part importante dans les débats intellectuels, notamment autour
de la figure de Roland Barthes, le concept de sémiologie 
graphique ait émergé
autour de la personne de Jacques Bertin [@bertin1967semiologie; @palsky2017semiologie]. 
Cette approche permet de réfléchir sur la pertinence des
techniques mises en oeuvre pour transmettre un message
graphique et de nombreuses visualisations, si elles
suivaient quelques-unes de ces règles, pourraient
être améliorées à peu de frais. 

Eric Mauvière, statisticien français héritier
de l'école de la sémiologie graphique de Bertin, 
propose d'excellents contenus sur le sujet. Certaines
des présentations qu'il a pu faire, notamment
celle pour le [`SSPHub`](https://ssphub.netlify.app/)
présentées dans la @nte-mauviere
devraient être visionnées dans toutes les formations
de _data science_ tant elles évoquent les nombreux
écueils rencontrés par les _data scientists_. 

![Un exemple de deux visualisations faites sur le même jeu de données par Eric Mauvière, voir @nte-mauviere](https://raw.githubusercontent.com/InseeFrLab/ssphub/main/talk/2024-02-29-mauviere/mauviere.png)



::: {#nte-mauviere .callout-note collapse="true"}
## Une conférence d'Eric Mauvière sur le sujet

```{ojs}
//| echo: false
html`${slides_button}`
```


{{< video src="https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20240229-dataviz-mauviere/video1991622347.mp4" controls="yes" >}}


```{ojs}
//| echo: false
slides = "https://minio.lab.sspcloud.fr/lgaliana/ssphub/replay/20240229-dataviz-mauviere/conf_ssphub_icem7.pdf"
```

```{ojs}
//| echo: false
slides_button = html`<p class="text-center">
  <a class="btn btn-primary btn-lg cv-download" href="${slides}" target="_blank">
    <i class="fa-solid fa-file-arrow-down"></i>&ensp;Télécharger les slides
  </a>
</p>`
```

:::


# Communiquer, une ouverture à la mise à disposition d'applications

L'objectif de ce cours est d'introduire aux principaux outils
et à la démarche que doivent adopter les _data scientists_
face à divers jeux de données. Il devient néanmoins de
plus en plus commun pour les _data scientists_
de développer et mettre à disposition des applications
interactives proposant un certain nombre d'explorations
et de visualisations automatisées de données. 
Il s'agit d'enjeux plus avancés que ce cours mais qui constituent
souvent un point d'entrée vers la _data science_ pour des
publics proches des _data scientists_. 

Nous évoquerons certains des outils privilégiés pour faire
cela, notamment les écosystèmes liés aux applications _web_
et aux outils `Javascript`. Ce besoin, devenu assez standard
pour les _data scientists_, fait la passerelle avec la mise
en production,
l'enjeu principal d'un cours de 3e année de l'ENSAE
construit par Romain Avouac et moi-même ([site web du cours](https://ensae-reproductibilite.github.io/website/)). Le présent site web, par exemple, est construit
selon ce principe grâce à des outils permettant d'exécuter de manière 
reproductible du `Python` sur des serveurs standardisés et ensuite
mettre à disposition ce code par le biais d'un site web. 


# L'écosystème `Python` {{< fa brands python >}}

Pour revenir à notre cours,
nous présenterons dans cette partie quelques librairies
et visualisations basiques en `Python` permettant de
partir sur de bonnes bases. Les ressources pour 
approfondir et progresser dans l'art de la visualisation
ne manquent pas, comme [cet ouvrage](https://clauswilke.com/dataviz/) [@wilke2019fundamentals]. 

## Les _packages_ de visualisations de données

L'écosystème `Python` pour la visualisation de données est très riche et
très éclaté.
Il est
possible de consacrer des livres entiers à celui-ci [@dale2022data].
`Python` propose
de nombreuses librairies pour produire de manière rapide et relativement
simple des visualisations de données[^1]. 

Les librairies graphiques se distinguent principalement en deux familles:

- Les librairies de __représentations figées__. Celles-ci ont plutôt vocation à être intégrées
dans des publications figées type PDF ou documents texte. Nous présenterons 
principalement `Matplotlib` et `Seaborn` mais il en existe d'autres, en pleine émergence,
comme [`Plotnine`](https://plotnine.readthedocs.io/en/stable/), l'adaptation de [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html) à l'écosystème `Python`.
- Les librairies de __représentations réactives__.  Celles-ci sont adaptées à des représentations
_web_ et offrent la possibilité aux lecteurs d'agir sur la représentation graphique affichée. 
Les librairies qui proposent ces fonctionnalités reposent généralement sur `JavaScript`, l'écosystème
du développement _web_, pour lequel elles offrent un point d'entrée via `Python`. 
Nous évoquerons principalement `Plotly` et `Folium` dans cette famille mais il existe de nombreux
autres _frameworks_ dans ce domaine[^2].


[^1]: Pour être honnête, pendant longtemps `Python` a été sur ce point un peu moins agréable
que `R` qui bénéficie de
l'incontournable librairie [`ggplot2`](https://juba.github.io/tidyverse/08-ggplot2.html).

    N'étant pas
construite sur la [grammaire des graphiques](http://r.qcbs.ca/workshop03/book-fr/la-grammaire-des-graphiques-gg.html),
la principe librairie de graphiques en `Python` qu'est `Matplotlib` est plus fastidieuse
à utiliser que `ggplot2`. 

    [`seaborn`](https://seaborn.pydata.org/), que nous présenterons,
facilite un peu le travail de représentation graphique mais, là encore, il est difficile de faire
plus malléable et universel que `ggplot2`.

    La librairie [`plotnine`](https://plotnine.readthedocs.io/en/stable/) vise à proposer une implémentation similaire
à `ggplot` pour les utilisateurs de `Python`. Son développement est à suivre. 


[^2]: A cet égard, je recommande vivement de suivre l'actualité de la _dataviz_
sur la plateforme [`Observable`](https://observablehq.com/) qui tend à
rapprocher les communautés des spécialistes de la _dataviz_ et des analystes
de données. La librairie [`Plot`](https://observablehq.com/plot/) pourrait devenir
un nouveau standard dans les prochaines années, sorte d'intermédiaire
entre `ggplot` et `d3`. 

Il est tout à fait possible
de faire des visualisations sophistiquées avec
une chaine de bout en bout `Python` puisqu'il s'agit d'un langage couteau-suisse
dont l'écosystème est très 
riche. Néanmoins, `Python` n'est pas la panacée et il peut parfois
être utile, pour obtenir un produit fini parfaitement poli,
de finaliser le travail avec d'autres langages, comme `Javascript` 
pour les visualisations réactives ou `QGIS` pour le
travail cartographique. Ce cours donnera les outils minimums
pour faire un travail rapide et plaisant mais le diable étant dans
les détails, il ne faut pas s'arcbouter à vouloir utiliser
`Python` pour tout et n'importe quoi. 

Dans le domaine de la visualisation, ce cours adopte le parti pris
d'explorer quelques
librairies centrales à partir d'un nombre restreint d'exemples en
répliquant des graphiques qu'on peut trouver sur le site d'*open data* de la 
mairie de Paris. 
La meilleure école pour la visualisation restant
la pratique sur des jeux de données, il est recommandé d'explorer la richesse
de l'écosystème de l'_open data_ pour expérimenter des visualisations. 


## Les applications de visualisation

Cette partie du cours se focalise sur des représentations synthétiques simples. 
Elle n'évoque pas (_encore ?_) la construction d'applications de visualisation
de données où un ensemble de graphiques se mettent à jour de manière synchrone
en fonction d'actions d'utilisateurs. 

Ceci dépasse en effet le cadre d'un cours d'introduction car construire
ces applications
impliquent
de maîtriser des concepts plus complexes comme l'interaction entre une page
_web_ et un serveur, d'avoir des rudiments de connaissance en `Linux`, etc.
Les concepts nécessaires à la compréhension de ces outils sont au coeur
du cours de 3e année ["Mise en production de projets de _data science_"](https://ensae-reproductibilite.github.io/website/)
que Romain Avouac donnons en 3e année d'ENSAE. 

Néanmoins, comme la valorisation de données sous une forme applicative est très
commune, il
il est utile _a minima_ d'évoquer la dualité entre sites statiques
et applications dynamiques afin de donner les bons gestes et pointer vers les 
outils adéquat. 
Dans le monde de l'applicatif, il est important de distinguer le _front_ (la page
visible par les utilisateurs de l'application) du _back office_ (le moteur
qui effectue des actions en fonction des paramètres choisis par l'utilisateur
de la page). 

Il existe principalement deux paradigmes pour faire
interagir ces deux éléments. La distinction principale entre ces deux approches est qu’elles s’appuient sur des serveurs différents. Un site statique repose sur un serveur web là où `Streamlit` s’appuie sur serveur classique en _backend_. La différence principale entre ces deux types de serveurs réside principalement dans leur fonction et leur utilisation:

* Un serveur _web_ est spécifiquement conçu pour stocker, traiter et livrer des pages web (le _front_) aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées. Cela n'empêche pas d'avoir des étapes complexes de valorisation de données, ni de la réactivité en embarquant du `Javascript` dans l'application mais les étapes de traitement en `Python` sont faites en amont de la mise à disposition de l'application. Pour les utilisateurs de `Python`, il existe plusieurs constructeurs de sites statiques avant une mise à disposition par le biais d'un hébergement sur [`Github Pages`](https://pages.github.com/). Les deux écosystèmes les plus communs sont [`Quarto Markdown`](https://quarto.org/) et [`Django`](https://www.djangoproject.com/), le premier étant plus simple d'usage et de maintenance que le second. Ce site, par exemple, est construit grâce à `Quarto` ce qui assure la reproductibilité des exemples présentés et une mise en forme ergonomique et paramétrable des résultats.  
* Un serveur _backend_ classique est conçu pour effectuer des opérations en réponse à un _front_, en l’occurrence une page _web_. Dans le contexte d’une application construite avec `Python`, il s’agit d’un serveur avec l’environnement `Python` _ad hoc_ pour exécuter le code nécessaire à répondre à toute action d’un utilisateur de l’application. Le code est exécuté à la volée et non une fois pour toute comme dans l'approche précédente. Il s'agit donc d'un paradigme pouvant permettre plus de complexité applicative mais représentant un défi supplémentaire lors de la phase de mise en production. Dans l'écosystème `Python`, les deux principaux outils permettant de construire de telles applications sont [`Streamlit`](https://streamlit.io/) et [`Dash`](https://dash.plotly.com/), le premier étant plus rapide à mettre en oeuvre que le second. Plus récemment, l'écosystème équivalent dominant en `R`, [`Shiny`](https://shiny.posit.co/) a été adapté en `Python` par `Posit`. 

::: {.callout-note collapse="true"}
## Fait-on toujours du `tkinter` ?

Les écosystèmes présentés ci-dessus pour les applications réactives sont des _frameworks web_. Il se distinguent des clients lourds comme [`tkinter`](https://docs.python.org/fr/3/library/tkinter.html),
l'outil historique pour faire des interfaces graphiques. Outre l'aspect plus rudimentaire des
interfaces `tkinter` par rapport à celles de `Streamlit`, `Dash` ou `Shiny`, il existe
des raisons fortes pour privilégier ces derniers à `tkinter`. 

Ce dernier est un client lourd. Autrement dit, il est adhérent à un système d'exploitation
et à des installations de _packages_ en amont du fonctionnement de l'interface.
Il est bien sûr possible de rendre portable celle-ci mais, comme cela est développé
dans le [cours de mise en production](https://ensae-reproductibilite.github.io/website/),
il y a de nombreuses raisons pour lesquelles cette approche peut provoquer des erreurs
ou des _bugs_ inattendus. Les _frameworks_ _web_ présentent l'intérêt de simplifier
cette mise à disposition en dissociant le _front_ (des pages HTML et du CSS) du _back_ (du
code `Python`). Ils se sont donc imposés naturellement même si on retrouve encore beaucoup 
de ressources en ligne datées sur le développement d'applications avec  `tkinter`.
:::

En ce qui concerne la construction d'applications, le premier réflexe
à avoir est: _"ai-je besoin de faire une application réactive ou un site
statique ne suffit-il pas ? "_. Ce dernier étant beaucoup plus facile à
mettre en oeuvre et ayant une charge de maintenance minimale, c'est souvent
un choix rationnel. S'il devient complexe de faire un site statique, par
exemple parce qu'ils impliquent des calculs sophistiqués qu'il serait
complexe de mettre en oeuvre sans compétences `JavaScript`, on peut alors
se poser la question de la séparation entre _front_ et _back_
en reportant les calculs vers une API, construite par exemple par le biais de [`FastAPI`](https://fastapi.tiangolo.com/). Il s'agit, par exemple, d'une méthode pratique pour mettre
à disposition un modèle de _machine learning_ comme le
dernier chapitre
de la partie modélisation l'évoquera. Si la mise en oeuvre d'une API
est compliquée ou bien est un bazooka pour tuer une mouche,
alors on pourra aller vers une application réactive du type
de `Streamlit`.

Encore une fois, la construction d'une application fait
appel à des concepts qui dépassent
un niveau introductif en `Python`. Avoir conscience des bons réflexes
peut néanmoins faire économiser un temps non négligeable en évitant de patauger
dans la semoule à cause d'un mauvais choix initial. 



## Résumé de cette partie

Pour en revenir au contenu de cette partie après cet _aparté_, celle-ci
est divisée en deux et chaque chapitre est lui-même
dual, selon qu'on s'intéresse aux représentations figées
ou dynamiques :

- Dans un premier temps, nous évoquerons des
représentations graphiques standards (histogrammes, diagrammes
en barre...) pour synthétiser certaines informations quantitatives ;
    + Les représentations fixes reposeront sur `Pandas`, `Matplotlib` et `Seaborn`
    + Les graphiques réactifs s'appuieront sur `Plotly`
- Dans un deuxième temps, nous présenterons les représentations
cartographiques:
    + Les cartes figées à partir de `Geopandas` ou de `plotnine`
    + Les cartes réactives avec `Folium` (adaptation `Python` de la librairie `Leaflet.js`)


## Références utiles

La visualisation de données est un art qui s'apprend, au début, principalement
par la pratique. Néanmoins, il n'est pas évident de produire
des visualisations lisibles et ergonomiques
et il est utile de s'inspirer d'exemples de
spécialistes (les grands titres de presse disposent d'excellentes visualisations).


Voici quelques ressources utiles sur ces sujets :

- [`Datawrapper`](https://blog.datawrapper.de/) propose un excellent blog sur les 
bonnes pratiques de visualisation, notamment
avec les articles de [Lisa Charlotte Muth](https://lisacharlottemuth.com/). Je recommande notamment cet article sur
les [couleurs](https://blog.datawrapper.de/emphasize-with-color-in-data-visualizations/) ou
celui-ci sur les [textes](https://blog.datawrapper.de/text-in-data-visualizations/) ;
- Le [blog d'Eric Mauvière](https://www.icem7.fr/) ;
- _["La Sémiologie graphique de Jacques Bertin a cinquante ans"](https://visionscarto.net/la-semiologie-graphique-a-50-ans)_ ;
- Les [visualisations _trending_](https://observablehq.com/explore) sur `Observable` ;
- Le _New York Times_ (les rois de la _dataviz_) revient tous les ans sur les meilleures visualisations
de l'année dans la veine du [_data scrollytelling_](https://makina-corpus-blog-scrollytelling.netlify.app/). Voir par exemple la [rétrospective de l'année 2022](https://www.nytimes.com/interactive/2022/12/28/us/2022-year-in-graphics.html).

::: {.callout-tip}
## Quelques ressources sur `Streamlit` ou `Dash`

Outre le [cours de 3e année](https://ensae-reproductibilite.github.io/website/) de l'ENSAE,
le lab de _data science_ de l'Insee a construit de nombreux tutoriels 
pour s'appropier les écosystèmes d'applications réactives en `Python` qui
sont l'un des produits les plus attractifs de l'écosystème `Python`. 

Voici par exemple un [tutoriel 101](https://inseefrlab.github.io/funathon2023_sujet4/) très détaillé sur `Streamlit` permettant de créer une [application type `Yuka`](https://myyuka.lab.sspcloud.fr/) sur les données de l'`openfoodfacts`. Un autre tutoriel pas à pas construit par l'Insee 
est consacré à `streamlit` et vise à proposer la construction d'un [tableau de bord du trafic aérien](https://inseefrlab.github.io/funathon2024_sujet2/). 

:::

Et quelques références supplémentaires, citées dans cette introduction :

::: {#refs}
:::


