::: {.content-visible when-profile="fr"}

# Pourquoi apprendre `Python` quand il existe des IA génératrices de code ?

Les assistants de code, notamment `Copilot` et `ChatGPT`, ont changé à jamais le développement de code. Ces outils font maintenant parti des outils quotidiens des _data scientists_ et offrent beaucoup de confort car ils peuvent, à l'aide d'instructions plus ou moins claires, générer du code `Python`. Et ces IA ayant étaient entraînées à partir de l'ensemble du code présent sur internet, et parfois spécialisées à répondre à des problèmes de développement, elles aident beaucoup. L'ambition du _vibe coding_ est d'aller une étape plus loin dans ce processus en renforçant la prise d'initiative des LLM qui ne passerait plus par l'intermédiaire de l'humain pour accéder aux ressources de calcul qui permettent d'exécuter le code proposé.

_Puisque maintenant les IA génèrent du code, pourquoi continuer à apprendre à coder ?_

Car coder ce n'est pas juste produire des lignes de code, c'est se confronter à un problème, adopter une stratégie par étape pour y répondre, réfléchir à plusieurs solutions possibles et choisir la meilleure en arbitrant entre plusieurs objectifs (vitesse, simplicité, etc.), tester et corriger, etc. Le code est un outil au service d'un problème d'ingenieurie. Les IA savent très bien coder, elles savent relier un problème à des ressources sur lesquelles elles ont appris et savent bien transposer un problème rencontré dans un autre langage à `Python`.

Mais encore faut-il savoir formuler le problème, savoir juger de la qualité de la réponse du LLM, être capable de remettre en question la proposition de l'assistant pour corriger une erreur ou obtenir une réponse plus satisfaisante. Les LLM sont une recherche _google_ très raffinée: si vous n'avez pas les bons mots clés pour faire une recherche Google, votre recherche sera décevante. Il en va de même avec les LLM même si l'aspect conversationnel en langage naturel réduit la barrière à l'entrée.

La confrontation à un jeu de données relève avant tout d’une démarche d’ingénierie. Le code n’est pas une fin en soi, mais un outil au service d’un raisonnement structuré, visant à résoudre un problème concret. Comme un ingénieur qui conçoit un pont à partir d’un besoin de traversée, le data scientist part d’un objectif opérationnel — construire un algorithme de sélection, mesurer l’impact d’un lancement produit, prédire une évolution de ventes — pour le formuler de manière exploitable. Cela implique de traduire des concepts scientifiques ou business en questions analytiques, puis de décomposer le problème en étapes logiques, chacune traduite en instructions que la machine peut exécuter.

Dans ce cadre, un LLM peut jouer un rôle d’assistant, mais seulement si le problème est bien posé. Si les étapes sont floues ou mal définies, la réponse du modèle sera approximative, voire inutile. Sur une tâche standard, le résultat pourra sembler correct, mais sur une question plus spécifique, il faudra souvent affiner, reformuler, itérer… et parfois ne jamais obtenir de réponse satisfaisante. Non pas parce que le modèle est mauvais, mais parce que l’ingénierie du problème en amont fait toute la différence[^ai-enstein].

[^ai-enstein]: Il n'est pas inutile sur ce sujet de lire le _post_ de Thomas Wolf (CSO de `HuggingFace`) ["The Einstein AI model"](https://thomwolf.io/blog/scientific-ai.html). Même si le _post_ évoque principalement les innovations de rupture en mettant quelque peu de côté les innovations marginales, il est intéressant de comprendre que les LLM, en dépit des grandes annonces prophétiques faites par les gourous de la tech, restent des outils qui certes ont de bonnes performances sur des tests standardisés mais restent des assistants, pour l'heure.

Une dernière raison pour laquelle se contenter d'une IA de code sans recul critique est que ces dernières sont forcément en retard par rapport aux usages puisqu'elles ont été entraînées sur des données passées. L'écosystème `Python` est très dynamique et, même si les IA des principaux fournisseurs de services sont fréquemment réentrainées et peuvent maintenant accéder à internet pour rafraichir leurs connaissances, certaines librairies peuvent rapidement s'imposer dans leur domaine.

Par exemple, en cette année 2025, [`uv`](https://docs.astral.sh/uv/) a connu une adoption rapide, comme [`ruff`](https://docs.astral.sh/ruff/) l'année d'avant. Il faudra encore un peu de temps pour que les IA génératives proposent d'elles-mêmes ce gestionnaire d'environnement plutôt que [`poetry`](https://python-poetry.org/). L'existence d'IA génératives ne dispense donc pas, comme avant, d'avoir une veille technique active et d'être vigilant sur l'évolution des pratiques.
:::

::: {.content-visible when-profile="en"}

# Why learn `Python` when code-generating AIs exist?

Code assistants like `Copilot` and `ChatGPT` have fundamentally transformed software development. These tools are now part of the everyday toolkit of a _data scientist_, offering remarkable convenience by generating `Python` code from more or less well-specified instructions. Trained on massive amounts of publicly available code—and often fine-tuned for solving development tasks—they can be extremely helpful. The concept of _vibe coding_ even pushes this further, aiming to let large language models (LLMs) take initiative without requiring human intermediaries to access the computational resources needed to run the code they generate.

So, if AIs can now generate code, why should we still learn how to code?

Because coding is not just about writing lines of code. It’s about understanding a problem, crafting a step-by-step strategy to tackle it, considering multiple solutions and trade-offs (e.g., speed, simplicity), testing and debugging. Code is a means to an engineering end. While AIs are very good at generating code, relating problems to known patterns, and even translating solutions across languages into `Python`, that’s only part of the picture.

Working with data is first and foremost an engineering process. Code is not the goal—it’s the tool that supports structured reasoning toward solving real-world problems. Just like an engineer designs a bridge to meet a practical need, a data scientist begins with an operational goal—such as building a recommendation system, evaluating the impact of a product launch, or forecasting sales—and reformulates it into an analytical task. This means translating scientific or business ideas into a set of questions, then breaking those down into logical steps, each of which can be executed by a computer.

In this context, an LLM can act as a valuable assistant—but only when the problem is well formulated. If the task is vague or ill-defined, the model’s answers will be approximate or even useless. On standard problems, the results may appear accurate. But for more specific, non-standard tasks, it often becomes necessary to iterate, refine the prompt, reframe the problem… and sometimes still fail to get a satisfactory result. Not because the model is poor, but because good problem formulation—the essence of problem engineering—makes all the difference[^ai-enstein-en].

[^ai-enstein-en]: On this topic, see Thomas Wolf’s blog post [_The Einstein AI model_](https://thomwolf.io/blog/scientific-ai.html). Although the post focuses on disruptive innovation and pays less attention to incremental progress, it’s insightful in understanding that LLMs—despite bold predictions from tech influencers—are still just tools. They may excel at standardized tasks, but for now, they remain assistants.

For instance, in the year 2025, [`uv`](https://docs.astral.sh/uv/) saw rapid adoption, as did [`ruff`](https://docs.astral.sh/ruff/) the year before. It will still be some time before generative AIs propose this environment manager on their own, rather than [`poetry`](https://python-poetry.org/). The existence of generative AIs does not, therefore, dispense us, as before, from keeping an active technical watch and being vigilant about changes in practices.
:::
