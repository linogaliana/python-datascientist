---
title: "L'environnement Python pour la data science"
date: 2020-07-22T12:00:00Z
draft: false
weight: 30
slug: "ecosystemeDS"
type: book
description: |
  `Python` propose un écosystème très riche pour la
  _data science_. Ce chapitre fait un tour
  d'horizon de celui-ci en présentant les principaux
  _packages_ qui seront présentés dans ce cours.
image: python_panda.jpg
categories:
  - Tutoriel
  - Rappels
---

La richesse des langages _open-source_ est la possibilité
d'utiliser des _packages_
développés par des spécialistes. `Python` est particulièrement
bien doté dans le domaine. Pour caricaturer, on lit parfois
que `Python` est le deuxième meilleur langage pour toutes les
tâches, ce qui en fait le meilleur langage. 

En effet, la malléabilité de `Python` fait qu'on peut
l'aborder de manière très différentes
selon que l'on est plutôt SysAdmin, développeur web ou 
data scientist. C'est ce dernier profil qui va ici nous
intéresser.

Le data scientist devant disposer de nombreuses cordes
à son arc. Cela se reflète sur l'écosystème de la _data science_
qui est assez éclaté. Cependant, ce foisonnement 
n'est pas propre à `Python` puisque `R` propose encore plus de
packages que `Python` où un certain nombre de _framework_
normalisés limitent l'éclatement de l'écosystème. De plus,
le foisonnement de l'environnement du _data scientist_ 
est une véritable opportunité puisqu'elle permet
aux packages de se spécialiser dans un 
domaine, où ils sont plus efficaces, et aux concepteurs
de package d'oser mettre en oeuvre de nouvelles méthodes, 
indispensables pour que le langage suive les évolutions
rapides de la recherche ou de la technologie.




## Les packages Python essentiels pour le cours et la vie des _data scientists_

![](https://pydsc.files.wordpress.com/2017/11/pythonenvironment.png?w=663)

Ce
[post](https://medium.com/data-science-library/ultimate-python-library-guide-for-data-science-2562148158bf),
dont l'image ci-dessus est tirée, résume la plupart des packages utiles
pour un data scientist ou un économiste/sociologue. Nous nous bornerons
ici à évoquer ceux utilisés quotidiennement.

### `numpy`

`numpy` gère tout ce qui est calcul matriciel.
Le langage `Python` est un des langages les plus lents qui soient[^1].
Tous les calculs rapides ne sont pas écrits en `Python` mais en `C++`, voire `Fortran`.
C’est le cas du package `numpy`. Celui-ci est incontournable
dès qu’on veut être rapide. Le package
`scipy` est une extension où l’on peut trouver
des fonctions statistiques, d’optimisation.

[^1]: `Python` est un langage interprété, comme `R`. Cela le rend très
intelligible, y compris par un non-expert. C'est une des raisons de son
succès. Le créateur de `Python`, Guido Van Rossum,
en a fait un des principes philosophiques
à l'origine de `Python`: un code est plus souvent lu qu'écrit. 
La contrepartie est qu'il s'agit d'une surcouche à des langages
plus bas-niveau, notamment `C`. Ces derniers proposent beaucoup moins de
surcouches. En réalité, les fonctions `Python` font appel, plus ou moins
directement, à du `C`. Une manière d'optimiser le code est ainsi d'arriver,
avec le moins de surcouches possible, à la fonction `C` sous-jacente,
beaucoup plus rapide.

La _Cheat Sheet_ de `numpy` est pratique: 
<https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf>

Comme `numpy` est la brique de base de l'analyse de données, un chapitre
de ce cours lui est consacré. 

### `pandas`

Avant tout, un bon _data scientist_ doit être capable de
s'approprier et manipuler des données rapidement. Pour cette raison,
`pandas` est incontournable.
Il gère la plupart des formats de données. Pour être efficace,
il est lui aussi implémenté en `C++`.
Le package est rapide si on utilise les méthodes pré-implémentées sur
des données d'une taille raisonnable (par rapport à la RAM disponible). Il faut
néanmoins s'en méfier avec des données volumineuses.
En règle générale, un jeu de données nécessite
trois fois plus d’espace en mémoire que les
données n’en prennent sur le disque.

La Cheat Sheet de pandas :
<https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf>

`pandas` étant un élément incontournable, deux chapitres y sont consacrés.


### `matplotlib` et `seaborn`

`matplotlib` existe depuis une vingtaine d'années pour doter `Python` de
fonctionalités graphiques. Il s'agit d'un package très flexible, offrant 
de nombreuses fonctionalités. Néanmoins, ces dernières années, 
`seaborn` a émergé pour simplifier la création de certains graphiques
standards de l'analyse de données (histogrammes, diagramme en barre, etc. ).
Le succès de `seaborn` n'éclipse néanmoins pas `matplotlib` puisque ce
dernier est souvent nécessaire pour finaliser la customisation d'un 
graphique produit par `seaborn`[^2]

[^2]: La situation est différente en `R` où `ggplot2` a quasiment éclipsé
l'outil de graphique de base de `R`.

### `scikit-learn`

`scikit-learn` est le module de *machine learning* le plus populaire pour
trois raisons:

* il s'appuie sur une API extrêmement consistante (méthodes *fit*, *transform*
  et *predict*, respectivement pour apprendre des données, appliquer des transformations et prédire sur de nouvelles données) ; 
* il permet de construire 
des analyses reproductibles en construisant des *pipelines* de données ;
* sa documentation est un modèle à suivre.

L'INRIA, institution française, est l'un des éléments moteurs dans 
la création et la maintenance de `scikit-learn`

### `TensorFlow`, `PyTorch` et `Keras`

Les librairies essentielles pour implémenter et utiliser des modèles
de *deep learning* en Python ont été développées par des acteurs du
numérique.

`TensorFlow` est la librairie la plus mature, mais pas nécessairement la plus facile à prendre en main. D'ailleurs, Google semble l'abandonner en usage interne pour lui 
préférer `JAX`.
`Keras` propose une interface *high-level*,
donc plus facile d'utilisation,
mais qui n'en reste pas moins suffisante pour une grande variété d'usages.
La documentation de `Keras` est très bien faite.

`PyTorch` est un _framework_ plus récent mais très complet,
dont la syntaxe plaira aux amateurs de programmation orienté-objet.
Développé par Facebook, 
il est très utilisé dans certains domaines de recherche, comme le NLP.
Il s'agit du _framework_ dont la dynamique récente a été la plus
ascensionnelle. 

### `statsmodels`

`statsmodels` plaira plus aux statisticiens, il implémente des modèles
économétriques similaires à `scikit-learn`. 
Par rapport à `scikit-learn`, 
`statsmodels` est plus orienté économétrie. La présentation des
résultats est très proche de ce qu’on trouve en `R`.

###  `requests` et `beautifulsoup`

`requests` est l'une des librairies de base de `Python`, dédiée
à gérer la connexion avec internet. Les amateurs d'API 
seront des utilisateurs fréquents de celle-ci. Les 
personnes plus spécialistes de _web scraping_ l'utiliseront avec
`beautifulsoup` qui offre une syntaxe extrêmement puissante
pour récupérer automatiquement du contenu de pages web.

### `nltk` et `spaCy` 

Dans le domaine du traitement automisé du langage, plus connu
sous son acronyme anglais NLP, les deux packages phares sont
`nltk` et `spaCy`.

`nltk` est le package historique. Il existe depuis les années
1990 et propose de nombreuses ressources utiles pour l'analyse
textuelle. Néanmoins, ces dernières années, `spaCy` est venu
moderniser l'approche en proposant une approche permettant
de mieux intégrer les différentes phases du traitement de données 
textuelles, une excellente documentation et un meilleur support
des langues non anglo-saxonnes, comme le Français. 

Mais `Python` est également un outil privilégié pour communiquer:

* Une bonne intégration de Python à `Markdown` (grâce notamment à ... `R Markdown`) qui facilite la construction de documents HTML ou PDF (via `Latex`)
* [Sphynx](https://www.sphinx-doc.org/en/master/) et [JupyterBook](https://jupyterbook.org/intro.html) proposent des modèles de documentation
très complets
* [`bokeh`](https://bokeh.org/) ou [`streamlit`](https://www.streamlit.io/) comme alternative à [shiny (R)](https://shiny.rstudio.com/)
* [`Django`](https://www.djangoproject.com/) et [`Flask`](https://flask.palletsprojects.com/en/2.0.x/) permettent de construire des applications web en `Python`
* Les librairies dynamiques, notamment
[`folium`](https://python-visualization.github.io/folium/) ou 
[`plotly`](https://plotly.com/), sont très appréciées pour construire des
visualisations dynamiques qui sont pratiques dans une analyse exploratoire
mais également lorsqu'il faut valoriser ses travaux auprès de
publics non experts de la donnée.

L'un des nouveaux arrivants dans cet écosystème déjà riche
est [`FastAPI`](https://fastapi.tiangolo.com/)). Avec ce _package_, 
il est très facile de transformer un code `Python` en `API` ce qui facilite
la mise à disposition de données mais aussi de productions par `Python` (comme
la mise à disposition d'une API pour permettre à des personnes de tester
les résultats d'un modèle de machine learning).

Ce n'est qu'une petite partie de l'écosystème `Python`, d'une richesse rare.


## Environnement autour de Python

Python est un langage très riche, grâce à sa logique open-source. Mais l'un
des principaux intérêts réside dans le riche écosystème avec lequel Python
s'intègre. On peut donner quelques éléments, dans un inventaire à la Prévert non exaustif.

En premier lieu, des éléments reliés au traitement des données:

* [`Spark`](https://fr.wikipedia.org/wiki/Apache_Spark),
le *framework* dominant dans le domaine du traitement des *big-data*, très bien
interfacé avec `Python` (grâce à l'API `pyspark`), qui facilite le traitement des données volumineuses. Son utilisation nécessite cependant d'avoir accès à une
infrastructure de calculs distribuée.
* [`Cython`](https://cython.org/) permet d'intégrer facilement du code `C`, très
efficace avec `Python` (équivalent de `Rcpp` pour `R`).
* [`Julia`](https://julialang.org/) est un langage récent, qui propose une syntaxe familière aux utilisateurs de languages scientifiques (Python, R, MATLAB), tout en permettant des performances proches du `C` grâce à une compilation à la volée.



Enfin, des éléments permettant un déploiement de résultats ou d'applications
en continu :
* Les images `Docker` de `Jupyterhub` facilitent l'usage de l'intégration continue
pour construire des modules, les tester et déployer des site web.
* Les services type `Binder`, `Google Colab` et `Kaggle` proposent des kernels
`Python`

## Rester au courant des évolutions 

L'écosystème riche et foisonnant de `Python` a comme contrepartie
qu'il faut rester attentif à ses évolutions pour ne pas 
voir son capital humain vieillir et ainsi devenir _has-been_.
Alors qu'avec des langages 
monolithiques comme
`SAS` ou `Stata` on pouvait se permettre de ne faire de vieille technique
mais seulement consulter la documentation officielle, avec `Python` 
ou `R` c'est impossible. Ce cours lui-même est en évolution continue, ce
qui est assez exigeant :sweating:, pour épouser les évolutions
de l'écosystème. 

`Twitter` est une excellente source d'information pour être rapidement 
au courant des évolutions du monde de la data science. Les agrégateurs
de contenu comme `medium` ou `towardsdatascience` proposent des _posts_
de qualité hétérogène mais il peut être utile de recevoir par mail
le _feed_ des nouveaux _posts_ : au bout d'un certain temps, cela peut
permettre de dégager les nouvelles tendances. Le site
`realpython` propose généralement de très bon posts, complets et 
pédagogiques. 

En ce qui concerne les ouvrages papiers, certains sont de très bonne qualité.
Cependant, il convient de faire attention à la date de mise à jour de ceux-ci :
la vitesse d'évolution de certains éléments de l'écosystème peut les
périmer très rapidement. 

