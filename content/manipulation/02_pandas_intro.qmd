---
title: "Introduction à Pandas"
title-en: "Introduction to Pandas"
tags:
  - Pandas
  - Pollution
  - Ademe
  - Tutoriel
  - Manipulation
categories:
  - Tutoriel
  - Exercices
  - Manipulation
description: |
  `Pandas` est l'élément central de l'écosystème `Python` pour la _data science_. Ce chapitre présente les premières
  manipulations de données qu'on peut faire grâce à `Pandas` pour explorer
  la structure de son jeu de données.
description-en: |
  `Pandas` is the central piece of the `Python` ecosystem for data science. This chapter presents the first data exploration that can be performed with `Pandas` in order to explore the structure of a dataset.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda_stretching.png
links:
- icon: journal-text
  name: Documentation Pandas
  url: https://pandas.pydata.org/docs/
echo: false
---

::: {.content-visible when-format="html"}

:::: {.content-visible when-profile="fr"}
Pour essayer les exemples présents dans ce tutoriel : 
::::
:::: {.content-visible when-profile="en"}
If you want to try the examples in this tutorial:
::::

{{< include "../../build/_printBadges.qmd" >}}

:::

::: {.content-visible when-format="ipynb"}
{{warninglang}}
:::

::: {.content-visible when-profile="fr"}

:::: {.tip}
## Compétences à l'issue de ce chapitre

- Importer un jeu de données sous forme de _dataframe_ `Pandas` et explorer sa structure ;
- Effectuer des manipulations sur les colonnes et les lignes ;
- Construire des statistiques agrégées et chaîner les opérations ;
- Utiliser les méthodes graphiques de `Pandas` pour se représenter rapidement la distribution des données.

::::

:::

::: {.content-visible when-profile="en"}

:::: {.tip}
## Skills to be acquired by the end of this chapter

- Import a dataset as a `Pandas` dataframe and explore its structure;
- Perform manipulations on columns and rows;
- Construct aggregate statistics and chain operations;
- Use `Pandas` graphical methods to quickly represent data distribution.

::::

:::



# Introduction

::: {.content-visible when-profile="fr"}


Le _package_ `Pandas` est l'une des briques centrales de l'écosystème de
la _data science_ depuis une dizaine d'années. Le _DataFrame_, 
objet central dans des langages comme `R` 
ou `Stata`, a longtemps était un grand absent dans l'écosystème `Python`. 
Pourtant, grâce à `Numpy`, toutes les briques de base étaient présentes
mais méritaient d'être réagencées pour convenir aux besoins
des _data scientists_. 

Wes McKinney, lorsqu'il a construit `Pandas`
pour proposer un _dataframe_ s'appuyant, en arrière-plan, sur la librairie
de calcul numérique `Numpy`, a permis un grand bond en avant pour `Python`
dans l'analyse de données qui explique sa popularité dans l'écosystème
de la _data science_. `Pandas` n'est pas sans limite[^tidyverse], nous aurons l'occasion
d'en évoquer quelques unes, mais la grande richesse des méthodes d'analyses
proposées simplifie énormément le travail d'analyse de données.
Pour en savoir plus sur ce _package_, l'ouvrage
de référence de @mckinney2012python présente de nombreuses fonctionnalités du _package_.

[^tidyverse]: L'écosystème équivalent en `R`, le [`tidyverse`](https://www.tidyverse.org/), développé
par _Posit_, est de conception plus récente que `Pandas`. Sa philosophie
a ainsi pu s'inspirer de celle de `Pandas` tout en pouvant remédier à quelques limites 
de la syntaxe `Pandas`. Les deux syntaxes étant une mise en oeuvre en `Python` ou `R` 
de la philosophie `SQL`, il est naturel qu'elles se ressemblent beaucoup et
qu'il soit pertinent pour les _data scientists_ de connaître les deux langages.


Nous nous concentrerons dans ce chapitre sur les éléments les plus pertinents
dans le cadre d'une introduction à la _data science_ et laisserons
les utilisateurs intéressés approfondir leurs connaissances
dans les ressources foisonnantes qu'il existe sur le sujet. 

Comme les jeux de données prennent généralement de la valeur
en associant plusieurs sources, par exemple pour mettre en relation
un enregistrement avec une donnée contextuelle ou pour lier deux bases
clients afin d'avoir une donnée faisant sens, le chapitre suivant présentera
la manière d'associer des jeux de données
différents avec `Pandas`. 
A l'issue du chapitre suivant, grâce à des croisements
de données, nous diposerons d'une base fine sur les empreintes
carbone des Français[^empreinte]. 

[^empreinte]: A vrai dire, ce n'est pas l'empreinte carbone
mais l'__inventaire national__
puisque la base de données correspond à une vision production,
pas consommation. Les émissions faites dans une commune pour satisfaire
la consommation d'une autre seront imputées à la première là 
où le concept d'empreinte carbone voudrait qu'on l'impute
aux secondes. De plus, les émissions présentées ici ne comportent pas
les émissions produites par des biais produits à l'étranger. Il ne s'agit pas, avec cet exercice, de construire une 
statistique fiable mais plutôt de comprendre la logique de 
l'association de données pour construire des statistiques descriptives. 

:::

::: {.content-visible when-profile="en"}

The `Pandas` package has been the central piece of the data science ecosystem for about a decade. The _DataFrame_, a central object in languages like `R` or `Stata`, had long been absent in the `Python` ecosystem. Yet, thanks to `Numpy`, all the basic components were present but needed to be reconfigured to meet the needs of data scientists.

Wes McKinney, when he built `Pandas` to provide a dataframe leveraging the numerical computation library `Numpy` in the background, enabled a significant leap forward for `Python` in data analysis, explaining its popularity in the data science ecosystem. `Pandas` is not without limitations[^tidyverse], which we will have the opportunity to discuss, but the vast array of analysis methods it offers greatly simplifies data analysis work. For more information on this package, the reference book by @mckinney2012python presents many of the package's features.

[^tidyverse]: The equivalent ecosystem in `R`, the [`tidyverse`](https://www.tidyverse.org/), developed by _Posit_, is of more recent design than `Pandas`. Its philosophy could thus draw inspiration from `Pandas` while addressing some limitations of the `Pandas` syntax. Since both syntaxes are an implementation in `Python` or `R` of the `SQL` philosophy, it is natural that they resemble each other and that it is pertinent for data scientists to know both languages.

In this chapter, we will focus on the most relevant elements in the context of an introduction to data science, leaving interested users to deepen their knowledge with the abundant resources available on the subject.

As datasets generally gain value by associating multiple sources, for example, to relate a record to contextual data or to link two client databases to obtain meaningful data, the next chapter will present how to merge different datasets with `Pandas`. By the end of the next chapter, thanks to data merging, we will have a detailed database on the carbon footprints of the French[^empreinte].

[^empreinte]: Actually, it is not the carbon footprint but the __national inventory__ since the database corresponds to a production view, not consumption. Emissions made in one municipality to satisfy the consumption of another will be attributed to the former where the carbon footprint concept would attribute it to the latter. Moreover, the emissions presented here do not include those produced by goods made abroad. This exercise is not about constructing a reliable statistic but rather understanding the logic of data merging to construct descriptive statistics.

:::


::: {.content-visible when-profile="fr"}

## Données

Dans ce tutoriel `Pandas`, nous allons utiliser :

* Les émissions de gaz à effet de serre estimées au niveau communal par l'ADEME. Le jeu de données est 
disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)
et requêtable directement dans `Python` avec
[cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) ;

Le [chapitre suivant](/content/manipulation/02_pandas_suite.qmd) permettra de mettre en application des éléments présents dans ce chapitre avec
les données ci-dessus associées à des données de contexte au niveau communal.

:::


::: {.content-visible when-profile="en"}
## Data used in this chapter

In this `Pandas` tutorial, we will use:

* Greenhouse gas emissions estimated at the municipal level by ADEME. The dataset is available on [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_) and can be queried directly in `Python` with [this URL](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert).

The [next chapter](/content/manipulation/02_pandas_suite.qmd) will allow us to apply the elements presented in this chapter with the above data combined with contextual data at the municipal level.
:::

::: {.content-visible when-profile="fr"}

## Environnement

Nous suivrons les conventions habituelles dans l'import des packages :

:::

::: {.content-visible when-profile="en"}
## Environment

We will follow the usual conventions in importing packages:
:::


```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

{{< include "02_pandas_intro/reproducible.qmd" >}}

::: {.content-visible when-profile="fr"}

Au cours de cette démonstration des principales fonctionalités de `Pandas`, et
lors du chapitre suivant,
je recommande de se référer régulièrement aux ressources suivantes : 

* L'[aide officielle de `Pandas`](https://pandas.pydata.org/docs/user_guide/index.html).
Notamment, la
[page de comparaison des langages](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/index.html)
qui est très utile ;
* [Ce tutoriel](https://observablehq.com/@observablehq/data-wrangling-translations),
pensé certes pour les utilisateurs d'[`Observable Javascript`](https://observablehq.com),
mais qui offre de nombreux exemples intéressants pour les afficionados de `Pandas` ;
* La _cheatsheet suivante_, [issue de ce post](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463)

Pour rappel, afin d'exécuter les exemples de code dans un _notebook_ interactif, vous pouvez utiliser les raccourcis en haut de la page pour lancer celui-ci dans votre environnement de prédilection.

:::

::: {.content-visible when-profile="en"}
Throughout this demonstration of the main `Pandas` functionalities, and in the next chapter, I recommend regularly referring to the following resources:

* The [official `Pandas` documentation](https://pandas.pydata.org/docs/user_guide/index.html), especially the [language comparison page](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/index.html), which is very useful;
* [This tutorial](https://observablehq.com/@observablehq/data-wrangling-translations), designed for users of [`Observable Javascript`](https://observablehq.com), but offering many interesting examples for `Pandas` aficionados;
* The following [cheatsheet from this post](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463).

As a reminder, to execute the code examples in an interactive notebook, you can use the shortcuts at the top of the page to launch it in your preferred environment.
:::

::: {.content-visible when-profile="fr"}
# Logique de `Pandas`

## Anatomie d'une table `Pandas`

L'objet central dans la logique `Pandas` est le `DataFrame`.
Il s'agit d'une structure particulière de données
à deux dimensions, structurées en alignant des lignes et colonnes.
Contrairement à une matrice, les colonnes
peuvent être de types différents.

Un `DataFrame` est composé des éléments suivants :

* l'indice de la ligne ;
* le nom de la colonne ;
* la valeur de la donnée ;

![Structuration d'un _DataFrame_ `Pandas`,
empruntée à <https://x.com/epfl_exts/status/997506000600084480>](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/pandasDF.png)

:::

::: {.content-visible when-profile="en"}
# `Pandas` logic

## Anatomy of a `Pandas` table

The central object in the `Pandas` logic is the `DataFrame`. It is a special data structure with two dimensions, structured by aligning rows and columns. Unlike a matrix, columns can be of different types.

A `DataFrame` consists of the following elements:

* the row index;
* the column name;
* the data value;

![Structure of a `Pandas` _DataFrame_, borrowed from <https://x.com/epfl_exts/status/997506000600084480>](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/pandasDF.png)
:::


::: {.content-visible when-profile="fr"}

## Avant le `DataFrame`, la `Serie`

En fait, un _DataFrame_ est une collection d'objets appelés `pandas.Series`.
Ces `Series` sont des objets d'une dimension qui sont des extensions des
array-unidimensionnels `Numpy`[^numpyarrow]. En particulier, pour faciliter le traitement
de données catégorielles ou temporelles, des types de variables
supplémentaires sont disponibles dans `Pandas` par rapport à
`Numpy` (`categorical`, `datetime64` et `timedelta64`). Ces
types sont associés à des méthodes optimisées pour faciliter le traitement
de ces données.

[^numpyarrow]: L'objectif originel de `Pandas` est de fournir une librairie haut-niveau vers des couches basses plus abstraites que sont les _array_ `Numpy`. `Pandas` est progressivement en train de changer ces couches basses pour privilégier `Arrow` à `Numpy`
sans déstabiliser les commandes haut-niveau auxquelles les utilisateurs de `Pandas` sont habitués. Ce changement s'explique par le fait qu'`Arrow`, une librairie bas niveau de calcul, est plus puissante et plus flexible que `Numpy`. Cette dernière, par exemple, propose des types textuels limités là où `Arrow` offre une plus grande liberté.  

Il existe plusieurs types possibles pour un `pandas.Series`, extension des types de données de base en `Python` qui détermineront ensuite le comportement de cette variable. En effet, de nombreuses opérations n'ont pas le même sens selon qu'on a une valeur numérique ou non.

Les types les plus simples (`int` ou `float`) correspondent aux valeurs numériques:

:::

::: {.content-visible when-profile="en"}
## Before seeing `DataFrame`, we need to know `Pandas` `Series`

In fact, a _DataFrame_ is a collection of objects called `pandas.Series`. These `Series` are one-dimensional objects that are extensions of the one-dimensional `Numpy` arrays[^numpyarrow]. In particular, to facilitate the handling of categorical or temporal data, additional variable types are available in `Pandas` compared to `Numpy` (`categorical`, `datetime64`, and `timedelta64`). These types are associated with optimized methods to facilitate the processing of this data.

[^numpyarrow]: The original goal of `Pandas` is to provide a high-level library for more abstract low-level layers, such as `Numpy` arrays. `Pandas` is gradually changing these low-level layers to favor `Arrow` over `Numpy` without destabilizing the high-level commands familiar to `Pandas` users. This shift is due to the fact that `Arrow`, a low-level computation library, is more powerful and flexible than `Numpy`. For example, `Numpy` offers limited textual types, whereas `Arrow` provides greater freedom.

There are several possible types for a `pandas.Series`, extending the basic data types in `Python`, which will determine the behavior of this variable. Indeed, many operations do not have the same meaning depending on whether the value is numeric or not.

The simplest types (`int` or `float`) correspond to numeric values:
:::


```{python}
#| echo: true
poids = pd.Series(
    [3, 7, 12]
)
poids
```

::: {.content-visible when-profile="fr"}

:::: {.important}
## Attention aux valeurs manquantes !

De manière générale, si `Pandas` détecte exclusivement des valeurs entières dans une variable, il utilisera le type `int` pour optimiser la mémoire. Ce choix fait sens. Il a néanmoins un inconvénient: `Numpy`, et donc par extension `Pandas`, ne sait se représenter des valeurs manquantes pour le type `int` (plus d'éléments sur les valeurs manquantes ci-dessous). 

En attendant le changement de couche basse en faveur d'`Arrow`, qui lui sait gérer les valeurs manquantes dans les `int`, la méthode à mettre en oeuvre est de convertir au type `float` si la variable sera amenée à avoir des valeurs manquantes, ce qui est assez simple:

::::

Pour des données textuelles, c'est tout aussi simple:

:::

::: {.content-visible when-profile="en"}
:::: {.important}
## Missing Values are tricky !

In general, if `Pandas` exclusively detects integer values in a variable, it will use the `int` type to optimize memory. This choice makes sense. However, it has a drawback: `Numpy`, and therefore by extension `Pandas`, cannot represent missing values for the `int` type (more on missing values below).

Pending the shift to `Arrow`, which can handle missing values in `int`, the method to use is to convert to the `float` type if the variable will have missing values, which is quite simple:

::::

For textual data, it is just as simple:

:::

```{python}
#| echo: true
animal = pd.Series(
  ['chat', 'chien', 'koala']
)
animal
```

::: {.content-visible when-profile="fr"}
Le type `object` correspond est une voiture-balais pour les types de données exclusivement textuelles (type `str`) ou mélangeant données textuelles et numériques (type `mixed`). Historiquement, c'était un type intermédiaire entre le `factor` et le `character` de `R`. Cependant, depuis quelques temps, il existe un type équivalent au `factor` de `R` dans `Pandas` pour
les variables dont le nombre de valeurs
est une liste finie et relativement courte, le type `category`. Le type `object` pouvant provoquer des erreurs inattendues par sa nature mixte, il est recommandé, lorsqu'on désire se concentrer sur une variable, de faire un choix sur la nature de celle-ci et de la convertir:
:::

::: {.content-visible when-profile="en"}
The `object` type is a catch-all for exclusively textual data types (type `str`) or a mix of textual and numerical data (type `mixed`). Historically, it was an intermediate type between the `factor` and `character` of `R`. However, recently, there is an equivalent type to `factor` in `Pandas` for variables with a finite and relatively short list of values, the `category` type. The `object` type can cause unexpected errors due to its mixed nature, so it is recommended to choose the nature of a variable and convert it:
:::


```{python}
#| echo: true
animal.astype("category") #<1>
animal.astype(str) #<2>
```
1. Pour convertir en `category` (le choix qui fait sens ici)
2. Pour convertir en `str` (si on désire faire des opérations textuelles ultérieures)

::: {.content-visible when-profile="fr"}
Il faut bien examiner les types de ses objets `Pandas` et les convertir s'ils ne font pas sens ; `Pandas` fait certes des choix optimisés mais il est parfois nécessaire de les corriger car `Pandas` ne connaît pas vos usages ultérieurs des données. C'est l'une des opérations à faire lors
du _feature engineering_, ensemble des étapes de préparation des données pour un usage ultérieur.  
:::
::: {.content-visible when-profile="en"}
It is important to examine the types of your `Pandas` objects and convert them if they do not make sense; `Pandas` makes optimized choices, but it may be necessary to correct them because `Pandas` does not know your future data usage. This is one of the tasks to do during _feature engineering_, the set of steps for preparing data for future use.
:::

::: {.content-visible when-profile="fr"}
# De la `Serie` au `DataFrame`

Nous avons crée deux séries indépendantes, `animal`
et `poids` qui sont pourtant reliées. Dans le monde matriciel, cela correspondrait à passer du vecteur à la matrice. Dans le monde de `Pandas`, cela veut dire passer de la `Serie` au `DataFrame`.

Cela se fait naturellement avec `Pandas`:
:::

::: {.content-visible when-profile="en"}
# From `Series` to `DataFrame`

We have created two independent series, `animal` and `poids`, which are related. In the matrix world, this would correspond to moving from a vector to a matrix. In the `Pandas` world, this means moving from a `Series` to a `DataFrame`.

This is done naturally with `Pandas`:
:::

```{python}
#| echo: true
animaux = pd.DataFrame(
  zip(animal, poids), #<1>
  columns = ['animal','poids']
)
animaux
```
::: {.content-visible when-profile="fr"}
1. On doit utiliser `zip` ici car `Pandas` attend une structure du type `{"var1": [val1, val2], "var2": [val1, val2]}` qui n'est pas celle que nous avons préparé précédemment. Nous verrons néanmoins que cette approche n'est pas la plus usuelle pour créer un `DataFrame`
:::
::: {.content-visible when-profile="en"}
1. We need to use `zip` here because `Pandas` expects a structure like `{"var1": [val1, val2], "var2": [val1, val2]}`, which is not what we prepared previously. However, we will see that this approach is not the most common for creating a `DataFrame`.
:::

::: {.content-visible when-profile="fr"}

## L'indexation

La différence essentielle entre une `Series` et un objet `Numpy` est l'indexation.
Dans `Numpy`,
l'indexation est implicite ; elle permet d'accéder à une donnée (celle à
l'index situé à la position *i*).
Avec une `Series`, on peut bien sûr utiliser un indice de position mais on peut
surtout faire appel à des indices plus explicites.

Ceci permet d'accéder à la donnée de manière plus naturelle, en utilisant les noms de colonne par exemple:
:::

::: {.content-visible when-profile="en"}
The essential difference between a `Series` and a `Numpy` object is indexing. In `Numpy`, indexing is implicit; it allows accessing data (the one at the index located at position *i*). With a `Series`, you can, of course, use a positional index, but more importantly, you can use more explicit indices.

This allows accessing data more naturally, using column names, for example:
:::

```{python}
#| echo: true
animaux['poids']
```

::: {.content-visible when-profile="fr"}
L'existence d'indice rend le *subsetting*, c'est-à-dire
la sélection de lignes ou de colonnes, particulièrement aisé. Les _DataFrames_ pendant présentent deux indices: ceux des lignes et ceux des colonnes. On pourra faire des sélections sur ces deux dimensions. 
En anticipant sur les exercices
ultérieurs, on peut voir que cela va nous faciliter la sélection de ligne:
:::

::: {.content-visible when-profile="en"}
The existence of an index makes subsetting, that is, selecting rows or columns, particularly easy. DataFrames have two indices: those for rows and those for columns. You can make selections on both dimensions. Anticipating later exercises, you can see that this will facilitate row selection:
:::


```{python}
#| echo: true
animaux.loc[animaux['animal'] == "chat", 'poids']
```

::: {.content-visible when-profile="fr"}
Cette instruction est équivalente à la commande `SQL`:

```sql
SELECT poids FROM animaux WHERE animal == "chat"
```

Si on revient sur notre jeu de données `animaux`,
on peut voir sur la gauche l'affichage du numéro de la ligne:

```{python}
#| echo: true
animaux
```

Il s'agit de l'indice par défaut pour la dimension ligne car nous n'en n'avons pas configuré un. Ce n'est pas obligatoire, il est tout à fait possible d'avoir un indice correspondant à une variable d'intérêt (nous découvrirons cela lorsque nous explorerons `groupby` dans le prochain chapitre). Cependant, cela peut être piégeux et il est recommandé que cela ne soit que transitoire, d'où l'intérêt de faire régulièrement des `reset_index`. 

:::

::: {.content-visible when-profile="en"}
This instruction is equivalent to the SQL command:

```sql
SELECT poids FROM animaux WHERE animal == "chat"
```

If we return to our `animaux` dataset, we can see the row number displayed on the left:

```{python}
#| echo: true
animaux
```

This is the default index for the row dimension because we did not configure one. It is not mandatory; it is quite possible to have an index corresponding to a variable of interest (we will discover this when we explore `groupby` in the next chapter). However, this can be tricky, and it is recommended that this be only transitory, hence the importance of regularly performing `reset_index`.
:::

::: {.content-visible when-profile="fr"}

## Le concept de _tidy data_

Le concept de __*tidy*__ data, popularisé par Hadley Wickham via ses packages `R` (voir @wickham2023r),
est parfaitement pertinent pour décrire la structure d'un _DataFrame_ `Pandas`.
Les trois règles des données _tidy_ sont les suivantes :

* Chaque __variable__ possède sa propre colonne ;
* Chaque __observation__ possède sa propre ligne ;
* Une __valeur__, matérialisant une observation d'une variable,
se trouve sur une unique cellule.


![Illustration du concept de _tidy data_ (emprunté à H. Wickham)](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

Ces principes peuvent vous apparaître de bon sens mais vous découvrirez que de nombreux formats de données ne correspondent pas à ce principe. Par exemple, des tableurs Excel proposent régulièrement des valeurs à cheval sur plusieurs colonnes ou plusieurs lignes fusionnées. Restructurer cette donnée selon le principe des _tidy data_ sera un enjeu pour être en mesure d'effectuer des traitements sur celle-ci.  

:::

::: {.content-visible when-profile="en"}
## The concept of _tidy data_

The concept of __*tidy*__ data, popularized by Hadley Wickham through his `R` packages (see @wickham2023r), is highly relevant for describing the structure of a `Pandas` DataFrame. The three rules of tidy data are as follows:

* Each __variable__ has its own column;
* Each __observation__ has its own row;
* A __value__, representing an observation of a variable, is located in a single cell.

![Illustration of the _tidy data_ concept (borrowed from H. Wickham)](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

These principles may seem like common sense, but you will find that many data formats do not adhere to them. For example, Excel spreadsheets often have values spanning multiple columns or several merged rows. Restructuring this data according to the tidy data principle will be crucial to performing analysis on it.
:::

::: {.content-visible when-profile="fr"}

# Importer des données avec `Pandas`

S'il fallait créer à la main tous ses _DataFrames_
à partir de vecteurs, `Pandas` ne serait pas pratique. `Pandas` propose de nombreuses fonctions pour lire
des données stockées dans des formats différents. 

Les données les plus simples à lire sont les données tabulaires stockées dans un format adéquat. Les deux principaux formats à connaître sont le `CSV` et le format `Parquet`. Le premier présente l'avantage de la simplicité - il est universel, connu de tous et est lisible par n'importe quel éditeur de texte. Le second gagne en popularité dans l'écosystème de la donnée, car il règle certaines limites du CSV (stockage optimisé, types des variables prédéfénis...) mais présente l'inconvénient de ne pas être lisible sans un outil adéquat (qui heureusement sont de plus en plus présents dans les éditeurs de code standard comme `VSCode`). Pour en savoir plus sur la différence entre `CSV` et `Parquet`, se reporter aux [chapitre d'approfondissement](/content/modern-ds/s3.qmd) sur le sujet.

Les donées stockées dans les autres formats textes dérivés du `CSV` (`.txt`, `.tsv`...) ou les formats type `JSON` sont lisibles avec `Pandas` mais il faut parfois itérer pour trouver les bons paramètres de lecture. Nous découvrirons dans d'autres chapitres que d'autres formats de données liés à des structures de données différentes sont tout aussi lisibles avec `Python`. 

Les formats plats (`.csv`, `.txt`...) et le format `Parquet` sont simples à utiliser avec `Pandas` parce que d'une part ils ne sont pas propriétaires et d'autre part ils stockent la donnée sous
une forme _tidy_. Les données issues de tableurs, `Excel` ou `LibreOffice`, sont plus ou moins compliquées
à importer selon qu'elles suivent ce schéma ou non. 
Cela vient du fait que ces outils sont utilisés à tord et à travers. Alors que dans le monde de la _data science_ ceux-ci devraient servir principalement à diffuser des tableaux finaux pour du _reporting_, ils servent souvent à diffuser une donnée brute qui pourrait l'être par des canaux plus adaptés. 
L'une des principales difficultés liées aux tableurs vient du fait que les données sont généralement associées à de la documentation dans le même onglet - par exemple le tableur comporte quelques lignes décrivant les sources avant le tableau - ce qui nécessite de l'intelligence humaine pour assister `Pandas` lors de la phase d'import. Celle-ci sera toujours possible mais lorsqu'il existe une alternative sous forme de fichier plat, il n'y a pas d'hésitation à avoir. 
:::

::: {.content-visible when-profile="en"}
# Importing Data with `Pandas`

If you had to manually create all your DataFrames from vectors, `Pandas` would not be practical. `Pandas` offers many functions to read data stored in different formats.

The simplest data to read are tabular data stored in an appropriate format. The two main formats to know are `CSV` and `Parquet`. The former has the advantage of simplicity - it is universal, known to all, and readable by any text editor. The latter is gaining popularity in the data ecosystem because it addresses some limitations of CSV (optimized storage, predefined variable types...) but has the disadvantage of not being readable without a suitable tool (which fortunately are increasingly present in standard code editors like `VSCode`). For more information on the difference between `CSV` and `Parquet`, refer to the [deep dive chapters](/content/modern-ds/s3.qmd) on the subject.

Data stored in other text-derived formats from `CSV` (`.txt`, `.tsv`...) or formats like `JSON` are readable with `Pandas`, but sometimes it takes a bit of iteration to find the correct reading parameters. In other chapters, we will discover that other data formats related to different data structures are also readable with `Python`.

Flat formats (`.csv`, `.txt`...) and the `Parquet` format are easy to use with `Pandas` because they are not proprietary and they store data in a tidy form. Data from spreadsheets, `Excel` or `LibreOffice`, are more or less complicated to import depending on whether they follow this schema or not. This is because these tools are used indiscriminately. While in the data science world, they should primarily be used to disseminate final tables for reporting, they are often used to disseminate raw data that could be better shared through more suitable channels.

One of the main difficulties with spreadsheets is that the data are usually associated with documentation in the same tab - for example, the spreadsheet has some lines describing the sources before the table - which requires human intelligence to assist `Pandas` during the import phase. This will always be possible, but when there is an alternative in the form of a flat file, there should be no hesitation.
:::

::: {.content-visible when-profile="fr"}

## Lire des données depuis un chemin local

Cet exercice vise à présenter l'intérêt d'utiliser un chemin relatif
plutôt qu'un chemin absolu pour favoriser la reproductibilité du code. Nous préconiserons néanmoins par la suite de privilégier directement la lecture depuis internet lorsqu'elle est possible et n'implique pas le téléchargement récurrent d'un fichier volumineux.

Pour préparer cet exercice, le code suivant permettra de
télécharger des données qu'on va écrire en local

{{< include "02_pandas_intro/_exo0_preliminary.qmd" >}}
{{< include "02_pandas_intro/_exo0.qmd" >}}
{{< include "02_pandas_intro/_exo0_solution.qmd" >}}

Le principal problème de la lecture depuis des
fichiers stockés en local est le risque de 
se rendre adhérant à un système de fichier
qui n'est pas forcément partagé. Il vaut mieux,
lorsque c'est possible, directement lire la donnée
avec un lien `HTTPS`, ce que `Pandas` sait faire.
De plus, lorsqu'on travaille sur de l'_open data_ cela assure qu'on utilise la dernière donnée
disponible et non une duplication en local qui peut ne pas être à jour. 

:::

::: {.content-visible when-profile="en"}
## Reading Data from a Local Path

This exercise aims to demonstrate the benefit of using a relative path rather than an absolute path to enhance code reproducibility. However, we will later recommend reading directly from the internet when possible and when it does not involve the recurrent download of a large file.

To prepare for this exercise, the following code will allow you to download data and write it locally:

{{< include "02_pandas_intro/_exo0_preliminary.qmd" >}}
{{< include "02_pandas_intro/_exo0.qmd" >}}
{{< include "02_pandas_intro/_exo0_solution.qmd" >}}


The main issue with reading from files stored locally is the risk of becoming dependent on a file system that is not necessarily shared. It is better, whenever possible, to directly read the data with an `HTTPS` link, which `Pandas` can handle. Moreover, when working with open data, this ensures that you are using the latest available data and not a local duplication that may not be up to date.
:::


::: {.content-visible when-profile="fr"}

## Lecture depuis un CSV disponible sur internet

L'URL d'accès aux données peut être conservé dans une variable _ad hoc_ :

```{python}
#| echo: true
url = "https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert"
```

L'objectif du prochain exercice est de se familiariser à l'import et l'affichage de données
avec `Pandas` et à l'affichage de quelques observations. 

:::

::: {.content-visible when-profile="en"}
## Reading from a CSV Available on the Internet

The URL to access the data can be stored in an ad hoc variable:

```{python}
#| echo: true
url = "https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert"
```

The goal of the next exercise is to get familiar with importing and displaying data using `Pandas` and displaying a few observations.

:::

{{< include "02_pandas_intro/_exo1.qmd" >}}
{{< include "02_pandas_intro/_exo1_solution.qmd" >}}

::: {.content-visible when-profile="fr"}

Comme l'illustre cet exercice, l'affichage des _DataFrames_ dans les _notebooks_ est assez ergonomique.
Les premières et dernières lignes s'affichent
automatiquement. Pour des tables de valorisation présentes dans un
rapport ou un article de recherche, le chapitre suivant
présente `great_tables` qui offre de très riches
fonctionnalités de mise en forme des tableaux. 

:::: {.warning}

Il faut faire attention au `display` et aux
commandes qui révèlent des données (`head`, `tail`, etc.)
dans un _notebook_ qui exploite
des données confidentielles lorsqu'on utilise 
le logiciel de contrôle de version `Git` (cf. chapitres dédiés).

En effet, on peut se
retrouver à partager des données, involontairement, dans l'historique
`Git`. Comme cela sera expliqué dans le chapitre dédié à `Git`,
un fichier, nommé le `.gitignore`, suffit pour créer quelques règles
évitant le partage involontaire de données avec `Git`. 

::::

:::

::: {.content-visible when-profile="en"}

As illustrated by this exercise, displaying DataFrames in notebooks is quite ergonomic. The first and last rows are displayed automatically. For valuation tables present in a report or research article, the next chapter introduces `great_tables`, which offers very rich table formatting features.

:::: {.warning}

Be careful with `display` and commands that reveal data (`head`, `tail`, etc.) in a notebook that handles confidential data when using version control software like `Git` (see dedicated chapters).

Indeed, you may end up sharing data inadvertently in the `Git` history. As explained in the chapter dedicated to `Git`, a file named `.gitignore` is sufficient to create some rules to avoid unintentional sharing of data with `Git`.

::::

:::

::: {.content-visible when-profile="fr"}

# Explorer la structure d'un _DataFrame_

`Pandas` propose donc un schéma de données assez familier aux utilisateurs
de logiciels statistiques comme `R`. A l'instar des
principaux paradigmes de traitement de la données comme le _tidyverse_ (`R`),
la grammaire de `Pandas` est héritière de la logique `SQL`.
La philosophie est très proche : on effectue des opérations
de sélection de ligne, de colonne, des tris de ligne en fonction
de valeurs de certaines colonnes, des traitements standardisés sur des
variables, etc. De manière générale, on privilégie les traitements
faisant appel à des noms de variables à des numéros de ligne ou de
colonne. 

Que vous soyez familiers de `SQL` ou de `R`, vous retrouverez une logique
similaire à celle que vous connaissez quoique les noms puissent diverger:
`df.loc[df['y']=='b']` s'écrira peut-être `df %>% filter(y=='b')` (`R`)
ou `SELECT * FROM df WHERE y == 'b'` (`SQL`) mais la logique
est la même.

`Pandas` propose énormément de fonctionnalités pré-implémentées. 
Il est vivement recommandé, avant de se lancer dans l'écriture d'une
fonction, de se poser la question de son implémentation native dans `Numpy`, `Pandas`, etc.
La plupart du temps, s'il existe une solution implémentée dans une librairie, il convient
de l'utiliser car elle sera plus efficace que celle que vous mettrez en oeuvre.

Pour présenter les méthodes les plus pratiques pour l'analyse de données,
on peut partir de l'exemple des consommations de CO2 communales issues
des données de l'Ademe auquel les exercices précédents
étaient dédiés. 

:::

::: {.content-visible when-profile="en"}
# Exploring the Structure of a DataFrame

`Pandas` offers a data schema quite familiar to users of statistical software like `R`. Similar to the main data processing paradigms like the _tidyverse_ (`R`), the grammar of `Pandas` inherits from `SQL` logic. The philosophy is very similar: operations are performed to select rows, columns, sort rows based on column values, apply standardized treatments to variables, etc. Generally, operations that reference variable names are preferred over those that reference row or column numbers.

Whether you are familiar with `SQL` or `R`, you will find a similar logic to what you know, although the names might differ: `df.loc[df['y']=='b']` may be written as `df %>% filter(y=='b')` (`R`) or `SELECT * FROM df WHERE y == 'b'` (`SQL`), but the logic is the same.

`Pandas` offers a plethora of pre-implemented functionalities. It is highly recommended, before writing a function, to consider if it is natively implemented in `Numpy`, `Pandas`, etc. Most of the time, if a solution is implemented in a library, it should be used as it will be more efficient than what you would implement.

To present the most practical methods for data analysis, we can use the example of the municipal CO2 consumption data from Ademe, which was the focus of the previous exercises.

:::


```{python}
#| echo: true
df = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
df
```

::: {.content-visible when-profile="fr"}

## Dimensions et structure d'un _DataFrame_

Les premières méthodes utiles permettent d'afficher quelques
attributs d'un `DataFrame`.

::: 

::: {.content-visible when-profile="en"}
## Dimensions and Structure of a DataFrame

The first useful methods allow displaying some attributes of a `DataFrame`.
:::


```{python}
#| echo: true
df.axes
```

```{python}
#| echo: true
df.columns
```

```{python}
#| echo: true
df.index
```

::: {.content-visible when-profile="fr"}

Pour connaître les dimensions d'un DataFrame, on peut utiliser quelques méthodes
pratiques :

:::

::: {.content-visible when-profile="en"}
## Dimensions and Structure of a DataFrame

The first useful methods allow displaying some attributes of a `DataFrame`.
:::

```{python}
#| echo: true
df.ndim
```

```{python}
#| echo: true
df.shape
```

```{python}
#| echo: true
df.size
```

::: {.content-visible when-profile="fr"}

Pour déterminer le nombre de valeurs uniques d'une variable, plutôt que chercher à écrire soi-même une fonction,
on utilise la
méthode `nunique`. Par exemple,

```{python}
#| echo: true
df['Commune'].nunique()
```

`Pandas` propose énormément de méthodes utiles. 
Voici un premier résumé de celles relatives à la structure des données, accompagné d'un comparatif avec `R` :

| Opération                     | pandas       | dplyr (`R`)    | data.table (`R`)           |
|-------------------------------|--------------|----------------|----------------------------|
| Récupérer le nom des colonnes | `df.columns` | `colnames(df)` | `colnames(df)`             |
| Récupérer les dimensions      | `df.shape` | `dim(df)` | `dim(df)` |
| Récupérer le nombre de valeurs uniques d'une variable | `df['myvar'].nunique()` | `df %>%  summarise(distinct(myvar))` | `df[,uniqueN(myvar)]` |

:::

::: {.content-visible when-profile="en"}
To know the dimensions of a DataFrame, some practical methods can be used:

```{python}
#| echo: true
df.ndim
```

```{python}
#| echo: true
df.shape
```

```{python}
#| echo: true
df.size
```

To determine the number of unique values of a variable, rather than writing a function yourself, use the `nunique` method. For example,

```{python}
#| echo: true
df['Commune'].nunique()
```

`Pandas` offers many useful methods. Here is a summary of those related to data structure, accompanied by a comparison with `R`:

| Operation                     | pandas                  | dplyr (`R`)           | data.table (`R`)          |
|-------------------------------|-------------------------|-----------------------|---------------------------|
| Retrieve column names         | `df.columns`            | `colnames(df)`        | `colnames(df)`            |
| Retrieve dimensions           | `df.shape`              | `dim(df)`             | `dim(df)`                 |
| Retrieve unique values of a variable | `df['myvar'].nunique()` | `df %>%  summarise(distinct(myvar))` | `df[,uniqueN(myvar)]` |
:::

::: {.content-visible when-profile="fr"}

## Accéder à des éléments d'un DataFrame

En SQL, effectuer des opérations sur les colonnes se fait avec la commande
`SELECT`. Avec `Pandas`,
pour accéder à une colonne dans son ensemble on peut
utiliser plusieurs approches :

* `dataframe.variable`, par exemple `df.Energie`.
Cette méthode requiert néanmoins d'avoir des
noms de colonnes sans espace ou caractères spéciaux, ce qui exclut
souvent des jeux de données réels. Elle n'est pas recommandée. ;
* `dataframe[['variable']]` pour renvoyer la variable sous
forme de `DataFrame`. Cette méthode peut être assez piégeuse pour une variable seule, il vaut mieux lui privilégier `dataframe.loc[:,['variable']]` qui est plus explicite sur la nature de l'objet qu'on désire en sortie ;
* `dataframe['variable']` pour
renvoyer la variable sous forme de `Series`. Par exemple, `df[['Autres transports']]`
ou `df['Autres transports']`. C'est une manière préférable de procéder.

Pour récupérer plusieurs colonnes à la fois, il y a deux approches, la seconde étant préférable :

* `dataframe[['variable1', 'variable2']]` ;
* `dataframe.loc[:, ['variable1', 'variable2']]`

Cela est équivalent à `SELECT variable1, variable2 FROM dataframe` en SQL.  

Le `.loc` peut apparaître excessivement verbeux. Il permet néanmoins de s'assurer qu'on effectue bien un _subset_ sur la dimension des colonnes. Les _DataFrame_ ayant deux indices, ceux des lignes et des colonnes, on peut avoir parfois des surprises avec l'implicite, il est plus fiable d'être explicite.

:::

::: {.content-visible when-profile="en"}
## Accessing elements of a DataFrame

In SQL, performing operations on columns is done with the `SELECT` command. With `Pandas`, to access an entire column, several approaches can be used:

* `dataframe.variable`, for example `df.Energie`. This method requires column names without spaces or special characters, which excludes many real datasets. It is not recommended.
* `dataframe[['variable']]` to return the variable as a `DataFrame`. This method can be tricky for a single variable; it's better to use `dataframe.loc[:,['variable']]`, which is more explicit about the nature of the resulting object.
* `dataframe['variable']` to return the variable as a `Series`. For example, `df[['Autres transports']]` or `df['Autres transports']`. This is the preferred method.

To retrieve multiple columns at once, there are two approaches, with the second being preferable:

* `dataframe[['variable1', 'variable2']]`
* `dataframe.loc[:, ['variable1', 'variable2']]`

This is equivalent to `SELECT variable1, variable2 FROM dataframe` in SQL.

Using `.loc` may seem excessively verbose, but it ensures that you are performing a subset operation on the column dimension. DataFrames have two indices, for rows and columns, and implicit operations can sometimes cause surprises, so it's more reliable to be explicit.
:::

::: {.content-visible when-profile="fr"}

## Accéder à des lignes

Pour accéder à une ou plusieurs valeurs d'un `DataFrame`,
il existe deux manières conseillées de procéder, selon la
forme des indices de lignes ou colonnes utilisées :

* `df.iloc` : utilise les indices. C'est une méthode moyennement fiable car les indices d'un _DataFrame_ peuvent évoluer au cours d'un traitement (notamment lorsqu'on fait des opérations par groupe). 
* `df.loc` : utilise les labels. Cette méthode est recommandée. 

:::: {.warning}

Les bouts de code utilisant la structure `df.ix`
sont à bannir car la fonction est *deprecated* et peut
ainsi disparaître à tout moment.

::::

`iloc` va se référer à l'indexation de 0 à *N* où *N* est égal à `df.shape[0]` d'un
`pandas.DataFrame`. `loc` va se référer aux valeurs de l'index
de `df`.
Par exemple, avec le `pandas.DataFrame` `df_example`:

:::

::: {.content-visible when-profile="en"}
## Accessing rows

To access one or more values in a `DataFrame`, there are two recommended methods, depending on the form of the row or column indices used:

* `df.iloc`: uses indices. This method is somewhat unreliable because a DataFrame's indices can change during processing (especially when performing group operations).
* `df.loc`: uses labels. This method is recommended.

:::: {.warning}
Code snippets using the `df.ix` structure should be avoided as the function is deprecated and may disappear at any time.
::::

`iloc` refers to the indexing from 0 to *N*, where *N* equals `df.shape[0]` of a `pandas.DataFrame`. `loc` refers to the values of `df`'s index. For example, with the `pandas.DataFrame` `df_example`:

:::

```{python}
#| echo: true
df_example = pd.DataFrame(
    {'month': [1, 4, 7, 10], 'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})
df_example = df_example.set_index('month')
df_example
```

::: {.content-visible when-profile="fr"}

- `df_example.loc[1, :]` donnera la première ligne de `df` (ligne où l'indice `month` est égal à 1) ;
- `df_example.iloc[1, :]` donnera la deuxième ligne (puisque l'indexation en `Python` commence à 0) ;
- `df_example.iloc[:, 1]` donnera la deuxième colonne, suivant le même principe.

Les exercices ultérieurs permettront de pratiquer cette syntaxe sur notre jeu de données des émissions de gaz carbonique.

:::

::: {.content-visible when-profile="en"}
- `df_example.loc[1, :]` will return the first row of `df` (row where the `month` index equals 1);
- `df_example.iloc[1, :]` will return the second row (since Python indexing starts at 0);
- `df_example.iloc[:, 1]` will return the second column, following the same principle.

Later exercises will allow practicing this syntax on our dataset of carbon emissions.

:::

::: {.content-visible when-profile="fr"}

# Principales manipulations de données

Les opérations les plus fréquentes en `SQL` sont résumées par le tableau suivant.
Il est utile de les connaître (beaucoup de syntaxes de maniement de données
reprennent ces termes) car, d'une
manière ou d'une autre, elles couvrent la plupart
des usages de manipulation des données. Nous allons en décrire, par le suite, quelques unes:

| Opération | SQL | pandas | dplyr (`R`) | data.table (`R`) |
|-----|-----------|--------|-------------|------------------|
| Sélectionner des variables par leur nom | `SELECT` | `df[['Autres transports','Energie']]` | `df %>% select(Autres transports, Energie)` | `df[, c('Autres transports','Energie')]` |
| Sélectionner des observations selon une ou plusieurs conditions; | `FILTER` | `df[df['Agriculture']>2000]` | `df %>% filter(Agriculture>2000)` | `df[Agriculture>2000]` |
| Trier la table selon une ou plusieurs variables | `SORT BY` | `df.sort_values(['Commune','Agriculture'])` | `df %>% arrange(Commune, Agriculture)` | `df[order(Commune, Agriculture)]` |
| Ajouter des variables qui sont fonction d’autres variables; | `SELECT *, LOG(Agriculture) AS x FROM df` | `df['x'] = np.log(df['Agriculture'])`  |  `df %>% mutate(x = log(Agriculture))` | `df[,x := log(Agriculture)]` |
| Effectuer une opération par groupe | `GROUP BY` | `df.groupby('Commune').mean()` | `df %>% group_by(Commune) %>% summarise(m = mean)` | `df[,mean(Commune), by = Commune]` |
| Joindre deux bases de données (*inner join*) | `SELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x` | `table1.merge(table2, left_on = 'id', right_on = 'x')` | `table1 %>% inner_join(table2, by = c('id'='x'))` | `merge(table1, table2, by.x = 'id', by.y = 'x')` |

:::

::: {.content-visible when-profile="en"}
# Main data wrangling routine

The most frequent operations in `SQL` are summarized in the following table. It is useful to know them (many data manipulation syntaxes use these terms) because, one way or another, they cover most data manipulation uses. We will describe some of them later:

| Operation | SQL | pandas | dplyr (`R`) | data.table (`R`) |
|-----------|-----|--------|-------------|------------------|
| Select variables by name | `SELECT` | `df[['Autres transports','Energie']]` | `df %>% select(Autres transports, Energie)` | `df[, c('Autres transports','Energie')]` |
| Select observations based on one or more conditions | `FILTER` | `df[df['Agriculture']>2000]` | `df %>% filter(Agriculture>2000)` | `df[Agriculture>2000]` |
| Sort the table by one or more variables | `SORT BY` | `df.sort_values(['Commune','Agriculture'])` | `df %>% arrange(Commune, Agriculture)` | `df[order(Commune, Agriculture)]` |
| Add variables that are functions of other variables | `SELECT *, LOG(Agriculture) AS x FROM df` | `df['x'] = np.log(df['Agriculture'])` | `df %>% mutate(x = log(Agriculture))` | `df[,x := log(Agriculture)]` |
| Perform an operation by group | `GROUP BY` | `df.groupby('Commune').mean()` | `df %>% group_by(Commune) %>% summarise(m = mean)` | `df[,mean(Commune), by = Commune]` |
| Join two databases (inner join) | `SELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x` | `table1.merge(table2, left_on = 'id', right_on = 'x')` | `table1 %>% inner_join(table2, by = c('id'='x'))` | `merge(table1, table2, by.x = 'id', by.y = 'x')` |

:::

::: {.content-visible when-profile="fr"}

## Opérations sur les colonnes : ajouter ou retirer des variables, les renommer, etc. 

Sur le plan technique, les `DataFrames` `Pandas` sont des objets *mutables* en langage `Python`,
c'est-à-dire qu'il est possible de faire évoluer le `DataFrame` au grès
des opérations mises en oeuvre. 

L'opération la plus classique consiste à ajouter ou retirer des variables à la table de données.
La manière la plus simple d'opérer pour ajouter des colonnes est
d'utiliser la réassignation. Par exemple, pour créer une variable `dep` qui correspond aux deux premiers numéros du code commune (code Insee), il suffit de prendre la variable et lui appliquer le traitement adapté (en l'occurrence ne garder que ses deux premières caractères):

```{python}
#| echo: true
df['dep'] = df['INSEE commune'].str[:2]
df.head(3)
```

En SQL, la manière de procéder dépend du moteur d'exécution. En pseudo-code cela donne

```sql
SELECT everything(), SUBSTR("code_insee", 2) AS dep FROM df
```

Il est possible d'appliquer cette approche de création de colonnes sur plusieurs colonnes. Un des
intérêts de cette approche est qu'elle permet de recycler le nom de colonnes.

:::

::: {.content-visible when-profile="en"}
## Operations on Columns: Adding or Removing Variables, Renaming Them, etc.

Technically, `Pandas` DataFrames are *mutable* objects in the `Python` language, meaning it is possible to change the DataFrame as needed during processing.

The most classic operation is adding or removing variables to the data table. The simplest way to add columns is by reassignment. For example, to create a `dep` variable that corresponds to the first two digits of the commune code (INSEE code), simply take the variable and apply the appropriate treatment (in this case, keep only its first two characters):

```{python}
#| echo: true
df['dep'] = df['INSEE commune'].str[:2]
df.head(3)
```

In SQL, the method depends on the execution engine. In pseudo-code, it would be:

```sql
SELECT everything(), SUBSTR("code_insee", 2) AS dep FROM df
```

It is possible to apply this approach to creating columns on multiple columns. One of the advantages of this approach is that it allows recycling column names.

:::

```{python}
#| echo: true
vars = ['Agriculture', 'Déchets', 'Energie']

df[[v + "_log" for v in vars]] = np.log(df.loc[:, vars])
df.head(3)
```

::: {.content-visible when-profile="fr"}

La requête équivalente en SQL serait assez fastidieuse à écrire. Sur ce genre d'opérations, on voit bien l'intérêt d'avoir une librairie haut niveau comme `Pandas`. 

:::: {.warning}

Cela est possible grâce à la vectorisation native des opérations de `Numpy` et à la magie `Pandas` qui réarrange tout ceci. Ce n'est pas utilisable avec n'importe quelle fonction. Pour d'autres fonctions, il faudra utiliser `assign`, généralement par le biais de _lambda functions_, des fonctions temporaires faisant office de passe plat. Par exemple, pour créer une variable selon cette approche, il faudrait faire:

```{python}
#| echo: true
df.assign(
  Energie_log = lambda x: np.log(x['Energie'])
)
```

Avec des méthodes de `Pandas` ou de `Numpy` comme ici, cela n'a pas d'intérêt, c'est même contreproductif car cela ralentit le code.

::::

On peut facilement renommer des variables avec la méthode `rename` qui
fonctionne bien avec des dictionnaires. Pour renommer des colonnes il faut
préciser le paramètre `axis = 'columns'` ou `axis=1`. Le paramètre `axis` est souvent nécessaire car par défaut de nombreuses méthodes de `Pandas` supposent que l'indice sur lequel les opérations sont faites est l'indice des lignes :

:::

::: {.content-visible when-profile="en"}
The equivalent SQL query would be quite tedious to write. For such operations, the benefit of a high-level library like `Pandas` becomes clear.

::: {.warning}

This is possible thanks to the native vectorization of `Numpy` operations and the magic of `Pandas` that rearranges everything. This is not usable with just any function. For other functions, you will need to use `assign`, generally through lambda functions, temporary functions acting as pass-throughs. For example, to create a variable using this approach, you would do:

```{python}
#| echo: true
df.assign(
  Energie_log = lambda x: np.log(x['Energie'])
)
```

With methods from `Pandas` or `Numpy` like this, it is not beneficial and even counterproductive as it slows down the code.

:::

Variables can be easily renamed using the `rename` method, which works well with dictionaries. To rename columns, specify the parameter `axis = 'columns'` or `axis=1`. The `axis` parameter is often necessary because many `Pandas` methods assume by default that operations are done on the row index:

:::

```{python}
#| echo: true
df = df.rename({"Energie": "eneg", "Agriculture": "agr"}, axis=1)
df.head()
```

::: {.content-visible when-profile="fr"}
Enfin, pour effacer des colonnes, on utilise la méthode `drop` avec l'argument
`columns`:
:::
::: {.content-visible when-profile="en"}
Finally, to delete columns, use the `drop` method with the `columns` argument:
:::

```{python}
#| echo: true
df = df.drop(columns = ["eneg", "agr"])
```

::: {.content-visible when-profile="fr"}
## Réordonner les observations

La méthode `sort_values` permet de réordonner les observations d'un `DataFrame`, en laissant l'ordre des colonnes identiques. 

Par exemple,
si on désire classer par ordre décroissant de consommation de CO2 du secteur
résidentiel, on fera

```{python}
#| echo: true
df = df.sort_values("Résidentiel", ascending = False)
df.head(3)
```

Ainsi, en une ligne de code, on identifie les villes où le secteur
résidentiel consomme le plus. En SQL on ferait

```sql
SELECT * FROM df ORDER BY DESC "Résidentiel"
```
:::

::: {.content-visible when-profile="en"}
## Reordering observations

The `sort_values` method allows reordering observations in a `DataFrame`, keeping the column order the same.

For example, to sort in descending order of CO2 consumption in the residential sector, you would do:

```{python}
#| echo: true
df = df.sort_values("Résidentiel", ascending = False)
df.head(3)
```

Thus, in one line of code, you can identify the cities where the residential sector consumes the most. In SQL, you would do:

```sql
SELECT * FROM df ORDER BY "Résidentiel" DESC
```
:::

::: {.content-visible when-profile="fr"}
## Filter

L'opération de sélection de lignes s'appelle `FILTER` en SQL. Elle s'utilise
en fonction d'une condition logique (clause `WHERE`). On sélectionne les
données sur une condition logique. 

Il existe plusieurs méthodes en `Pandas`. La plus simple est d'utiliser les *boolean mask*, déjà vus dans le chapitre
[`numpy`](/content/manipulation/01_numpy.qmd).

Par exemple, pour sélectionner les communes dans les Hauts-de-Seine, on
peut commencer par utiliser le résultat de la méthode `str.startswith` (qui renvoie
`True` ou `False`) :

```{python}
#| echo: true
df['INSEE commune'].str.startswith("92")
```

`str.` est une méthode particulière en `Pandas` qui permet de traiter chaque valeur d'un vecteur comme un `string` natif en `Python` sur lequel appliquer une méthode ultérieure (en l'occurrence `startswith`). 

L'instruction ci-dessus renvoie un vecteur de booléens. Nous avons vu précédemment que la méthode `loc` servait à faire du _subsetting_ sur l'indice des lignes comme des colonnes. Elle fonctionne avec des vecteurs booléens. Dans ce cas, si on fait un _subsetting_ sur la dimension ligne (resp. colonne), elle va renvoyer toutes les observations (resp. variable) qui 
satisfont cette condition. 

Ainsi, en mettant bout à bout ces deux élements, on peut filter nos données pour n'avoir que les résultats
du 92:

```{python}
#| echo: true
df.loc[df['INSEE commune'].str.startswith("92")].head(2)
```

Le code SQL équivalent peu varier selon le moteur d'exécution (`DuckDB`, `PostGre`, `MySQL`) mais prendrait une forme similaire à celle-ci:

```sql
SELECT * FROM df WHERE STARTSWITH("INSEE commune", "92")
```
:::
::: {.content-visible when-profile="en"}
## Filtering

The operation of selecting rows is called `FILTER` in SQL. It is used based on a logical condition (clause `WHERE`). Data is selected based on a logical condition.

There are several methods in `Pandas`. The simplest is to use boolean masks, as seen in the chapter [`numpy`](/content/manipulation/01_numpy.qmd).

For example, to select the municipalities in Hauts-de-Seine, you can start by using the result of the `str.startswith` method (which returns `True` or `False`):

```{python}
#| echo: true
df['INSEE commune'].str.startswith("92")
```

`str.` is a special method in `Pandas` that allows treating each value of a vector as a native `string` in `Python` to which a subsequent method (in this case, `startswith`) is applied.

The above instruction returns a vector of booleans. We previously saw that the `loc` method is used for subsetting on both row and column indices. It works with boolean vectors. In this case, if subsetting on the row dimension (or column), it will return all observations (or variables) that satisfy this condition.

Thus, by combining these two elements, we can filter our data to get only the results for the 92:

```{python}
#| echo: true
df.loc[df['INSEE commune'].str.startswith("92")].head(2)
```

The equivalent SQL code may vary depending on the execution engine (`DuckDB`, `PostGre`, `MySQL`) but would take a form similar to this:

```sql
SELECT * FROM df WHERE STARTSWITH("INSEE commune", "92")
```
:::

::: {.content-visible when-profile="fr"}
## Résumé des principales opérations

Les principales manipulations sont les suivantes:

:::: {layout-ncol=2}

![Sélectionner des colonnes](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/select_pandas.png)
![Renommer des colonnes](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/rename_pandas.png)

![Créer de nouvelles colonnes](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/mutate_pandas.png)
![Sélectionner des lignes](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/filter_pandas.png)

![Réordonner le _DataFrame_](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/arrange_pandas.png)

::::

:::

::: {.content-visible when-profile="en"}
## Summary of main operations

The main manipulations are as follows:

:::: {layout-ncol=2}

![Selecting Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/select_pandas.png)
![Renaming Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/rename_pandas.png)

![Creating New Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/mutate_pandas.png)
![Selecting Rows](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/filter_pandas.png)

![Reordering the DataFrame](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/arrange_pandas.png)

::::

:::

::: {.content-visible when-profile="fr"}

# Statistiques descriptives

Pour repartir de la source brute, recréeons notre jeu de données pour les exemples:

:::

::: {.content-visible when-profile="en"}
# Descriptive Statistics

To start again from the raw source, let's recreate our dataset for the examples:
:::

```{python}
#| echo: true
df = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
df.head(3)
```

::: {.content-visible when-profile="fr"}

`Pandas` embarque plusieurs méthodes pour construire des statistiques agrégées: somme, nombre de valeurs unique, nombre de valeurs non manquantes, moyenne, variance, etc.  

La méthode la plus générique est `describe`

```{python}
#| echo: true
df.describe()
```

qui ressemble à la méthode éponyme en `Stata` ou à la `PROC FREQ` de `SAS`, deux langages propriétaires. 
Elle nous donne néanmoins beaucoup d'information et on est souvent noyé par celle-ci. Il est donc plus pratique de directement travailler sur quelques colonnes ou de choisir soi-même les statistiques qu'on désire utiliser.
:::

::: {.content-visible when-profile="en"}
`Pandas` includes several methods to construct aggregate statistics: sum, count of unique values, count of non-missing values, mean, variance, etc.

The most generic method is `describe`

```{python}
#| echo: true
df.describe()
```

which is similar to the eponymous method in `Stata` or the `PROC FREQ` of `SAS`, two proprietary languages. However, it provides a lot of information, which can often be overwhelming. Therefore, it is more practical to work directly on a few columns or choose the statistics you want to use.
:::

::: {.content-visible when-profile="fr"}

## Comptages

Le premier type de statistiques qu'on peut vouloir mettre en oeuvre relève du comptage ou dénombrement des valeurs. 

Si on désire, par exemple, connaître le nombre de communes dans notre jeu de données, on pourra utiliser la méthode `count` ou alors `nunique` si on s'intéresse aux valeurs non dupliquées.

:::

::: {.content-visible when-profile="en"}
## Counts

The first type of statistics you might want to implement involves counting or enumerating values.

For example, if you want to know the number of municipalities in your dataset, you can use the `count` method or `nunique` if you are interested in unique values.

:::

```{python}
#| echo: true
df['Commune'].count()
```

```{python}
#| echo: true
df['Commune'].nunique()
```

::: {.content-visible when-profile="fr"}

En SQL, la première instruction serait `SELECT COUNT(Commune) FROM df`, la seconde `SELECT COUNT DISTINCT Commune FROM df`.
Ici, cela ne permet donc de comprendre qu'il peut y avoir des doublons dans la colonne `Commune`, ce dont il faudra tenir compte si on désire identifier nos communes de manière unique (ce sera un sujet lorsque le prochain chapitre abordera la question du croisement des données).

La cohérence de la syntaxe `Pandas` permet de faire cela pour plusieurs colonnes de manière simultanée. En SQL cela serait possible mais le code à mettre en oeuvre commence à devenir assez verbeux:

:::



::: {.content-visible when-profile="en"}
In SQL, the first instruction would be `SELECT COUNT(Commune) FROM df`, the second `SELECT COUNT DISTINCT Commune FROM df`. Here, this allows us to understand that there may be duplicates in the `Commune` column, which needs to be considered if we want to uniquely identify our municipalities (this will be addressed in the next chapter on data merging).

The coherence of the `Pandas` syntax allows you to do this for several columns simultaneously. In SQL, this would be possible but the code would start to become quite verbose:
:::

```{python}
#| echo: true
df.loc[:, ['Commune', 'INSEE commune']].count()
```

```{python}
#| echo: true
df.loc[:, ['Commune', 'INSEE commune']].nunique()
```

::: {.content-visible when-profile="fr"}
Avec ces deux commandes simples, on comprend donc
que notre variable `INSEE commune` (le code Insee)
sera plus fiable pour identifier des communes
que les noms, qui ne sont pas forcément uniques. C'est justement l'objectif du code Insee de proposer 
un identifiant unique, a contrario du code postal qui peut
être partagé par plusieurs communes. 
:::

::: {.content-visible when-profile="en"}
With these two simple commands, we understand that our `INSEE commune` variable (the INSEE code) will be more reliable for identifying municipalities than the names, which are not necessarily unique. The purpose of the INSEE code is to provide a unique identifier, unlike the postal code which can be shared by several municipalities.
:::

::: {.content-visible when-profile="fr"}
## Statistiques agrégées

`Pandas` embarque plusieurs méthodes pour construire des statistiques sur plusieurs colonnes: somme, moyenne, variance, etc. 

Les méthodes sont assez transparentes:

```{python}
#| echo: true
df['Agriculture'].sum()
df['Agriculture'].mean()
```

Là encore, la cohérence de `Pandas` nous permet de généraliser le calcul de statistiques à plusieurs colonnes

```{python}
#| echo: true
df.loc[:, ['Agriculture', 'Résidentiel']].sum()
df.loc[:, ['Agriculture', 'Résidentiel']].mean()
```


Il est possible de généraliser ceci à toutes les colonnes. 
Cependant, il est nécessaire d'introduire le paramètre `numeric_only` pour ne faire la tâche d'agrégation que sur les variables pertinentes

```{python}
#| echo: true
df.mean(numeric_only = True)
```

:::: {.warning}

La version 2.0 de `Pandas` a introduit un changement
de comportement dans les méthodes d'agrégation. 

Il est dorénavant nécessaire de préciser quand on désire
effectuer des opérations si on désire ou non le faire
exclusivement sur les colonnes numériques. C'est pour cette 
raison qu'on explicite ici l'argument `numeric_only = True`. 
Ce comportement
était par le passé implicite. 

::::


La méthode pratique à connaître est `agg`. Celle-ci permet de définir les statistiques qu'on désire calculer pour chaque variable
:::

::: {.content-visible when-profile="en"}
## Aggregated statistics

`Pandas` includes several methods to construct statistics on multiple columns: sum, mean, variance, etc.

The methods are quite straightforward:

```{python}
#| echo: true
df['Agriculture'].sum()
df['Agriculture'].mean()
```

Again, the consistency of `Pandas` allows generalizing the calculation of statistics to multiple columns:

```{python}
#| echo: true
df.loc[:, ['Agriculture', 'Résidentiel']].sum()
df.loc[:, ['Agriculture', 'Résidentiel']].mean()
```

It is possible to generalize this to all columns. However, it is necessary to introduce the `numeric_only` parameter to perform the aggregation task only on the relevant variables:

```{python}
#| echo: true
df.mean(numeric_only = True)
```

:::: {.warning}

Version 2.0 of `Pandas` introduced a change in the behavior of aggregation methods.

It is now necessary to specify whether you want to perform operations exclusively on numeric columns. This is why we explicitly state the argument `numeric_only = True` here. This behavior was previously implicit.

::::

The practical method to know is `agg`. This allows defining the statistics you want to calculate for each variable:

:::

```{python}
#| echo: true
df.agg(
  {
    'Agriculture': ['sum', 'mean'],
    'Résidentiel': ['mean', 'std'],
    'Commune': 'nunique'
  }
)
```

::: {.content-visible when-profile="fr"}

La sortie des méthodes d'agrégation est une `Serie` indexée (les méthodes du type `df.sum()`) ou directement un `DataFrame` (la méthode `agg`). Il est en général plus pratique d'avoir un `DataFrame` qu'une `Serie` indexée si on désire retravailler le tableau pour en tirer des conclusions. Il est donc utile de transformer les sorties sous forme de `Serie` en `DataFrame` puis appliquer la méthode `reset_index` pour transformer l'indice en colonne. A partir de là, il sera possible de modifier le `DataFrame` pour rendre celui-ci plus lisible.

Par exemple si on s'intéresse à la part de chaque secteur dans les émissions totales, on pourra procéder en deux temps.
D'abord on va créer une observation par secteur représentant les émissions totales de celui-ci:
:::

::: {.content-visible when-profile="en"}
The output of aggregation methods is an indexed `Series` (methods like `df.sum()`) or directly a `DataFrame` (the `agg` method). It is generally more practical to have a `DataFrame` than an indexed `Series` if you want to rework the table to draw conclusions. Therefore, it is useful to transform the outputs from `Series` to `DataFrame` and then apply the `reset_index` method to convert the index into a column. From there, you can modify the `DataFrame` to make it more readable.

For example, if you are interested in the share of each sector in total emissions, you can proceed in two steps. First, create an observation per sector representing its total emissions:

:::

```{python}
#| echo: true
# Etape 1: création d'un DataFrame propre
emissions_totales = (
  pd.DataFrame(
    df.sum(numeric_only = True),
    columns = ["emissions"]
  )
  .reset_index(names = "secteur")
)
emissions_totales
```

::: {.content-visible when-profile="fr"}

Il ne reste plus qu'à travailler _a minima_ le jeu de données afin d'avoir déjà quelques conclusions intéressantes sur la structure des émissions en France:

:::

::: {.content-visible when-profile="en"}
Then, work minimally on the dataset to get some interesting conclusions about the structure of emissions in France:
:::

```{python}
#| echo: true
emissions_totales['emissions (%)'] = (
  100*emissions_totales['emissions']/emissions_totales['emissions'].sum()
)
(emissions_totales
  .sort_values("emissions", ascending = False).
  round()
)
```

::: {.content-visible when-profile="fr"}

Ce tableau n'est pas vraiment mis en forme donc encore loin d'être communiquable mais il présente déjà un intérêt dans une perspective exploratoire. Il nous permet de comprendre les secteurs les plus émetteurs, à savoir le transport, l'agriculture et l'industrie, hors énergie. Le fait que l'énergie soit relativement peu émettrice s'explique bien du fait du _mix énergétique_ français où le nucléaire représente une majorité de la production électrique. 

Pour aller plus loin dans la mise en forme de ce tableau afin d'avoir des statistiques communiquables en dehors de `Python`, nous découvrirons au prochain chapitre `great_tables`. 

:::: {.note}

La structure de données issue de `df.sum` est assez pratique (elle est _tidy_). On pourrait faire exactement la même opération que `df.sum(numeric_only = True)` avec le code suivant:

```{python}
#| echo: true
df.select_dtypes(include='number').agg(func = sum)
```

::::

:::

::: {.content-visible when-profile="en"}
This table is not well-formatted and far from being presentable, but it is already useful from an exploratory perspective. It helps us understand the most emitting sectors, namely transport, agriculture, and industry, excluding energy. The fact that energy is relatively low in emissions can be explained by the French energy mix, where nuclear power represents a majority of electricity production.

To go further in formatting this table to have communicable statistics outside of `Python`, we will explore `great_tables` in the next chapter.

:::: {.note}

The data structure resulting from `df.sum` is quite practical (it is tidy). We could do exactly the same operation as `df.sum(numeric_only = True)` with the following code:

```{python}
#| echo: true
df.select_dtypes(include='number').agg(func=sum)
```

::::

:::

::: {.content-visible when-profile="fr"}

## Valeurs manquantes

Jusqu'à présent nous n'avons pas évoqué ce qui pourrait être un caillou dans la chaussure du _data scientist_, les valeurs manquantes. 

Les jeux de données réels sont rarement complets et les valeurs manquantes peuvent refléter de nombreuses réalités: problème de remontée d'information, variable non pertinente pour cette observation... 

Sur le plan technique, `Pandas` ne rencontre pas de problème à gérer les valeurs manquantes (sauf pour les variables `int` mais c'est une exception). 
Par défaut, les valeurs manquantes sont affichées `NaN` et sont de type `np.nan` (pour
les valeurs temporelles, i.e. de type `datatime64`, les valeurs manquantes sont
`NaT`).
On a un comportement cohérent d'agrégation lorsqu'on combine deux colonnes dont l'une comporte des valeurs manquantes.

:::

::: {.content-visible when-profile="en"}
## Missing values

So far, we haven't discussed what could be a stumbling block for a data scientist: missing values.

Real datasets are rarely complete, and missing values can reflect many realities: data retrieval issues, irrelevant variables for a given observation, etc.

Technically, `Pandas` handles missing values without issues (except for `int` variables, but that's an exception). By default, missing values are displayed as `NaN` and are of type `np.nan` (for temporal values, i.e., of type `datetime64`, missing values are `NaT`). We get consistent aggregation behavior when combining two columns, one of which has missing values.

:::

```{python}
#| echo: true
ventes = pd.DataFrame(
    {'prix': np.random.uniform(size = 5),
     'client1': [i+1 for i in range(5)],
     'client2': [i+1 for i in range(4)] + [np.nan],
     'produit': [np.nan] + ['yaourt','pates','riz','tomates']
    }
)
ventes
```

::: {.content-visible when-profile="fr"}

`Pandas` va bien refuser de faire l'agrégation car pour lui une valeur manquante n'est pas un zéro:

:::


::: {.content-visible when-profile="en"}
`Pandas` will refuse to aggregate because, for it, a missing value is not zero:
:::

```{python}
#| echo: true
ventes["client1"] + ventes["client2"]
```

::: {.content-visible when-profile="fr"}

Il est possible de supprimer les valeurs manquantes grâce à `dropna()`.
Cette méthode va supprimer toutes les lignes où il y a au moins une valeur manquante.

:::

::: {.content-visible when-profile="en"}
It is possible to remove missing values using `dropna()`. This method will remove all rows where there is at least one missing value.
:::


```{python}
#| echo: true
ventes.dropna()
```

::: {.content-visible when-profile="fr"}

En l'occurrence, on perd deux lignes. Il est aussi possible de supprimer seulement les colonnes où il y a des valeurs manquantes
dans un DataFrame avec `dropna()` en utilisant le paramètre `subset`.

:::

::: {.content-visible when-profile="en"}
In this case, we lose two rows. It is also possible to remove only the columns where there are missing values in a DataFrame with `dropna()` using the `subset` parameter.
:::

```{python}
#| echo: true
ventes.dropna(subset=["produit"])
```

::: {.content-visible when-profile="fr"}
Cette fois on ne perd plus qu'une ligne, celle où `produit` est manquant.

`Pandas` donne la possibilité d'imputer les valeurs manquantes grâce à la méthode `fillna()`. Par exemple, si on pense que les valeurs manquantes dans `produit` sont des valeurs nulles, on pourra faire

:::

::: {.content-visible when-profile="en"}
This time we lose only one row, the one where `produit` is missing.

`Pandas` provides the ability to impute missing values using the `fillna()` method. For example, if you think the missing values in `produit` are zeros, you can do:

:::

```{python}
#| echo: true
ventes.dropna(subset=["produit"]).fillna(0)
```

::: {.content-visible when-profile="fr"}
Si on désire faire une imputation à la médiane pour la variable `client2`, on changera marginalement ce code
en encapsulant à l'intérieur le calcul de la médiane
:::
::: {.content-visible when-profile="en"}
If you want to impute the median for the `client2` variable, you can slightly change this code by encapsulating the median calculation inside:
:::


```{python}
#| echo: true
(ventes["client2"]
  .fillna(
    ventes["client2"].median()
  )
)
```

::: {.content-visible when-profile="fr"}
Sur des jeux de données du monde réel, il est utile d'utiliser la méthode `isna` (ou `isnull`) combinées avec `sum` ou `mean` pour connaître l'ampleur des valeurs manquantes dans un jeu de données.
:::

::: {.content-visible when-profile="en"}
For real-world datasets, it is useful to use the `isna` (or `isnull`) method combined with `sum` or `mean` to understand the extent of missing values in a dataset.
:::


```{python}
#| echo: true
df.isnull().mean().sort_values(ascending = False)
```

::: {.content-visible when-profile="fr"}
Cette étape préparatoire est utile pour anticiper la question de l'imputation ou du filtre sur les valeurs manquantes: sont-elles _missing at random_ ou reflètent-elles un sujet sur la remontée des données ? Les choix relatifs au traitement des valeurs manquantes ne sont pas des choix méthodologiques neutres. `Pandas` donne les outils techniques
pour faire ceci mais la question de la légitimité de ces choix et de la pertinence est propre à chaque donnée. Les explorations sur les données visent à détecter des indices pour faire un choix éclairé. 
:::

::: {.content-visible when-profile="en"}
This preparatory step is useful for anticipating the question of imputation or filtering on missing values: are they missing at random or do they reflect an issue in data retrieval? The choices related to handling missing values are not neutral methodological choices. `Pandas` provides the technical tools to do this, but the legitimacy and relevance of these choices are specific to each dataset. Data explorations aim to detect clues to make an informed decision.
:::

::: {.content-visible when-profile="fr"}
# Représentations graphiques rapides

Les tableaux de nombre sont certes une informations utile
pour comprendre la structure d'un jeu de données mais dont
l'aspect dense rend l'appropriation difficile. Avoir un graphique simplement peut être utile pour se représenter, en un coup d'oeil, la distribution des données et ainsi connaître le caractère plus ou moins normal d'une observation. 

`Pandas` embarque des méthodes graphiques rudimentaires pour répondre à ce besoin. Elles sont pratiques pour
produire rapidement un graphique, notamment après des opérations
complexes de maniement de données.
Nous approfondirons cette problématique des visualisations de données dans la partie [Communiquer](/content/visualisation.qmd). 

On peut appliquer la méthode `plot()` directement à une `Serie` :
:::

::: {.content-visible when-profile="en"}
# Quick graphical representations

Numerical tables are certainly useful for understanding the structure of a dataset, but their dense aspect makes them difficult to grasp. Having a simple graph can be useful to visualize the distribution of the data at a glance and thus understand the normality of an observation.

`Pandas` includes basic graphical methods to meet this need. They are practical for quickly producing a graph, especially after complex data manipulation operations. We will delve deeper into the issue of data visualizations in the [Communicate](/content/visualisation.qmd) section.

You can apply the `plot()` method directly to a `Series`:
:::

```{python}
#| echo: true
df['Déchets'].plot()
```

::: {.content-visible when-profile="fr"}
Le code équivalent avec `matplotlib` serait:
:::
::: {.content-visible when-profile="en"}
The equivalent code with `matplotlib` would be:
:::

```{python}
#| echo: true
import matplotlib.pyplot as plt
plt.plot(df.index, df['Déchets'])
```

::: {.content-visible when-profile="fr"}
Par défaut, la visualisation obtenue est une série. Ce n'est
pas forcément celle attendue puisqu'elle n'a de sens que pour des séries temporelles. En tant que _data scientist_ sur microdonnées, on s'intéresse plus fréquemment à un histogramme pour avoir une idée de la distribution des données. 
Pour cela, il suffit d'ajouter l'argument `kind = 'hist'`:
:::
::: {.content-visible when-profile="en"}
By default, the obtained visualization is a series. This is not necessarily what is expected since it only makes sense for time series. As a data scientist working with microdata, you are more often interested in a histogram to get an idea of the data distribution. To do this, simply add the argument `kind = 'hist'`:
:::

```{python}
#| echo: true
df['Déchets'].hist()
```

::: {.content-visible when-profile="fr"}
Avec des données dont la distribution est non normalisée,
ce qui représente de nombreuses variables du monde réel,
les histogrammes sont généralement peu instructifs. Le _log_ peut être une solution
pour remettre à une échelle comparable certaines valeurs extrêmes:
:::
::: {.content-visible when-profile="en"}
With data that has a non-normalized distribution, which represents many real-world variables, histograms are generally not very informative. The log can be a solution to bring some extreme values to a comparable scale:
:::


```{python}
#| echo: true
df['Déchets'].plot(kind = 'hist', logy = True)
```

::: {.content-visible when-profile="fr"}
La sortie est un objet `matplotlib`. La *customisation* de ces
figures est ainsi
possible (et même désirable car les graphiques `matplotlib`
sont, par défaut, assez rudimentaires).
Cependant, il s'agit d'une méthode rapide pour la construction
de figures qui nécessite du travail pour une visualisation
finalisée. Cela passe par un travail approfondi sur l'objet
`matplotlib` ou l'utilisation d'une librairie plus haut
niveau pour la représentation graphique (`seaborn`, `plotnine`, `plotly`, etc.).

La partie de ce cours consacrée à la visualisation de données présentera succinctement ces différents
paradigmes de visualisation. Ceux-ci ne dispensent pas de faire
preuve de bon sens dans le choix du graphique utilisé
pour représenter une statistique descriptive (cf. [cette conférence d'Eric Mauvière](https://ssphub.netlify.app/talk/2024-02-29-mauviere/) ).
:::

::: {.content-visible when-profile="en"}
The output is a `matplotlib` object. Customizing these figures is thus possible (and even desirable because the default `matplotlib` graphs are quite basic). However, this is a quick method for constructing figures that require work for a finalized visualization. This involves thorough work on the `matplotlib` object or using a higher-level library for graphical representation (`seaborn`, `plotnine`, `plotly`, etc.).

The part of this course dedicated to data visualization will briefly present these different visualization paradigms. These do not exempt you from using common sense in choosing the graph used to represent a descriptive statistic (see [this conference by Eric Mauvière](https://ssphub.netlify.app/talk/2024-02-29-mauviere/)).
:::


::: {.content-visible when-profile="fr"}
# Exercice de synthèse

Cette exercice synthétise plusieurs étapes de préparation et d'exploration de données pour mieux comprendre la structure
du phénomène qu'on désire étudier à savoir les émissions de gaz carbonique en France. 

Il est recommandé de repartir d'une session vierge (dans un _notebook_ il faut faire `Restart Kernel`) pour ne pas avoir un environnement pollué par d'autres objets. Vous pouvez ensuite exécuter le code suivant pour avoir la base nécessaire:
:::

::: {.content-visible when-profile="en"}
# Synthesis exercise

This exercise synthesizes several steps of data preparation and exploration to better understand the structure of the phenomenon we want to study, namely carbon emissions in France.

It is recommended to start from a clean session (in a notebook, you should do `Restart Kernel`) to avoid an environment polluted by other objects. You can then run the following code to get the necessary base:
:::

```{python}
#| echo: true
import pandas as pd

emissions = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
emissions.head(2)
```

{{< include "02_pandas_intro/_exo2_fr.qmd" >}}
{{< include "02_pandas_intro/_exo2_solution.qmd" >}}


::: {.content-visible when-profile="fr"}

A l'issue de la question 8, on comprend un peu mieux les facteurs qui peuvent expliquer une forte émission au niveau communal. Si on regarde les trois principales communes émettrices, on peut remarquer qu'il s'agit de villes avec des raffineries:

```{python}
#| output: true
emissions_top.head(3)
```

Grâce à nos explorations minimales avec `Pandas`, on voit que ce jeu de données nous donne donc une information sur la nature du tissu productif français et des conséquences environnementales de certaines activités. 

# Références {.unnumbered}

* Le site
[pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html)
fait office de référence

* Le livre `Modern Pandas` de Tom Augspurger : https://tomaugspurger.github.io/modern-1-intro.html


:::

::: {.content-visible when-profile="en"}
At the end of question 8, we better understand the factors that can explain high emissions at the municipal level. If we look at the top three emitting municipalities, we can see that they are cities with refineries:

```{python}
#| output: true
emissions_top.head(3)
```

Thanks to our minimal explorations with `Pandas`, we see that this dataset provides information about the nature of the French productive fabric and the environmental consequences of certain activities.

# References {.unnumbered}

* The site [pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html) serves as a reference

* The book `Modern Pandas` by Tom Augspurger: https://tomaugspurger.github.io/modern-1-intro.html
:::


::: {#refs}
:::



