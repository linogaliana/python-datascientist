---
title: "Introduction to Pandas"
tags:
  - Pandas
  - Pollution
  - Ademe
  - Tutorial
  - Manipulation
categories:
  - Tutorial
  - Exercises
  - Manipulation
description: |
  `Pandas` is the central piece of the `Python` ecosystem for data science. This chapter presents the first data exploration that can be performed with `Pandas` in order to explore the structure of a dataset.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda_stretching.png
links:
- icon: journal-text
  name: Pandas Documentation
  url: https://pandas.pydata.org/docs/
echo: false
---

::: {.content-visible when-format="html"}
To try the examples in this tutorial:

{{< include "../../build/_printBadges.qmd" >}}
:::

# Introduction

The `Pandas` package has been the central piece of the data science ecosystem for about a decade. The _DataFrame_, a central object in languages like `R` or `Stata`, had long been absent in the `Python` ecosystem. Yet, thanks to `Numpy`, all the basic components were present but needed to be reconfigured to meet the needs of data scientists.

Wes McKinney, when he built `Pandas` to provide a dataframe leveraging the numerical computation library `Numpy` in the background, enabled a significant leap forward for `Python` in data analysis, explaining its popularity in the data science ecosystem. `Pandas` is not without limitations[^tidyverse], which we will have the opportunity to discuss, but the vast array of analysis methods it offers greatly simplifies data analysis work. For more information on this package, the reference book by @mckinney2012python presents many of the package's features.

[^tidyverse]: The equivalent ecosystem in `R`, the [`tidyverse`](https://www.tidyverse.org/), developed by _Posit_, is of more recent design than `Pandas`. Its philosophy could thus draw inspiration from `Pandas` while addressing some limitations of the `Pandas` syntax. Since both syntaxes are an implementation in `Python` or `R` of the `SQL` philosophy, it is natural that they resemble each other and that it is pertinent for data scientists to know both languages.

In this chapter, we will focus on the most relevant elements in the context of an introduction to data science, leaving interested users to deepen their knowledge with the abundant resources available on the subject.

As datasets generally gain value by associating multiple sources, for example, to relate a record to contextual data or to link two client databases to obtain meaningful data, the next chapter will present how to merge different datasets with `Pandas`. By the end of the next chapter, thanks to data merging, we will have a detailed database on the carbon footprints of the French[^empreinte].

[^empreinte]: Actually, it is not the carbon footprint but the __national inventory__ since the database corresponds to a production view, not consumption. Emissions made in one municipality to satisfy the consumption of another will be attributed to the former where the carbon footprint concept would attribute it to the latter. Moreover, the emissions presented here do not include those produced by goods made abroad. This exercise is not about constructing a reliable statistic but rather understanding the logic of data merging to construct descriptive statistics.


## Data used in this chapter

In this `Pandas` tutorial, we will use:

* Greenhouse gas emissions estimated at the municipal level by ADEME. The dataset is available on [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_) and can be queried directly in `Python` with [this URL](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert).

The [next chapter](/content/manipulation/02_pandas_suite.qmd) will allow us to apply the elements presented in this chapter with the above data combined with contextual data at the municipal level.

::: {.tip}
## Skills to be acquired by the end of this chapter

- Import a dataset as a `Pandas` dataframe and explore its structure;
- Perform manipulations on columns and rows;
- Construct aggregate statistics and chain operations;
- Use `Pandas` graphical methods to quickly represent data distribution.

:::

## Environment

We will follow the usual conventions in importing packages:

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

To obtain reproducible results, you can set the seed of the pseudo-random number generator.

```{python}
#| echo: true
np.random.seed(123)
```

Throughout this demonstration of the main `Pandas` functionalities, and in the next chapter, I recommend regularly referring to the following resources:

* The [official `Pandas` documentation](https://pandas.pydata.org/docs/user_guide/index.html), especially the [language comparison page](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/index.html), which is very useful;
* [This tutorial](https://observablehq.com/@observablehq/data-wrangling-translations), designed for users of [`Observable Javascript`](https://observablehq.com), but offering many interesting examples for `Pandas` aficionados;
* The following [cheatsheet from this post](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463).

As a reminder, to execute the code examples in an interactive notebook, you can use the shortcuts at the top of the page to launch it in your preferred environment.


# `Pandas` logic

## Anatomy of a `Pandas` table

The central object in the `Pandas` logic is the `DataFrame`. It is a special data structure with two dimensions, structured by aligning rows and columns. Unlike a matrix, columns can be of different types.

A `DataFrame` consists of the following elements:

* the row index;
* the column name;
* the data value;

![Structure of a `Pandas` _DataFrame_, borrowed from <https://x.com/epfl_exts/status/997506000600084480>](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/pandasDF.png)

## Before seeing `DataFrame`, we need to know `Pandas` `Series`

In fact, a _DataFrame_ is a collection of objects called `pandas.Series`. These `Series` are one-dimensional objects that are extensions of the one-dimensional `Numpy` arrays[^numpyarrow]. In particular, to facilitate the handling of categorical or temporal data, additional variable types are available in `Pandas` compared to `Numpy` (`categorical`, `datetime64`, and `timedelta64`). These types are associated with optimized methods to facilitate the processing of this data.

[^numpyarrow]: The original goal of `Pandas` is to provide a high-level library for more abstract low-level layers, such as `Numpy` arrays. `Pandas` is gradually changing these low-level layers to favor `Arrow` over `Numpy` without destabilizing the high-level commands familiar to `Pandas` users. This shift is due to the fact that `Arrow`, a low-level computation library, is more powerful and flexible than `Numpy`. For example, `Numpy` offers limited textual types, whereas `Arrow` provides greater freedom.

There are several possible types for a `pandas.Series`, extending the basic data types in `Python`, which will determine the behavior of this variable. Indeed, many operations do not have the same meaning depending on whether the value is numeric or not.

The simplest types (`int` or `float`) correspond to numeric values:

```{python}
#| echo: true
poids = pd.Series([3, 7, 12])
poids
```

::: {.important}
## Missing Values are tricky !

In general, if `Pandas` exclusively detects integer values in a variable, it will use the `int` type to optimize memory. This choice makes sense. However, it has a drawback: `Numpy`, and therefore by extension `Pandas`, cannot represent missing values for the `int` type (more on missing values below).

Pending the shift to `Arrow`, which can handle missing values in `int`, the method to use is to convert to the `float` type if the variable will have missing values, which is quite simple:

:::

For textual data, it is just as simple:

```{python}
#| echo: true
animal = pd.Series(['cat', 'dog', 'koala'])
animal
```

The `object` type is a catch-all for exclusively textual data types (type `str`) or a mix of textual and numerical data (type `mixed`). Historically, it was an intermediate type between the `factor` and `character` of `R`. However, recently, there is an equivalent type to `factor` in `Pandas` for variables with a finite and relatively short list of values, the `category` type. The `object` type can cause unexpected errors due to its mixed nature, so it is recommended to choose the nature of a variable and convert it:

```{python}
#| echo: true
animal.astype("category") #<1>
animal.astype(str) #<2>
```
1. To convert to `category` (the logical choice here)
2. To convert to `str` (if you want to perform subsequent textual operations)

It is important to examine the types of your `Pandas` objects and convert them if they do not make sense; `Pandas` makes optimized choices, but it may be necessary to correct them because `Pandas` does not know your future data usage. This is one of the tasks to do during _feature engineering_, the set of steps for preparing data for future use.


# From `Series` to `DataFrame`

We have created two independent series, `animal` and `poids`, which are related. In the matrix world, this would correspond to moving from a vector to a matrix. In the `Pandas` world, this means moving from a `Series` to a `DataFrame`.

This is done naturally with `Pandas`:

```{python}
#| echo: true
animaux = pd.DataFrame(
  zip(animal, poids), #<1>
  columns = ['animal','poids']
)
animaux
```
1. We need to use `zip` here because `Pandas` expects a structure like `{"var1": [val1, val2], "var2": [val1, val2]}`, which is not what we prepared previously. However, we will see that this approach is not the most common for creating a `DataFrame`.

## Indexing

The essential difference between a `Series` and a `Numpy` object is indexing. In `Numpy`, indexing is implicit; it allows accessing data (the one at the index located at position *i*). With a `Series`, you can, of course, use a positional index, but more importantly, you can use more explicit indices.

This allows accessing data more naturally, using column names, for example:

```{python}
#| echo: true
animaux['poids']
```

The existence of an index makes subsetting, that is, selecting rows or columns, particularly easy. DataFrames have two indices: those for rows and those for columns. You can make selections on both dimensions. Anticipating later exercises, you can see that this will facilitate row selection:

```{python}
#| echo: true
animaux.loc[animaux['animal'] == "chat", 'poids']
```

This instruction is equivalent to the SQL command:

```sql
SELECT poids FROM animaux WHERE animal == "chat"
```

If we return to our `animaux` dataset, we can see the row number displayed on the left:

```{python}
#| echo: true
animaux
```

This is the default index for the row dimension because we did not configure one. It is not mandatory; it is quite possible to have an index corresponding to a variable of interest (we will discover this when we explore `groupby` in the next chapter). However, this can be tricky, and it is recommended that this be only transitory, hence the importance of regularly performing `reset_index`.

## The concept of _tidy data_

The concept of __*tidy*__ data, popularized by Hadley Wickham through his `R` packages (see @wickham2023r), is highly relevant for describing the structure of a `Pandas` DataFrame. The three rules of tidy data are as follows:

* Each __variable__ has its own column;
* Each __observation__ has its own row;
* A __value__, representing an observation of a variable, is located in a single cell.

![Illustration of the _tidy data_ concept (borrowed from H. Wickham)](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

These principles may seem like common sense, but you will find that many data formats do not adhere to them. For example, Excel spreadsheets often have values spanning multiple columns or several merged rows. Restructuring this data according to the tidy data principle will be crucial to performing analysis on it.

# Importing Data with `Pandas`

If you had to manually create all your DataFrames from vectors, `Pandas` would not be practical. `Pandas` offers many functions to read data stored in different formats.

The simplest data to read are tabular data stored in an appropriate format. The two main formats to know are `CSV` and `Parquet`. The former has the advantage of simplicity - it is universal, known to all, and readable by any text editor. The latter is gaining popularity in the data ecosystem because it addresses some limitations of CSV (optimized storage, predefined variable types...) but has the disadvantage of not being readable without a suitable tool (which fortunately are increasingly present in standard code editors like `VSCode`). For more information on the difference between `CSV` and `Parquet`, refer to the [deep dive chapters](/content/modern-ds/s3.qmd) on the subject.

Data stored in other text-derived formats from `CSV` (`.txt`, `.tsv`...) or formats like `JSON` are readable with `Pandas`, but sometimes it takes a bit of iteration to find the correct reading parameters. In other chapters, we will discover that other data formats related to different data structures are also readable with `Python`.

Flat formats (`.csv`, `.txt`...) and the `Parquet` format are easy to use with `Pandas` because they are not proprietary and they store data in a tidy form. Data from spreadsheets, `Excel` or `LibreOffice`, are more or less complicated to import depending on whether they follow this schema or not. This is because these tools are used indiscriminately. While in the data science world, they should primarily be used to disseminate final tables for reporting, they are often used to disseminate raw data that could be better shared through more suitable channels.

One of the main difficulties with spreadsheets is that the data are usually associated with documentation in the same tab - for example, the spreadsheet has some lines describing the sources before the table - which requires human intelligence to assist `Pandas` during the import phase. This will always be possible, but when there is an alternative in the form of a flat file, there should be no hesitation.


## Reading Data from a Local Path

This exercise aims to demonstrate the benefit of using a relative path rather than an absolute path to enhance code reproducibility. However, we will later recommend reading directly from the internet when possible and when it does not involve the recurrent download of a large file.

To prepare for this exercise, the following code will allow you to download data and write it locally:

{{< include "02_pandas_intro/_exo0_preliminary.qmd" >}}
{{< include "02_pandas_intro/_exo0_en.qmd" >}}
{{< include "02_pandas_intro/_exo0_solution.qmd" >}}


The main issue with reading from files stored locally is the risk of becoming dependent on a file system that is not necessarily shared. It is better, whenever possible, to directly read the data with an `HTTPS` link, which `Pandas` can handle. Moreover, when working with open data, this ensures that you are using the latest available data and not a local duplication that may not be up to date.

## Reading from a CSV Available on the Internet

The URL to access the data can be stored in an ad hoc variable:

```{python}
#| echo: true
url = "https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert"
```

The goal of the next exercise is to get familiar with importing and displaying data using `Pandas` and displaying a few observations.

{{< include "02_pandas_intro/_exo1_en.qmd" >}}
{{< include "02_pandas_intro/_exo1_solution.qmd" >}}


As illustrated by this exercise, displaying DataFrames in notebooks is quite ergonomic. The first and last rows are displayed automatically. For valuation tables present in a report or research article, the next chapter introduces `great_tables`, which offers very rich table formatting features.

::: {.warning}

Be careful with `display` and commands that reveal data (`head`, `tail`, etc.) in a notebook that handles confidential data when using version control software like `Git` (see dedicated chapters).

Indeed, you may end up sharing data inadvertently in the `Git` history. As explained in the chapter dedicated to `Git`, a file named `.gitignore` is sufficient to create some rules to avoid unintentional sharing of data with `Git`.

:::


# Exploring the Structure of a DataFrame

`Pandas` offers a data schema quite familiar to users of statistical software like `R`. Similar to the main data processing paradigms like the _tidyverse_ (`R`), the grammar of `Pandas` inherits from `SQL` logic. The philosophy is very similar: operations are performed to select rows, columns, sort rows based on column values, apply standardized treatments to variables, etc. Generally, operations that reference variable names are preferred over those that reference row or column numbers.

Whether you are familiar with `SQL` or `R`, you will find a similar logic to what you know, although the names might differ: `df.loc[df['y']=='b']` may be written as `df %>% filter(y=='b')` (`R`) or `SELECT * FROM df WHERE y == 'b'` (`SQL`), but the logic is the same.

`Pandas` offers a plethora of pre-implemented functionalities. It is highly recommended, before writing a function, to consider if it is natively implemented in `Numpy`, `Pandas`, etc. Most of the time, if a solution is implemented in a library, it should be used as it will be more efficient than what you would implement.

To present the most practical methods for data analysis, we can use the example of the municipal CO2 consumption data from Ademe, which was the focus of the previous exercises.

```{python}
#| echo: true
df = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
df
```

## Dimensions and Structure of a DataFrame

The first useful methods allow displaying some attributes of a `DataFrame`.

```{python}
#| echo: true
df.axes
```

```{python}
#| echo: true
df.columns
```

```{python}
#| echo: true
df.index
```

To know the dimensions of a DataFrame, some practical methods can be used:

```{python}
#| echo: true
df.ndim
```

```{python}
#| echo: true
df.shape
```

```{python}
#| echo: true
df.size
```

To determine the number of unique values of a variable, rather than writing a function yourself, use the `nunique` method. For example,

```{python}
#| echo: true
df['Commune'].nunique()
```

`Pandas` offers many useful methods. Here is a summary of those related to data structure, accompanied by a comparison with `R`:

| Operation                     | pandas                  | dplyr (`R`)           | data.table (`R`)          |
|-------------------------------|-------------------------|-----------------------|---------------------------|
| Retrieve column names         | `df.columns`            | `colnames(df)`        | `colnames(df)`            |
| Retrieve dimensions           | `df.shape`              | `dim(df)`             | `dim(df)`                 |
| Retrieve unique values of a variable | `df['myvar'].nunique()` | `df %>%  summarise(distinct(myvar))` | `df[,uniqueN(myvar)]` |

## Accessing Elements of a DataFrame

In SQL, performing operations on columns is done with the `SELECT` command. With `Pandas`, to access an entire column, several approaches can be used:

* `dataframe.variable`, for example `df.Energie`. This method requires column names without spaces or special characters, which excludes many real datasets. It is not recommended.
* `dataframe[['variable']]` to return the variable as a `DataFrame`. This method can be tricky for a single variable; it's better to use `dataframe.loc[:,['variable']]`, which is more explicit about the nature of the resulting object.
* `dataframe['variable']` to return the variable as a `Series`. For example, `df[['Autres transports']]` or `df['Autres transports']`. This is the preferred method.

To retrieve multiple columns at once, there are two approaches, with the second being preferable:

* `dataframe[['variable1', 'variable2']]`
* `dataframe.loc[:, ['variable1', 'variable2']]`

This is equivalent to `SELECT variable1, variable2 FROM dataframe` in SQL.

Using `.loc` may seem excessively verbose, but it ensures that you are performing a subset operation on the column dimension. DataFrames have two indices, for rows and columns, and implicit operations can sometimes cause surprises, so it's more reliable to be explicit.

## Accessing Rows

To access one or more values in a `DataFrame`, there are two recommended methods, depending on the form of the row or column indices used:

* `df.iloc`: uses indices. This method is somewhat unreliable because a DataFrame's indices can change during processing (especially when performing group operations).
* `df.loc`: uses labels. This method is recommended.

::: {.warning}
Code snippets using the `df.ix` structure should be avoided as the function is deprecated and may disappear at any time.
:::

`iloc` refers to the indexing from 0 to *N*, where *N* equals `df.shape[0]` of a `pandas.DataFrame`. `loc` refers to the values of `df`'s index. For example, with the `pandas.DataFrame` `df_example`:

```{python}
#| echo: true
df_example = pd.DataFrame(
    {'month': [1, 4, 7, 10], 'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})
df_example = df_example.set_index('month')
df_example
```

- `df_example.loc[1, :]` will return the first row of `df` (row where the `month` index equals 1);
- `df_example.iloc[1, :]` will return the second row (since Python indexing starts at 0);
- `df_example.iloc[:, 1]` will return the second column, following the same principle.

Later exercises will allow practicing this syntax on our dataset of carbon emissions.


# Main Data Manipulations

The most frequent operations in `SQL` are summarized in the following table. It is useful to know them (many data manipulation syntaxes use these terms) because, one way or another, they cover most data manipulation uses. We will describe some of them later:

| Operation | SQL | pandas | dplyr (`R`) | data.table (`R`) |
|-----------|-----|--------|-------------|------------------|
| Select variables by name | `SELECT` | `df[['Autres transports','Energie']]` | `df %>% select(Autres transports, Energie)` | `df[, c('Autres transports','Energie')]` |
| Select observations based on one or more conditions | `FILTER` | `df[df['Agriculture']>2000]` | `df %>% filter(Agriculture>2000)` | `df[Agriculture>2000]` |
| Sort the table by one or more variables | `SORT BY` | `df.sort_values(['Commune','Agriculture'])` | `df %>% arrange(Commune, Agriculture)` | `df[order(Commune, Agriculture)]` |
| Add variables that are functions of other variables | `SELECT *, LOG(Agriculture) AS x FROM df` | `df['x'] = np.log(df['Agriculture'])` | `df %>% mutate(x = log(Agriculture))` | `df[,x := log(Agriculture)]` |
| Perform an operation by group | `GROUP BY` | `df.groupby('Commune').mean()` | `df %>% group_by(Commune) %>% summarise(m = mean)` | `df[,mean(Commune), by = Commune]` |
| Join two databases (inner join) | `SELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x` | `table1.merge(table2, left_on = 'id', right_on = 'x')` | `table1 %>% inner_join(table2, by = c('id'='x'))` | `merge(table1, table2, by.x = 'id', by.y = 'x')` |

## Operations on Columns: Adding or Removing Variables, Renaming Them, etc.

Technically, `Pandas` DataFrames are *mutable* objects in the `Python` language, meaning it is possible to change the DataFrame as needed during processing.

The most classic operation is adding or removing variables to the data table. The simplest way to add columns is by reassignment. For example, to create a `dep` variable that corresponds to the first two digits of the commune code (INSEE code), simply take the variable and apply the appropriate treatment (in this case, keep only its first two characters):

```{python}
#| echo: true
df['dep'] = df['INSEE commune'].str[:2]
df.head(3)
```

In SQL, the method depends on the execution engine. In pseudo-code, it would be:

```sql
SELECT everything(), SUBSTR("code_insee", 2) AS dep FROM df
```

It is possible to apply this approach to creating columns on multiple columns. One of the advantages of this approach is that it allows recycling column names.

```{python}
#| echo: true
vars = ['Agriculture', 'Déchets', 'Energie']

df[[v + "_log" for v in vars]] = np.log(df.loc[:, vars])
df.head(3)
```

The equivalent SQL query would be quite tedious to write. For such operations, the benefit of a high-level library like `Pandas` becomes clear.

::: {.warning}

This is possible thanks to the native vectorization of `Numpy` operations and the magic of `Pandas` that rearranges everything. This is not usable with just any function. For other functions, you will need to use `assign`, generally through lambda functions, temporary functions acting as pass-throughs. For example, to create a variable using this approach, you would do:

```{python}
#| echo: true
df.assign(
  Energie_log = lambda x: np.log(x['Energie'])
)
```

With methods from `Pandas` or `Numpy` like this, it is not beneficial and even counterproductive as it slows down the code.

:::

Variables can be easily renamed using the `rename` method, which works well with dictionaries. To rename columns, specify the parameter `axis = 'columns'` or `axis=1`. The `axis` parameter is often necessary because many `Pandas` methods assume by default that operations are done on the row index:

```{python}
#| echo: true
df = df.rename({"Energie": "eneg", "Agriculture": "agr"}, axis=1)
df.head()
```

Finally, to delete columns, use the `drop` method with the `columns` argument:

```{python}
#| echo: true
df = df.drop(columns = ["eneg", "agr"])
```


## Reordering Observations

The `sort_values` method allows reordering observations in a `DataFrame`, keeping the column order the same.

For example, to sort in descending order of CO2 consumption in the residential sector, you would do:

```{python}
#| echo: true
df = df.sort_values("Résidentiel", ascending = False)
df.head(3)
```

Thus, in one line of code, you can identify the cities where the residential sector consumes the most. In SQL, you would do:

```sql
SELECT * FROM df ORDER BY "Résidentiel" DESC
```

## Filtering

The operation of selecting rows is called `FILTER` in SQL. It is used based on a logical condition (clause `WHERE`). Data is selected based on a logical condition.

There are several methods in `Pandas`. The simplest is to use boolean masks, as seen in the chapter [`numpy`](/content/manipulation/01_numpy.qmd).

For example, to select the municipalities in Hauts-de-Seine, you can start by using the result of the `str.startswith` method (which returns `True` or `False`):

```{python}
#| echo: true
df['INSEE commune'].str.startswith("92")
```

`str.` is a special method in `Pandas` that allows treating each value of a vector as a native `string` in `Python` to which a subsequent method (in this case, `startswith`) is applied.

The above instruction returns a vector of booleans. We previously saw that the `loc` method is used for subsetting on both row and column indices. It works with boolean vectors. In this case, if subsetting on the row dimension (or column), it will return all observations (or variables) that satisfy this condition.

Thus, by combining these two elements, we can filter our data to get only the results for the 92:

```{python}
#| echo: true
df.loc[df['INSEE commune'].str.startswith("92")].head(2)
```

The equivalent SQL code may vary depending on the execution engine (`DuckDB`, `PostGre`, `MySQL`) but would take a form similar to this:

```sql
SELECT * FROM df WHERE STARTSWITH("INSEE commune", "92")
```

## Summary of Main Operations

The main manipulations are as follows:

::: {layout-ncol=2}

![Selecting Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/select_pandas.png)
![Renaming Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/rename_pandas.png)

![Creating New Columns](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/mutate_pandas.png)
![Selecting Rows](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/filter_pandas.png)

![Reordering the DataFrame](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/arrange_pandas.png)

:::


# Descriptive Statistics

To start again from the raw source, let's recreate our dataset for the examples:

```{python}
#| echo: true
df = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
df.head(3)
```

`Pandas` includes several methods to construct aggregate statistics: sum, count of unique values, count of non-missing values, mean, variance, etc.

The most generic method is `describe`

```{python}
#| echo: true
df.describe()
```

which is similar to the eponymous method in `Stata` or the `PROC FREQ` of `SAS`, two proprietary languages. However, it provides a lot of information, which can often be overwhelming. Therefore, it is more practical to work directly on a few columns or choose the statistics you want to use.

## Counts

The first type of statistics you might want to implement involves counting or enumerating values.

For example, if you want to know the number of municipalities in your dataset, you can use the `count` method or `nunique` if you are interested in unique values.

```{python}
#| echo: true
df['Commune'].count()
```

```{python}
#| echo: true
df['Commune'].nunique()
```

In SQL, the first instruction would be `SELECT COUNT(Commune) FROM df`, the second `SELECT COUNT DISTINCT Commune FROM df`. Here, this allows us to understand that there may be duplicates in the `Commune` column, which needs to be considered if we want to uniquely identify our municipalities (this will be addressed in the next chapter on data merging).

The coherence of the `Pandas` syntax allows you to do this for several columns simultaneously. In SQL, this would be possible but the code would start to become quite verbose:

```{python}
#| echo: true
df.loc[:, ['Commune', 'INSEE commune']].count()
```

```{python}
#| echo: true
df.loc[:, ['Commune', 'INSEE commune']].nunique()
```

With these two simple commands, we understand that our `INSEE commune` variable (the INSEE code) will be more reliable for identifying municipalities than the names, which are not necessarily unique. The purpose of the INSEE code is to provide a unique identifier, unlike the postal code which can be shared by several municipalities.



## Aggregated Statistics

`Pandas` includes several methods to construct statistics on multiple columns: sum, mean, variance, etc.

The methods are quite straightforward:

```{python}
#| echo: true
df['Agriculture'].sum()
df['Agriculture'].mean()
```

Again, the consistency of `Pandas` allows generalizing the calculation of statistics to multiple columns:

```{python}
#| echo: true
df.loc[:, ['Agriculture', 'Résidentiel']].sum()
df.loc[:, ['Agriculture', 'Résidentiel']].mean()
```

It is possible to generalize this to all columns. However, it is necessary to introduce the `numeric_only` parameter to perform the aggregation task only on the relevant variables:

```{python}
#| echo: true
df.mean(numeric_only = True)
```

::: {.warning}

Version 2.0 of `Pandas` introduced a change in the behavior of aggregation methods.

It is now necessary to specify whether you want to perform operations exclusively on numeric columns. This is why we explicitly state the argument `numeric_only = True` here. This behavior was previously implicit.

:::

The practical method to know is `agg`. This allows defining the statistics you want to calculate for each variable:

```{python}
#| echo: true
df.agg(
  {
    'Agriculture': ['sum', 'mean'],
    'Résidentiel': ['mean', 'std'],
    'Commune': 'nunique'
  }
)
```

The output of aggregation methods is an indexed `Series` (methods like `df.sum()`) or directly a `DataFrame` (the `agg` method). It is generally more practical to have a `DataFrame` than an indexed `Series` if you want to rework the table to draw conclusions. Therefore, it is useful to transform the outputs from `Series` to `DataFrame` and then apply the `reset_index` method to convert the index into a column. From there, you can modify the `DataFrame` to make it more readable.

For example, if you are interested in the share of each sector in total emissions, you can proceed in two steps. First, create an observation per sector representing its total emissions:

```{python}
#| echo: true
# Step 1: create a clean DataFrame
emissions_totales = (
  pd.DataFrame(
    df.sum(numeric_only = True),
    columns = ["emissions"]
  )
  .reset_index(names = "secteur")
)
emissions_totales
```

Then, work minimally on the dataset to get some interesting conclusions about the structure of emissions in France:

```{python}
#| echo: true
emissions_totales['emissions (%)'] = (
  100 * emissions_totales['emissions'] / emissions_totales['emissions'].sum()
)
(emissions_totales
  .sort_values("emissions", ascending = False)
  .round()
)
```

This table is not well-formatted and far from being presentable, but it is already useful from an exploratory perspective. It helps us understand the most emitting sectors, namely transport, agriculture, and industry, excluding energy. The fact that energy is relatively low in emissions can be explained by the French energy mix, where nuclear power represents a majority of electricity production.

To go further in formatting this table to have communicable statistics outside of `Python`, we will explore `great_tables` in the next chapter.

::: {.note}

The data structure resulting from `df.sum` is quite practical (it is tidy). We could do exactly the same operation as `df.sum(numeric_only = True)` with the following code:

```{python}
#| echo: true
df.select_dtypes(include='number').agg(func=sum)
```

:::


## Missing Values

So far, we haven't discussed what could be a stumbling block for a data scientist: missing values.

Real datasets are rarely complete, and missing values can reflect many realities: data retrieval issues, irrelevant variables for a given observation, etc.

Technically, `Pandas` handles missing values without issues (except for `int` variables, but that's an exception). By default, missing values are displayed as `NaN` and are of type `np.nan` (for temporal values, i.e., of type `datetime64`, missing values are `NaT`). We get consistent aggregation behavior when combining two columns, one of which has missing values.

```{python}
#| echo: true
ventes = pd.DataFrame(
    {'prix': np.random.uniform(size = 5),
     'client1': [i+1 for i in range(5)],
     'client2': [i+1 for i in range(4)] + [np.nan],
     'produit': [np.nan] + ['yaourt','pates','riz','tomates']
    }
)
ventes
```

`Pandas` will refuse to aggregate because, for it, a missing value is not zero:

```{python}
#| echo: true
ventes["client1"] + ventes["client2"]
```

It is possible to remove missing values using `dropna()`. This method will remove all rows where there is at least one missing value.

```{python}
#| echo: true
ventes.dropna()
```

In this case, we lose two rows. It is also possible to remove only the columns where there are missing values in a DataFrame with `dropna()` using the `subset` parameter.

```{python}
#| echo: true
ventes.dropna(subset=["produit"])
```

This time we lose only one row, the one where `produit` is missing.

`Pandas` provides the ability to impute missing values using the `fillna()` method. For example, if you think the missing values in `produit` are zeros, you can do:

```{python}
#| echo: true
ventes.dropna(subset=["produit"]).fillna(0)
```

If you want to impute the median for the `client2` variable, you can slightly change this code by encapsulating the median calculation inside:

```{python}
#| echo: true
(ventes["client2"]
  .fillna(
    ventes["client2"].median()
  )
)
```

For real-world datasets, it is useful to use the `isna` (or `isnull`) method combined with `sum` or `mean` to understand the extent of missing values in a dataset.

```{python}
#| echo: true
df.isnull().mean().sort_values(ascending = False)
```

This preparatory step is useful for anticipating the question of imputation or filtering on missing values: are they missing at random or do they reflect an issue in data retrieval? The choices related to handling missing values are not neutral methodological choices. `Pandas` provides the technical tools to do this, but the legitimacy and relevance of these choices are specific to each dataset. Data explorations aim to detect clues to make an informed decision.


# Quick Graphical Representations

Numerical tables are certainly useful for understanding the structure of a dataset, but their dense aspect makes them difficult to grasp. Having a simple graph can be useful to visualize the distribution of the data at a glance and thus understand the normality of an observation.

`Pandas` includes basic graphical methods to meet this need. They are practical for quickly producing a graph, especially after complex data manipulation operations. We will delve deeper into the issue of data visualizations in the [Communicate](/content/visualisation.qmd) section.

You can apply the `plot()` method directly to a `Series`:

```{python}
#| echo: true
df['Déchets'].plot()
```

The equivalent code with `matplotlib` would be:

```{python}
#| echo: true
import matplotlib.pyplot as plt
plt.plot(df.index, df['Déchets'])
```

By default, the obtained visualization is a series. This is not necessarily what is expected since it only makes sense for time series. As a data scientist working with microdata, you are more often interested in a histogram to get an idea of the data distribution. To do this, simply add the argument `kind = 'hist'`:

```{python}
#| echo: true
df['Déchets'].hist()
```

With data that has a non-normalized distribution, which represents many real-world variables, histograms are generally not very informative. The log can be a solution to bring some extreme values to a comparable scale:

```{python}
#| echo: true
df['Déchets'].plot(kind = 'hist', logy = True)
```

The output is a `matplotlib` object. Customizing these figures is thus possible (and even desirable because the default `matplotlib` graphs are quite basic). However, this is a quick method for constructing figures that require work for a finalized visualization. This involves thorough work on the `matplotlib` object or using a higher-level library for graphical representation (`seaborn`, `plotnine`, `plotly`, etc.).

The part of this course dedicated to data visualization will briefly present these different visualization paradigms. These do not exempt you from using common sense in choosing the graph used to represent a descriptive statistic (see [this conference by Eric Mauvière](https://ssphub.netlify.app/talk/2024-02-29-mauviere/)).

# Synthesis Exercise

```{python}
#| echo: false
lang = "en"
```


This exercise synthesizes several steps of data preparation and exploration to better understand the structure of the phenomenon we want to study, namely carbon emissions in France.

It is recommended to start from a clean session (in a notebook, you should do `Restart Kernel`) to avoid an environment polluted by other objects. You can then run the following code to get the necessary base:

```{python}
#| echo: true
import pandas as pd

emissions = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
emissions.head(2)
```


{{< include "02_pandas_intro/_exo2_fr.qmd" >}}
{{< include "02_pandas_intro/_exo2_solution.qmd" >}}


At the end of question 8, we better understand the factors that can explain high emissions at the municipal level. If we look at the top three emitting municipalities, we can see that they are cities with refineries:

```{python}
#| output: true
emissions_top.head(3)
```

Thanks to our minimal explorations with `Pandas`, we see that this dataset provides information about the nature of the French productive fabric and the environmental consequences of certain activities.

# References {.unnumbered}

* The site [pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html) serves as a reference

* The book `Modern Pandas` by Tom Augspurger: https://tomaugspurger.github.io/modern-1-intro.html

::: {#refs}
:::
