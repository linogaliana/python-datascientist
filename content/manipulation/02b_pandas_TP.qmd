---
title: "Pratique de pandas : un exemple complet"
date: 2023-07-01T13:00:00Z
draft: false
weight: 30
tags:
  - pandas
  - pollution
  - Ademe
  - Exercice
  - Manipulation
categories:
  - Manipulation
  - Exercice
slug: pandasTP
type: book
description: |
  Après avoir présenté la logique de `Pandas` dans le chapitre précédent, 
  ce chapitre vise à illustrer les fonctionalités du _package_ 
  à partir de données d'émissions de gaz à effet de serre
  de l'[`Ademe`](https://data.ademe.fr/). 
echo: false
image: panda_stretching.png
---

Les exemples de ce TP sont visualisables sous forme de `Jupyter Notebooks`:

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/manipulation/02b_pandas_TP.qmd")
```
:::

Dans cette série d'exercices `Pandas`, 
nous allons découvrir comment manipuler plusieurs
jeux de données avec `Python`.

Si vous êtes intéressés par `R`,
une version très proche de ce TP est
disponible dans [ce cours](https://rgeo.linogaliana.fr/exercises/r-wrangling.html).


Dans ce tutoriel, nous allons utiliser deux sources de données :

* Les émissions de gaz à effet de serre estimées au niveau communal par l'`ADEME`. Le jeu de données est
disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)
et requêtable directement dans `Python` avec
[cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) (ce sera l'objet du premier exercice).
* Idéalement, on utiliserait directement les données
[disponibles sur le site de l'Insee](https://www.insee.fr/fr/statistiques/3560121) mais celles-ci nécessitent un peu de travail
de nettoyage qui n'entre pas dans le cadre de ce TP. 
Pour faciliter l'import de données Insee, il est recommandé d'utiliser le _package_
[`pynsee`](https://pynsee.readthedocs.io/en/latest/) qui simplifie l'accès aux données
de l'Insee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)
ou via des API. 

La librairie `pynsee` n'est pas installée par défaut avec `Python`. Avant de pouvoir l'utiliser,
il est nécessaire de l'installer :

```{python}
#| eval: false
#| echo: true
!pip install xlrd
!pip install pynsee
```

Toutes les dépendances indispensables étant installées, il suffit
maintenant d'importer les librairies qui seront utilisées
pendant ces exercices :

```{python}
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pynsee
import pynsee.download
```

## Importer les données

### Import d'un csv de l'Ademe

L'URL d'accès aux données peut être conservé dans une variable _ad hoc_ :

```{python}
#| echo: true
url = "https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert"
```

L'objectif du premier exercice est de se familiariser à l'import et l'affichage de données
avec `Pandas`. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 1: Importer un CSV et explorer la structure de données</h3>
```

1. Importer les données de l'Ademe à l'aide du package `Pandas` et de la commande consacrée pour l'import de csv. Nommer le `DataFrame` obtenu `emissions`[^nomdf].
2. Utiliser les méthodes adéquates afin d'afficher pour les 10 premières valeurs, les 15 dernières et un échantillon aléatoire de 10 valeurs grâce aux méthodes adéquates du _package_ `Pandas`. 
3. Tirer 5 pourcents de l'échantillon sans remise.
4. Ne conserver que les 10 premières lignes et tirer aléatoirement dans celles-ci pour obtenir un DataFrame de 100 données.
5. Faire 100 tirages à partir des 6 premières lignes avec une probabilité de 1/2 pour la première observation et une probabilité uniforme pour les autres.


<details>
<summary>
En cas de blocage à la question 1
</summary>

Lire la documentation de `read_csv` (très bien faite) ou chercher des exemples
en ligne pour découvrir cette fonction.

</details>


```{=html}
</div>
```
:::

[^nomdf]: Par manque d'imagination, on est souvent tenté d'appeler notre
_dataframe_ principal `df` ou `data`. C'est souvent une mauvaise idée puisque
ce nom n'est pas très informatif quand on relit le code quelques semaines
plus tard. L'autodocumentation, approche qui consiste à avoir un code
qui se comprend de lui-même, est une bonne pratique et il est donc recommandé
de donner un nom simple mais efficace pour connaître la nature du _dataset_ en question.

```{python}
#| label: exo1-q1
# Question 1
emissions = pd.read_csv(url, sep=",")
```

```{python}
#| output: false
#| label: exo1-q2
# Question 2
emissions.head(2)

emissions.head(10)
emissions.tail(15)
emissions.sample(10)

# Question 3
emissions.sample(frac = 0.05)

# Question 4
emissions.head(10).sample(n = 100, replace = True)

# Question 5
emissions.head(6).sample(n = 100, replace = True, weights = [0.5] + [0.1]*5)
```

## Premières manipulations de données

Le chapitre précédent évoquait quelques manipulations traditionnelles
de données. Les principales sont rappelées ici :

::: {.content-visible when-format="html"}

:::: {layout-ncol=2}

![Sélectionner des colonnes](select_pandas.png)
![Renommer des colonnes](rename_pandas.png)

![Créer de nouvelles colonnes](mutate_pandas.png)
![Sélectionner des lignes](filter_pandas.png)

![Réordonner le _DataFrame_](arrange_pandas.png)

::::

:::

::: {.content-visible when-format="ipynb"}

![Sélectionner des colonnes](select_pandas.png){fig-width="50%"}
![Renommer des colonnes](rename_pandas.png){fig-width="50%"}

![Créer de nouvelles colonnes](mutate_pandas.png){fig-width="50%"}
![Sélectionner des lignes](filter_pandas.png){fig-width="50%"}

![Réordonner le _DataFrame_](arrange_pandas.png){fig-width="50%"}

:::

La _cheatsheet_ suivante est très pratique puisqu'elle illustre ces différentes
fonctions. Il est recommandé de régulièrement
la consulter :

![Cheasheet `Pandas`](https://cdn-images-1.medium.com/max/2000/1*YhTbz8b8Svi22wNVvqzneg.jpeg){width=70%}

L'objectif du prochain exercice est de se familiariser aux principales manipulations de données
sur un sous-ensemble de la table des émissions de gaz carbonique. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 2: Découverte des verbes de <code>Pandas</code> pour manipuler des données</h3>
```

En premier lieu, on propose de se familiariser avec les opérations sur
les colonnes.

1. Créer un _dataframe_ `emissions_copy` ne conservant que les colonnes
`INSEE commune`, `Commune`, `Autres transports` et `Autres transports international`

<details>
<summary>
Indice pour cette question
</summary>

![](./select_pandas.png){fig-width=50%}

</details>

2. Comme les noms de variables sont peu pratiques, les renommer de la
manière suivante:
    + `INSEE commune` $\to$ `code_insee`
    + `Autres transports` $\to$ `transports`
    + `Autres transports international` $\to$ `transports_international`

<details>
<summary>
Indice pour cette question
</summary>

![](./rename_pandas.png){fig-width=50%}

</details>

3. On propose, pour simplifier, de remplacer les valeurs manquantes (`NA`)
par la valeur 0. Utiliser la
méthode [`fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)
pour transformer les valeurs manquantes en 0.

4. Créer les variables suivantes:
    - `dep`: le département. Celui-ci peut être créé grâce aux deux premiers caractères de `code_insee` en appliquant la méthode `str` ;
    - `transports_total`: les émissions du secteur transports (somme des deux variables)

<details>
<summary>
Indice pour cette question
</summary>

![](mutate_pandas.png){fig-width=50%}

</details>


5. Ordonner les données du plus gros pollueur au plus petit 
puis ordonner les données 
du plus gros pollueur au plus petit par département (du 01 au 95). 

<details>
<summary>
Indice pour cette question
</summary>

![](arrange_pandas.png){fig-width=50%}

</details>

6. Ne conserver que les communes appartenant aux départements 13 ou 31. 
Ordonner ces communes du plus gros pollueur au plus petit.

<details>
<summary>
Indice pour cette question
</summary>

![](./filter_pandas.png){fig-width=50%}

</details>

7. Calculer les émissions totales par départements

<details>
<summary>
Indice pour cette question
</summary>

* _"Grouper par"_ = `groupby`
* _"émissions totales"_ = `agg({***: "sum"})`

</details>


```{=html}
</div>
```
:::

```{python}
#| output: false
# Question 1
emissions_copy = emissions.loc[
  :,
  ["INSEE commune", "Commune", "Autres transports", "Autres transports international"]
]

# Question 2
emissions_copy = emissions_copy.rename({
  "INSEE commune": "code_insee",
  "Autres transports": "transports",
  "Autres transports international": "transports_international"
}, axis = 1)

# Question 3
emissions_copy = emissions_copy.fillna(0)

# Question 4
emissions_copy['dep'] = emissions_copy['code_insee'].str[:2]
emissions_copy['transports_total'] = emissions_copy['transports'] + emissions_copy['transports_international']

# Question 5
data_sorted = emissions_copy.sort_values("transports_total", ascending = False)
data_sorted
emissions_copy.sort_values(by = ["dep","transports_total"], ascending = [True, False])

# Question 6
emissions_hg_bdr = (emissions_copy
    .loc[emissions_copy['dep'].isin(["13","31"])]
    .sort_values("transports", ascending = False)
)

# Question 7
emissions_copy.groupby("dep").agg({"transports_total": "sum"})
```

A la question 5, quand on ordonne les communes exclusivement à partir de la variable
`transports_total`, on obtient ainsi:

```{python}
data_sorted.head(3)
```

A la question 6, on obtient ce classement :

```{python}
emissions_hg_bdr.head(3)
```


### Import des données de l'Insee

En ce qui concerne nos informations communales, on va utiliser l'une des
sources de l'Insee les plus utilisées : les données [`Filosofi`](https://www.insee.fr/fr/metadonnees/source/serie/s1172). 
Afin de faciliter la récupération de celles-ci, nous allons
utiliser le _package_ communautaire `pynsee` :



::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Le _package_ `pynsee` comporte deux principaux points d'entrée:

- Les API de l'Insee, ce qui sera illustré dans le chapitre consacré.
- Quelques jeux de données directement issus du site web de
l'Insee ([insee.fr](https://www.insee.fr/fr/accueil))

Dans ce chapitre, nous allons exclusivement utiliser cette deuxième
approche. Cela se fera par le module `pynsee.download`.  

La liste des données disponibles depuis ce _package_ est [ici](https://inseefrlab.github.io/DoReMIFaSol/articles/donnees_dispo.html).
La fonction `download_file` attend un identifiant unique
pour savoir quelle base de données aller chercher et
restructurer depuis le
site [insee.fr](https://www.insee.fr/fr/accueil). 

<details>
<summary>
Connaître la liste des bases disponibles
</summary>

Pour connaître la liste des bases disponibles, vous
pouvez utiliser la fonction `meta = pynsee.get_file_list()`
après avoir fait `import pynsee`. 
Celle-ci renvoie un `DataFrame` dans lequel on peut
rechercher, par exemple grâce à une recherche
de mots-clefs : 

```{python}
#| echo: true

meta = pynsee.get_file_list()
meta.loc[meta['label'].str.contains(r"Filosofi.*2016")]
```

Ici, `meta['label'].str.contains(r"Filosofi.*2016")` signifie:
"_`pandas` trouve moi tous les labels où sont contenus les termes Filosofi et 2016._"
 (`.*` signifiant "_peu m'importe le nombre de mots ou caractères entre_")

</details>

```{=html}
</div>
```
:::

On va utiliser les données Filosofi (données de revenus) au niveau communal de 2016. 
Ce n'est pas la même année que les données d'émission de CO2, ce n'est donc pas parfaitement rigoureux,
mais cela permettra tout de même d'illustrer 
les principales fonctionnalités de `Pandas`

Le point d'entrée principal de la fonction `pynsee` est la fonction `download_file`.

Le code pour télécharger les données est le suivant :

```{python}
#| echo: true
#| output: false
from pynsee.download import download_file
filosofi = download_file("FILOSOFI_COM_2016")
```

Le _DataFrame_ en question a l'aspect suivant :

```{python}
filosofi.sample(5)
```

`Pandas` a géré automatiquement les types de variables. Il le fait relativement bien, mais une vérification est toujours utile pour les variables qui ont un statut spécifique.

Pour les variables qui ne sont pas en type `float` alors qu’elles devraient l’être, on modifie leur type.

```{python}
#| echo: true
filosofi.loc[:, filosofi.columns[2:]] = (
  filosofi.loc[:, filosofi.columns[2:]]
  .apply(pd.to_numeric, errors='coerce')
)
```



Un simple coup d'oeil sur les données
donne une idée assez précise de la manière dont les données sont organisées.
On remarque que certaines variables de `filosofi` semblent avoir beaucoup de valeurs manquantes (secret statistique)
alors que d'autres semblent complètes. 
Si on désire exploiter `filosofi`, il faut faire attention à la variable choisie.


Notre objectif à terme va être de relier l'information contenue entre ces
deux jeux de données. En effet, sinon, nous risquons d'être frustré : nous allons
vouloir en savoir plus sur les émissions de gaz carbonique mais seront très
limités dans les possibilités d'analyse sans ajout d'une information annexe
issue de `filosofi`.  


## Les indices

Les indices sont des éléments spéciaux d'un `DataFrame` puisqu'ils permettent d'identifier certaines observations.
Il est tout à fait possible d'utiliser plusieurs indices, par exemple si on a des niveaux imbriqués.

Pour le moment, on va prendre comme acquis que les codes communes (dits aussi codes Insee) permettent
d'identifier de manière unique une commune. Un exercice ultérieur permettra de s'en assurer. 

`Pandas` propose un système d'indice qui permet d'ordonner les variables mais également de gagner
en efficacité sur certains traitements, comme des recherches d'observations. Le prochain 
exercice illustre cette fonctionnalité. 


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 3 : Les indices</h3>
```


On suppose ici qu'on peut se fier aux codes communes. En effet, on a un même ordre de grandeur de communes dans les deux bases. 

```{python}
#| echo: true
print(emissions[['INSEE commune', 'Commune']].nunique())
print(filosofi[['CODGEO', 'LIBGEO']].nunique())
```


1. Fixer comme indice la variable de code commune dans les deux bases.
Regarder le changement que cela induit sur le *display* du `DataFrame`

2. Les deux premiers chiffres des codes communes sont le numéro de département.
Créer une variable de département `dep` dans `emissions` et `filosofi`

3. Calculer les émissions totales par secteur pour chaque département.
Mettre en _log_ ces résultats dans un objet `emissions_log`.
Garder 5 départements et produire un `barplot` grâce à la méthode _plot_ (la
figure n'a pas besoin d'être vraiment propre, c'est seulement pour illustrer
cette méthode)

4. Repartir de `emissions`.
Calculer les émissions totales par département et sortir la liste
des 10 principaux émetteurs de CO2 et des 5 départements les moins émetteurs.
Essayer de comprendre pourquoi ce sont ces départements qui apparaissent en tête
du classement. Pour cela, il peut être utile de regarder les caractéristiques de ces
départements dans `filosofi`

```{=html}
</div>
```
:::

```{python}
#| output: false

# Question 1
display(emissions)
emissions = emissions.set_index('INSEE commune')
display(emissions)

display(filosofi)
filosofi =  filosofi.set_index('CODGEO') 
display(filosofi)
```

```{python}
#| output: false

# Question 2
emissions['dep'] = emissions.index.str[:2]
filosofi['dep'] = filosofi.index.str[:2]
```

```{python}
#| output: false

# Question 3
emissions_log = emissions.groupby('dep').sum(numeric_only = True).apply(np.log)
print(emissions_log.head())
emissions_log.sample(5).plot(kind = "barh")
```

```{python}
#| output: false

# Question 4
## Emissions totales par département (df)
emissions_totales = (
  emissions
  .reset_index()
  .groupby("dep")
  .sum(numeric_only = True)
  .sum(axis = 1, numeric_only = True)
)
gros_emetteurs = emissions_totales.sort_values(ascending = False).head(10)
petits_emetteurs = emissions_totales.sort_values().head(5)

## Caractéristiques des départements (df_city)
print("gros emetteurs")
display(
  filosofi.loc[
    filosofi['dep'].isin(gros_emetteurs.index), ['NBPERSMENFISC16','MED16']
    ].mean()
  )
print("\npetits emetteurs")
display(
  filosofi.loc[
    filosofi['dep'].isin(petits_emetteurs.index), ['NBPERSMENFISC16','MED16']
    ].mean()
  )
# Les petits emetteurs sont en moyenne plus pauvres et moins peuplés que les gros emetteurs
```

En pratique, l'utilisation des indices en `Pandas` peut être piégeuse, notamment lorsqu'on
associe des sources de données.  
Il est plutôt recommandé de ne pas les utiliser ou de les utiliser avec parcimonie,
cela pourra éviter de mauvaises surprises. 


## Restructurer les données

Quand on a plusieurs informations pour un même individu ou groupe, on
retrouve généralement deux types de structure de données : 
    
* format __wide__ : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes 
* format __long__ : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d'observations

Un exemple de la distinction entre les deux peut être pris à l'ouvrage de référence d'Hadley Wickham, [*R for Data Science*](https://r4ds.hadley.nz/):

![Données _long_ et _wide_](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png)


L'aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :

![](reshape.png){fig-width=60%}

Le fait de passer d'un format *wide* au format *long* (ou vice-versa)
peut être extrêmement pratique car certaines fonctions sont plus adéquates sur une forme de données ou sur l'autre.

En règle générale, avec `Python` comme avec `R`, les __formats *long* sont souvent préférables__.
Les formats _wide_ sont plutôt pensés pour des tableurs comme `Excel` ou on dispose d'un nombre réduit
de lignes à partir duquel faire des tableaux croisés dynamiques. 

Le prochain exercice propose donc une telle restructuration de données. 
Les données de l'ADEME, et celles de l'Insee également, sont au format
_wide_. 
Le prochain exercice illustre l'intérêt de faire la conversion _long_ $\to$ _wide_
avant de faire un graphique.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 5: Restructurer les données : wide to long</h3>
```

1. Créer une copie des données de l'`ADEME` en faisant `df_wide = emissions_wide.copy()`

2. Restructurer les données au format *long* pour avoir des données d'émissions par secteur en gardant comme niveau d'analyse la commune (attention aux autres variables identifiantes).

3. Faire la somme par secteur et représenter graphiquement

4. Garder, pour chaque département, le secteur le plus polluant

```{=html}
</div>
```
:::

```{python}
#| output: false
#| label: question1
# Question 1

emissions_wide = emissions.copy()
emissions_wide[['Commune','dep', "Agriculture", "Tertiaire"]].head() 
```

```{python}
#| output: false
#| label: question2
# Question 2
emissions_wide.reset_index().melt(id_vars = ['INSEE commune','Commune','dep'],
                          var_name = "secteur", value_name = "emissions")
```

```{python}
#| output: false
#| label: question3
# Question 3

emissions_totales = (
  emissions_wide.reset_index()
 .melt(
    id_vars = ['INSEE commune','Commune','dep'],
    var_name = "secteur", value_name = "emissions"
    )
 .groupby('secteur')
 .sum(numeric_only = True)
)

emissions_totales.plot(kind = "barh")
```

```{python}
#| output: false
#| label: question4
# Question 4

top_commune_dep = (
  emissions_wide
  .reset_index()
  .melt(
    id_vars = ['INSEE commune','Commune','dep'],
    var_name = "secteur", value_name = "emissions")
 .groupby(['secteur','dep'])
 .sum(numeric_only=True).reset_index()
 .sort_values(['dep','emissions'], ascending = False)
 .groupby('dep').head(1)
)
display(top_commune_dep)
```

## Combiner les données

### Travail préliminaire

Jusqu'à présent lorsque nous avons produit des statistiques descriptives,
celles-ci étaient univariées, c'est-à-dire que nous produisions de l'information
sur une variable mais nous ne la mettions pas en lien avec une autre. Pourtant,
on est rapidement amené à désirer expliquer certaines statistiques agrégées à partir
de caractéristiques issues d'une autre source de données. Cela implique
donc d'associer des jeux de données,
autrement dit de mettre en lien deux jeux de données
présentant le même niveau d'information. 

On appelle ceci faire un _merge_ ou un _join_. De manière illustrée,
ceci revient à effectuer ce type d'opération :

![](https://pandas.pydata.org/docs/_images/merging_concat_axis1_inner.png)


Avant de faire ceci, il est néanmoins nécessaire de s'assurer que les variables
communes entre les bases de données présentent le bon niveau d'information. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 6: vérification des clés de jointure</h3>
```

On commence par vérifier les dimensions des `DataFrames` et la structure de certaines variables clés.
En l'occurrence, les variables fondamentales pour lier nos données sont les variables communales.
Ici, on a deux variables géographiques: un code commune et un nom de commune. 

1. Vérifier les dimensions des _DataFrames_.

2. Identifier dans `filosofi` les noms de communes qui correspondent à plusieurs codes communes et sélectionner leurs codes. En d'autres termes, identifier les `LIBGEO` tels qu'il existe des doublons de `CODGEO` et les stocker dans un vecteur `x` (conseil: faire attention à l'index de `x`).

On se focalise temporairement sur les observations où le libellé comporte plus de deux codes communes différents

* _Question 3_. Regarder dans `filosofi` ces observations.

* _Question 4_. Pour mieux y voir, réordonner la base obtenue par order alphabétique.

* _Question 5_. Déterminer la taille moyenne (variable nombre de personnes: `NBPERSMENFISC16`) et quelques statistiques descriptives de ces données.
Comparer aux mêmes statistiques sur les données où libellés et codes communes coïncident.

* _Question 6_. Vérifier les grandes villes (plus de 100 000 personnes),
la proportion de villes pour lesquelles un même nom est associé à différents codes commune.

* _Question 7_. Vérifier dans `filosofi` les villes dont le libellé est égal à Montreuil.
Vérifier également celles qui contiennent le terme _'Saint-Denis'_.

```{=html}
</div>
```
:::

```{python}
#| output: false

# Question 1
print(emissions.shape)
print(filosofi.shape)
```

```{python}
#| output: false

# Question 2
filosofi = filosofi.reset_index(drop=False)
doublons = filosofi.groupby('LIBGEO').count()['CODGEO']
doublons = doublons.loc[doublons>1]
doublons = doublons.reset_index()
doublons
```

```{python}
#| output: false
# Question 3
filosofi.loc[
  filosofi['LIBGEO'].isin(doublons['LIBGEO'])
  ]
```

```{python}
#| output: false
# Question 4
filosofi.loc[
  filosofi['LIBGEO'].isin(doublons['LIBGEO'])
  ].sort_values('LIBGEO')
```

```{python}
#| output: false
# Question 5
print(10*"--" + "Communes dupliquées" + 10*"--")
print(
  filosofi.loc[
    filosofi['LIBGEO'].isin(doublons['LIBGEO']), 'NBPERSMENFISC16'
    ].describe()
  )
print(10*"--" + "Communes non dupliquées" + 10*"--")
print(
  filosofi.loc[
    ~filosofi['LIBGEO'].isin(doublons['LIBGEO']), 'NBPERSMENFISC16'
    ].describe()
  )
```

```{python}
#| output: false
# Question 6
emissions_big_city = filosofi.loc[filosofi['NBPERSMENFISC16']>100000].copy()
emissions_big_city['probleme'] = emissions_big_city['LIBGEO'].isin(doublons['LIBGEO'])
emissions_big_city['probleme'].mean() 
emissions_big_city[emissions_big_city['probleme']]
print(100*emissions_big_city['probleme'].mean()) #8,33 %
```

```{python}
#| output: false
# Question 7
filosofi[filosofi['LIBGEO'] == 'Montreuil']
filosofi[filosofi['LIBGEO'].str.contains('Saint-Denis')].head(10)
```

Ce petit exercice permet de se rassurer car les libellés dupliqués
sont en fait des noms de commune identiques mais qui ne sont pas dans le même département.
Il ne s'agit donc pas d'observations dupliquées.
On peut donc se fier aux codes communes, qui eux sont uniques.


### Associer des données


Une information que l'on cherche à obtenir s'obtient de moins en moins à partir d'une unique base de données. Il devient commun de devoir combiner des données issues de sources différentes. 

Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation où une information permet d'apparier de manière exacte deux bases de données (autrement nous serions dans une situation, beaucoup plus complexe, d'appariement flou). La situation typique est l'appariement entre deux sources de données selon un identifiant individuel ou un identifiant de code commune, ce qui est notre cas.

Il est recommandé de lire [ce guide assez complet sur la question des jointures avec R](https://www.book.utilitr.org/03_fiches_thematiques/fiche_joindre_donnees) qui donne des recommandations également utiles en `Python`.

Dans le langage courant du statisticien,
on utilise de manière indifférente les termes *merge* ou *join*. Le deuxième terme provient de la syntaxe `SQL`.
Quand on fait du `Pandas`, on utilise plutôt la commande _merge_. 


![](pandas_join.png)


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 7: Calculer l'empreinte carbone par habitant</h3>
```

En premier lieu, on va calculer l'empreinte carbone de chaque commune. 


1. Créer une variable `emissions` qui correspond aux émissions totales d'une commune

2. Faire une jointure à gauche entre les données d'émissions et les données de cadrage[^notebiais].

3. Calculer l'empreinte carbone (émissions totales / population). 

A ce stade nous pourrions avoir envie d'aller vers la modélisation pour essayer d'expliquer
les déterminants de l'empreinte carbone à partir de variables communales. 
Une approche inférentielle nécessite néanmoins pour être pertinente de
vérifier en amont des statistiques descriptives.

4. Sortir un histogramme en niveau puis en log de l'empreinte carbone communale.

Avec une meilleure compréhension de nos données, nous nous rapprochons
de la statistique inférentielle. Néanmoins, nous avons jusqu'à présent
construit des statistiques univariées mais n'avons pas cherché à comprendre
les résultats en regardant le lien avec d'autres variables. 
Cela nous amène vers la statistique bivariée, notamment l'analyse des corrélations. 
Ce travail est important puisque toute modélisation ultérieure consistera à 
raffiner l'analyse des corrélations pour tenir compte des corrélations croisées
entre multiples facteurs. On propose ici de faire cette analyse
de manière minimale. 


5. Regarder la corrélation entre les variables de cadrage et l'empreinte carbone. Certaines variables semblent-elles pouvoir potentiellement influer sur l'empreinte carbone ?

```{=html}
</div>
```
:::

[^notebiais]: Idéalement, il serait nécessaire de s'assurer que cette jointure n'introduit
pas de biais. En effet, comme nos années de référence ne sont pas forcément identiques,
il peut y avoir un _mismatch_ entre nos deux sources. Le TP étant déjà long, nous n'allons pas dans cette voie.
Les lecteurs intéressés pourront effectuer une telle analyse en exercice supplémentaire.


```{python}
#| output: false
# Question 1

emissions['emissions'] = emissions.sum(axis = 1, numeric_only = True)
```

```{python}
#| output: false
# Question 2

emissions_merged = (
    emissions.reset_index()
    .merge(filosofi, left_on = "INSEE commune", right_on = "CODGEO")
)
```

```{python}
#| output: false
# Question 3

emissions_merged['empreinte'] = emissions_merged['emissions']/emissions_merged['NBPERSMENFISC16']
emissions_merged['empreinte'] = emissions_merged['empreinte'].astype(float)
```

```{python}
#| output: false
# Question 4

emissions_merged['empreinte'].plot(kind = 'hist')
np.log(emissions_merged['empreinte']).plot(kind = 'hist')
emissions_merged['empreinte'].describe()
```

A l'issue de la question 5, le graphique des corrélations est le suivant:

```{python}
#| output: false
# Question 5

correlation = emissions_merged.corr(numeric_only=True)['empreinte']
correlation = correlation.reset_index()
correlation = correlation.loc[~correlation['index'].isin(["empreinte","emissions"])]
correlation['empreinte'].nlargest(10)
```

```{python}
correlation.set_index("index")['empreinte'].plot(kind = "barh")
```


## Exercices bonus

Les plus rapides d'entre vous sont invités à aller un peu plus loin en s'entraînant avec des exercices bonus qui proviennent du  [site de Xavier Dupré](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3). 3 notebooks en lien avec `numpy` et `pandas` vous y sont proposés : 

1. Calcul Matriciel, Optimisation : [énoncé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/td2a_cenonce_session_2A.html) / [corrigé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/td2a_correction_session_2A.html)
2. DataFrame et Graphes : [énoncé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/td2a_cenonce_session_1.html) / [corrigé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/td2a_correction_session_1.html)
3. Pandas et itérateurs : [énoncé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/pandas_iterator.html) / [corrigé](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/pandas_iterator_correction.html)

