::: {.content-visible when-profile="fr"}
La première question nous permet de récupérer un premier jeu de données
:::

::: {.content-visible when-profile="en"}
The first question allows us to retrieve an initial dataset.
:::

```{python}
#| label: exercise2-api-education-q1
import requests

url_annuaire_education = "https://data.education.gouv.fr/api/explore/v2.1/catalog/datasets/fr-en-annuaire-education/records"

school_q1_exo2 = pd.DataFrame(
  requests
  .get(url_annuaire_education)
  .json()
  .get("results")
)

school_q1_exo2.head(2)
```

::: {.content-visible when-profile="fr"}
Néanmoins, on a deux problèmes : le nombre de lignes et le département d'intérêt. Essayons déjà avec la question 2 de changer ce dernier.
:::

::: {.content-visible when-profile="en"}
However, there are two issues: the number of rows and the department of interest. Let’s first address the latter with question 2.
:::

```{python}
#| label: exercise2-api-education-q2
url_31_limite10 = "https://data.education.gouv.fr/api/explore/v2.1/catalog/datasets/fr-en-annuaire-education/records?where=code_departement%20like%20%22031%22"

school_q2_exo2 = pd.DataFrame(
  requests
  .get(url_31_limite10)
  .json()
  .get("results")
)
school_q2_exo2.head()
```

::: {.content-visible when-profile="fr"}
C'est mieux, mais nous avons toujours seulement 10 observations. Si on essaie d'ajuster le nombre de lignes (question 3), on obtient le retour suivant de l'API :
:::

::: {.content-visible when-profile="en"}
This is better, but we still only have 10 observations. If we try to adjust the number of rows (question 3), we get the following response from the API:
:::



```{python}
url_31_limite200 = "https://data.education.gouv.fr/api/explore/v2.1/catalog/datasets/fr-en-annuaire-education/records?where=code_departement%20like%20%22031%22&limit=200"

requests.get(url_31_limite200).content
```


::: {.content-visible when-profile="fr"}
Essayons avec des données plus exhaustives : le fichier brut sur `data.gouv`. Comme on peut le voir dans les métadonnées, on sait qu'on a plus de 1000 écoles dont on peut récupérer des données, mais qu'on en a ici extrait seulement 20. Le champ `next` nous donne directement l'URL à utiliser pour récupérer les 20 pages suivantes : c'est grâce à lui qu'on a une chance de récupérer toutes nos données d'intérêt.
:::

::: {.content-visible when-profile="en"}
Let’s try using more comprehensive data: the raw file on `data.gouv`. As seen in the metadata, we know there are over 1,000 schools for which data can be retrieved, but only 20 have been extracted here. The `next` field directly provides the URL to fetch the next 20 pages: this is how we can ensure we retrieve all our data of interest.
:::


```{python}
#| label: exercise2-api-datagouv
url_api_datagouv = "https://tabular-api.data.gouv.fr/api/resources/b22f04bf-64a8-495d-b8bb-d84dbc4c7983/data/?Code_departement__exact=031"

call_api_datagouv = requests.get(url_api_datagouv).json()
```

::: {.content-visible when-profile="fr"}
La partie intéressante pour automatiser la récupération de nos données est la clé `links` du JSON :
:::

::: {.content-visible when-profile="en"}
The key part for automating the retrieval of our data is the `links` key in the JSON:
:::

```{python}
call_api_datagouv.get('links')
```

::: {.content-visible when-profile="fr"}
En bouclant sur celui-ci pour parcourir la liste des URL accessibles, on peut récupérer des données. Comme le code d'automatisation est assez fastidieux à écrire, le voici :
:::

::: {.content-visible when-profile="en"}
By looping over it to traverse the list of accessible URLs, we can retrieve the data. Since the automation code is rather tedious to write, here it is:
:::

```{python}
#| echo: true
#| output: false
#| label: exercise2-api-tabular
import requests
import pandas as pd

# Initialize the initial API URL
url_api_datagouv = "https://tabular-api.data.gouv.fr/api/resources/b22f04bf-64a8-495d-b8bb-d84dbc4c7983/data/?Code_departement__exact=031&page_size=50"

# Initialize an empty list to store all data entries
all_data = []

# Initialize the URL for pagination
current_url = url_api_datagouv

# Loop until there is no next page
while current_url:
    try:
        # Make a GET request to the current URL
        response = requests.get(current_url)
        response.raise_for_status()  # Raise an exception for HTTP errors

        # Parse the JSON response
        json_response = response.json()

        # Extract data and append to the all_data list
        page_data = json_response.get('data', [])
        all_data.extend(page_data)
        print(f"Fetched {len(page_data)} records from {current_url}")

        # Get the next page URL
        links = json_response.get('links', {})
        current_url = links.get('next')  # This will be None if there's no next page

    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
        break

```

::: {.content-visible when-profile="fr"}
Le _DataFrame_ obtenu est le suivant:
:::

::: {.content-visible when-profile="en"}
The resulting _DataFrame_ is as follows:
:::


```{python}
#| echo: true
schools_dep31 = pd.DataFrame(all_data)
schools_dep31.head()
```

::: {.content-visible when-profile="fr"}
On peut fusionner ces nouvelles données avec nos données précédentes pour enrichir celles-ci. Pour faire une production fiable, il faudrait faire attention aux écoles qui ne s'apparient pas, mais ce n'est pas grave pour cette série d'exercices.
:::

::: {.content-visible when-profile="en"}
We can merge this new data with our previous dataset to enrich it. For reliable production, care should be taken with schools that do not match, but this is not critical for this series of exercises.
:::


```{python}
#| echo: true
#| label: exercise2-bpe-enriched
bpe_enriched = bpe.merge(
  schools_dep31,
  left_on = "SIRET",
  right_on = "SIREN_SIRET"
)
bpe_enriched.head(2)
```

::: {.content-visible when-profile="fr"}
Cela nous donne des données enrichies de nouvelles caractéristiques sur les établissements. Il y a des coordonnées géographiques dans celles-ci, mais nous allons faire comme s'il n'y en avait pas pour réutiliser notre API de géolocalisation.
:::

::: {.content-visible when-profile="en"}
This provides us with data enriched with new characteristics about the institutions. Although there are geographic coordinates in the dataset, we will pretend there aren't to reuse our geolocation API.
:::
