---
title: "R√©cup√©rer des donn√©es avec des API depuis Python"
date: 2023-07-08T13:00:00Z
draft: false
weight: 80
slug: api
type: book
tags:
  - API
  - JSON
  - openfood
  - Exercice
  - Manipulation
categories:
  - Exercice
  - Manipulation
description: |
  Les __API__ (_Application Programming Interface_) sont un mode d'acc√®s aux
  donn√©es en expansion. Gr√¢ce aux API, l'automatisation de scripts
  est facilit√©e puisqu'il n'est plus n√©cessaire de stocker un fichier,
  et g√©rer ses diff√©rentes versions, mais uniquement de requ√™ter une base
  et laisser au producteur de donn√©es le soin de g√©rer les mises √† jour de
  la base.  
image: map_buffer.png
echo: false
---

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/manipulation/04c_API_TP.qmd")
```
:::



__La partie utilisant l'API DVF n'est plus √† jour, elle sera mise √† jour prochainement.__

## Introduction : Qu'est-ce qu'une API ?  

### D√©finition

Pour expliquer le principe d'une API, je vais reprendre le d√©but de 
la fiche d√©di√©e dans la documentation collaborative
[utilitR](https://www.book.utilitr.org/api.html) que je recommande de lire :

> Une *Application Programming Interface* (ou API) est une interface de programmation qui permet d‚Äôutiliser une application existante pour restituer des donn√©es. Le terme d‚ÄôAPI peut √™tre para√Ætre intimidant, mais il s‚Äôagit simplement d‚Äôune fa√ßon de mettre √† disposition des donn√©es : plut√¥t que de laisser l‚Äôutilisateur consulter directement des bases de donn√©es (souvent volumineuses et complexes), l‚ÄôAPI lui propose de formuler une requ√™te qui est trait√©e par le serveur h√©bergeant la base de donn√©es, puis de recevoir des donn√©es en r√©ponse √† sa requ√™te.
> 
> D‚Äôun point de vue informatique, une API est une porte d‚Äôentr√©e clairement identifi√©e par laquelle un logiciel offre des services √† d‚Äôautres logiciels (ou utilisateurs). L‚Äôobjectif d‚Äôune API est de fournir un point d‚Äôacc√®s √† une fonctionnalit√© qui soit facile √† utiliser et qui masque les d√©tails de la mise en oeuvre. Par exemple, l‚ÄôAPI Sirene permet de r√©cup√©rer la raison sociale d‚Äôune entreprise √† partir de son identifiant Siren en interrogeant le r√©f√©rentiel disponible sur Internet directement depuis un script R, sans avoir √† conna√Ætre tous les d√©tails du r√©pertoire Sirene.
>
> √Ä l‚ÄôInsee comme ailleurs, la connexion entre les bases de donn√©es pour les nouveaux projets tend √† se r√©aliser par des API. L‚Äôacc√®s √† des donn√©es par des API devient ainsi de plus en plus commun et est amen√© √† devenir une comp√©tence de base de tout utilisateur de donn√©es.
>
> [`utilitR`](https://www.book.utilitr.org/api.html)


### Avantages des API

A nouveau, citons la documentation [utilitR](https://www.book.utilitr.org/api.html) :

Les API pr√©sentent de multiples avantages :

> * Les API rendent les programmes plus reproductibles. En effet, gr√¢ce aux API, il est possible de mettre √† jour facilement les donn√©es utilis√©es par un programme si celles-ci √©voluent. Cette flexibilit√© accrue pour l‚Äôutilisateur √©vite au producteur de donn√©es d‚Äôavoir √† r√©aliser de multiples extractions, et r√©duit le probl√®me de la coexistence de versions diff√©rentes des donn√©es.
> * Gr√¢ce aux API, l‚Äôutilisateur peut extraire facilement une petite partie d‚Äôune base de donn√©es plus cons√©quente.
> * Les API permettent de mettre √† disposition des donn√©es tout en limitant le nombre de personnes ayant acc√®s aux bases de donn√©es elles-m√™mes.
> * Gr√¢ce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un acc√®s sp√©cifique pour les gros utilisateurs).
>
> [`utilitR`](https://www.book.utilitr.org/api.html)

L'utilisation accrue d'API dans le cadre de strat√©gies open-data est l'un
des piliers des 15 feuilles de route minist√©rielles
en mati√®re d‚Äôouverture, de circulation et de valorisation des donn√©es publiques.

### Utilisation des API

Citons encore une fois
la documentation [`utilitR`](https://www.book.utilitr.org/api.html) :

> Une API peut souvent √™tre utilis√©e de deux fa√ßons : par une interface Web, et par l‚Äôinterm√©diaire d‚Äôun logiciel (R, Python‚Ä¶). Par ailleurs, les API peuvent √™tre propos√©es avec un niveau de libert√© variable pour l‚Äôutilisateur :
> 
> * soit en libre acc√®s (l‚Äôutilisation n‚Äôest pas contr√¥l√©e et l‚Äôutilisateur peut utiliser le service comme bon lui semble)‚ÄØ;
> * soit via la g√©n√©ration d‚Äôun compte et d‚Äôun jeton d‚Äôacc√®s qui permettent de s√©curiser l‚Äôutilisation de l‚ÄôAPI et de limiter le nombre de requ√™tes.
>
> [`utilitR`](https://www.book.utilitr.org/api.html)

De nombreuses API n√©cessitent une authentification, c'est-√†-dire un 
compte utilisateur afin de pouvoir acc√©der aux donn√©es. 
Dans un premier temps, 
nous regarderons exclusivement les API ouvertes sans restriction d'acc√®s.  
Certains exercices et exemples permettront n√©anmoins d'essayer des API
avec restrictions d'acc√®s. 

## Requ√™ter une API

### Principe g√©n√©ral

> L‚Äôutilisation de l‚Äôinterface Web est utile dans une d√©marche exploratoire mais trouve rapidement ses limites, notamment lorsqu‚Äôon consulte r√©guli√®rement l‚ÄôAPI. L‚Äôutilisateur va rapidement se rendre compte qu‚Äôil est beaucoup plus commode d‚Äôutiliser une API via un logiciel de traitement pour automatiser la consultation ou pour r√©aliser du t√©l√©chargement de masse. De plus, l‚Äôinterface Web n‚Äôexiste pas syst√©matiquement pour toutes les API.
> 
> Le mode principal de consultation d‚Äôune API consiste √† adresser une requ√™te √† cette API via un logiciel adapt√© (R, Python, Java‚Ä¶). Comme pour l‚Äôutilisation d‚Äôune fonction, l‚Äôappel d‚Äôune API comprend des param√®tres qui sont d√©taill√©es dans la documentation de l‚ÄôAPI. 
>
> [`utilitR`](https://www.book.utilitr.org/api.html)


Voici les √©l√©ments importants √† avoir en t√™te sur les requ√™tes (j'emprunte encore
√† [`utilitR`](https://www.book.utilitr.org/api.html)) :

* Le __point d‚Äôentr√©e__ d‚Äôun service offert par une API se pr√©sente sous la forme d‚Äôune URL (adresse web).
Chaque service propos√© par une API a sa propre URL. Par exemple, dans le cas de l‚ÄôOpenFood Facts,
l'URL √† utiliser pour obtenir des informations sur un produit particulier (l'identifiant `737628064502`)
est <https://world.openfoodfacts.org/api/v0/product/737628064502.json>
* Cette URL doit √™tre compl√©t√©e avec diff√©rents param√®tres qui pr√©cisent la requ√™te (par exemple l‚Äôidentifiant Siren). Ces param√®tres viennent s‚Äôajouter √† l‚ÄôURL, souvent √† la suite de `?`. Chaque service propos√© par une API a ses propres param√®tres, d√©taill√©s dans la documentation. 
* Lorsque l‚Äôutilisateur soumet sa requ√™te, l‚ÄôAPI lui renvoie une r√©ponse structur√©e contenant l‚Äôensemble des informations demand√©es. Le r√©sultat envoy√© par une API est majoritairement aux formats `JSON` ou `XML` (deux formats dans lesquels les informations sont hi√©rarchis√©es de mani√®re emboit√©e). Plus rarement, certains services proposent une information sous forme plate (de type csv).

Du fait de la dimension hi√©rarchique des formats `JSON` ou `XML`,
le r√©sultat n‚Äôest pas toujours facile √† r√©cup√©rer mais
`Python` propose d'excellents outils pour cela (meilleurs que ceux de `R`).
Certains packages, notamment `json`, facilitent l‚Äôextraction de champs d‚Äôune sortie d‚ÄôAPI.
Dans certains cas, des _packages_ sp√©cifiques √† une API ont √©t√© cr√©√©s pour simplifier l‚Äô√©criture d‚Äôune requ√™te ou la r√©cup√©ration du r√©sultat. Par exemple, le package
[`pynsee`](https://github.com/InseeFrLab/Py-Insee-Data/tree/master/pynsee)
propose des options qui seront retranscrites automatiquement dans l'URL de 
requ√™te pour faciliter le travail sur les donn√©es Insee. 

### Illustration avec une API de l'Ademe pour obtenir des diagnostics energ√©tiques

Le diagnostic de performance √©nerg√©tique (DPE)
renseigne sur la performance √©nerg√©tique d‚Äôun logement ou d‚Äôun b√¢timent,
en √©valuant sa consommation d‚Äô√©nergie et son impact en terme d‚Äô√©missions de gaz √† effet de serre.

Les donn√©es des performances √©nerg√©tiques des b√¢timents sont 
mises √† disposition par l'[Ademe](https://data.ademe.fr/datasets/dpe-france).
Comme ces donn√©es sont relativement
volumineuses, une API peut √™tre utile lorsqu'on ne s'int√©resse
qu'√† un sous-champ des donn√©es.
Une documentation et un espace de test de l'API sont disponibles
sur le site [API GOUV](https://api.gouv.fr/documentation/api_dpe_logements)^[La documentation est √©galement disponible [ici](https://koumoul.com/openapi-viewer/?proxy=false&hide-toolbar=true&url=https://koumoul.com/s/data-fair/api/v1/datasets/dpe-france/api-docs.json)].

Supposons qu'on d√©sire r√©cup√©rer une centaine de valeurs pour la commune
de Villieu-Loyes-Mollon dans l'Ain (code Insee 01450).

L'API comporte plusieurs points d'entr√©e. Globalement, la racine 
commune est :

> https://koumoul.com/data-fair/api/v1/datasets/dpe-france

Ensuite, en fonction de l'API d√©sir√©e, on va ajouter des √©l√©ments
√† cette racine. En l'occurrence, on va utiliser 
l'API `field` qui permet de r√©cup√©rer des lignes en fonction d'un
ou plusieurs crit√®res (pour nous, la localisation g√©ographique):

L'exemple donn√© dans la documentation technique est 

> GET https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/{field}

ce qui en `Python` se traduira par l'utilisation de la m√©thode `get` du
package `Request`
sur un url dont la structure est la suivante :

- il commencera par `https://koumoul.com/data-fair/api/v1/datasets/dpe-france/values/` ;
- il sera ensuite suivi par des param√®tres de recherche. Le champ `{field}` 
commence ainsi g√©n√©ralement par un `?` qui permet ensuite de sp√©cifier des param√®tres
sous la forme `nom_parameter=value`

A la lecture de la documentation, les premiers param√®tres qu'on d√©sire :

- Le nombre de pages, ce qui nous permet d'obtenir un certain nombre d'√©chos. On
va seulement r√©cup√©rer 10 pages ce qui correspond √† une centaine d'√©chos. On va
n√©anmoins pr√©ciser qu'on veut 100 √©chos
- Le format de sortie. On va privil√©gier le `JSON` qui est un format standard dans le
monde des API. `Python` offre beaucoup de flexibilit√© gr√¢ce √† l'un de
ses objets de base, √† savoir le dictionnaire (type `dict`), pour manipuler de tels 
fichiers
- Le code commune des donn√©es qu'on d√©sire obtenir. Comme on l'a √©voqu√©,
on va r√©cup√©rer les donn√©es dont le code commune est `01450`. D'apr√®s la doc, 
il convient de passer le code commune sous le format:
`code_insee_commune_actualise:{code_commune}`. Pour √©viter tout risque de 
mauvais formatage, on va utiliser `%3A` pour signifier `:`,  `%2A` pour signifier `*` et 
`%22` pour signifier `"`.
- D'autres param√®tres annexes, sugg√©r√©s par la documentation

Cela nous donne ainsi un URL dont la structure est la suivante :

```{python}
#| echo: true
code_commune = "01450"
size = 100
api_root = "https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines"
url_api = f"{api_root}?page=1&after=10&format=json&q_mode=simple&qs=code_insee_commune_actualise" + "%3A%22" + f"{code_commune}" + "%22" + f"&size={size}&select=" + "%2A&sampling=neighbors"
```

Si vous introduisez cet URL dans votre navigateur, vous devriez aboutir
sur un `JSON` non format√©[^1]. En `Python`,
on peut utiliser `requests` pour r√©cup√©rer les donn√©es[^2] :


[^1]: Le JSON est un format tr√®s appr√©ci√© dans le domaine du *big data*
car il permet d'empiler des donn√©es
qui ne sont pas compl√®tes. Il 
s'agit d'un des formats privil√©gi√©s du paradigme No-SQL pour lequel
cet [excellent cours](http://b3d.bdpedia.fr/) propose plus de d√©tails.

[^2]: Suivant les API, nous avons soit besoin de rien de plus si nous parvenons directement √† obtenir un json, soit devoir utiliser un *parser* comme `BeautifulSoup` dans le cas contraire. Ici, le JSON peut √™tre format√© relativement ais√©ment. 

```{python}
#| echo: true
import requests
import pandas as pd

req = requests.get(url_api)
wb = req.json()
```

Prenons par exemple les 1000 premiers caract√®res du r√©sultat, pour se donner
une id√©e du r√©sultat et se convaincre que notre filtre au niveau
communal est bien pass√© :

```{python}
#| echo: true
#| output: asis
print(req.content[:1000])
```


Ici, il n'est m√™me pas n√©cessaire en premi√®re approche 
d'utiliser le package `json`, l'information
√©tant d√©j√† tabul√©e dans l'√©cho renvoy√© (on a la m√™me information pour tous les pays):
On peut donc se contenter de `Pandas` pour transformer nos donn√©es en 
`DataFrame` et `Geopandas` pour convertir en donn√©es
g√©ographiques :

```{python}
#| echo: true
import pandas as pandas
import geopandas as gpd

def get_dpe_from_url(url):

    req = requests.get(url)
    wb = req.json()
    df = pd.json_normalize(wb["results"])

    dpe = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs = 4326)
    dpe = dpe.dropna(subset = ['longitude', 'latitude'])

    return dpe

dpe = get_dpe_from_url(url_api)
dpe.head(2)
```

Essayons de repr√©senter sur une carte ces DPE avec les
ann√©es de construction des logements.
Avec `Folium`, on obtient la carte interactive suivante :

```{python}
#| echo: true
import seaborn as sns
import folium

palette = sns.color_palette("coolwarm", 8)

def interactive_map_dpe(dpe):

    # convert in number
    dpe['color'] = [ord(dpe.iloc[i]['classe_consommation_energie'].lower()) - 96 for i in range(len(dpe))]
    dpe = dpe.loc[dpe['color']<=7]
    dpe['color'] = [palette.as_hex()[x] for x in dpe['color']]


    center = dpe[['latitude', 'longitude']].mean().values.tolist()
    sw = dpe[['latitude', 'longitude']].min().values.tolist()
    ne = dpe[['latitude', 'longitude']].max().values.tolist()

    m = folium.Map(location = center, tiles='OpenStreetMap')

    # I can add marker one by one on the map
    for i in range(0,len(dpe)):
        folium.Marker([dpe.iloc[i]['latitude'], dpe.iloc[i]['longitude']],
                    popup=f"Ann√©e de construction : {dpe.iloc[i]['annee_construction']}, <br>DPE : {dpe.iloc[i]['classe_consommation_energie']}",
                    icon=folium.Icon(color="black", icon="home", icon_color = dpe.iloc[i]['color'])).add_to(m)

    m.fit_bounds([sw, ne])

    return m

m = interactive_map_dpe(dpe)
```

```{python}
#| echo : true
# Afficher la carte
m
```

### Un catalogue incomplet d'API existantes

De plus en plus de sites mettent des API √† disposition des d√©veloppeurs et autres curieux. 

Pour en citer quelques-unes tr√®s connues : 

- `Twitter` <i class="fab fa-twitter"></i> : <https://dev.twitter.com/rest/public>
- `Facebook` <i class="fab fa-facebook"></i> : <https://developers.facebook.com/>
- `Instagram` <i class="fab fa-instagram"></i> : <https://www.instagram.com/developer/>
- `Spotify` <i class="fab fa-spotify"></i> : <https://developer.spotify.com/web-api/>

Cependant, il est int√©ressant de ne pas se restreindre √† celles-ci dont les 
donn√©es ne sont pas toujours les plus int√©ressantes. Beaucoup
de producteurs de donn√©es, priv√©s comme publics, mettent √† disposition 
leurs donn√©es sous forme d'API.

- [API gouv](https://api.gouv.fr/) : beaucoup d'API officielles de l'Etat fran√ßais 
et acc√®s √† de la documentation
- Insee : https://api.insee.fr/catalogue/ et [`pynsee`](https://github.com/InseeFrLab/Py-Insee-Data/tree/master/pynsee)
- P√¥le Emploi : https://www.emploi-store-dev.fr/portail-developpeur-cms/home.html
- SNCF : https://data.sncf.com/api
- Banque Mondiale : https://datahelpdesk.worldbank.org/knowledgebase/topics/125589

## L'API DVF : acc√©der √† des donn√©es de transactions immobili√®res simplement

‚ö†Ô∏è __Cette partie n√©cessite une mise √† jour pour privil√©gier l'API DVF du Cerema__.

Le site `DVF` (demandes de valeurs fonci√®res) permet de visualiser toutes les donn√©es relatives aux mutations √† titre on√©reux (ventes de maisons, appartements, garages...) r√©alis√©es durant les 5 derni√®res ann√©es.

Un site de visualisation est disponible sur <https://app.dvf.etalab.gouv.fr/>.

Ce site est tr√®s complet quand il s'agit de conna√Ætre le prix moyen au m√®tre
carr√© d'un quartier ou de comparer des r√©gions entre elles. 
L'API DVF permet d'aller plus loin afin de r√©cup√©rer les r√©sultats dans 
un logiciel de traitement de donn√©es. Elle a √©t√© r√©alis√©e par
[Christian Quest](https://github.com/cquest) et le code
source est disponible sur Github <a href="https://github.com/cquest/dvf_as_api" class="github"><i class="fab fa-github"></i></a>.

Les crit√®res de recherche sont les suivants :
- `code_commune` = code INSEE de la commune (ex: 94068)
- `section` = section cadastrale (ex: 94068000CQ)
- `numero_plan` = identifiant de la parcelle, (ex: 94068000CQ0110)
- `lat` + `lon` + `dist` (optionnel): pour une recherche g√©ographique, dist est par d√©faut un rayon de 500m
- `code_postal`

Les filtres de s√©lection compl√©mentaires :
- `nature_mutation` (Vente, etc)
- `type_local` (Maison, Appartement, Local, D√©pendance)

Les requ√™tes sont de la forme : `http://api.cquest.org/dvf?code_commune=29168`. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 1 : Exploiter l'API DVF</h3>
```

1. Rechercher toutes les transactions existantes dans DVF √† Plogoff (code commune `29168`, en Bretagne).
Afficher les cl√©s du JSON et en d√©duire le nombre de transactions r√©pertori√©es. 
2. N'afficher que les transactions portant sur des maisons. 
3. Utiliser l'[API geo](https://api.gouv.fr/documentation/api-geo) pour
r√©cup√©rer le d√©coupage communal de la ville de Plogoff.
4. Repr√©senter l'histogramme des prix de vente.

N'h√©sitez pas √† aller plus loin en jouant sur des variables de
groupes par exemple.

```{=html}
</div>
```
:::


```{python}
#| echo: false
#| include: false
#| eval: false

# Question 1
import json
import requests
import pandas as pd
data_immo = requests.get("http://api.cquest.org/dvf?code_commune=29168").json()
print(data_immo.keys())
print(data_immo["nb_resultats"])
ventes = pd.json_normalize(data_immo["resultats"])
ventes.head()
```

Le r√©sultat de la question 2 devrait
ressembler au `DataFrame` suivant :

```{python}
#| echo: false
#| eval: false

# Question 2
maisons = requests.get("http://api.cquest.org/dvf?code_commune=29168&type_local=Maison").json()
pd.json_normalize(maisons["resultats"])
```


```{python}
#| echo: false
#| eval: false

# Question 3
#!pip install geopandas
import geopandas as gpd
plgf = gpd.read_file("https://geo.api.gouv.fr/communes/29168?fields=nom,code,codesPostaux,codeDepartement,codeRegion,population&format=geojson&geometry=contour")
plgf.head()
```

L'histogramme des prix de vente (question 4) aura l'aspect suivant :

```{python}
#| echo: false
#| include: false
#| eval: false

# Question 4
p = ventes["valeur_fonciere"].plot(kind = "hist")
p
```


On va faire une carte des ventes en affichant le prix de l'achat.
La cartographie r√©active sera pr√©sent√©e dans les chapitres
consacr√©s √† la visualisation de donn√©es.

Supposons que le DataFrame des ventes s'appelle `ventes`. Il faut d'abord le
convertir
en objet `geopandas`. 

```{python}
#| eval: false

ventes = ventes.dropna(subset = ['lat','lon'])
ventes = gpd.GeoDataFrame(ventes, geometry=gpd.points_from_xy(ventes.lon, ventes.lat))
ventes
```

Avant de faire une carte, on va convertir 
les limites de la commune de Plogoff en geoJSON pour faciliter 
sa repr√©sentation avec `folium`
([voir la doc `geopandas` √† ce propos](https://geopandas.readthedocs.io/en/latest/gallery/polygon_plotting_with_folium.html#Add-polygons-to-map)):

```{python}
#| eval: false
geo_j = plgf.to_json()
```

Pour repr√©senter graphiquement, on peut utiliser le code suivant (essayez de
le comprendre et pas uniquement de l'ex√©cuter). 

```{python}
#| output: hide
#| eval: false
import folium
import numpy as np

ventes['map_color'] = pd.qcut(ventes['valeur_fonciere'], [0,0.8,1], labels = ['lightblue','red'])
ventes['icon'] = np.where(ventes['type_local']== 'Maison', "home", "")
ventes['num_voie_clean'] = np.where(ventes['numero_voie'].isnull(), "", ventes['numero_voie'])
ventes['text'] = ventes.apply(lambda s: "Adresse: {num} {voie} <br>Vente en {annee} <br>Prix {prix:.0f} ‚Ç¨".format(
                        num = s['num_voie_clean'],
                        voie = s["voie"],
                        annee = s['date_mutation'].split("-")[0],
                        prix = s["valeur_fonciere"]),
             axis=1)
             
center = ventes[['lat', 'lon']].mean().values.tolist()
sw = ventes[['lat', 'lon']].min().values.tolist()
ne = ventes[['lat', 'lon']].max().values.tolist()

m = folium.Map(location = center, tiles='OpenStreetMap')

# I can add marker one by one on the map
for i in range(0,len(ventes)):
    folium.Marker([ventes.iloc[i]['lat'], ventes.iloc[i]['lon']],
                  popup=ventes.iloc[i]['text'],
                  icon=folium.Icon(color=ventes.iloc[i]['map_color'], icon=ventes.iloc[i]['icon'])).add_to(m)

m.fit_bounds([sw, ne])
```


```{python}
#| echo : true
#| eval: false
# Afficher la carte
m
```



## G√©ocoder des donn√©es gr√¢ce aux API officielles

Pour pouvoir faire cet exercice 

```{python}
#| output: false
#| echo: true
!pip install xlrd
```


Jusqu'√† pr√©sent, nous avons travaill√© sur des donn√©es o√π la dimension
g√©ographique √©tait d√©j√† pr√©sente ou relativement facile √† int√©grer. 

Ce cas id√©al ne se rencontre pas n√©cessairement dans la pratique. 
On dispose parfois de localisations plus ou moins pr√©cises et plus ou
moins bien formatt√©es pour d√©terminer la localisation de certains
lieux. 

Depuis quelques ann√©es, un service officiel de g√©ocodage a √©t√© mis en place.
Celui-ci est gratuit et permet de mani√®re efficace de coder des adresses
√† partir d'une API. Cette API, connue sous le
nom de la __Base d'Adresses Nationale (BAN)__ a b√©n√©fici√© de la mise en commun de donn√©es de plusieurs
acteurs (collectivit√©s locales, Poste) et de comp√©tences d'acteurs
comme Etalab. La documentation de celle-ci est disponible √† l'adresse
<https://api.gouv.fr/les-api/base-adresse-nationale>.


Pour illustrer la mani√®re de g√©ocoder des donn√©es avec `Python`, nous
allons partir de la base
[des r√©sultats des auto-√©coles √† l'examen du permis sur l'ann√©e 2018](https://www.data.gouv.fr/fr/datasets/taux-de-reussite-auto-ecole-par-auto-ecole-en-2018/).

Ces donn√©es n√©cessitent un petit peu de travail pour √™tre propres √† une 
analyse statistique.
Apr√®s avoir renomm√© les colonnes, nous n'allons conserver que
les informations relatives au permis B (permis voiture classique) et
les auto-√©coles ayant pr√©sent√© au moins 20 personnes √† l'examen. 


```{python}
#| echo: true
import pandas as pd
import xlrd
import geopandas as gpd

df = pd.read_excel("https://www.data.gouv.fr/fr/datasets/r/d4b6b072-8a7d-4e04-a029-8cdbdbaf36a5", header = [0,1])

# Le Excel a des noms de colonne emboit√©es, 
# on nettoie
index_0 = ["" if df.columns[i][0].startswith("Unnamed") else df.columns[i][0] for i in range(len(df.columns))]
index_1 = [df.columns[i][1] for i in range(len(df.columns))]
keep_index = [True if el in ('', "B") else False for el in index_0] 
cols = [index_0[i] + " " + index_1[i].replace("+", "_") for i in range(len(df.columns))]
df.columns = cols
df = df.loc[:, keep_index]
df.columns = df.columns.str.replace("(^ |¬∞)", "", regex = True).str.replace(" ", "_")

# On garde le sous-√©chantillon d'int√©r√™t
df = df.dropna(subset = ['B_NB'])
df = df.loc[~df["B_NB"].astype(str).str.contains("(\%|\.)"),:]
df['B_NB'] = df['B_NB'].astype(int)
df['B_TR'] = df['B_TR'].str.replace(",", ".").str.replace("%","").astype(float)
df = df.loc[df["B_NB"]>20]
```

```{python}
#| echo: false
#| output: asis
moyenne_nationale = (df['B_NB']*df['B_TR']).sum()/df['B_NB'].sum()
print("Sur cet √©chantillon, le taux de r√©ussite moyen √©tait, en 2018, de {:.2%}".format(moyenne_nationale/100))
```

Nos informations g√©ographiques prennent la forme suivante : 

```{python}
#| echo: true
df.loc[:,['Adresse','CP','Ville']].head(5)
```

Autrement dit, nous disposons d'une adresse, d'un code postal et d'un nom
de ville. Ces informations peuvent servir √† faire une recherche
sur la localisation d'une auto-√©cole puis, √©ventuellement, de se restreindre
√† un sous-√©chantillon. 

### Utiliser l'API BAN

La [documentation officielle de l'API](https://adresse.data.gouv.fr/api-doc/adresse)
propose un certain nombre d'exemples de mani√®re de g√©olocaliser des donn√©es. 
Dans notre situation, deux points d'entr√©e paraissent int√©ressants:

* __L'API `/search/`__ qui repr√©sente un point d'entr√©e avec des URL de la forme
`https://api-adresse.data.gouv.fr/search/?q=\<adresse\>&postcode=\<codepostal\>&limit=1`
* __L'API `/search/csv`__ qui prend un CSV en entr√©e et retourne ce m√™me CSV avec
les observations g√©ocod√©es. La requ√™te prend la forme suivante, en apparence
moins simple √† mettre en oeuvre : 
`curl -X POST -F data=@search.csv -F columns=adresse -F columns=postcode https://api-adresse.data.gouv.fr/search/csv/`

La tentation serait forte d'utiliser la premi√®re m√©thode avec une boucle sur les
lignes de notre `DataFrame` pour g√©ocoder l'ensemble de notre jeu de donn√©es. 
Cela serait n√©anmoins une mauvaise id√©e car les communications entre notre
session `Python` et les serveurs de l'API seraient beaucoup trop nombreuses
pour offrir des performances satisfaisantes. 

Pour vous en convaincre, vous pouvez ex√©cuter le code suivant sur un petit
√©chantillon de donn√©es (par exemple 100 comme ici) et remarquer que le temps
d'ex√©cution est assez important

```{python}
#| echo: true
#| eval: false
import time

dfgeoloc = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(","," "))
dfgeoloc['url'] = (dfgeoloc['Adresse'] + "+" + dfgeoloc['Ville'].str.replace("-",'+')).str.replace(" ","+")
dfgeoloc['url'] = 'https://api-adresse.data.gouv.fr/search/?q=' + dfgeoloc['url'] + "&postcode=" + df['CP'] + "&limit=1"
dfgeoloc = dfgeoloc.dropna()

start_time = time.time()

def get_geoloc(i):
    print(i)
    return gpd.GeoDataFrame.from_features(requests.get(dfgeoloc['url'].iloc[i]).json()['features'])

local = [get_geoloc(i) for i in range(len(dfgeoloc.head(10)))]
print("--- %s seconds ---" % (time.time() - start_time))
```

Comme l'indique la documentation, si on d√©sire industrialiser notre processus
de g√©ocodage, on va privil√©gier l'API CSV.

Pour obtenir une requ√™te `CURL` coh√©rente avec le format d√©sir√© par l'API
on va √† nouveau utiliser `Requests` mais cette fois avec des param√®tres
suppl√©mentaires:

* `data` va nous permettre de passer des param√®tres √† `CURL` (√©quivalents aux `-F`
de la requ√™te `CURL`) :
    + `columns`: Les colonnes utilis√©es pour localiser une donn√©e. En l'occurrence,
on utilise l'adresse et la ville (car les codes postaux n'√©tant pas uniques, 
un m√™me nom de voirie peut se trouver dans plusieurs villes partageant le m√™me
code postal) ;
    + `postcode`: Le code postal de la ville. Id√©alement nous aurions utilis√©
le code Insee mais nous ne l'avons pas dans nos donn√©es ;
    + `result_columns`: on restreint les donn√©es √©chang√©es avec l'API aux 
colonnes qui nous int√©ressent. Cela permet d'acc√©l√©rer les processus (on
√©change moins de donn√©es) et de r√©duire l'impact carbone de notre activit√©
(moins de transferts = moins d'√©nergie d√©pens√©e). En l'occurrence, on ne ressort
que les donn√©es g√©olocalis√©es et un score de confiance en la g√©olocalisation ;
* `files`: permet d'envoyer un fichier via `CURL`.

Les donn√©es sont r√©cup√©r√©es avec `request.post`. Comme il s'agit d'une
cha√Æne de caract√®re, nous pouvons directement la lire avec `Pandas` en 
utilisant `io.StringIO` pour √©viter d'√©crire des donn√©es interm√©diaires.

Le nombre d'√©chos semblant √™tre limit√©, il
est propos√© de proc√©der par morceaux
(ici, le jeu de donn√©es est d√©coup√© en 5 morceaux).


```{python}
#| echo: true
#| output: false
import requests
import io   
import numpy as np
import time

params = {
    'columns': ['Adresse', 'Ville'],
    'postcode': 'CP',
    'result_columns': ['result_score', 'latitude', 'longitude'],
}

df[['Adresse','CP','Ville']] = df.loc[:, ['Adresse','CP','Ville']].apply(lambda s: s.str.lower().str.replace(","," "))

def geoloc_chunk(x):
    dfgeoloc = x.loc[:, ['Adresse','CP','Ville']]
    dfgeoloc.to_csv("datageocodage.csv", index=False)
    response = requests.post('https://api-adresse.data.gouv.fr/search/csv/', data=params, files={'data': ('datageocodage.csv', open('datageocodage.csv', 'rb'))})
    geoloc = pd.read_csv(io.StringIO(response.text), dtype = {'CP': 'str'})
    return geoloc
    
start_time = time.time()
geodata = [geoloc_chunk(dd) for dd in np.array_split(df, 10)]
print("--- %s seconds ---" % (time.time() - start_time))
```

Cette m√©thode est beaucoup plus rapide et permet ainsi, une fois retourn√© √† nos
donn√©es initiales, d'avoir un jeu de donn√©es g√©olocalis√©.

```{python}
#| echo: true

# Retour aux donn√©es initiales
geodata = pd.concat(geodata, ignore_index = True)
df_xy = df.merge(geodata, on = ['Adresse','CP','Ville'])
df_xy = df_xy.dropna(subset = ['latitude','longitude'])

# Mise en forme pour le tooltip
df_xy['text'] = (
    df_xy['Raison_Sociale'] + '<br>' +
    df_xy['Adresse'] + '<br>' +
    df_xy['Ville'] + '<br>Nombre de candidats:' + df_xy['B_NB'].astype(str)
)
df_xy.filter(
    ['Raison_Sociale','Adresse','CP','Ville','latitude','longitude'],
    axis = "columns"
).sample(10)
```

Il ne reste plus qu'√† utiliser `Geopandas`
et nous serons en mesure de faire une carte des localisations des auto-√©coles :

```{python}
#| echo: true

# Transforme en geopandas pour les cartes
import geopandas as gpd
dfgeo = gpd.GeoDataFrame(
    df_xy,
    geometry = gpd.points_from_xy(df_xy.longitude, df_xy.latitude)
)
```

Nous allons repr√©senter les stations dans l'Essonne avec un zoom initialement
sur les villes de Massy et Palaiseau. Le code est le suivant :

```{python}
#| echo: true
#| output: false

import folium

# Repr√©senter toutes les auto√©coles de l'Essonne
df_91 = df_xy.loc[df_xy["Dept"] == "091"]

# Centrer la vue initiale sur Massy-Palaiseau
df_pal = df_xy.loc[df_xy['Ville'].isin(["massy", "palaiseau"])]
center = df_pal[['latitude', 'longitude']].mean().values.tolist()
sw = df_pal[['latitude', 'longitude']].min().values.tolist()
ne = df_pal[['latitude', 'longitude']].max().values.tolist()

m = folium.Map(location = center, tiles='OpenStreetMap')

# I can add marker one by one on the map
for i in range(0,len(df_91)):
    folium.Marker([df_91.iloc[i]['latitude'], df_91.iloc[i]['longitude']],
                  popup=df_91.iloc[i]['text'],
                  icon=folium.Icon(icon='car', prefix='fa')).add_to(m)

m.fit_bounds([sw, ne])
```

Ce qui permet d'obtenir la carte:

```{python}
#| echo : true
# Afficher la carte
m
```

Vous pouvez aller plus loin avec l'exercice suivant.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 2 : Quelles sont les auto-√©coles les plus proches de chez moi ?</h3>
```

On va supposer que vous cherchez, dans un rayon donn√© autour d'un centre ville,
les auto-√©coles disponibles.

<details>
<summary>
Fonction n√©cessaire pour cet exercice
</summary>
Cet exercice n√©cessite une fonction pour cr√©er un cercle
autour d'un point
(source [ici](https://gis.stackexchange.com/questions/289044/creating-buffer-circle-x-kilometers-from-point-using-python/289923)).
La voici :

```python
from functools import partial
import pyproj
from shapely.ops import transform
from shapely.geometry import Point

proj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')


def geodesic_point_buffer(lat, lon, km):
    # Azimuthal equidistant projection
    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'
    project = partial(
        pyproj.transform,
        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),
        proj_wgs84)
    buf = Point(0, 0).buffer(km * 1000)  # distance in metres
    return transform(project, buf).exterior.coords[:]
```
</details>


1. Pour commencer, utiliser l'[API Geo](https://geo.api.gouv.fr/decoupage-administratif)
pour la ville de Palaiseau.
2. Appliquer la fonction `geodesic_point_buffer` au centre ville de Palaiseau
3. Ne conserver que les auto-√©coles dans ce cercle et les ordonner 

__Si vous avez la r√©ponse √† la question 3, n'h√©sitez pas √† la soumettre sur `Github` afin que je compl√®te la correction__ üòâ !

```{=html}
</div>
```
:::


```{python}
from functools import partial
import pyproj
from shapely.ops import transform
from shapely.geometry import Point

proj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')


def geodesic_point_buffer(lat, lon, km):
    # Azimuthal equidistant projection
    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'
    project = partial(
        pyproj.transform,
        pyproj.Proj(aeqd_proj.format(lat=lat, lon=lon)),
        proj_wgs84)
    buf = Point(0, 0).buffer(km * 1000)  # distance in metres
    return transform(project, buf).exterior.coords[:]
```


```{python}
#| echo: false

# Cette fois on veut bien le centre, pas le contour
pal = gpd.read_file("https://geo.api.gouv.fr/communes?nom=Palaiseau&format=geojson")
```


```{python}
#| echo: false
b = geodesic_point_buffer(pal.loc[0].geometry.y, pal.loc[0].geometry.x, 10.0)
circle = pd.DataFrame(b, columns = ['y','x'])
circle = gpd.GeoDataFrame(circle, geometry=gpd.points_from_xy(circle.y, circle.x), crs = pal.crs)
```

Pour se convaincre, de notre cercle constitu√© lors de
la question 2, on peut repr√©senter une carte.
On a bien un cercle centr√© autour de Palaiseau :

```{python}
import matplotlib.pyplot as plt
import contextily as ctx

fig,ax = plt.subplots(figsize=(10, 10))
circle.to_crs("EPSG:3857").plot(ax = ax, color = 'red')
pal.to_crs("EPSG:3857").plot(ax = ax, color = 'green')
#ctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)
ax
```

```{python}
#| output: false
import matplotlib.pyplot as plt
import contextily as ctx

fig,ax = plt.subplots(figsize=(10, 10))
circle.to_crs("EPSG:3857").plot(ax = ax, color = 'red')
pal.to_crs("EPSG:3857").plot(ax = ax, color = 'green')
#ctx.add_basemap(ax, source = ctx.providers.Stamen.Toner)
ax

plt.savefig('map_buffer.png', bbox_inches='tight')
```


## Exercices suppl√©mentaires

### D√©couvrir l'API d'`OpenFoodFacts`

Pour vous aidez, vous pouvez regarder une exemple de structure du JSON ici :
<https://world.openfoodfacts.org/api/v0/product/3274080005003.json> en particulier la cat√©gorie `nutriments`.


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 3 : Retrouver des produits dans l'openfood facts üçï</h3>
```

Voici une liste de code-barres:
`3274080005003,  5449000000996, 8002270014901,
3228857000906, 3017620421006, 8712100325953`

Utiliser l'[API d'openfoodfacts](https://world.openfoodfacts.org/data)
(l'API, pas depuis le CSV !)
pour retrouver les produits correspondants
et leurs caract√©ristiques nutritionnelles.

Le panier para√Æt-il √©quilibr√© ? üç´

R√©cup√©rer l'URL d'une des images et l'afficher dans votre navigateur.

```{=html}
</div>
```
:::


```{python}
#| output: false

import json
import requests
import pandas as pd
```


```{python}
#| output: false

# Param√®tres n√©cessaires
df = pd.DataFrame([3274080005003,  5449000000996, 8002270014901,
3228857000906, 3017620421006, 8712100325953], columns = ['code_ean'])
nutri = ['energy_100g', 'nutriscore_grade', 'nova_group', 'fat_100g', 'saturated-fat_100g', 'carbohydrates_100g', 'sugars_100g', 'salt_100g', 'fiber_100g', 'proteins_100g', 'calcium_100g', 'iron_100g', 'sodium_100g', 'cholesterol_100g']
cols_api = ['code', 'product_name', 'categories', 'categories_tags'] + ["nutriments.{}".format(i) for i in nutri]
```

```{python}
#| output: false

def get_products_api(barcode, col = cols_api):
    url = "https://world.openfoodfacts.org/api/v0/product/{}.json".format(str(barcode))
    #print(url)
    res = requests.get(url)
    results = res.json()
    product = results["product"]
    openfood = pd.json_normalize(product)
    openfood = openfood[list(set(col) & set(openfood.columns))]
    return openfood
  
# Exemple
get_products_api(3274080005003, col=["code","nutriments.fat_100g"])
```


```{python}
#| output: false

openfood = [get_products_api(barcode) for barcode in df['code_ean'].dropna().astype(str).unique()]
openfood = pd.concat(openfood)
openfood.head(10)
```

Voici par exemple la photo du produit ayant le code-barre `5449000000996`. Vous le reconnaissez ?


```{python}
#| echo: false

url_image = get_products_api(5449000000996, col = ["image_front_small_url"])["image_front_small_url"].iloc[0]
```

```{python}
#| echo: false
#| output: asis
print("![]({url})".format(url = url_image))
```

