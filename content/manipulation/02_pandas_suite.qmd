---
title: "Manipuler des données avec Pandas"
title-en: "Data wrangling with Pandas"
tags:
  - Pandas
  - Pollution
  - Ademe
  - Tutoriel
  - Manipulation
categories:
  - Tutoriel
  - Manipulation
description: |
  Le chapitre d'introduction à `Pandas` a permis de présenter le principe de données organisées sous une forme de _DataFrame_ et la praticité de l'écosystème `Pandas` pour effectuer des opérations simples sur un jeu de données. Ce chapitre consolide ces principes en présentant deux types de traitements classiques de la boite à outil des _data scientists_ : statistiques par groupe et associations de données. 
description-en: |
  The introductory chapter to `Pandas` presented how data 
  were organized as _DataFrames_ and how the `Pandas` ecosystem
  can be useful to
  perform simple operations on datasets. This chapter consolidates these principles by introducing two classic types of operations from the data scientist's toolbox: group statistics and data merging.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/pandas_weight.webp
echo: false
links:
- icon: journal-text
  name: Documentation Pandas
  url: https://pandas.pydata.org/docs/
eval: false
---


{{< badges
    printMessage="true"
>}}


::: {.content-visible when-format="ipynb"}
{{warninglang}}
:::

::: {.content-visible when-profile="fr"}

:::: {.tip}
## Compétences à l'issue de ce chapitre

- Savoir construire des statistiques agrégées fines grâce aux méthodes de `Pandas` ;
- Savoir restructurer ses données et joindre plusieurs `DataFrames` ensemble ;
- Créer des tableaux attractifs pour communiquer sur des résultats agrégés ;
- Connaître les limites de `Pandas` et les _packages_ alternatifs.

::::

:::

::: {.content-visible when-profile="en"}

:::: {.tip}
## Skills to be acquired by the end of this chapter

- How to construct fine aggregate statistics using `Pandas` methods;
- Restructure your data and join several DataFrames together;
- Create attractive tables to communicate aggregated results;
- Know the limits of `Pandas` and alternative _packages_.

::::

:::



# Introduction

::: {.content-visible when-profile="fr"}


Le [chapitre d'introduction à `Pandas`](/content/manipulation/02_pandas_intro.qmd) a permis de présenter le principe de données organisées sous une forme de _DataFrame_ et la praticité de l'écosystème `Pandas` pour effectuer des opérations simples sur un jeu de données.

Il est rare de travailler exclusivement sur une source brute. Un jeu de données prend généralement de la valeur lorsqu'il est comparé à d'autres sources. Pour des chercheurs, cela permettra de contextualiser l'information présente dans une source en la comparant ou en l'associant à d'autres sources. Pour des _data scientists_ dans le secteur privé, il s'agira souvent d'associer des informations sur une même personne dans plusieurs bases clientes ou comparer les clients entre eux.

L'un des apports des outils modernes de _data science_, notamment `Pandas` est la simplicité par laquelle ils permettent de restructurer des sources pour travailler sur plusieurs données sur un projet. 
Ce chapitre consolide ainsi les principes vus précédemment en raffinant les traitements faits sur les données. Il va explorer principalement deux types d'opérations:

- les statistiques descriptives par groupe ;
- l'association de données par des caractéristiques communes.

Effectuer ce travail de manière simple, fiable et efficace est indispensable pour les _data scientists_ tant cette tâche est courante. Heureusement `Pandas` permet de faire cela très bien avec des données structurées. Nous verrons dans les prochains chapitres, mais aussi dans l'ensemble de la [partie sur le traitement des données textuelles](/content/nlp/index.qmd), comment faire avec des données moins structurées.

Grâce à ce travail, nous allons approfondir notre compréhension d'un phénomène réel par le biais de statistiques descriptives fines. Cela est une étape indispensable avant de basculer vers la [statistique inférentielle](https://fr.wikipedia.org/wiki/Inf%C3%A9rence_statistique#:~:text=L'inf%C3%A9rence%20statistique%20est%20l,%3A%20la%20probabilit%C3%A9%20d'erreur.), l'approche qui consiste à formaliser et généraliser des liens de corrélation ou de causalité entre des caractéristiques observées et un phénomène.  

:::: {.tip}
## Compétences à l'issue de ce chapitre

- Récupérer un jeu de données officiel de l'Insee ;
- Construire des statistiques descriptives par groupe et jongler entre les niveaux des données ;
- Associer des données (_reshape_, _merge_) pour leur donner plus de valeur ;
- Faire un beau tableau pour communiquer des statistiques descriptives.

::::

:::

::: {.content-visible when-profile="en"}
The [introductory chapter to `Pandas`](/content/manipulation/02_pandas_intro.qmd) presented the concept of data organized in the form of a _DataFrame_ and the practicality of the `Pandas` ecosystem for performing simple operations on a dataset.

It is rare to work exclusively on a raw source. A dataset generally gains value when compared to other sources. For researchers, this allows contextualizing the information present in one source by comparing or associating it with other sources. For data scientists in the private sector, it often involves linking information about the same person in multiple customer databases or comparing customers with each other.

One of the benefits of modern data science tools, especially `Pandas`, is the ease with which they allow restructuring sources to work on multiple datasets in a project. This chapter consolidates the principles previously seen by refining the data processing. It will mainly explore two types of operations:

- Group descriptive statistics;
- Data merging by common characteristics.

Performing this work simply, reliably, and efficiently is essential for data scientists as this task is common. Fortunately, `Pandas` handles this very well with structured data. In the following chapters, and also throughout the [section on text data processing](/content/nlp/index.qmd), we will see how to handle less structured data.

Through this work, we will deepen our understanding of a real world phenomenon through detailed descriptive statistics. This is an essential step before moving on to [inferential statistics](https://en.wikipedia.org/wiki/Statistical_inference), the approach that consists of formalizing and generalizing correlations or causal relationships between observed characteristics and a phenomenon.

:::: {.tip}
## Skills at the end of this chapter

- Retrieve an official dataset from Insee;
- Build group descriptive statistics and switch between data levels;
- Merge data (_reshape_, _merge_) to add value;
- Create a beautiful table to communicate descriptive statistics.

::::

:::

::: {.content-visible when-profile="fr"}

## Environnement

Le chapitre précédent utilisait quasi exclusivement la librairie `Pandas`. Nous allons dans ce chapitre utiliser d'autres _packages_ en complément de celui-ci. 

Comme expliqué ci-dessous, nous allons utiliser une librairie nommée `pynsee` pour récupérer les données de l'Insee utiles à enrichir notre jeu de données de l'Ademe. Cette librairie n'est pas installée par défaut dans `Python`. Avant de pouvoir l'utiliser,
il est nécessaire de l'installer, comme la librairie `great_tables` que nous verrons à la fin de ce chapitre:
:::

::: {.content-visible when-profile="en"}
## Environment

The previous chapter used almost exclusively the `Pandas` library. In this chapter, we will use other packages in addition to it.

As explained below, we will use a library called `pynsee` to retrieve Insee data useful for enriching our Ademe dataset. This library is not installed by default in `Python`. Before using it, it is necessary to install it, along with the `great_tables` library that we will see at the end of this chapter:
:::


```{python}
#| eval: false
#| echo: true
!pip install xlrd
!pip install pynsee
!pip install great_tables
```

::: {.content-visible when-profile="fr"}
L'instruction `!pip install <pkg>` est une manière de faire comprendre à `Jupyter`, le moteur d'exécution derrière les _notebooks_ que la commande qui suit (`pip install` ce `<pkg>`)
est une commande système, à exécuter hors de `Python` (dans le terminal par exemple pour un système `Linux`).  

Les premiers _packages_ indispensables pour démarrer ce chapitre sont les suivants:
:::

::: {.content-visible when-profile="en"}
The instruction `!pip install <pkg>` is a way to tell `Jupyter`, the execution engine behind notebooks, that the following command (`pip install <pkg>`) is a system command to be executed outside of `Python` (in the terminal, for example, for a `Linux` system).

The essential packages to start this chapter are as follows:
:::
```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pynsee
import pynsee.download
```

{{< include "02_pandas_intro/_reproducible.qmd" >}}

::: {.content-visible when-profile="fr"}

## Données utilisées

Ce tutoriel continue l'exploration du jeu de données du chapitre précédent:

* Les émissions de gaz à effet de serre estimées au niveau communal par l'ADEME. Le jeu de données est 
disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)
et requêtable directement dans `Python` avec
[cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) ;


Les problématiques d'enrichissement de données (association d'une source à une autre à partir de caractéristiques communes) seront présentées à partir de deux sources produites par l'Insee:

* Le 
[code officiel géographique](https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv),
un référentiel
produit par l'Insee utilisé pour identifier les communes à partir d'un code unique, contrairement au code postal ;
* Les données [_Filosofi_](https://www.insee.fr/fr/metadonnees/source/serie/s1172), une source sur les revenus des Français à une échelle spatiale fine construite par l'Insee à partir des déclarations fiscales et d'informations sur les prestations sociales. En l'occurrence, nous allons utiliser les niveaux de revenu et les populations[^poplegales] au niveau communal afin de les mettre en regard de nos données d'émissions.


[^poplegales]: Idéalement il serait plus cohérent, pour les données démographiques, d'utiliser les [populations légales](https://www.insee.fr/fr/information/2008354), issues du recensement. Néanmoins cette base n'est pas encore intégrée nativement dans la librairie `pynsee` que nous allons utiliser dans ce chapitre. Un exercice d'ouverture est proposé pour construire des agrégats de population à partir des jeux de données individuels anonymisés du recensement (les [fichiers détails](https://www.insee.fr/fr/information/2383306)).


Pour faciliter l'import de données Insee, il est recommandé d'utiliser le _package_
[`pynsee`](https://pynsee.readthedocs.io/en/latest/) qui simplifie l'accès aux principaux jeux de données
de l'Insee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)
ou via des API. 

:::: {.note}

Le _package_ `pynsee` comporte deux principaux points d'entrée :

- Les API de l'Insee, ce qui sera illustré dans le chapitre consacré.
- Quelques jeux de données directement issus du site web de
l'Insee ([insee.fr](https://www.insee.fr/fr/accueil))

Dans ce chapitre, nous allons exclusivement utiliser cette deuxième
approche. Cela se fera par le module `pynsee.download`.  

La liste des données disponibles depuis ce _package_ est [ici](https://inseefrlab.github.io/DoReMIFaSol/articles/donnees_dispo.html).
La fonction `download_file` attend un identifiant unique
pour savoir quelle base de données aller chercher et
restructurer depuis le
site [insee.fr](https://www.insee.fr/fr/accueil). 

<details>
<summary>
Connaître la liste des bases disponibles
</summary>

Pour connaître la liste des bases disponibles, vous
pouvez utiliser la fonction `meta = pynsee.get_file_list()`
après avoir fait `import pynsee`. 
Celle-ci renvoie un `DataFrame` dans lequel on peut
rechercher, par exemple grâce à une recherche
de mots-clefs : 

```{python}
#| echo: true
#| eval: false
import pynsee
meta = pynsee.get_file_list()
meta.loc[meta['label'].str.contains(r"Filosofi.*2016")]
```

Ici, `meta['label'].str.contains(r"Filosofi.*2016")` signifie:
"_`pandas` trouve moi tous les labels où sont contenus les termes Filosofi et 2016._"
 (`.*` signifiant "_peu m'importe le nombre de mots ou caractères entre_")

</details>

::::

:::

::: {.content-visible when-profile="en"}

## Data used

This tutorial continues the exploration of the dataset from the previous chapter:

* Greenhouse gas emissions estimated at the municipal level by ADEME. The dataset is available on [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_) and can be directly queried in Python with [this URL](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert);

The issues of data enrichment (associating one source with another based on common characteristics) will be presented using two sources produced by Insee:

* The [official geographic code](https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv), a reference produced by Insee used to identify municipalities with a unique code, unlike the postal code;
* The [_Filosofi_](https://www.insee.fr/fr/metadonnees/source/serie/s1172) data, a source on French income at a fine spatial scale constructed by Insee from tax returns and social benefit information. In this case, we will use income levels and populations[^poplegales] at the municipal level to compare them with our emissions data.

[^poplegales]: Ideally, it would be more coherent, for demographic data, to use the [legal populations](https://www.insee.fr/fr/information/2008354), from the census. However, this base is not yet natively integrated into the `pynsee` library that we will use in this chapter. An open exercise is proposed to construct population aggregates from anonymized individual census data (the [detailed files](https://www.insee.fr/fr/information/2383306)).

To facilitate the import of Insee data, it is recommended to use the [`pynsee`](https://pynsee.readthedocs.io/en/latest/) package, which simplifies access to the main Insee datasets available on the [insee.fr](https://www.insee.fr/fr/accueil) website or via APIs.

:::: {.note}

The `pynsee` package has two main entry points:

- The Insee APIs, which will be illustrated in the dedicated chapter.
- Some datasets directly from the Insee website ([insee.fr](https://www.insee.fr/fr/accueil))

In this chapter, we will exclusively use the second approach through the `pynsee.download` module.

The list of available data from this package is [here](https://inseefrlab.github.io/DoReMIFaSol/articles/donnees_dispo.html). The `download_file` function expects a unique identifier to know which database to fetch and restructure from the [insee.fr](https://www.insee.fr/fr/accueil) website.

<details>
<summary>
Knowing the list of available databases
</summary>

To know the list of available databases, you can use the `meta = pynsee.get_file_list()` function after importing `pynsee`. This returns a `DataFrame` in which you can search, for example, using a keyword search:

```{python}
#| echo: true
#| eval: false
import pynsee
meta = pynsee.get_file_list()
meta.loc[meta['label'].str.contains(r"Filosofi.*2016")]
```

Here, meta['label'].str.contains(r"Filosofi.*2016") means: "pandas find me all labels containing the terms Filosofi and 2016." (.* means "no matter the number of words or characters in between").

</details>

::::


:::


::: {.content-visible when-profile="fr"}
# Récupération des jeux de données

## Données d'émission de l'Ademe

Comme expliqué au chapitre précédent, ces données peuvent être importées très simplement avec `Pandas`
:::

::: {.content-visible when-profile="en"}
# Retrieving data for this chapter

## French carbon emissions dataset

As explained in the previous chapter, these data can be imported very simply with `Pandas`:

:::

```{python}
#| echo: true
import pandas as pd

url = "https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert"
emissions = pd.read_csv(url)
emissions.head(2)
```

::: {.content-visible when-profile="fr"}
Nous allons d'ores et déjà conserver le nom des secteurs émetteurs présents dans la base de données pour simplifier des utilisations ultérieures:
:::
::: {.content-visible when-profile="en"}
We will already keep the names of the emitting sectors present in the database to simplify subsequent uses:
:::

```{python}
#| echo: true
secteurs = emissions.select_dtypes(include='number').columns
```

::: {.content-visible when-profile="fr"}
Les exploitations ultérieures de ces données utiliseront la dimension départementale dont nous avons montré la construction au chapitre précédent:
:::
::: {.content-visible when-profile="en"}
Subsequent exploitations of these data will use the departmental dimension, the construction of which we demonstrated in the previous chapter:
:::

```{python}
#| echo: true
emissions['dep'] = emissions["INSEE commune"].str[:2]
```

::: {.content-visible when-profile="fr"}

## Données _Filosofi_

On va utiliser les données Filosofi (données de revenus) au niveau communal de 2016. 
Ce n'est pas la même année que les données d'émission de CO2, ce n'est donc pas parfaitement rigoureux,
mais cela permettra tout de même d'illustrer 
les principales fonctionnalités de `Pandas`

Le point d'entrée principal de la fonction `pynsee` est la fonction `download_file`.

Le code pour télécharger les données est le suivant :
:::


::: {.content-visible when-profile="en"}
We will use the Filosofi data (income data) at the municipal level for 2016. It is not the same year as the CO2 emissions data, so it is not perfectly rigorous, but it will still illustrate the main functionalities of `Pandas`.

The main entry point for the `pynsee` function is the `download_file` function.

The code to download the data is as follows:
:::

```{python}
#| echo: true
#| output: false
#| eval: false
from pynsee.download import download_file
filosofi = download_file("FILOSOFI_COM_2016")
```

```{python}
#| echo: false
#| output: false
import pandas as pd
backup_filosofi = "https://minio.lab.sspcloud.fr/lgaliana/data/python-ENSAE/filosofi.parquet"
filosofi = pd.read_parquet(backup_filosofi)
```


::: {.content-visible when-profile="fr"}
Le _DataFrame_ en question a l'aspect suivant :
:::

::: {.content-visible when-profile="en"}
The resulting `DataFrame` looks like this:
:::


```{python}
#| echo: true
filosofi.sample(3)
```

::: {.content-visible when-profile="fr"}
`Pandas` a géré automatiquement les types de variables. Il le fait relativement bien, mais une vérification est toujours utile pour les variables qui ont un statut spécifique.

Pour les variables qui ne sont pas en type `float` alors qu’elles devraient l’être, on modifie leur type.
:::

::: {.content-visible when-profile="en"}
`Pandas` automatically handled the variable types. It does this relatively well, but a check is always useful for variables that have a specific status.

For variables that are not of type `float` but should be, we modify their type.
:::

```{python}
#| echo: true
filosofi = (
  filosofi
  .astype(
    {c: "float" for c in filosofi.columns[2:]}
  )
)
```

::: {.content-visible when-profile="fr"}
Un simple coup d'oeil sur les données
donne une idée assez précise de la manière dont les données sont organisées.
On remarque que certaines variables de `filosofi` semblent avoir beaucoup de valeurs manquantes (secret statistique)
alors que d'autres semblent complètes. 
Si on désire exploiter `filosofi`, il faut faire attention à la variable choisie.


Notre objectif à terme va être de relier l'information contenue entre ces
deux jeux de données. En effet, sinon, nous risquons d'être frustré : nous allons
vouloir en savoir plus sur les émissions de gaz carbonique mais seront très
limités dans les possibilités d'analyse sans ajout d'une information annexe
issue de `filosofi`.  
:::

::: {.content-visible when-profile="en"}
A quick glance at the data gives a fairly precise idea of how the data are organized. We notice that some variables in `filosofi` seem to have many missing values (statistical secrecy), while others seem complete. If we want to exploit `filosofi`, we need to pay attention to the chosen variable.

Our ultimate goal will be to link the information contained between these two datasets. Otherwise, we risk being frustrated: we will want to know more about carbon emissions but will be very limited in the possibilities of analysis without adding additional information from `filosofi`.
:::

::: {.content-visible when-profile="fr"}
# Statistiques descriptives par groupe

## Principe

Nous avons vu, lors du chapitre précédent, comment obtenir
une statistique agrégée simplement grâce à `Pandas`. 
Il est néanmoins commun d'avoir des données avec des strates
intermédiaires d'analyse pertinentes: des variables géographiques, l'appartenance à des groupes socio-démographiques liés à des caractéristiques renseignées, des indicatrices de période temporelle, etc. 
Pour mieux comprendre la structure de ses données, les _data scientists_ sont donc souvent amenés à construire des statistiques descriptives sur des sous-groupes présents dans les données. Pour reprendre l'exemple sur les émissions, nous avions précédemment construit des statistiques d'émissions au niveau national. Mais qu'en est-il du profil d'émission des différents départements ? Pour répondre à cette question, il sera utile d'agréger nos données au niveau départemental. Ceci nous donnera une information différente du jeu de données initial (niveau communal) et du niveau le plus agrégé (niveau national).

En `SQL`, il est très simple de découper des données pour
effectuer des opérations sur des blocs cohérents et recollecter des résultats
dans la dimension appropriée.
La logique sous-jacente est celle du *split-apply-combine* qui est repris
par les langages de manipulation de données, auxquels `pandas`
[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html). 

L'image suivante, issue de
[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/),
représente bien la manière dont fonctionne l'approche
`split`-`apply`-`combine`:

![Split-apply-combine (Source: [unlhcc.github.io](https://unlhcc.github.io/r-novice-gapminder/16-plyr/))](https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png){fig-width=70%}

En `Pandas`, on utilise `groupby` pour découper les données selon un ou
plusieurs axes (ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet
est particulièrement utile).
L'ensemble des opérations d'agrégation (comptage, moyennes, etc.) que nous avions vues précédemment peut être mise en oeuvre par groupe.

Techniquement, cette opération consiste à créer une association
entre des labels (valeurs des variables de groupe) et des
observations. Utiliser la méthode `groupby` ne déclenche pas d'opérations avant la mise en oeuvre d'une statistique, cela créé seulement une relation formelle entre des observations et des regroupemens qui seront utilisés _a posteriori_:
:::

::: {.content-visible when-profile="en"}
# Descriptive statistics by group

## Principle

In the previous chapter, we saw how to obtain an aggregated statistic easily with `Pandas`. However, it is common to have data with intermediate analysis strata that are relevant: geographical variables, membership in socio-demographic groups related to recorded characteristics, temporal period indicators, etc. To better understand the structure of the data, data scientists are often led to construct descriptive statistics on sub-groups present in the data. For example, we previously constructed emission statistics at the national level. But what about the emission profiles of different departments? To answer this question, it will be useful to aggregate our data at the departmental level. This will give us different information from the initial dataset (municipal level) and the most aggregated level (national level).

In `SQL`, it is very simple to segment data to perform operations on coherent blocks and recollect results in the appropriate dimension. The underlying logic is that of *split-apply-combine*, which is adopted by data manipulation languages, including `pandas` [which is no exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).

The following image, from [this site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/), well represents how the `split`-`apply`-`combine` approach works:

![Split-apply-combine (Source: [unlhcc.github.io](https://unlhcc.github.io/r-novice-gapminder/16-plyr/))](https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png){fig-width=70%}

In `Pandas`, we use `groupby` to segment the data according to one or more axes (this [tutorial](https://realpython.com/pandas-groupby/) on the subject is particularly useful). All the aggregation operations (counting, averages, etc.) that we saw earlier can be applied by group.

Technically, this operation involves creating an association between labels (values of group variables) and observations. Using the `groupby` method does not trigger operations until a statistic is implemented; it simply creates a formal relationship between observations and groupings that will be used later:
:::

```{python}
#| echo: true
filosofi["dep"] = filosofi["CODGEO"].str[:2]
filosofi.groupby('dep').__class__
```

::: {.content-visible when-profile="fr"}
Tant qu'on n'appelle pas une action sur un `DataFrame` par groupe, du type
`head` ou `display`, `pandas` n'effectue aucune opération. On parle de
*lazy evaluation*. Par exemple, le résultat de `df.groupby('dep')` est
une transformation qui n'est pas encore évaluée :
:::

::: {.content-visible when-profile="en"}
As long as we do not call an action on a `DataFrame` by group, such as `head` or `display`, `pandas` performs no operations. This is called *lazy evaluation*. For example, the result of `df.groupby('dep')` is a transformation that has not yet been evaluated:
:::

```{python}
#| echo: true
filosofi.groupby('dep')
```


::: {.content-visible when-profile="fr"}

## Illustration 1: dénombrement par groupe

Pour illustrer l'application de ce principe à un comptage, on peut dénombrer le nombre de communes par département en 2023 (chaque année cette statistique change du fait des fusions de communes). Pour cela, il suffit de prendre le référentiel des communes françaises issu du code officiel géographique (COG) et dénombrer par département grâce à `count`:
:::

::: {.content-visible when-profile="en"}
## Illustration 1: counting by group

To illustrate the application of this principle to counting, we can count the number of municipalities by department in 2023 (this statistic changes every year due to municipal mergers). For this, we simply take the reference of French municipalities from the official geographical code (COG) and count by department using `count`:

:::


```{python}
#| echo: true
import requests
from io import StringIO
import pandas as pd

url_cog_2023 = "https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv"
url_backup = "https://minio.lab.sspcloud.fr/lgaliana/data/python-ENSAE/cog_2023.csv"

# Try-except clause to avoid timout issue sometimes
# Without timeout problem, pd.read_csv(url_cog_2023) would be sufficient 
try: 
  response = requests.get(url_cog_2023)
  response.raise_for_status()
  cog_2023 = pd.read_csv(StringIO(response.text))
except requests.exceptions.Timeout:
  print("Failing back to backup")
  cog_2023 = pd.read_csv(url_backup)
```




::: {.content-visible when-profile="fr"}

Grâce à ce jeu de données, sans avoir recours aux statistiques par groupe, on peut déjà savoir combien on a, respectivement, de communes, départements et régions en France:

```{python}
#| echo: true
communes = cog_2023.loc[cog_2023['TYPECOM']=="COM"] #<1>
communes.loc[:, ['COM', 'DEP', 'REG']].nunique()
```
1. On se restreint au statut "Commune" car ce fichier comporte également les codes Insee pour d'autres status, comme les "Arrondissements municipaux" de Paris, Lyon et Marseille. 
:::

::: {.content-visible when-profile="en"}

With this dataset, without resorting to group statistics, we can already know how many municipalities, departments, and regions we have in France, respectively:

```{python}
#| echo: true
communes = cog_2023.loc[cog_2023['TYPECOM']=="COM"] #<1>
communes.loc[:, ['COM', 'DEP', 'REG']].nunique()
```
1. We restrict to the status "Commune" because this file also contains Insee codes for other statuses, such as the "Municipal Arrondissements" of Paris, Lyon, and Marseille.
:::


::: {.content-visible when-profile="fr"}
Maintenant, intéressons nous aux départements ayant le plus de communes. Il s'agit de la même fonction de dénombrement où on joue, cette fois, sur le groupe à partir duquel est calculé la statistique. 

Calculer cette statistique se fait de manière assez transparente lorsqu'on connaît le principe d'un calcul de statistiques avec `Pandas`:
:::

::: {.content-visible when-profile="en"}
Now, let's look at the departments with the most municipalities. It is the same counting function where we play, this time, on the group from which the statistic is calculated.

Calculating this statistic is quite straightforward when you understand the principle of calculating statistics with `Pandas`:
:::

```{python}
#| echo: true
communes = cog_2023.loc[cog_2023['TYPECOM']=="COM"] #<1>
communes.groupby('DEP').agg({'COM': 'nunique'})
```

::: {.content-visible when-profile="fr"}
En SQL, on utiliserait la requête suivante:
:::
::: {.content-visible when-profile="en"}
In SQL, we would use the following query:
:::

```sql
SELECT dep, COUNT DISTINCT "COM" AS COM 
FROM communes
GROUP BY dep 
WHERE TYPECOM == 'COM';
```

::: {.content-visible when-profile="fr"}
La sortie est une `Serie` indexée. Ce n'est pas très pratique comme nous avons pu l'évoquer au cours du chapitre précédent. Il est plus pratique de transformer cet objet en `DataFrame` avec `reset_index`. Enfin, avec `sort_values`, on obtient la statistique désirée:
:::
::: {.content-visible when-profile="en"}
The output is an indexed `Series`. This is not very convenient as we mentioned in the previous chapter. It is more practical to transform this object into a `DataFrame` with `reset_index`. Finally, with `sort_values`, we obtain the desired statistic:
:::


```{python}
#| echo: true
(
    communes
    .groupby('DEP')
    .agg({'COM': 'nunique'})
    .reset_index()
    .sort_values('COM', ascending = False)
)
```

::: {.content-visible when-profile="fr"}

## Illustration 2: agrégats par groupe

Pour illustrer les agrégats par groupe nous pouvons prendre le jeu de données de l'Insee `filosofi` et compter la population grâce à la variable `NBPERSMENFISC16`.

Pour calculer le total au niveau France entière nous pouvons faire de deux manières :

:::

::: {.content-visible when-profile="en"}
## Illustration 2: aggregates by group

To illustrate aggregates by group, we can use the Insee `filosofi` dataset and count the population using the variable `NBPERSMENFISC16`.

To calculate the total for the whole of France, we can do it in two ways:

:::


```{python}
#| echo: true
filosofi['NBPERSMENFISC16'].sum()* 1e-6
```

```{python}
#| echo: true
filosofi.agg({"NBPERSMENFISC16": "sum"}).div(1e6)
```

::: {.content-visible when-profile="fr"}
où les résultats sont reportés en millions de personnes. La logique est identique lorsqu'on fait des statistiques par groupe, il s'agit seulement de remplacer `filosofi` par `filosofi.groupby('dep')` pour créer une version partitionnée par département de notre jeu de données:

```{python}
#| echo: true
filosofi.groupby('dep')['NBPERSMENFISC16'].sum() #<1>
```
1. Avec cette approche, il faut faire attention à l'ordre des opérations: d'abord on effectue le `groupby` puis on conserve la colonne d'intérêt
:::

::: {.content-visible when-profile="en"}
where the results are reported in millions of people. The logic is the same when doing group statistics, it's just a matter of replacing `filosofi` with `filosofi.groupby('dep')` to create a partitioned version of our dataset by department:

```{python}
#| echo: true
filosofi.groupby('dep')['NBPERSMENFISC16'].sum() #<1>
```
1. With this approach, you need to pay attention to the order of operations: first, perform the `groupby` and then select the column of interest
:::



```{python}
#| echo: true
filosofi.groupby('dep').agg({"NBPERSMENFISC16": "sum"})
```

::: {.content-visible when-profile="fr"}
La seconde approche est plus pratique car elle donne directement un `DataFrame` `Pandas` et non une série indexée sans nom. A partir de celle-ci, quelques manipulations basiques peuvent suffire pour avoir un tableau diffusables sur la démographie départementale. Néanmoins, celui-ci, serait quelques peu brut de décoffrage car nous ne possédons à l'heure actuelle que les numéros de département. Pour avoir le nom de départements, il faudrait utiliser une deuxième base de données et croiser les informations communes entre elles (en l'occurrence le numéro du département). C'est l'objet de la prochaine partie.
:::

::: {.content-visible when-profile="en"}
The second approach is more practical because it directly gives a `Pandas` `DataFrame` and not an unnamed indexed series. From this, a few basic manipulations can suffice to have a shareable table on departmental demographics. However, this table would be somewhat rudimentary as we currently only have the department numbers. To get the names of the departments, we would need to use a second dataset and merge the common information between them (in this case, the department number). This is the subject of the next part.
:::


## Exercice d'application

::: {.content-visible when-profile="fr"}
Cet exercice d'application s'appuie sur le jeu de données de l'Ademe nommé `emissions` précédemment.
:::

::: {.content-visible when-profile="en"}
This application exercise uses the `Ademe` dataset named `emissions` previously discussed.
:::

{{< include "02_pandas_suite/_exo1.qmd" >}}
{{< include "02_pandas_suite/_exo1_solution.qmd" >}}

::: {.content-visible when-profile="fr"}
Ces résultats sont assez logiques ; les départements ruraux ont une part plus importante de leur émission issue de l'agriculture, les départements urbains ont plus d'émissions issues du secteur tertiaire, ce qui est lié à la densité plus importante de ces espaces. 

Grâce à ces statistiques on progresse dans la connaissance de notre jeu de données et donc de la nature des émissions de C02 en France.
Les statistiques descriptives par groupe nous permettent de mieux saisir l'hétérogénéité spatiale de notre phénomène. 

Cependant, on reste limité dans notre capacité à interpréter les statistiques obtenues sans recourir à l'utilisation d'information annexe. Pour donner du sens et de la valeur à une statistique, il faut généralement associer celle-ci à de la connaissance annexe sous peine qu'elle soit désincarnée.

Dans la suite de ce chapitre, nous envisagerons une première voie qui est le croisement avec des données complémentaires. On appelle ceci un enrichissement de données. Ces données peuvent être des observations à un niveau identique à celui de la source d'origine. Par exemple, l'un des croisements les plus communs est d'associer une base client à une base d'achats afin de mettre en regard un comportement d'achat avec des caractéristiques pouvant expliquer celui-ci. Les associations de données peuvent aussi se faire à des niveaux conceptuels différents, en général à un niveau plus agrégé pour contextualiser la donnée plus fine et comparer une observation à des mesures dans un groupe similaire. Par exemple, on peut associer des temps et des modes de transports individuels à ceux d'une même classe d'âge ou de personnes résidant dans la même commune pour pouvoir comparer la différence entre certains individus et un groupe sociodémographique similaire. 
:::

::: {.content-visible when-profile="en"}
These results are quite logical; rural departments have a larger share of their emissions from agriculture, while urban departments have higher emissions from the tertiary sector, which is related to the higher density of these areas.

With these statistics, we progress in understanding our dataset and, consequently, the nature of CO2 emissions in France. Descriptive statistics by group help us better grasp the spatial heterogeneity of our phenomenon.

However, we remain limited in our ability to interpret the obtained statistics without using additional information. To give meaning and value to a statistic, it is generally necessary to associate it with additional knowledge; otherwise, it remains detached.

In the rest of this chapter, we will consider a primary approach which is the merging of complementary data. This process is called data enrichment. These data can be observations at the same level as the original source. For example, one of the most common merges is associating a customer database with a purchase database to relate purchasing behavior to characteristics that may explain it. Data merges can also occur at different conceptual levels, generally at a more aggregated level to contextualize finer data and compare an observation to measures within a similar group. For instance, we can associate individual travel times and modes with those of the same age group or people living in the same municipality to compare the differences between certain individuals and a similar sociodemographic group.
:::

::: {.content-visible when-profile="fr"}
# Restructurer les données

## Principe

Quand on a plusieurs informations pour un même individu ou groupe, on
retrouve généralement deux types de structure de données : 

* format __wide__ : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes 
* format __long__ : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d'observations

Un exemple de la distinction entre les deux peut être pris à l'ouvrage de référence d'Hadley Wickham, [*R for Data Science*](https://r4ds.hadley.nz/):

![Données _long_ et _wide_ (Source:  [*R for Data Science*](https://r4ds.hadley.nz/))](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png)

L'aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/reshape.png){fig-width=60%}

Le fait de passer d'un format *wide* au format *long* (ou vice-versa)
peut être extrêmement pratique car certaines fonctions sont plus adéquates sur une forme de données ou sur l'autre.

En règle générale, avec `Python` comme avec `R`, les __formats *long* sont souvent préférables__.
Les formats _wide_ sont plutôt pensés pour des tableurs comme `Excel` ou on dispose d'un nombre réduit
de lignes à partir duquel faire des tableaux croisés dynamiques. 
:::

::: {.content-visible when-profile="en"}
# Restructuring datasets

## Principle

When we have multiple pieces of information for the same individual or group, we generally find two types of data structures:

* __Wide__ format: the data contains repeated observations for the same individual (or group) in different columns.
* __Long__ format: the data contains repeated observations for the same individual in different rows, with a column distinguishing the observation levels.

An example of the distinction between the two can be taken from Hadley Wickham's reference book, [*R for Data Science*](https://r4ds.hadley.nz/):

![Wide and Long Data Formats (Source: [*R for Data Science*](https://r4ds.hadley.nz/))](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png)

The following cheat sheet will help remember the functions to apply if needed:

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/reshape.png){fig-width=60%}

Switching from a *wide* format to a *long* format (or vice versa) can be extremely practical because certain functions are more suitable for one form of data than the other.

Generally, with `Python` as with `R`, __long formats are often preferable__. Wide formats are rather designed for spreadsheets like `Excel`, where we have a limited number of rows to create pivot tables from.
:::

::: {.content-visible when-profile="fr"}
## Exercice d'application

Les données de l'ADEME, et celles de l'Insee également, sont au format
_wide_. 
Le prochain exercice illustre l'intérêt de faire la conversion _long_ $\to$ _wide_
avant de faire un graphique avec la méthode `plot` vue au chapitre précédent
:::

::: {.content-visible when-profile="en"}
## Application

The ADEME data, and the Insee data as well, are in the _wide_ format. The next exercise illustrates the benefit of converting from _long_ to _wide_ before creating a plot with the `plot` method seen in the previous chapter.
:::

{{< include "02_pandas_suite/_exo2.qmd" >}}
{{< include "02_pandas_suite/_exo2_solution.qmd" >}}

::: {.content-visible when-profile="fr"}

# Joindre des données

## Principe

Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation
où une information permet d'apparier de manière exacte deux bases de données[^flou]. 
C'est un besoin quotidien des _data scientists_ d'associer des informations présentes dans plusieurs fichiers. Par exemple, dans des bases de données d'entreprises, les informations clients (adresse, âge, etc.) seront dans un fichier, les ventes dans un autre et les caractéristiques des produits dans un troisième fichier. Afin d'avoir une base complète mettant en regard toutes ces informations, il sera dès lors nécessaire de joindre ces trois fichiers sur la base d'informations communes. 

Cette pratique découle du fait que de nombreux systèmes d'information prennent la forme d'un schéma en étoile:

![Illustration du schéma en étoile (Source: [Databricks](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png))](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png){fig-width="80%"}

Cette structuration de l'information est très liée au modèle des tables relationnelles des années 1980. Aujourd'hui, il existe des modèles de données plus flexibles où l'information est empilée dans un _data lake_ sans structure _a priori_. Néanmoins ce modèle du schéma en étoile conserve une pertinence parce qu'il permet de partager l'information qu'à ceux qui en ont besoin laissant le soin à ceux qui ont besoin de lier des données entre elles de le faire. 

Puisque la logique du schéma en étoile vient historiquement des bases relationnelles, il est naturel qu'il s'agisse d'une approche intrinsèquement liée à la philosophie du SQL, jusque dans le vocabulaire. On parle souvent de jointure de données, un héritage du terme `JOIN` de SQL, et la manière de décrire les jointures (_left join_, _right join_...) est directement issue des instructions SQL associées. 

On parle généralement de base de gauche et de droite pour illustrer les jointures:

![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/join_initial.png){fig-width="50%"}

:::

::: {.content-visible when-profile="en"}
# Joining data

## Principle

Here we will focus on the most favorable case, which is the situation where information allows for an exact match between two databases[^fuzzy]. It is a daily necessity for data scientists to merge information present in multiple files. For example, in business databases, customer information (address, age, etc.) will be in one file, sales in another, and product characteristics in a third file. To have a complete base that compares all this information, it will be necessary to join these three files based on common information.

This practice stems from the fact that many information systems take the form of a star schema:

![Illustration of the star schema (Source: [Databricks](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png))](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png){fig-width="80%"}

This structuring of information is closely related to the model of relational tables from the 1980s. Today, there are more flexible data models where information is stacked in a data lake without an a priori structure. Nevertheless, this star schema model retains relevance because it allows sharing information only with those who need it, leaving it to those who need to link data to do so.

Since the logic of the star schema historically comes from relational databases, it is natural that it is an approach intrinsically linked to the philosophy of SQL, even in the vocabulary. The term "data join" is often used, inherited from the SQL `JOIN` term, and the way to describe joins (left join, right join...) comes directly from the associated SQL instructions.

We generally speak of left and right bases to illustrate the joins:

![Joins](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/join_initial.png){fig-width="50%"}
:::

::: {.content-visible when-profile="fr"}

## Mise en oeuvre avec `Pandas`

En `Pandas`, la méthode la plus pratique pour associer des jeux de données à partir de caractéristiques communes est `merge`. Ses principaux arguments permettent de contrôler le comportement de jointure. Nous allons les explorer de manière visuelle. 

En l'occurrence, pour notre problématique de construction de statistiques
sur les émissions de gaz carbonique, la base de gauche sera le _DataFrame_ `emission` et la base de droite le _DataFrame_ `filosofi`:

:::

::: {.content-visible when-profile="en"}
## Implementation with `Pandas`

In `Pandas`, the most practical method to join datasets based on common characteristics is `merge`. Its main arguments allow for controlling the join behavior. We will explore them visually.

In our case, for constructing statistics on carbon emissions, the left base will be the `emissions` DataFrame, and the right base will be the `filosofi` DataFrame:
:::


```{python}
#| echo: true
emissions.head(2)
```

```{python}
#| echo: true
filosofi.head(2)
```

::: {.content-visible when-profile="fr"}

On parle de clé(s) de jointure pour nommer la ou les variable(s) nécessaire(s) à la fusion de données. Ce sont les variables communes aux deux jeux de données. Il n'est pas nécessaire qu'elles aient le même nom en revanche elles doivent partager des valeurs communes autrement l'intersection entre ces deux bases est l'ensemble vide. 

On peut jouer sur deux dimensions dans la jointure (ceci sera plus clair ensuite avec les exemples graphiques). 

* Il existe principalement trois types de fusions: _left join_ et _right join_ ou un combo des deux selon le type de pivot qu'on désire mettre en oeuvre. 
* Ensuite, il existe deux manières de fusionner les valeurs une fois qu'on a choisi un pivot: _inner_ ou _outer join_. Dans le premier cas, on ne conserve que les observations où les clés de jointures sont présentes dans les deux bases, dans le second on conserve toutes les observations de la clé de jointure des variables pivot quitte à avoir des valeurs manquantes si la deuxième base de données n'a pas de telles observations. 

Dans les exemples ci-dessous, nous allons utiliser les codes communes et les départements comme variables de jointure. En soi, l'usage du département n'est pas nécessaire puisqu'il se déduit directement du code commune mais cela permet d'illustrer le principe des jointures sur plusieurs variables. A noter que le nom de la commune est volontairement mis de côté pour effectuer des jointures alors que c'est une information commune aux deux bases. Cependant, comme il s'agit d'un champ textuel, dont le formattage peut suivre une norme différente dans les deux bases, ce n'est pas une information fiable pour faire une jointure exacte. 

Pour illustrer le principe du pivot à gauche ou à droite, on va créer deux variables identificatrices de la ligne de nos jeux de données de gauche et de droite. Cela nous permettra de trouver facilement les lignes présentes dans un jeu de données mais pas dans l'autre.

:::

::: {.content-visible when-profile="en"}
We refer to join keys as the variable(s) necessary for merging data. These are the variables common to both datasets. They do not need to have the same name, but they must share common values; otherwise, the intersection between these two datasets is the empty set.

We can manipulate two dimensions in the join (this will be clearer later with graphical examples):

* There are mainly three types of merges: left join, right join, or a combination of the two, depending on the type of pivot we want to implement.
* Then, there are two ways to merge the values once we have chosen a pivot: inner or outer join. In the first case, we only keep the observations where the join keys are present in both datasets; in the second, we keep all observations of the pivot key variables, even if the second dataset does not have such observations, resulting in missing values.

In the examples below, we will use the commune codes and departments as join keys. Using the department is not necessary since it is directly deduced from the commune code, but it helps illustrate the principle of joins on multiple variables. Note that the name of the commune is intentionally set aside for joins, even though it is common information to both datasets. However, as it is a textual field, which may follow different formatting norms in the two datasets, it is not reliable for an exact join.

To illustrate the principle of the left or right pivot, we will create two identifier variables for the row in our left and right datasets. This will allow us to easily find rows present in one dataset but not in the other.
:::

```{python}
#| echo: true
emissions = emissions.reset_index(names = ['id_left'])
filosofi = filosofi.reset_index(names = ['id_right'])
```

### _Left join_

::: {.content-visible when-profile="fr"}
Commençons avec la jointure à gauche. Comme son nom l'indique, on va prendre la variable de gauche en pivot:
:::
::: {.content-visible when-profile="en"}
Let's start with the left join. As its name indicates, we will take the left variable as the pivot:
:::

![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/left_join.png)


```{python}
#| echo: true
left_merged = emissions.merge(
  filosofi,
  left_on = ["INSEE commune", "dep"],
  right_on = ["CODGEO", "dep"],
  how = "left"
)
left_merged.head(3)
```

::: {.content-visible when-profile="fr"}
Il est recommandé de toujours expliciter les clés de jointures par le biais des arguments `left_on`, `right_on` ou `on` si les noms de variables sont communs dans les deux bases.
Si on a des noms de variables communes entre les bases mais qu'elles ne sont pas définies comme clés de jointures, celles-ci ne seront pas utilisées pour joindre mais seront conservées avec un suffixe qui par défaut est `_x` et `_y` (paramétrable par le biais de l'argument `suffixes`). 

La syntaxe `Pandas` étant directement inspirée de SQL, on a une traduction assez transparente de l'instruction ci-dessus en SQL:
:::

::: {.content-visible when-profile="en"}
It is recommended to always explicitly specify the join keys using the `left_on`, `right_on`, or `on` arguments if the variable names are common between the two datasets.
If there are common variable names between the datasets that are not defined as join keys, they will not be used for the join but will be retained with a suffix that defaults to `_x` and `_y` (configurable using the `suffixes` argument).

The `Pandas` syntax is directly inspired by SQL, so we have a fairly transparent translation of the above instruction into SQL:
:::

```sql
SELECT *
FROM emissions
LEFT JOIN filosofi
  ON emissions.`INSEE commune` = filosofi.CODGEO
  AND emissions.dep = filosofi.dep;
```

::: {.content-visible when-profile="fr"}
En faisant une jointure à gauche, on doit en principe avoir autant de lignes que la base de données à gauche:
:::

::: {.content-visible when-profile="en"}
By performing a left join, we should, in principle, have as many rows as in the left dataset:
:::

```{python}
#| echo: true
left_merged.shape[0] == emissions.shape[0]
```

::: {.content-visible when-profile="fr"}
Autrement, cela est signe qu'il y a une clé dupliquée à droite. Grâce à notre variable `id_right`, on peut savoir les codes communes à droite qui n'existent pas à gauche:
:::
::: {.content-visible when-profile="en"}
Otherwise, it indicates that there is a duplicate key on the right. Thanks to our `id_right` variable, we can identify the commune codes on the right that do not exist on the left:
:::


```{python}
#| echo: true
left_merged.loc[left_merged['id_right'].isna()].tail(3)
```

::: {.content-visible when-profile="fr"}
Cela vient du fait que nous utilisons des données qui ne sont pas de la même année de référence du code officiel géographique (2016 vs 2018). Pendant cet intervalle, il y a eu des changements de géographie, notamment des fusions de communes. Par exemple, la commune de Courcouronnes qu'on a vu ci-dessus peut être retrouvée regroupée avec Evry dans le jeu de données filosofi (base de droite):
:::
::: {.content-visible when-profile="en"}
This is because we are using data that are not from the same reference year of the official geographical code (2016 vs 2018). During this interval, there were geographical changes, notably commune mergers. For example, the commune of Courcouronnes seen above can be found merged with Evry in the filosofi dataset (right base):
:::

```{python}
#| echo: true
filosofi.loc[
  filosofi['LIBGEO']
  .str.lower()
  .str.contains("courcouronnes")
]
```

::: {.content-visible when-profile="fr"}
Dans un exercice de construction de statistiques publiques, on ne pourrait donc se permettre cette disjonction des années. 
:::
::: {.content-visible when-profile="en"}
In a public statistics construction exercise, we could not afford this discrepancy in years.
:::


### _Right join__

![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/right_join.png)

::: {.content-visible when-profile="fr"}
Le principe est le même mais cette fois c'est la base de droite qui est prise sous forme de pivot:
:::
::: {.content-visible when-profile="en"}
The principle is the same, but this time it is the right base that is taken as the pivot:
:::


```{python}
#| echo: true
right_merged = emissions.merge(
  filosofi,
  left_on = ["INSEE commune", "dep"],
  right_on = ["CODGEO", "dep"],
  how = "right"
)
right_merged.head(3)
```

::: {.content-visible when-profile="fr"}
L'instruction équivalente en SQL serait
:::
::: {.content-visible when-profile="en"}
The equivalent instruction in SQL would be:
:::


```sql
SELECT *
FROM filosofi
RIGHT JOIN emissions
  ON filosofi.CODGEO = emissions.`INSEE commune`
  AND filosofi.dep = emissions.dep;
```

::: {.content-visible when-profile="fr"}
On peut, comme précédemment, vérifier la cohérence des dimensions:
:::
::: {.content-visible when-profile="en"}
We can, as before, check the consistency of the dimensions:
:::

```{python}
#| echo: true
right_merged.shape[0] == filosofi.shape[0]
```

::: {.content-visible when-profile="fr"}
Pour vérifier le nombre de lignes des données Filosofi que nous n'avons pas dans notre jeu d'émissions de gaz carbonique, on peut faire
:::
::: {.content-visible when-profile="en"}
To check the number of rows in the Filosofi data that we do not have in our greenhouse gas emissions dataset, we can do:
:::

```{python}
#| echo: true
right_merged['id_left'].isna().sum()
```

::: {.content-visible when-profile="fr"}
C'est un nombre faible. Quelles sont ces observations ?
:::
::: {.content-visible when-profile="en"}
It's a small number. What are these observations?
:::

```{python}
#| echo: true
right_merged.loc[
  right_merged['id_left'].isna(),
  filosofi.columns.tolist() + emissions.columns.tolist()
]
```

::: {.content-visible when-profile="fr"}
Il est suprenant de voir que Paris, Lyon et Marseille sont présents
dans la base des statistiques communales mais pas dans celles des émissions. 
Pour comprendre pourquoi, recherchons dans nos données d'émissions les observations liées à Marseille: 
:::
::: {.content-visible when-profile="en"}
It is surprising to see that Paris, Lyon, and Marseille are present in the communal statistics dataset but not in the emissions dataset. To understand why, let's search in our emissions data for observations related to Marseille:
:::

```{python}
#| echo: true
emissions.loc[
  emissions["Commune"]
  .str.lower()
  .str.contains('MARSEILLE')
]
```

::: {.content-visible when-profile="fr"}
Cela vient du fait que le jeu de données des émissions de l'Ademe propose de l'information sur les arrondissements dans les trois plus grandes villes
là où le jeu de données de l'Insee ne fait pas cette décomposition.
:::
::: {.content-visible when-profile="en"}
This is because the Ademe emissions dataset provides information on districts in the three largest cities, whereas the Insee dataset does not have this breakdown.
:::



### _Inner join_

![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/inner.png)

::: {.content-visible when-profile="fr"}
Il s'agit du jeu de données où les clés sont retrouvées à l'intersection des deux tables. 
:::
::: {.content-visible when-profile="en"}
This is the dataset where the keys are found at the intersection of the two tables.
:::


```{python}
#| echo: true
inner_merged = emissions.merge(
  filosofi,
  left_on = ["INSEE commune", "dep"],
  right_on = ["CODGEO", "dep"],
  how = "inner"
)
inner_merged.head(3)
```

::: {.content-visible when-profile="fr"}
En SQL, cela donne
:::
::: {.content-visible when-profile="en"}
In SQL, this would be:
:::

```sql
SELECT *
FROM emissions
INNER JOIN filosofi
  ON emissions.`INSEE commune` = filosofi.CODGEO
  AND emissions.dep = filosofi.dep;
```

::: {.content-visible when-profile="fr"}
Le nombre de lignes dans notre jeu de données peut être comparé au jeu de droite et de gauche:
:::
::: {.content-visible when-profile="en"}
The number of rows in our dataset can be compared to the left and right datasets:
:::

```{python}
#| echo: true
inner_merged.shape[0] == (
  left_merged.shape[0] - left_merged['id_right'].isna().sum()
)
```

```{python}
#| echo: true
inner_merged.shape[0] == (
  right_merged.shape[0] - right_merged['id_left'].isna().sum()
)
```


### _Full join_

::: {.content-visible when-profile="fr"}
Le _full join_ est un pivot à gauche puis à droite pour les informations qui n'ont pas été trouvées
:::
::: {.content-visible when-profile="en"}
The full join is a pivot to the left and then to the right for the information that was not found.
:::


![](https://minio.lab.sspcloud.fr/lgaliana/python-ENSAE/inputs/merge_pandas/full_join.png)

```{python}
#| echo: true
full_merged = emissions.merge(
  filosofi,
  left_on = ["INSEE commune", "dep"],
  right_on = ["CODGEO", "dep"],
  how = "outer"
)
full_merged.head(3)
```

::: {.content-visible when-profile="fr"}
Comme d'habitude, la traduction en SQL est presque immédiate:
:::
::: {.content-visible when-profile="en"}
As usual, the translation to SQL is almost immediate:
:::

```sql
SELECT *
FROM emissions
FULL OUTER JOIN filosofi
  ON emissions.`INSEE commune` = filosofi.CODGEO
  AND emissions.dep = filosofi.dep;
```

::: {.content-visible when-profile="fr"}
Cette fois, on a une combinaison de nos trois jeux de données initiaux:

* Le _inner join_ ;
* Le _left join_ sur les observations sans clé de droite ;
* Le _right join_ sur les observations sans clé de gauche ;
:::

::: {.content-visible when-profile="en"}
This time, we have a combination of our three initial datasets:

* The inner join;
* The left join on observations without the right key;
* The right join on observations without the left key;
:::

```{python}
#| echo: true
(
  full_merged['id_left'].isna().sum() + full_merged['id_right'].isna().sum()
) == (
  left_merged['id_right'].isna().sum() + right_merged['id_left'].isna().sum()
)
```

::: {.content-visible when-profile="fr"}
### En résumé
:::

::: {.content-visible when-profile="en"}
### In summary
:::

![](https://external-preview.redd.it/yOLzCR0qSzul2WpjQorxINB0xpU3_N9twmFVsgbGJwQ.jpg?auto=webp&s=4feedc91302ba635b3028a21b98d047def5cdc2b){fig-align="center"}

::: {.content-visible when-profile="fr"}
## Exemples d'identifiants dans les données françaises

### Le Code officiel géographique (COG): l'identifiant des données géographiques

Pour les données géographiques, il existe de nombreux identifiants selon la problématique d'étude.
Parmi les besoins principaux, on retrouve le fait d'apparier des données géographiques à partir d'un identifiant administratif commun. Par exemple, associer deux jeux de données au niveau communal. 

Pour cela, l'identifiant de référence est le code Insee, issu du [Code officiel géographique (COG)](https://www.insee.fr/fr/information/2560452) que nous utilisons depuis le dernier chapitre et que nous aurons amplement l'occasion d'exploiter au cours des différents chapitres de ce cours. 
La géographie administrative étant en évolution perpétuelle, la base des code Insee est une base vivante. Le site et les API de l'Insee permettent de récupérer l'historique d'après-guerre afin de pouvoir faire de l'analyse géographique sur longue période. 

Les codes postaux ne peuvent être considérés comme un identifiant : ils peuvent regrouper plusieurs communes ou, au contraire, une même commune peut avoir plusieurs codes postaux. Il s'agit d'un système de gestion de la Poste qui n'a pas été construit pour l'analyse statistique.

Pour se convaincre du problème, à partir des données mises à disposition par La Poste, on peut voir que le code postal 11420 correspond à 11 communes:
:::

::: {.content-visible when-profile="en"}
## Examples of identifiers in French data

### The Official Geographic Code (COG): The identifier for geographic data

For geographic data, there are many identifiers depending on the study problem.
Among the main needs is the ability to match geographic data using a common administrative identifier. For example, associating two datasets at the municipal level.

For this, the reference identifier is the Insee code, derived from the [Official Geographic Code (COG)](https://www.insee.fr/fr/information/2560452), which we have been using since the last chapter and will extensively use throughout the different chapters of this course.
Given that the administrative geography is constantly evolving, the Insee code database is a living base. The Insee website and APIs provide access to the post-war historical data for long-term geographic analysis.

Postal codes cannot be considered as an identifier: they can group several municipalities or, conversely, one municipality can have several postal codes. It is a system managed by La Poste that was not designed for statistical analysis.

To illustrate the problem, from the data provided by La Poste, we can see that postal code 11420 corresponds to 11 municipalities:
:::


```{python}
#| echo: true
codes_postaux = pd.read_csv(
  "https://datanova.laposte.fr/data-fair/api/v1/datasets/laposte-hexasmal/raw",
  sep = ";", encoding = "latin1",
  dtype = {"Code_postal": "str", "#Code_commune_INSEE": "str"}
)
codes_postaux.loc[codes_postaux['Code_postal'] == "11420"]
```

::: {.content-visible when-profile="fr"}
En anticipant sur les compétences développées lors des prochains chapitres, nous pouvons représenter le problème sous forme cartographique en prenant l'exemple de l'Aude. Le code pour produire la carte des codes communes est donné tel quel, il n'est pas développé car il fait appel à des concepts et librairies qui seront présentés lors du prochain chapitre:

```{python}
#| echo: true
#| fig-cap: Géographie des codes postaux et des communes dans l'Aude (11)
from cartiflette import carti_download
shp_communes = carti_download(
  values = ["11"],
  crs = 4326,
  borders = "COMMUNE",
  simplification=50,
  filter_by="DEPARTEMENT",
  source="EXPRESS-COG-CARTO-TERRITOIRE",
  year=2022) #<1>

codes_postaux11 = shp_communes.merge(
  codes_postaux,
  left_on = "INSEE_COM",
  right_on = "#Code_commune_INSEE"
) #<2>
codes_postaux11 = codes_postaux11.dissolve(by = "Code_postal") #<3>

# Carte #<4>
ax = shp_communes.plot(color='white', edgecolor='blue', linewidth = 0.5)
ax = codes_postaux11.plot(ax = ax, color='none', edgecolor='black')
ax.set_axis_off()
```
1. Récupération des contours officiels de l'Aude produits par l'IGN par le biais de la librairie [`cartiflette`](https://github.com/InseeFrLab/cartiflette)
2. Jointure par le biais du code commune entre les deux sources de données
3. On agrège la géométrie au niveau des codes postaux
4. On crée une carte à partir de nos deux couches

:::

::: {.content-visible when-profile="en"}
Anticipating on the skills developed in the upcoming chapters, we can represent the problem cartographically by taking the example of the Aude department. The code to produce the map of commune codes is given as is, not developed, as it uses concepts and libraries that will be presented in the next chapter:

```{python}
#| echo: true
#| fig-cap: Geography of postal codes and municipalities in Aude (11)
from cartiflette import carti_download
shp_communes = carti_download(
  values = ["11"],
  crs = 4326,
  borders = "COMMUNE",
  simplification=50,
  filter_by="DEPARTEMENT",
  source="EXPRESS-COG-CARTO-TERRITOIRE",
  year=2022) #<1>

codes_postaux11 = shp_communes.merge(
  codes_postaux,
  left_on = "INSEE_COM",
  right_on = "#Code_commune_INSEE"
) #<2>
codes_postaux11 = codes_postaux11.dissolve(by = "Code_postal") #<3>

# Map #<4>
ax = shp_communes.plot(color='white', edgecolor='blue', linewidth = 0.5)
ax = codes_postaux11.plot(ax = ax, color='none', edgecolor='black')
ax.set_axis_off()
```
1. Downloading the official contours of Aude produced by IGN using the [`cartiflette`](https://github.com/InseeFrLab/cartiflette) library
2. Joining using the commune code between the two data sources
3. Aggregating the geometry at the postal code level
4. Creating a map from our two layers
:::

::: {.content-visible when-profile="fr"}

### Sirene: l'identifiant dans les données d'entreprises

Pour relier les microdonnées d'entreprises françaises, il existe un numéro unique d'identification : le [numéro `Siren`](https://entreprendre.service-public.fr/vosdroits/F32135). Il s'agit d'un numéro d'identification dans un répertoire légal d'entreprise indispensable pour toutes démarches juridiques, fiscales... Pour les entreprises qui possèdent plusieurs établissements - par exemple dans plusieurs villes - il existe un identifiant dérivé qui s'appelle le [`Siret`](https://www.economie.gouv.fr/cedef/numero-siret): aux 9 chiffres du numéro Sirene s'ajoutent 5 chiffres d'identifications de l'établissement. D'ailleurs, les administrations publiques sont également concernées par le numéro Siren: étant amenées à effectuer des opérations de marchés (achat de matériel, locations de biens, etc.) elles disposent également d'un identifiant Siren. Etant inscrits dans des répertoires légaux pour lesquels les citoyens sont publics, les numéros Siren et les noms des entreprises associées sont disponibles en _open data_, par exemple sur [annuaire-entreprises.data.gouv.fr/](https://annuaire-entreprises.data.gouv.fr/) pour une recherche ponctuelle, sur [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/).

Cette base Sirene est une mine d'information, parfois comique, sur les entreprises françaises. Par exemple, le site [tif.hair/](https://tif.hair/) s'est amusé à répertorier la part des salons de coiffures proposant des jeux de mots dans le nom du salon. Lorsqu'un entrepreneur déclare la création d'une entreprise, il reçoit un numéro Sirene et un code d'activité (le [code APE](https://entreprendre.service-public.fr/vosdroits/F33050)) relié à la description qu'il a déclaré de l'activité de son entreprise. Ce code permet de classer l'activité d'une entreprise dans la [Nomenclature d'activités françaises (NAF)](https://www.insee.fr/fr/information/2406147) ce qui servira à l'Insee pour la publication de statistiques sectorielles. En l'occurrence, pour les coiffeurs, le code dans la NAF est [`96.02A`](https://www.insee.fr/fr/metadonnees/nafr2/sousClasse/96.02A?champRecherche=false). Il est possible à partir de la base disponible en _open data_ d'avoir en quelques lignes de `Python` la liste de tous les coiffeurs puis de s'amuser à explorer ces données (objet du prochain exercice optionnel).

L'exercice suivant, optionnel, propose de s'amuser à reproduire de manière simplifiée le recensement fait par [tif.hair/](https://tif.hair/)
des jeux de mots dans les salons de coiffure. Il permet de pratiquer quelques méthodes de manipulation textuelle, en avance de phase sur le chapitre consacré aux [expressions régulières](/content/manipulation/04b_regex_TP.qmd).

Le jeu de données de l'ensemble des entreprises étant assez volumineux (autour de 4Go en CSV après décompression), il est plus pratique de partir sur un jeu de données au format `Parquet`, plus optimisé (plus de détails sur ce format dans le [chapitre d'approfondissement](/content/modern-ds/s3.qmd) qui lui est consacré).

Pour lire ce type de fichiers de manière optimale, il est conseillé d'utiliser la librairie `DuckDB` qui permet de ne consommer que les données nécessaires et non de télécharger l'ensemble du fichier pour n'en lire qu'une partie comme ce serait le cas avec `Pandas` (voir la fin de ce chapitre, section "Aller au-delà de `Pandas`"). La requête SQL suivante se traduit en langage naturel par l'instruction suivante: _"A partir du fichier `Parquet`, je ne veux que quelques colonnes du fichier pour les coiffeurs (APE: 96.02A) dont le nom de l'entreprise (`denominationUsuelleEtablissement`) est renseigné"_:

:::

::: {.content-visible when-profile="en"}

### Sirene: the identifier in business data

To connect French business microdata, there is a unique identification number: the [Siren number](https://entreprendre.service-public.fr/vosdroits/F32135). It is an identification number in a legal business directory essential for all legal, fiscal, and other procedures. For companies that have multiple establishments—for example, in several cities—there is a derived identifier called the [Siret](https://www.economie.gouv.fr/cedef/numero-siret): the 9 digits of the Siren number are followed by 5 establishment identification digits. Moreover, public administrations are also concerned with the Siren number: being involved in market operations (purchasing equipment, renting goods, etc.), they also have a Siren identifier. As they are registered in legal directories whose information is public, the Siren numbers and the associated company names are available in open data, for example, on [annuaire-entreprises.data.gouv.fr/](https://annuaire-entreprises.data.gouv.fr/) for occasional searches, or on [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/).

This Sirene database is a treasure trove of information, sometimes amusing, about French companies. For example, the site [tif.hair/](https://tif.hair/) cataloged the proportion of hair salons with puns in their names. When an entrepreneur declares the creation of a business, they receive a Siren number and an activity code (the [APE code](https://entreprendre.service-public.fr/vosdroits/F33050)) related to the description of their business activity. This code allows the classification of a business activity in the [French Classification of Activities (NAF)](https://www.insee.fr/fr/information/2406147), which will be used by Insee for the publication of sectoral statistics. In the case of hairdressers, the code in the NAF is [96.02A](https://www.insee.fr/fr/metadonnees/nafr2/sousClasse/96.02A?champRecherche=false). From the open data available, it is possible, in a few lines of `Python`, to get the list of all hairdressers and then explore this data (the subject of the next optional exercise).

The following optional exercise suggests replicating, in a simplified manner, the survey done by [tif.hair/](https://tif.hair/) on puns in hair salon names. It allows practicing some text manipulation methods, ahead of the chapter dedicated to [regular expressions](/content/manipulation/04b_regex_TP.qmd).

Since the dataset of all companies is quite large (around 4GB in CSV after decompression), it is more practical to use a dataset in `Parquet` format, which is more optimized (more details on this format in the [advanced chapter](/content/modern-ds/s3.qmd) dedicated to it).

To read this type of file optimally, it is recommended to use the `DuckDB` library, which allows consuming only the necessary data instead of downloading the entire file to read only a part of it as would be the case with `Pandas` (see the end of this chapter, section "Beyond `Pandas`"). The following SQL query translates into natural language as: _"From the `Parquet` file, I only want a few columns of the file for hairdressers (APE: 96.02A) whose business name (`denominationUsuelleEtablissement`) is provided"_:

:::


{{< include "02_pandas_suite/_exo_optionnel.qmd" >}}

{{< include "02_pandas_suite/_exo_optionnel_solution.qmd" >}}

```{python}
#| echo: false
#| output: false
ojs_define(coiffeurs = coiffeurs_tif)
```

```{ojs}
function underlineTif(x) {
  // Use a regular expression to find all occurrences of "tif"
  const modx = x.replace(/TIF/g, match => 
    `<span style="text-transform: capitalize; border-bottom: solid 2px blue; margin-bottom: -2px;">${match}</span>`
  );

  console.log(modx)
  
  // Return the modified string wrapped in an HTML template
  return html`${modx}`;
}
```

::: {.content-visible when-profile="fr"}

Voici sous une forme plus interactive l'ensemble des coiffeurs qui possèdent les termes `tif` dans le nom de leur entreprise déposée dans les données officielles:

:::


::: {.content-visible when-profile="en"}

In a more interactive form, here's a list of all the hairdressers who have the word `tif` in the name of their registered business in the official data:

:::

::: {.content-visible when-format="html"}

```{ojs}
viewof coiffeursSearch = Inputs.search(
  transpose(coiffeurs),
  {label: "Nom du salon"}
)
```


```{ojs}
Inputs.table(coiffeursSearch, {
  columns: [
    "siren",
    "denominationUsuelleEtablissement",
    "enseigne1Etablissement"
  ],
  format: {
    denominationUsuelleEtablissement: x => underlineTif(x),
    enseigne1Etablissement: x => underlineTif(x)
  }
})
```

:::

::: {.content-visible when-profile="fr"}

Bien sûr, pour aller plus loin, il faudrait mieux normaliser les données, vérifier que l'information recherchée n'est pas à cheval sur plusieurs colonnes et bien sûr faire de l'inspection visuelle pour détecter les jeux de mots cachés. Mais déjà, en quelques minutes, on a des statistiques partielles sur le phénomène des coiffeurs blagueurs.

:::

::: {.content-visible when-profile="en"}

Of course, to go further, it would be better to normalize the data more thoroughly, check that the information sought is not spread across multiple columns, and conduct visual inspections to detect hidden puns. But already, in just a few minutes, we have partial statistics on the phenomenon of punny hairdressers.

:::

::: {.content-visible when-profile="fr"}

### Le NIR et la question de la confidentialité des identifiants individuels

En ce qui concerne les individus, il existe un identifiant unique permettant de relier ceux-ci dans différentes sources de données : le [NIR](https://www.cnil.fr/fr/definition/nir-numero-dinscription-au-repertoire), aussi connu sous le nom de numéro Insee ou numéro de sécurité sociale.
Ce numéro est nécessaire à l'administration pour la gestion des droits à prestations sociales (maladie, vieillesse, famille...). Au-delà de cette fonction qui peut être utile au quotidien, ce numéro est un identifiant individuel unique dans le [Répertoire national d'identification des personnes physiques (RNIPP)](https://www.insee.fr/fr/metadonnees/definition/c1602).

Cet identifiant est principalement présent dans des bases de gestion, liées aux fiches de paie, aux prestations sociales, etc. Cependant, _a contrario_ du numéro Sirene, celui-ci contient en lui-même plusieurs informations sensibles - en plus d'être intrinsèquement relié à la problématique sensible des droits à la sécurité sociale.

![Le numéro de sécurité sociale (Source: [Améli](https://www.ameli.fr/assure/droits-demarches/principes/numero-securite-sociale))](https://www.ameli.fr/sites/default/files/styles/webp_ckeditor/public/thumbnails/image/infographie_assures-regle-identification-assures.gif.webp?itok=j2owVDrB){fig-width="80%"}

Pour pallier ce problème, a récémment été mis en oeuvre le [code statistique non signifiant (CSNS)](https://www.insee.fr/fr/information/7635825?sommaire=7635842) ou NIR haché, un identifiant individuel anonyme non identifiant. L'objectif de cet identifiant anonymisé est de réduire la dissémination d'une information personnelle qui permettait certes aux fonctionnaires et chercheurs de relier de manière déterministe de nombreuses bases de données mais donnait une information non indispensable aux analystes sur les personnes en question. 


[^flou]: Autrement, on rentre dans le monde des appariements flous ou des appariements probabilistes. Les appariements flous sont des situations où on ne dispose plus d'un identifiant exact pour associer deux bases mais d'une information partiellement bruitée entre deux sources pour faire cette mise en relation. Par exemple, dans une base de données produit on aura `Coca Cola 33CL` et dans une autre `Coca Cola canette` mais sous ces deux noms sont cachés le même produit. Le chapitre d'[ouverture aux enjeux de recherche textuelle avec `ElasticSearch`](/content/modern-ds/elastic.qmd) est consacré à cette problématique. Les appariements probabilistes sont un autre type d'approche. Dans ceux-ci, on associe des observations dans deux bases non pas sur la base d'un identifiant mais sur la distance entre un ensemble de caractéristiques dans les deux bases. Cette technique est très utilisée dans les statistiques médicales ou dans l'évaluation de politiques publiques sur la base du [_propensity score matching_](https://en.wikipedia.org/wiki/Propensity_score_matching).

:::

::: {.content-visible when-profile="en"}

### The social security number and the issue of individual identifiers' confidentiality

For individuals, there exists a unique identifier that allows linking them across different data sources: the [NIR](https://www.cnil.fr/fr/definition/nir-numero-dinscription-au-repertoire), also known as the INSEE number or social security number.
This number is necessary for the administration to manage social benefits (health, retirement, family...). Beyond this function, which can be useful daily, this number is a unique individual identifier in the [National Register of Physical Persons (RNIPP)](https://www.insee.fr/fr/metadonnees/definition/c1602).

This identifier is mainly present in management databases related to payroll, social benefits, etc. However, unlike the Sirene number, it contains several sensitive pieces of information and is inherently linked to the sensitive issue of social security rights.

![Social security number (Source: [Améli](https://www.ameli.fr/assure/droits-demarches/principes/numero-securite-sociale))](https://www.ameli.fr/sites/default/files/styles/webp_ckeditor/public/thumbnails/image/infographie_assures-regle-identification-assures.gif.webp?itok=j2owVDrB){fig-width="80%"}

To address this problem, the [non-significant statistical code (CSNS)](https://www.insee.fr/fr/information/7635825?sommaire=7635842) or hashed NIR, a non-identifying anonymous individual identifier, was recently implemented. The goal of this anonymized identifier is to reduce the dissemination of personal information that, although allowing civil servants and researchers to deterministically link numerous databases, provided analysts with non-essential information about the individuals in question.

[^flou]: Otherwise, we enter the realm of fuzzy matching or probabilistic matching. Fuzzy matching occurs when we no longer have an exact identifier to link two databases but have partially noisy information between two sources to make the connection. For example, in a product database, we might have `Coca Cola 33CL` and in another `Coca Cola canette`, but these names hide the same product. The chapter on [Introduction to Textual Search with ElasticSearch](/content/modern-ds/elastic.qmd) addresses this issue. Probabilistic matching is another approach. In these, observations in two databases are associated not based on an identifier but on the distance between a set of characteristics in both databases. This technique is widely used in medical statistics or in the evaluation of public policies based on [_propensity score matching_](https://en.wikipedia.org/wiki/Propensity_score_matching).

:::

::: {.content-visible when-profile="fr"}

## Exercices d'application

### Pourquoi a-t-on besoin d'un code commune quand on a déjà son nom ?

Cet exercice va revenir un peu en arrière afin de saisir pourquoi nous avons pris comme hypothèse ci-dessus que le code commune était la clé de jointure.

:::

::: {.content-visible when-profile="en"}

### Why do we need a commune code when we already have its name?

This exercise will take a step back to understand why we assumed above that the commune code was the key for joining data.
:::


{{< include "02_pandas_suite/_exo3.qmd" >}}
{{< include "02_pandas_suite/_exo3_solution.qmd" >}}

::: {.content-visible when-profile="fr"}

Ce petit exercice permet donc de se rassurer car les libellés dupliqués
sont en fait des noms de commune identiques mais qui ne sont pas dans le même département.
Il ne s'agit donc pas d'observations dupliquées.
On peut donc se fier aux codes communes, qui eux sont uniques.

:::

::: {.content-visible when-profile="en"}

This small exercise reassures us as the duplicated labels are actually the same commune names but in different departments. So, these are not duplicated observations. We can thus rely on the commune codes, which are unique.

:::

::: {.content-visible when-profile="fr"}

### Calculer une empreinte carbone grâce à l'association entre des sources

:::

::: {.content-visible when-profile="en"}

### Associating different sources to compute carbon footprints 

:::


{{< include "02_pandas_suite/_exo4.qmd" >}}
{{< include "02_pandas_suite/_exo4_solution.qmd" >}}


::: {.content-visible when-profile="fr"}

# Formatter des tableaux de statistiques descriptives

Un _dataframe_ `Pandas`
est automatiquement mis en forme lorsqu'il est visualisé depuis un _notebook_ sous forme de table HTML à la mise en forme minimaliste.
Cette mise en forme est pratique pour voir
les données, une tâche indispensable pour les _data scientists_
mais ne permet pas d'aller vraiment au-delà. 

Dans une phase
exploratoire, il peut être pratique d'avoir un tableau
un peu plus complet, intégrant notamment des visualisations
minimalistes, pour mieux connaître ses données. Dans la phase
finale d'un projet, lorsqu'on communique sur un projet, il 
est avantageux de disposer d'une visualisation attrative. 
Pour ces deux besoins, les sorties des _notebooks_ sont
une réponse peu satisfaisante, en plus de nécessiter
le _medium_ du _notebook_ qui peut en rebuter certains. 

Heureusement, le tout jeune _package_ [`great_tables`](https://posit-dev.github.io/great-tables/get-started/) permet, simplement, de manière programmatique, la création de tableaux
qui n'ont rien à envier à des productions manuelles fastidieuses faites dans `Excel`
et difficilement répliquables. Ce _package_ est un portage en `Python` du _package_ [`GT`](https://gt.rstudio.com/).
`great_tables` construit des tableaux
_html_ ce qui offre une grande richesse dans la mise en forme et permet une excellente intégration avec [`Quarto`](https://quarto.org/), l'outil de publication reproductible développé par 
L'exercice suivant proposera de construire un tableau avec
ce _package_, pas à pas. 

Afin de se concentrer sur la construction du tableau, 
les préparations de données à faire en amont sont données
directement. Nous allons repartir de ce jeu de données:

:::

::: {.content-visible when-profile="en"}

# Formatting descriptive statistics tables

A `Pandas` DataFrame is automatically formatted when viewed from a notebook as a minimally styled HTML table. This formatting is convenient for viewing data, a necessary task for data scientists, but it doesn't go much beyond that.

In an exploratory phase, it can be useful to have a more complete table, including minimal visualizations, to better understand the data. In the final phase of a project, when communicating about it, having an attractive visualization is advantageous. The outputs of notebooks are not a satisfactory solution for these needs and require the medium of the notebook, which can deter some users.

Fortunately, the young package [`great_tables`](https://posit-dev.github.io/great-tables/get-started/) allows for the creation of tables programmatically that rival tedious manual productions in `Excel` and are difficult to replicate. This package is a `Python` port of the `GT` package. `great_tables` builds _HTML_ tables, offering great formatting richness and excellent integration with [`Quarto`](https://quarto.org/), the reproducible publishing tool developed by RStudio.

The following exercise will propose building a table with this package, step by step.

To focus on table construction, the necessary data preparations are provided directly. We will start from this dataset:

:::

```{python}
#| echo: false
emissions_merged.head(3)
```

{{< include "02_pandas_suite/_exo5_preliminary.qmd" >}}
{{< include "02_pandas_suite/_exo5.qmd" >}}


::: {.content-visible when-profile="fr"}
Grâce à celui-ci, on peut déjà comprendre que notre définition
de l'empreinte carbone est certainement défaillante. Il apparaît
peu plausible que les habitants du 77 aient une empreinte 500 fois
supérieure à celle de Paris intra-muros. La raison principale ? 
On n'est pas sur un concept d'émissions à la consommation mais à la
production, ce qui pénalise les espaces industriels ou les espaces
avec des aéroports...

Pour aller plus loin sur la construction de tableaux
avec `great_tables`, vous pouvez répliquer 
cet [exercice](https://rgeo.linogaliana.fr/exercises/eval.html)
de production de tableaux électoraux
que j'ai proposé pour un cours de `R` avec `gt`, l'équivalent
de `great_tables` pour `R`.
:::

::: {.content-visible when-profile="en"}
Thanks to this, we can already understand that our definition of the carbon footprint is certainly flawed. It seems unlikely that the inhabitants of the 77th department have a carbon footprint 500 times greater than that of intra-muros Paris. The main reason? We are not dealing with a concept of consumption emissions but production emissions, which penalizes industrial areas or areas with airports...

To learn more about constructing tables with `great_tables`, you can replicate this [exercise](https://rgeo.linogaliana.fr/exercises/eval.html) on producing electoral tables that I proposed for an `R` course with `gt`, the equivalent of `great_tables` for `R`.
:::

::: {.content-visible when-profile="fr"}

# `Pandas`: vers la pratique et au-delà 

## `Pandas` dans une chaine d'opérations

En général, dans un projet, le nettoyage de données va consister en un ensemble de
méthodes appliquées à un `DataFrame` ou alors une `Serie` lorsqu'on travaille exclusivement sur une colonne. 
Autrement dit, ce qui est généralement attendu lorsqu'on fait du `Pandas` c'est d'avoir une chaîne qui prend un `DataFrame` en entrée et ressort ce même `DataFrame` enrichi, ou une version agrégée de celui-ci, en sortie.

Cette manière de procéder est le coeur de la syntaxe `dplyr` en `R` mais n'est pas forcément native en `Pandas` selon les opérations qu'on désire mettre en oeuvre. En effet, la manière naturelle de mettre à jour un _dataframe_ en `Pandas` passe souvent par une syntaxe du type:
:::

::: {.content-visible when-profile="en"}

# `Pandas`: towards practice and beyond 

## `Pandas` in a chain of operations

Generally, in a project, data cleaning will consist of a series of methods applied to a `DataFrame` or a `Series` when working exclusively on a single column. In other words, what is usually expected when working with `Pandas` is to have a chain that takes a `DataFrame` as input and outputs the same `DataFrame` enriched or an aggregated version of it.

This way of proceeding is at the heart of the `dplyr` syntax in `R` but is not necessarily native in `Pandas` depending on the operations you want to implement. Indeed, the natural way to update a dataframe in `Pandas` often involves syntax like:
:::

```{python}
#| echo: true
import numpy as np
import pandas as pd

data = [[8000, 1000], [9500, np.nan], [5000, 2000]]
df = pd.DataFrame(data, columns=['salaire', 'autre_info'])
df['salaire_net'] = df['salaire']*0.8
```

::: {.content-visible when-profile="fr"}
En `SQL` on pourrait directement mettre à jour notre base de données avec la nouvelle colonne :
:::
::: {.content-visible when-profile="en"}
In `SQL` you could directly update your database with the new column:
:::

```sql
SELECT *, salaire*0.8 AS salaire_net FROM df
```

::: {.content-visible when-profile="fr"}
L'écosystème du _tidyverse_ en `R`, l'équivalent de `Pandas`, fonctionne selon la même logique que SQL de mise à jour de table. On ferait en effet la commande suivante avec `dplyr`:
:::
::: {.content-visible when-profile="en"}
The `tidyverse` ecosystem in `R`, the equivalent of `Pandas`, works according to the same logic as SQL table updates. Indeed, you would use the following command with `dplyr`:
:::

```r
df %>% mutate(salaire_net = salaire*0.8) 
```

::: {.content-visible when-profile="fr"}
Techniquement on pourrait faire ceci avec un `assign` en `Pandas`

```{python}
#| echo: true
df = df.drop("salaire_net", axis = "columns") #<1>
df = df.assign(salaire_net = lambda s: s['salaire']*0.8)
```
1. Pour effacer la variable afin de repartir de l'exemple initial

:::

::: {.content-visible when-profile="en"}
Technically, you could do this with an `assign` in `Pandas`:

```{python}
#| echo: true
df = df.drop("salaire_net", axis = "columns") #<1>
df = df.assign(salaire_net = lambda s: s['salaire']*0.8)
```
1. To delete the variable to start from the initial example
:::

::: {.content-visible when-profile="fr"}

Cependant cette syntaxe `assign` n'est pas très naturelle. Il est nécessaire de lui passer une _lambda function_ qui attend comme _input_ un `DataFrame` là où on voudrait une colonne. Il ne s'agit donc pas vraiment d'une syntaxe lisible et pratique. 

Il est néanmoins possible d'enchaîner des opérations sur des jeux de données grâce aux [_pipes_](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html). Ceux-ci reprennent la même philosophie que celle de `dplyr`, elle-même inspirée du _pipe_ Linux. 
Cette approche permettra de rendre plus lisible le code en définissant des fonctions effectuant des opérations sur une ou plusieurs colonnes d'un DataFrame. Le premier argument à indiquer à la fonction est le `DataFrame`, les autres sont ceux permettant de contrôler son comportement

:::

::: {.content-visible when-profile="en"}
However, this `assign` syntax is not very natural. It requires passing a lambda function that expects a `DataFrame` as input where you would want a column. So, it is not really a readable and practical syntax.

It is nevertheless possible to chain operations on datasets using [pipes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html). These follow the same philosophy as `dplyr`, itself inspired by the Linux pipe. This approach will make the code more readable by defining functions that perform operations on one or more columns of a DataFrame. The first argument to the function is the `DataFrame`, the others are those controlling its behavior:
:::


```{python}
#| echo: true
def calcul_salaire_net(df: pd.DataFrame, col: str, taux: float = 0.8):
  df["salaire_net"] = df[col]*taux
  return df
```

::: {.content-visible when-profile="fr"}
Ce qui transforme notre chaine de production en
:::
::: {.content-visible when-profile="en"}
This transforms our production chain into:
:::

```{python}
#| echo: true
(
  df
  .pipe(calcul_salaire_net, "salaire")
)
```

::: {.content-visible when-profile="fr"}
## Quelques limites sur la syntaxe de `Pandas` 

Il y a un avant et un après `Pandas` dans l'analyse de données en `Python`. Sans ce _package_ ô combien pratique `Python`, malgré toutes les forces de ce langage, aurait eu du mal à s'installer dans le paysage de l'analyse de données. Cependant, si `Pandas` propose une syntaxe cohérente sur de nombreux aspects, elle n'est pas parfaite non plus. Les paradigmes plus récents d'analyse de données en `Python` ont d'ailleurs parfois l'ambition de corriger ces imperfections syntaxiques là. 

Parmi les points les plus génants au quoditien il y a le besoin de régulièrement faire des `reset_index` lorsqu'on construit des statistiques descriptives. En effet, il peut être dangereux de garder des indices qu'on ne contrôle pas bien car, sans attention de notre part lors des phases de _merge_, ils peuvent être utilisés à mauvais escient par `Pandas` pour joindre les données ce qui peut provoquer des suprises. 

`Pandas` est extrêmement bien fait pour restructurer des données du format _long_ to _wide_ ou _wide_ to _long_. Cependant, ce n'est pas la seule manière de restructurer un jeu de données qu'on peut vouloir mettre en oeuvre. Il arrive régulièrement qu'on désire comparer la valeur d'une observation à celle d'un groupe à laquelle elle appartient. C'est notamment particulièrement utile dans une phase d'analyse des anomalies, valeurs aberrantes ou lors d'une investigation de détection de fraude. De manière native, en `Pandas`, il faut construire une statistique agrégée par groupe et refaire un _merge_ aux données initiales par le biais de la variable de groupe. C'est un petit peu fastidieux:
:::

::: {.content-visible when-profile="en"}

## Some limitations regarding `Pandas` syntax

There is a before and after `Pandas` in data analysis with `Python`. Without this incredibly practical package, Python, despite all its strengths, would have struggled to establish itself in the data analysis landscape. However, while `Pandas` offers a coherent syntax in many aspects, it is not perfect either. More recent data analysis paradigms in `Python` sometimes aim to correct these syntactical imperfections.

Among the most annoying points in everyday use is the need to regularly perform `reset_index` when building descriptive statistics. Indeed, it can be dangerous to keep indices that are not well controlled because, if we are not careful during the merge phases, they can be misused by `Pandas` to join data, leading to surprises.

`Pandas` is extremely well-designed for restructuring data from long to wide format or vice versa. However, this is not the only way to restructure a dataset that we might want to implement. It often happens that we want to compare the value of an observation to that of a group to which it belongs. This is particularly useful in anomaly analysis, outlier detection, or fraud investigation. Natively, in `Pandas`, you need to build an aggregate statistic by group and then merge it back to the initial data using the group variable. This is somewhat tedious:

:::

```{python}
#| echo: true
emissions_moyennes = emissions.groupby("dep").agg({"Agriculture": "mean"}).reset_index()
emissions_enrichies = (
  emissions
  .merge(emissions_moyennes, on = "dep", suffixes = ['', '_moyenne_dep'])
)
emissions_enrichies['relatives'] = emissions_enrichies["Agriculture"]/emissions_enrichies["Agriculture_moyenne_dep"]
emissions_enrichies.head()
```

::: {.content-visible when-profile="fr"}
Dans le _tidyverse_, cette opération en deux temps pourrait être faite en une seule étape, ce qui est plus pratique
:::
::: {.content-visible when-profile="en"}
In the `tidyverse`, this two-step operation could be done in a single step, which is more convenient:
:::

```r
emissions %>%
  group_by(dep) %>%
  mutate(relatives = Agriculture/mean(Agriculture))
```

::: {.content-visible when-profile="fr"}

Ce n'est pas si grave mais cela alourdit la longueur des chaines de traitement faites en `Pandas` et donc la charge de maintenance pour les faire durer dans le temps.  

De manière plus générale, les chaînes de traitement `Pandas` peuvent être assez verbeuses, car il faut régulièrement redéfinir le `DataFrame` qu'on utilise plutôt que simplement les colonnes. Par exemple, pour faire un filtre sur les lignes et les colonnes, il faudra faire:

:::

::: {.content-visible when-profile="en"}

This isn't too bad, but it does make `Pandas` processing chains longer and therefore increases the maintenance burden to keep them running over time.

More generally, `Pandas` processing chains can be quite verbose because it is often necessary to redefine the `DataFrame` rather than just the columns. For example, to filter rows and columns, you have to:

:::

```{python}
#| echo: true
(
  emissions
  .loc[
    (emissions["dep"] == "12") & (emissions["Routier"]>500), ['INSEE commune', 'Commune']
  ]
  .head(5)
)
```
::: {.content-visible when-profile="fr"}
En SQL on pourrait se contenter de faire référence aux colonnes dans le filter
:::
::: {.content-visible when-profile="en"}
In SQL, you could simply refer to the columns in the filter:
:::

```sql
SELECT "INSEE commune", 'Commune'
FROM emissions 
WHERE dep=="12" AND Routier>500
```

::: {.content-visible when-profile="fr"}
Dans le _tidyverse_ (`R`) on pourrait aussi faire ceci simplement
:::

::: {.content-visible when-profile="en"}
In the `tidyverse` (R), you could also do this simply:
:::


```r
df %>%
  filter(dep=="12", Routier>500) %>%
  select(`INSEE commune`, `Commune`)
```

::: {.content-visible when-profile="fr"}

# Les autres paradigmes

Ces deux chapitres ont permis d'explorer en profondeur la richesse de l'écosystème `Pandas` qui est un indispensable dans la boite à outil du _data scientist_. Malgré toutes les limites que nous avons pu évoquer, et les solutions alternatives que nous allons présenter, `Pandas` reste LE _package_ central de l'écosystème de la donnée avec `Python`. Nous allons voir dans les prochains chapitres son intégration native à l'écosystème `Scikit` pour le _machine learning_ ou l'extension de `Pandas` aux données spatiales avec `GeoPandas`. 

Les autres solutions techniques que nous allons ici évoquer peuvent être pertinentes si on désire traiter des volumes de données importants ou si on désire utiliser des syntaxes alternatives. 

Les principales alternatives à `Pandas` sont [`Polars`](https://pola.rs/), [`DuckDB`](https://duckdb.org/) et [`Spark`](https://spark.apache.org/docs/latest/api/python/index.html). Il existe également [`Dask`](https://www.dask.org/), une librairie pour paralléliser des traitements écris en `Pandas`.

:::

::: {.content-visible when-profile="en"}
# Other paradigms

These two chapters have explored in depth the richness of the `Pandas` ecosystem, which is indispensable in the data scientist's toolbox. Despite all the limitations we have mentioned, and the alternative solutions we will present, `Pandas` remains the central package of the data ecosystem with `Python`. In the following chapters, we will see its native integration with the `Scikit` ecosystem for machine learning or the extension of `Pandas` to spatial data with `GeoPandas`.

Other technical solutions that we will discuss here may be relevant if you want to handle large volumes of data or if you want to use alternative syntaxes.

The main alternatives to `Pandas` are [`Polars`](https://pola.rs/), [`DuckDB`](https://duckdb.org/), and [`Spark`](https://spark.apache.org/docs/latest/api/python/index.html). There is also [`Dask`](https://www.dask.org/), a library for parallelizing `Pandas` operations.
:::

::: {.content-visible when-profile="fr"}

## `Polars`

`Polars` est certainement le paradigme le plus inspiré de `Pandas`, jusqu'au choix du nom. La première différence fondamentale est dans les couches internes utilisées. `Polars` s'appuie sur l'implémentation `Rust` de `Arrow` là où `Pandas` s'appuie sur `Numpy` ce qui est facteur de perte de performance. Cela permet à `Polars` d'être plus efficace sur de gros volumes de données, d'autant que de nombreuses opérations sont parallélisées et reposent sur l'évaluation différées (_lazy evaluation_) un principe de programmation qui permet d'optimiser les requêtes pour ne pas les exécuter dans l'ordre de définition mais dans un ordre logique plus optimal.

Une autre force de `Polars` est la syntaxe plus cohérente, qui bénéficie du recul d'une quinzaine d'années d'existence de `Pandas` et d'une petite dizaine d'années de `dplyr` (le _package_ de manipulation de données au sein du paradigme du _tidyverse_ en `R`). Pour reprendre l'exemple précédent, il n'est plus nécessaire de forcer la référence au _DataFrame_, dans une chaîne d'exécution toutes les références ultérieures seront faites au regard du _DataFrame_ de départ

:::

::: {.content-visible when-profile="en"}

## `Polars`

`Polars` is certainly the paradigm most inspired by `Pandas`, even in the choice of name. The first fundamental difference lies in the internal layers used. `Polars` relies on the `Rust` implementation of `Arrow`, whereas `Pandas` relies on `Numpy`, which results in performance loss. This allows `Polars` to be more efficient on large volumes of data, especially since many operations are parallelized and rely on lazy evaluation, a programming principle that optimizes queries for logical rather than defined execution order.

Another strength of `Polars` is its more coherent syntax, benefiting from over fifteen years of `Pandas` existence and almost a decade of `dplyr` (the data manipulation package within the `R` tidyverse paradigm). To take the previous example, there is no longer a need to force the reference to the DataFrame; in an execution chain, all subsequent references will be made with respect to the initial DataFrame.
²
:::


```{python}
#| eval: false
!pip install polars
```

```{python}
#| echo: true
import polars as pl
emissions_polars = pl.from_pandas(emissions)
(
  emissions_polars
  .filter(pl.col("dep") == "12", pl.col("Routier") > 500)
  .select('INSEE commune', 'Commune')
  .head(5)
)
```

::: {.content-visible when-profile="fr"}

Pour découvrir `Polars`, de nombreuses ressources en ligne sont accessibles, notamment [ce _notebook_](https://github.com/InseeFrLab/ssphub/blob/main/post/polars/polars-tuto.ipynb) construit pour le réseau des _data scientists_ de la statistique publique. 

:::

::: {.content-visible when-profile="en"}

To learn about `Polars`, many online resources are available, including [this notebook](https://github.com/InseeFrLab/ssphub/blob/main/post/polars/polars-tuto.ipynb) built for the public statistics data scientists network.
:::

::: {.content-visible when-profile="fr"}

## `DuckDB`

_DuckDB_ est le nouveau venu dans l'écosystème de l'analyse de données repoussant les limites des données pouvant être traitées avec `Python` sans passer par des outils _big data_ comme `Spark`.
_DuckDB_ est la quintessence d'un nouveau paradigme, celui du [_"Big data is dead"_](https://motherduck.com/blog/big-data-is-dead/), où on peut traiter des données de volumétrie importante sans recourir à des infrastructures imposantes.

Outre sa grande efficacité, puisqu'avec _DuckDB_ on peut traiter des données d'une volumétrie supérieure à la mémoire vive de l'ordinateur ou du serveur, _DuckDB_ présente l'avantage de proposer une syntaxe uniforme quelle que soit le langage qui appelle _DuckDB_ (`Python`, `R`, `C++` ou `Javascript`). _DuckDB_ privilégie la syntaxe SQL pour traiter les données avec de nombreuses fonctions pré-implementées pour simplifier certaines transformations de données (par exemple pour les [données textuelles](https://duckdb.org/docs/sql/functions/char.html), les [données temporelles](https://duckdb.org/docs/sql/functions/time), etc.).

Par rapport à d'autres systèmes s'appuyant sur SQL, comme [`PostGreSQL`](https://www.bing.com/search?go=Rechercher&q=PostGreSQL&qs=ds&form=QBRE), `DuckDB` est très simple d'installation, ce n'est qu'une librairie `Python` là où beaucoup d'outils comme `PostGreSQL` nécessite une infrastructure adaptée. 

:::

::: {.content-visible when-profile="en"}
## `DuckDB`

`DuckDB` is the newcomer in the data analysis ecosystem, pushing the limits of data processing with `Python` without resorting to big data tools like `Spark`. `DuckDB` epitomizes a new paradigm, the ["Big data is dead"](https://motherduck.com/blog/big-data-is-dead/) paradigm, where large data volumes can be processed without imposing infrastructures.

Besides its great efficiency, as `DuckDB` can handle data volumes larger than the computer or server's RAM, it offers the advantage of a uniform syntax across languages that call `DuckDB` (`Python`, `R`, `C++`, or `Javascript`). `DuckDB` favors SQL syntax for data processing with many pre-implemented functions to simplify certain data transformations (e.g., for [text data](https://duckdb.org/docs/sql/functions/char.html), [time data](https://duckdb.org/docs/sql/functions/time), etc.).

Compared to other SQL-based systems like [`PostGreSQL`](https://www.postgresql.org/), `DuckDB` is very simple to install, as it is just a `Python` library, whereas many tools like `PostGreSQL` require an appropriate infrastructure.

:::


```{python}
#| eval: false
!pip install duckdb
```

::: {.content-visible when-profile="fr"}
Pour reprendre l'exemple précédent, on peut utiliser directement le code SQL précédent
:::
::: {.content-visible when-profile="en"}
To reuse the previous example, we can directly use the SQL code mentioned earlier.
:::

```{python}
#| echo: true
import duckdb
duckdb.sql(
  """
  SELECT "INSEE commune", "Commune"
  FROM emissions
  WHERE dep=='12' AND Routier>500
  LIMIT 5
  """)
```

::: {.content-visible when-profile="fr"}
Ici la clause `FROM emissions` vient du fait qu'on peut directement exécuter du SQL depuis un objet `Pandas` par le biais de `DuckDB`. Si on fait la lecture directement dans la requête, celle-ci se complexifie un petit peu mais la logique est la même
:::

::: {.content-visible when-profile="en"}
Here, the clause `FROM emissions` comes from the fact that we can directly execute SQL from a `Pandas` object via `DuckDB`. If we read directly in the query, it gets slightly more complex, but the logic remains the same.
:::


```{python}
#| echo: true
import duckdb
duckdb.sql(
  f"""
  SELECT "INSEE commune", "Commune"
  FROM read_csv_auto("{url}")
  WHERE
    substring("INSEE commune",1,2)=='12'
    AND
    Routier>500
  LIMIT 5
  """)
```

::: {.content-visible when-profile="fr"}

Le rendu du _DataFrame_  est légèrement différent de `Pandas` car, comme `Polars` et de nombreux systèmes de traitement de données volumineuses, `DuckDB` repose sur l'évaluation différée et donc ne présente en _display_ qu'un échantillon de données. 
`DuckDB` et `Polars` sont d'ailleurs très bien intégrés l'un à l'autre. On peut très bien faire du SQL sur un objet `Polars` via `DuckDB` ou appliquer des fonctions `Polars` sur un objet initialement lu avec `DuckDB`.

L'un des intérêts de `DuckDB` est son excellente intégration  avec  l'écosystème `Parquet`, le format de données déjà mentionné qui devient un standard dans le partage de données (il s'agit, par exemple, de la pierre angulaire du partage de données sur la plateforme _HuggingFace_). Pour en savoir plus sur `DuckDB` et découvrir son intérêt pour lire les données du recensement de la population française, vous pouvez consulter [ce post de blog](https://ssphub.netlify.app/post/parquetrp/). 
:::

::: {.content-visible when-profile="en"}
The rendering of the DataFrame is slightly different from `Pandas` because, like `Polars` and many large data processing systems, `DuckDB` relies on lazy evaluation and thus only displays a sample of data. `DuckDB` and `Polars` are also well integrated with each other. You can run SQL on a `Polars` object via `DuckDB` or apply `Polars` functions to an initially read `DuckDB` object.

One of the interests of `DuckDB` is its excellent integration with the `Parquet` ecosystem, the already mentioned data format that is becoming a standard in data sharing (for example, it is the cornerstone of data sharing on the HuggingFace platform). To learn more about `DuckDB` and discover its usefulness for reading data from the French population census, you can check out [this blog post](https://ssphub.netlify.app/post/parquetrp/).
:::

::: {.content-visible when-profile="fr"}

## `Spark` et le _big data_

`DuckDB` a repoussé les frontières du _big data_ qu'on peut définir comme le volume de données à partir duquel on ne peut plus traiter celles-ci sur une machine sans mettre en oeuvre une stratégie de parallélisation.

Néanmoins, pour les données très volumineuses, `Python` est très bien armé grâce à la librairie [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html). Celle-ci est une API en Python pour le langage `Spark`, un langage _big data_ basé sur Scala. Ce paradigme est construit sur l'idée que les utilisateurs de `Python` y accèdent par le biais de _cluster_ avec de nombreux noeuds pour traiter la donnée de manière parallèle. Celle-ci sera lue par blocs, qui seront traités en parallèle en fonction du nombre de noeuds parallèles. L'API DataFrame de `Spark` présente une syntaxe proche de celle des paradigmes précédents avec une ingénieurie plus complexe en arrière-plan liée à la parallélisation native. 

:::

::: {.content-visible when-profile="en"}

## `Spark`

`DuckDB` has pushed the boundaries of big data, which can be defined as the volume of data that can no longer be processed on a single machine without implementing a parallelization strategy.

Nevertheless, for very large data volumes, `Python` is well-equipped with the [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html) library. This is a Python API for the Spark language, a big data language based on Scala. This paradigm is built on the idea that `Python` users access it via clusters with many nodes to process data in parallel. The data will be read in blocks, processed in parallel depending on the number of parallel nodes. The `Spark` DataFrame API has a syntax close to previous paradigms with more complex engineering in the background related to native parallelization.

:::