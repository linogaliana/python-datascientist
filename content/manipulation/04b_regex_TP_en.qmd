---
title: "Mastering regular expressions"
draft: false
weight: 70
slug: regex
tags:
  - regex
  - pandas
  - re
  - Manipulation
  - Tutoriel
categories:
  - Tutoriel
  - Manipulation
type: book
description: |
  Regular expressions provide a very practical framework for flexibly manipulating
  textual data. They are especially useful for natural language processing (__NLP__)
  tasks or cleaning textual data.
image: https://d2h1bfu6zrdxog.cloudfront.net/wp-content/uploads/2022/04/img_625491e9ce092.png
---

::: {.content-visible when-format="html"}
{{< include "../../build/_switch_lang.qmd" >}}

If you want to try the examples in this tutorial:

{{< include "../../build/_printBadges.qmd" >}}
:::

::: {.content-visible when-format="ipynb"}
{{warninglang}}
:::

# Introduction

`Python` offers a lot of very useful functionalities for handling textual data. This is one of the reasons for its success in the natural language processing (NLP) community (see the dedicated section).

In previous chapters, we sometimes needed to search for basic textual elements. This was possible with the `str.find` method from the `Pandas` package, which is a vectorized version of the basic `find` method. We could also use the basic method directly, especially when performing _web scraping_.

However, this search function quickly reaches its limits. For instance, if we want to find both the singular and plural occurrences of a term, we will need to use the `find` method at least twice. For conjugated verbs, it becomes even more complex, especially if their form changes according to the subject.

For complicated expressions, it is advisable to use __regular expressions__ or _"regex"_. This is a feature found in many programming languages. It is a form of grammar that allows for searching for patterns.

Part of the content in this section is an adaptation of the
[collaborative documentation on `R` called `utilitR`](https://www.book.utilitr.org/03_fiches_thematiques/fiche_donnees_textuelles#regex) to which I contributed. This chapter also draws from the book [_R for Data Science_](https://r4ds.hadley.nz/regexps.html) which presents a very pedagogical chapter on regex.

We will use the _package_ `re` to illustrate our examples of regular expressions. This is the reference package used by `Pandas` in the background to vectorize text searches.

```{python}
import re
import pandas as pd
```

::: {.tip}

**Regular expressions (*regex*) are notoriously difficult to master.** There are tools that make working with regular expressions easier.

- The reference tool for this is [https://regex101.com/] which allows you to test `regex` in `Python` with an explanation accompanying the test.

- Similarly, [this site](https://ole.michelsen.dk/tools/regex/) has a cheat sheet at the bottom of the page.

- The [Regex Crossword](https://regexcrossword.com/) games allow you to learn regular expressions while having fun.

It can be useful to ask assistant AIs, such as `Github Copilot` or `ChatGPT`, for a first version of a regex by explaining the content you want to extract. This can save a lot of time, except when the AI is overconfident and offers you a completely wrong regex...

:::

# Principle

**Regular expressions are a tool used to describe a set of possible strings according to a precise syntax, and thus define a `pattern`.** Regular expressions are used, for example, when you want to extract a part of a string or replace a part of a string. A regular expression takes the form of a string, which can contain both literal elements and special characters with logical meaning.

For example, `"ch.+n"` is a regular expression that describes the following pattern: the literal string `ch`, followed by any string of at least one character (`.+`), followed by the letter `n`. In the string `"J'ai un chien."`, the substring `"chien"` matches this pattern. The same goes for `"chapeau ron"` in `"J'ai un chapeau rond"`. In contrast, in the string `"La soupe est chaude."`, no substring matches this pattern (because no `n` appears after the `ch`).

To convince ourselves, we can look at the first two cases:

```{python}
pattern = "ch.+n"
print(re.search(pattern, "J'ai un chien."))
print(re.search(pattern, "J'ai un chapeau rond."))
```

However, in the last case, we do not find the desired pattern:

```{python}
print(re.search(pattern, "La soupe est chaude."))
```

The previous regex contained two types of characters:

- _Literal characters_: letters and numbers recognized literally.
- _Meta-characters_: symbols that have a special meaning in regex.

The main _meta-characters_ are `.`, `+`, `*`, `[`, `]`, `^`, and `$` but there are many others. Among this set, we mainly use quantifiers (`.`, `+`, `*`...), character classes (sets delimited by `[` and `]`), or anchors (`^`, `$`...)

In the previous example, we had two adjacent quantifiers `.+`. The first (`.`) means any character[^1]. The second (`+`) means _"repeat the previous pattern"_. In our case, the combination `.+` allows us to repeat any character before finding an _n_. The number of times is indeterminate: it may not be necessary to intersperse characters before the _n_ or it may be necessary to capture several:

```{python}
print(re.search(pattern, "J'ai un chino"))
print(re.search(pattern, "J'ai un chiot très mignon."))
```

[^1]: Any character except for the newline (`\n`). Keep this in mind; I have already spent hours trying to understand why my `.` did not capture what I wanted spanning multiple lines...

## Character classes

When searching, we are interested in characters and often in character classes: we look for a digit, a letter, a character in a specific set, or a character that does not belong to a specific set. Some sets are predefined, others must be defined using brackets.

To define a character set, you need to write this set within brackets. For example, `[0123456789]` denotes a digit. Since it is a sequence of consecutive characters, we can summarize this notation as `[0-9]`.

For example, if we want to find all patterns that start with a `c` followed by an `h` and then a vowel (a, e, i, o, u), we can try this regular expression:

```{python}
re.findall("[c][h][aeiou]", "chat, chien, veau, vache, chèvre")
```

It would be more practical to use `Pandas` in this case to isolate the lines that meet the logical condition (by adding the accents that are otherwise not included):

```{python}
import pandas as pd
txt = pd.Series("chat, chien, veau, vache, chèvre".split(", "))
txt.str.match("ch[aeéèiou]")
```

However, the usage of character classes as shown above is not the most common. They are preferred for identifying complex patterns rather than a sequence of literal characters. Memory aid tables illustrate some of the most common character classes (`[:digit:]` or `\d`...)

## Quantifiers

We encountered quantifiers with our first regular expression. They control the number of times a pattern is matched.

The most common are:

- `?` : 0 or 1 match;
- `+` : 1 or more matches;
- `*` : 0 or more matches.

For example, `colou?r` will match both the American and British spellings:

```{python}
re.findall("colou?r", "Did you write color or colour?")
```

These quantifiers can of course be combined with other types of characters, especially character classes. This can be extremely useful. For example, `\d+` will capture one or more digits, `\s?` will optionally add a space, `[\w]{6,8}` will match a word between six and eight letters.

It is also possible to define the number of repetitions with `{}`:

- `{n}` matches exactly _n_ times;
- `{n,}` matches at least _n_ times;
- `{n,m}` matches between _n_ and _m_ times.

However, the repetition of terms by default only applies to the last character preceding the quantifier. We can confirm this with the example above:

```{python}
print(re.match("toc{4}","toctoctoctoc"))
```

To address this issue, parentheses are used. The principle is the same as with numeric rules: parentheses allow for introducing hierarchy. To revisit the previous example, we get the expected result thanks to the parentheses:

```{python}
print(re.match("(toc){4}","toctoctoctoc"))
print(re.match("(toc){5}","toctoctoctoc"))
print(re.match("(toc){2,4}","toctoctoctoc"))
```

::: {.note}

The regular expression algorithm always tries to match the largest piece to the regular expression.

For example, consider an HTML string:

```{python}
s = "<h1>Super titre HTML</h1>"
```

The regular expression `re.findall("<.*>", s)` potentially matches three pieces:

*  `<h1>`
*  `</h1>`
* `<h1>Super titre HTML</h1>`

It is the last one that will be chosen, as it is the largest. To select the smallest, you need to write the quantifiers like this: `*?`, `+?`. Here are a few examples:

```{python}
s = "<h1>Super titre HTML</h1>\n<p><code>Python</code> est un langage très flexible</p>"
print(re.findall("<.*>", s))
print(re.findall("<p>.*</p>", s))
print(re.findall("<p>.*?</p>", s))
print(re.compile("<.*?>").findall(s))
```

:::

```markdown
## Cheat sheet

The table below serves as a cheat sheet for regex:

|Regular expression|Meaning |
|------------------|---------------------------------|
|`"^"`             | Start of the string |
|`"$"`             | End of the string |
|`"\\."`           | A dot |
|`"."`             | Any character |
|`".+"`            | Any non-empty sequence of characters |
|`".*"`            | Any sequence of characters, possibly empty |
|`"[:alnum:]"`     | An alphanumeric character |
|`"[:alpha:]"`     | A letter |
|`"[:digit:]"`     | A digit |
|`"[:lower:]"`     | A lowercase letter |
|`"[:punct:]"`     | A punctuation mark |
|`"[:space:]"`     | A space |
|`"[:upper:]"`     | An uppercase letter |
|`"[[:alnum:]]+"`  | A sequence of at least one alphanumeric character |
|`"[[:alpha:]]+"`  | A sequence of at least one letter |
|`"[[:digit:]]+"`  | A sequence of at least one digit |
|`"[[:lower:]]+"`  | A sequence of at least one lowercase letter |
|`"[[:punct:]]+"`  | A sequence of at least one punctuation mark |
|`"[[:space:]]+"`  | A sequence of at least one space |
|`"[[:upper:]]+"`  | A sequence of at least one uppercase letter |
|`"[[:alnum:]]*"`  | A sequence of alphanumeric characters, possibly empty |
|`"[[:alpha:]]*"`  | A sequence of letters, possibly empty |
|`"[[:digit:]]*"`  | A sequence of digits, possibly empty |
|`"[[:lower:]]*"`  | A sequence of lowercase letters, possibly empty |
|`"[[:upper:]]*"`  | A sequence of uppercase letters, possibly empty |
|`"[[:punct:]]*"`  | A sequence of punctuation marks, possibly empty |
|`"[^[:alpha:]]+"` | A sequence of at least one character that is not a letter |
|`"[^[:digit:]]+"` | A sequence of at least one character that is not a digit |
|`"\|"`             | Either the expression `x` or `y` is present |
|`[abyz]`         | One of the specified characters |
|`[abyz]+`        | One or more of the specified characters (possibly repeated) |
|`[^abyz]`        | None of the specified characters are present |

Some character classes have lighter syntax because they are very common. Among them:

|Regular expression|Meaning |
|------------------|---------------------------------|
| `\d`             | Any digit |
| `\D`             | Any character that is not a digit |
| `\s`             | Any space (space, tab, newline) |
| `\S`             | Any character that is not a space |
| `\w`             | Any word character (letters and numbers) |
| `\W`             | Any non-word character (letters and numbers) |

In the following exercise, you will be able to practice the previous examples on a slightly more complete `regex`. This exercise does not require knowledge of the nuances of the `re` package; you will only need `re.findall`.

This exercise will use the following string:


```{python}
s = """date 0 : 14/9/2000
date 1 : 20/04/1971     date 2 : 14/09/1913     date 3 : 2/3/1978
date 4 : 1/7/1986     date 5 : 7/3/47     date 6 : 15/10/1914
date 7 : 08/03/1941     date 8 : 8/1/1980     date 9 : 30/6/1976"""
s
```

::: {.exercise}
## Exercise 1

1. First, extract the day of birth.
    + The first digit of the day is 0, 1, 2, or 3. Translate this into a `[X-X]` sequence.
    + The second digit of the day is between 0 and 9. Translate this into the appropriate sequence.
    + Note that the first digit of the day is optional. Insert the appropriate quantifier between the two character classes.
    + Add the slash after the pattern.
    + Test with `re.findall`. You should get many more matches than needed. This is normal; at this stage, the regex is not yet finalized.
2. Follow the same logic for the months, noting that Gregorian calendar months never exceed the first dozen. Test with `re.findall`.
3. Do the same for the birth years, noting that, unless proven otherwise, for people alive today, the relevant millennia are limited. Test with `re.findall`.
4. This regex is not natural; one could be satisfied with generic character classes `\d`, even though they might practically select impossible birth dates (e.g., `43/78/4528`). This would simplify the regex, making it more readable. Don’t forget the usefulness of quantifiers.
5. How can the regex be adapted to always be valid for our cases but also capture dates of the type `YYYY/MM/DD`? Test with `1998/07/12`.

:::

At the end of question 1, you should have this result:

```python
#| echo: false
re.findall("[0-3]?[0-9]/", s)
```

At the end of question 2, you should have this result, which starts to take shape:

```python
#| echo: false
re.findall("[0-3]?[0-9]/[0-1]?[0-9]", s)
```

At the end of question 3, you should successfully extract the dates:

```python
#| echo: false
# Question 3
re.findall("[0-3]?[0-9]/[0-1]?[0-9]/[0-2]?[0-9]?[0-9][0-9]", s)
```

```python
#| echo: false
#| output: false
# Question 4
re.findall("\d{1,2}/\d{1,2}/\d{2,4}", s)
```

If all goes well, in question 5, your regex should work:

```python
#| echo: false
# Question 5
re.findall("\d{1,4}/\d{1,2}/\d{1,4}", s + "\n 1998/07/12")
```

# Main `re` functions

Here is a summary table of the main functions of the `re` package with examples.

We have mainly used `re.findall` so far, which is one of the most practical functions in the package. `re.sub` and `re.search` are also quite useful. The others are less critical but can be helpful in specific cases.

| Function         | Purpose        |
|------------------|-----------------|
| `re.match(<regex>, s)` | Find and return the __first__ match of the regular expression `<regex>` __from the beginning__ of the string `s` |
| `re.search(<regex>, s)` | Find and return the __first__ match of the regular expression `<regex>` __regardless of its position__ in the string `s` |
| `re.finditer(<regex>, s)` | Find and return an iterator storing all matches of the regular expression `<regex>` __regardless of their position(s)__ in the string `s`. Typically, a loop is performed over this iterator |
| `re.findall(<regex>, s)` | Find and return **all matches** of the regular expression `<regex>` __regardless of their position(s)__ in the string `s` as a __list__ |
| `re.sub(<regex>, new_text, s)` | Find and __replace all__ matches of the regular expression `<regex>` __regardless of their position(s)__ in the string `s` |

To illustrate these functions, here are some examples:

::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>re.match</code> 👇</summary>
```

`re.match` can only capture a pattern at the start of a string. Its utility is thus limited. Let’s capture `toto`:

```python
re.match("(to){2}", "toto at the beach")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>re.search</code> 👇</summary>
```

`re.search` is more powerful than `re.match`, allowing capture of terms regardless of their position in a string. For example, to capture _age_:

```python
re.search("age", "toto is of age to go to the beach")
```

And to capture exclusively _"age"_ at the end of the string:

```python
re.search("age$", "toto is of age to go to the beach")
```


```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>re.finditer</code> 👇</summary>
```

`re.finditer` is, in my opinion, less practical than `re.findall`. Its main use compared to `re.findall` is capturing the position within a text field:

```{python}
s = "toto a l'age d'aller à la plage"
for match in re.finditer("age", s):
    start = match.start()
    end = match.end()
    print(f'String match "{s[start:end]}" at {start}:{end}')
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>re.sub</code> 👇</summary>
```

`re.sub` allows capturing and replacing expressions. For example, let's replace _"age"_ with _"âge"_. But be careful, you don't want to do this when the pattern is present in _"beach"_. So, we'll add a negative condition: capture _"age"_ only if it is not at the end of the string (which translates to regex as `?!$`).

```{python}
re.sub("age(?!$)", "âge", "toto a l'age d'aller à la plage")
```

```{=html}
</details>
```
:::


::: {.tip}
## When to use `re.compile` and raw strings?

`re.compile` can be useful when you use a regular expression multiple times in your code. It allows you to compile the regular expression into an object recognized by `re`, which can be more efficient in terms of performance when the regular expression is used repeatedly or on large data sets.

Raw strings (`raw string`) are special strings in `Python` that start with `r`. For example, `r"toto at the beach"`. They can be useful to prevent escape characters from being interpreted by `Python`. For instance, if you want to search for a string containing a backslash `\` in a string, you need to use a raw string to prevent the backslash from being interpreted as an escape character (`\t`, `\n`, etc.). The tester [https://regex101.com/](https://regex101.com/) also assumes you are using raw strings, so it can be useful to get used to them.

:::

# Generalization with `pandas`

`pandas` methods are extensions of those in `re` that avoid looping to check each line with a regex. In practice, when working with `DataFrames`, the `pandas` API is preferred over `re`. Code of the form `df.apply(lambda x: re.<function>(<regex>,x), axis = 1)` should be avoided as it is very inefficient.

The names sometimes change slightly compared to their `re` equivalents.

| Method          | Description |
|-----------------|-------------|
| `str.count()`   | Count the number of occurrences of the pattern in each line |
| `str.replace()` | Replace the pattern with another value. Vectorized version of `re.sub()` |
| `str.contains()`| Test if the pattern appears, line by line. Vectorized version of `re.search()` |
| `str.extract()` | Extract groups that match a pattern and return them in a column |
| `str.findall()` | Find and return all occurrences of a pattern. If a line contains multiple matches, a list is returned. Vectorized version of `re.findall()` |

Additionally, there are `str.split()` and `str.rsplit()` methods which are quite useful.

::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>str.count</code> 👇</summary>
```

You can count the number of times a pattern appears with `str.count`:

```{python}
df = pd.DataFrame({"a": ["toto", "titi"]})
df['a'].str.count("to")
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>str.replace</code> 👇</summary>
```

Replace the pattern _"ti"_ at the end of the string:

```{python}
df = pd.DataFrame({"a": ["toto", "titi"]})
df['a'].str.replace("ti$", " punch")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>str.contains</code> 👇</summary>
```

Check the cases where our line ends with _"ti"_:

```{python}
df = pd.DataFrame({"a": ["toto", "titi"]})
df['a'].str.contains("ti$")
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary>Example of <code>str.findall</code> 👇</summary>
```

```{python}
df = pd.DataFrame({"a": ["toto", "titi"]})
df['a'].str.findall("to")
```

```{=html}
</details>
```
:::

::: {.warning}

Currently, it is not necessary to add the `regex = True` argument, but this should be the case in a future version of `pandas`. It might be worth getting into the habit of adding it.

:::



# For more information

- [Collaborative documentation on `R` named `utilitR`](https://www.book.utilitr.org/03_fiches_thematiques/fiche_donnees_textuelles#regex)
- [_R for Data Science_](https://r4ds.hadley.nz/regexps.html)
- [_Regular Expression HOWTO_ in the official `Python` documentation](https://docs.python.org/3/howto/regex.html)
- The reference tool [https://regex101.com/] for testing regular expressions
- [This site](https://ole.michelsen.dk/tools/regex/) which has a cheat sheet at the bottom of the page.
- The games on [Regex Crossword](https://regexcrossword.com/) allow you to learn regular expressions while having fun

# Additional exercises

## Extracting email addresses

This is a classic use of _regex_

```{python}
text_emails = 'Hello from toto@gmail.com to titi.grominet@yahoo.com about the meeting @2PM'
```

::: {.exercise}
## Exercise: Extracting email addresses

Use the structure of an email address `[XXXX]@[XXXX]` to retrieve this content.

:::


```{python}
#| echo: false

# \S` represents any non-space character 
# `+` denotes the presence of the preceding set of characters between 1 and infinity
list_emails = re.findall('\S+@\S+', text_emails)     
print(list_emails)
```

## Extracting years from a `pandas` `DataFrame`

The general objective of the exercise is to clean columns in a DataFrame using regular expressions.

::: {.exercise}
## Exercise

The dataset in question contains books from the British Library and some related information. The dataset is available here: https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv

The "Date of Publication" column is not always a year; sometimes there are other details. The goal of the exercise is to have **a clean book publication date** and to examine the **distribution of publication years**.

To do this, you can:

* Either choose to perform the exercise without help. Your **reading of the instructions ends here**. You should carefully examine the dataset and transform it yourself.

* Or follow the step-by-step instructions below.

```{=html}
<details><summary>Guided version 👇</summary>
```

1. Read the data from the URL `https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv`. Be careful with the separator.
2. Keep only the columns `['Identifier', 'Place of Publication', 'Date of Publication', 'Publisher', 'Title', 'Author']`.
3. Observe the _'Date of Publication'_ column and note the issues with some rows (e.g., row 13).
4. Start by looking at the number of missing values. We cannot do better after regex, and normally we should not have fewer...
5. Determine the regex pattern for a publication date. Presumably, there are 4 digits forming a year. Use the `str.extract()` method with the `expand = False` argument (to keep only the first date matching our pattern)?
6. We have 2 `NaN` values that were not present at the start of the exercise. What are they and why?
7. What is the distribution of publication dates in the dataset? You can, for example, display a histogram using the `plot` method with the `kind = "hist"` argument.

```{=html}
</summary>
```

:::


```{python}
#| echo: false

# Question 1
data_books = pd.read_csv('https://raw.githubusercontent.com/realpython/python-data-cleaning/master/Datasets/BL-Flickr-Images-Book.csv', sep=',')
```

```{python}
#| echo: false

# Question 2
data_books = data_books[['Identifier', 'Place of Publication',
       'Date of Publication', 'Publisher', 'Title', 'Author']]
```

Here is an example of the problem to detect in question 3:

```{python}
#| echo: false

# Question 3
data_books[['Date of Publication',"Title"]].iloc[13:20]
```

```{python}
#| echo: false

# Question 4
data_books['Date of Publication'].isna().sum()
```

With our regex (question 5), we obtain a `DataFrame` that is more in line with our expectations:

```{python}
#| echo: false

# Question 5
expression = "([0-2][0-9][0-9][0-9])"
data_books['year'] = data_books['Date of Publication'].str.extract(expression, expand=False)

data_books.loc[~(data_books['Date of Publication'] == data_books['year']), ['Date of Publication', 'year']]
```

As for the new `NaN` values, they are rows that did not contain any strings resembling years:

```{python}
#| echo: false
data_books.loc[~data_books['Date of Publication'].isna() & data_books['year'].isna(), ['Date of Publication', 'year']]
```

Finally, we obtain the following histogram of publication dates:

```{python}
#| echo: false
pd.to_numeric(data_books['year'], downcast='integer').plot(kind ="hist")
```
