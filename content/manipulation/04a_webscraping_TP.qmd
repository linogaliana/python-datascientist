---
title: "Web scraping avec Python"
title-en: "Web scraping with Python"
author: Lino Galiana
tags:
  - Webscraping
  - BeautifulSoup
  - Exercice
  - Manipulation
categories:
  - Exercice
  - Manipulation
type: book
description: |
  `Python` permet de facilement récupérer une page web pour en extraire des données à restructurer. Le _web scraping_, que les Canadiens nomment _"moissonnage du web"_, est une manière de plus en plus utilisée de récupérer une grande masse d'information en temps réel. Ce chapitre présente les deux principaux paradigmes par le biais de `BeautifulSoup` et `Selenium` et les principaux défis du _web scraping_.
description-en: |
  Python makes it easy to retrieve a web page and extract data for restructuring. Web scraping is an increasingly popular way of retrieving large amounts of information in real time. This chapter presents the two main paradigms through `BeautifulSoup` and `Selenium` and the main challenges of _web scraping_.
image: bulbasaur.jpg
echo: false
---

{{< badges
    printMessage="true"
>}}

:::: {.content-visible when-format="ipynb"}
::: {.warninglang .callout-warning}
:::
::::


:::: {.content-visible when-profile="fr"}

::: {.callout-tip collapse="true"}

## Compétences à l'issue de ce chapitre

- Comprendre les enjeux du web scraping, en particulier les questions de légalité (RGPD, zone grise), de stabilité des sites, et de fiabilité des données ;
- Appliquer de bonnes pratiques lors du scraping : consulter le fichier `robots.txt`, espacer les requêtes, éviter de surcharger les sites, privilégier les périodes creuses ;
- Saisir la structure HTML d’une page (balises, parent-enfant) pour cibler correctement les éléments à extraire ;
- Utiliser le package `requests` pour récupérer le contenu d’une page web, et `BeautifulSoup` pour analyser et naviguer dans le HTML (méthodes `find`, `find_all`) ;
- Mettre en pratique le scraping avec un exercice concret (liste des équipes de Ligue 1) ;
- Découvrir Selenium pour simuler le comportement d’un utilisateur sur des pages dynamiques générées par JavaScript ;
- Évaluer les limites du web scraping et justifier l’usage d’API plus robustes quand elles sont disponibles.

:::

::::

  
::::: {.content-visible when-profile="en"}

::: {.callout-tip collapse="true"}

## Skills you will acquire in this chapter

- Understand the key challenges of web scraping, including legal concerns (e.g. GDPR, grey areas), site stability, and data reliability  
- Follow best practices when scraping: check the `robots.txt` file, space out your requests, avoid overloading servers, and scrape during off-peak hours when possible  
- Navigate the HTML structure of a web page (tags, parent-child relationships) to accurately target the elements you want to extract  
- Use the `requests` library to fetch web page content, and `BeautifulSoup` to parse and explore the HTML using methods like `find` and `find_all`  
- Practice your scraping skills with a hands-on exercise involving the French Ligue 1 football team list  
- Explore Selenium for simulating user interactions on JavaScript-driven dynamic pages  
- Understand the limitations of web scraping and know when it’s better to use more stable and reliable APIs  
:::

::::

::: {.content-visible when-profile="fr"}
Le [_web  scraping_](https://fr.wikipedia.org/wiki/Web_scraping) désigne les techniques d'extraction du contenu des sites internet.
C'est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n'existant pas forcément sous la forme d'un tableau *Excel*.

Ce TP vous présente comment créer et exécuter des robots afin de recupérer rapidement des informations utiles à vos projets actuels ou futurs.
Il part de quelques cas d'usages concret.
Ce chapitre est très fortement inspiré et réadapté à partir de [celui de Xavier Dupré](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/notebooks/TD2A_Eco_Web_Scraping.html), l'ancien professeur de la matière.
:::

::: {.content-visible when-profile="en"}
[_Web scraping_](https://en.wikipedia.org/wiki/Web_scraping) refers to techniques for extracting content from websites.
It is a very useful practice for anyone looking to work with information available online, but not necessarily in the form of an *Excel* table.

This chapter introduces you to how to create and run bots to quickly retrieve useful information for your current or future projects.
It starts with some concrete use cases.
This chapter is heavily inspired and adapted from [Xavier Dupré's](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/notebooks/TD2A_Eco_Web_Scraping.html) work, the former professor of the subject.
:::

::: {.content-visible when-profile="fr"}
# Enjeux

Un certain nombre d'enjeux du _web scraping_ ne seront évoqués
que superficiellement dans le cadre de ce chapitre.

## La zone grise de la légalité du _web scraping_

En premier lieu, en ce qui concerne la question de la légalité
de la récupération d'information par _scraping_, il existe
une zone grise. Ce n'est pas parce qu'une information est
disponible sur internet, directement ou avec un peu de recherche,
qu'elle peut être récupérée et réutilisée.

L'excellent [cours d'Antoine Palazzolo](https://inseefrlab.github.io/formation-webscraping/) évoque un certain nombre de cas
médiatiques et judiciaires sur cette question.
Dans le champ français, la CNIL a publié en 2020
de nouvelles directives sur le *web scraping* reprécisant
que toute donnée ne peut être réutilisée à l'insu de la personne
à laquelle ces données appartiennent. Autrement dit, en principe,
les données collectées par _web scraping_ sont soumises au
RGPD, c'est-à-dire nécessitent le consentement des personnes
à partir desquelles la réutilisation des données est faite.

Il est donc recommandé d'__être vigilant avec les données récupérées__
par _web scraping_ pour ne pas se mettre en faute légalement.
:::

::: {.content-visible when-profile="en"}
# Issues

A number of issues related to _web scraping_ will only be briefly mentioned in this chapter.

## The Legal Gray Area of _Web Scraping_

First, regarding the legality of retrieving information through _scraping_, there is a gray area. Just because information is available on the internet, either directly or with a little searching, does not mean it can be retrieved and reused.

The excellent [course by Antoine Palazzolo](https://inseefrlab.github.io/formation-webscraping/) discusses several media and legal cases on this issue. In France, the CNIL published new guidelines in 2020 on *web scraping*, clarifying that any data cannot be reused without the knowledge of the person to whom the data belongs. In other words, in principle, data collected by _web scraping_ is subject to GDPR, meaning it requires the consent of the individuals from whom the data is reused.

It is therefore recommended to __be cautious with the data retrieved__ by _web scraping_ to avoid legal issues.
:::


::: {.content-visible when-profile="fr"}
## Stabilité et fiabilité des informations reçues

La récupération de données par _web scraping_
est certes pratique mais elle ne correspond pas nécessairement
à un usage pensé, ou désiré, par un fournisseur de données.
Les données étant coûteuses à collecter et à mettre à disposition,
certains sites ne désirent pas nécessairement que celles-ci soient
extraites gratuitement et facilement. _A fortiori_ lorsque la donnée
peut permettre à un concurrent de disposer d'une information
utile d'un point de vue commercial (prix d'un produit concurrent, etc.).

Les acteurs mettent donc souvent en oeuvre des stratégies pour bloquer ou
limiter la quantité de données scrapées. La méthode la plus
classique est la détection et le blocage
des requêtes faites par des robots plutôt que par des humains.
Pour des acteurs spécialisés, cette détection est très facile car
de nombreuses preuves permettent d'identifier si une visite du site _web_
provient d'un utilisateur
humain derrière un navigateur ou d'un robot. Pour ne citer que quelques indices :
vitesse de la navigation entre pages, rapidité à extraire la donnée,
empreinte digitale du navigateur utilisé, capacité à répondre à des
questions aléatoires (captcha)...
Les bonnes pratiques, évoquées par la suite, ont pour objectif de faire
en sorte qu'un robot se comporte de manière civile en adoptant un comportement
proche de celui de l'humain mais sans contrefaire le fait qu'il ne s'agit
pas d'un humain.

Il convient d'ailleurs
d'être prudent quant aux informations reçues par _web scraping_.
La donnée étant au coeur du modèle économique de certains acteurs, certains
n'hésitent pas à renvoyer des données fausses aux robots
plutôt que les bloquer. C'est de bonne guerre !
Une autre technique piège s'appelle le _honey pot_. Il s'agit de pages qu'un humain
n'irait jamais visiter - par exemple parce qu'elles n'apparaissent pas dans
l'interface graphique - mais sur lesquelles un robot, en recherche automatique
de contenu, va rester bloquer.

Sans aller jusqu'à la stratégie de blocage du _web scraping_, d'autres raisons
peuvent expliquer qu'une récupération de données ait fonctionné par
le passé mais ne fonctionne plus. La plus fréquente est un changement dans la structure
d'un site _web_. Le _web scraping_ présente en effet l'inconvénient d'aller chercher
de l'information dans une structure très hiérarchisée. Un changement dans cette structure
peut suffire à rendre un robot incapable  de récupérer du contenu. Or, pour rester
attractifs, les sites _web_ changent fréquemment ce qui peut facilement
rendre inopérant un robot.

De manière générale, l'un des principaux messages de ce
chapitre, à retenir, est que le
__web scraping est une solution de dernier ressort, pour des récupérations ponctuelles de données sans garantie de fonctionnement ultérieur__. Il est préférable de __privilégier les API lorsque celles-ci sont disponibles__.
Ces dernières ressemblent à un contrat (formel ou non) entre un fournisseur de données
et un utilisateur où sont définis des besoins (les données) mais aussi des
conditions d'accès (nombre de requêtes, volumétrie, authentification...) là
où le _web scraping_ est plus proche du comportement dans le _Far West_.
:::

::: {.content-visible when-profile="en"}
## Stability and Reliability of Retrieved Information

Data retrieval through _web scraping_ is certainly practical, but it does not necessarily align with the intended or desired use by a data provider. Since data is costly to collect and make available, some sites may not necessarily want it to be extracted freely and easily. Especially when the data could provide a competitor with commercially useful information (e.g., the price of a competing product).

As a result, companies often implement strategies to block or limit the amount of data scraped. The most common method is detecting and blocking requests made by bots rather than humans. For specialized entities, this detection is quite easy because numerous indicators can identify whether a website visit comes from a human user behind a browser or a bot. To mention just a few clues: browsing speed between pages, speed of data extraction, fingerprinting of the browser used, ability to answer random questions (captcha)...

The best practices mentioned later aim to ensure that a bot behaves civilly by adopting behavior close to that of a human without pretending to be one.

It's also essential to be cautious about the information received through _web scraping_. Since data is central to some business models, some companies don't hesitate to send false data to bots rather than blocking them. It's fair play! Another trap technique is called the _honey pot_. These are pages that a human would never visit—for example, because they don't appear in the graphical interface—but where a bot, automatically searching for content, might get stuck.

Without resorting to the strategy of blocking _web scraping_, other reasons can explain why a data retrieval that worked in the past may no longer work. The most frequent reason is a change in the structure of a website. _Web scraping_ has the disadvantage of retrieving information from a very hierarchical structure. A change in this structure can make a bot incapable of retrieving content. Moreover, to remain attractive, websites frequently change, which can easily render a bot inoperative.

In general, one of the key takeaways from this chapter is that __web scraping is a last resort solution for occasional data retrieval without any guarantee of future functionality__. It is preferable to __favor APIs when they are available__. The latter resemble a contract (formal or not) between a data provider and a user, where needs (the data) and access conditions (number of requests, volume, authentication...) are defined, whereas _web scraping_ is more akin to behavior in the _Wild West_.
:::

::: {.content-visible when-profile="fr"}
## Les bonnes pratiques

La possibilité de récupérer des données par l'intermédiaire
d'un robot ne signifie pas qu'on peut se permettre de ne pas être
civilisé. En effet, lorsqu'il est non-maîtrisé, le
_web scraping_ peut ressembler à une attaque informatique
classique pour faire sauter un site _web_ : le déni de service.
Le [cours d'Antoine Palazzolo](https://inseefrlab.github.io/formation-webscraping/) revient
sur certaines bonnes pratiques qui ont émergé dans la communauté
des _scrapeurs_. Il est recommandé de lire cette ressource
pour en apprendre plus sur ce sujet. Y sont évoquées
plusieurs conventions, parmi lesquelles :

- Se rendre, depuis la racine du site,
sur le fichier `robots.txt` pour vérifier les consignes
proposées par les développeurs du site _web_ pour
cadrer le comportement des robots ;
- Espacer chaque requêtes de plusieurs secondes, comme le ferait
un humain, afin d'éviter de surcharger le site _web_ et de le
faire sauter par déni de service ;
- Faire les requêtes dans les heures creuses de fréquentation du
site _web_ s'il ne s'agit pas d'un site consulté internationalement.
Par exemple, pour un site en français, lancer le robot
pendant la nuit en France métropolitaine, est une bonne pratique.
Pour lancer un robot depuis `Python` à une heure programmée
à l'avance, il existe les `cronjobs`.
:::

::: {.content-visible when-profile="en"}
## Best Practices

The ability to retrieve data through a bot does not mean one can afford to be uncivilized. Indeed, when uncontrolled, _web scraping_ can resemble a classic cyberattack aimed at taking down a website: a denial of service. The [course by Antoine Palazzolo](https://inseefrlab.github.io/formation-webscraping/) reviews some best practices that have emerged in the scraping community. It is recommended to read this resource to learn more about this topic. Several conventions are discussed, including:

- Navigate from the site's root to the `robots.txt` file to check the guidelines provided by the website's developers to regulate the behavior of bots;
- Space out each request by several seconds, as a human would, to avoid overloading the website and causing it to crash due to a denial of service;
- Make requests during the website's off-peak hours if it is not an internationally accessed site. For example, for a French-language site, running the bot during the night in metropolitan France is a good practice. To run a bot from `Python` at a pre-scheduled time, there are `cronjobs`.
:::


{{< include "04_webscraping/_principe.qmd" >}}

{{< include "04_webscraping/_beautifulsoup.qmd" >}}

::: {.content-visible when-profile="fr"}
# Exercice guidé : obtenir la liste des équipes de Ligue 1

Dans le premier paragraphe de la page _"Participants"_,
on a le tableau avec les résultats de l'année.
:::

::: {.content-visible when-profile="en"}
# Guided Exercise: Get the List of Ligue 1 Teams

In the first paragraph of the _"Participants"_ page,
there is a table with the results of the year.
:::

{{< include "04_webscraping/_exo1.qmd" >}}
{{< include "04_webscraping/_exo1_solution.qmd" >}}

{{< include "04_webscraping/_bs4_get_stadium.qmd" >}}

{{< include "04_webscraping/_bs4_pokemon.qmd" >}}

{{< include "04_webscraping/_selenium.qmd" >}}


