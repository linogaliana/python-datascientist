:::: {.content-visible when-profile="fr"}
::: {.callout-tip collapse="false"}
# Partie 1 : Du `CSV` au `Parquet`

* Créer un notebook `benchmark_parquet.ipynb` afin de réaliser les différentes comparaisons de performance de l'application
* Créons notre décorateur, en charge de _benchmarker_ le code `Python`:

<details>

<summary>
Dérouler pour retrouver le code du décorateur permettant de mesurer la performance
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_profiler.qmd" >}}

</details>

* Reprendre ce code pour encapsuler un code de construction d'une pyramide des âges dans une fonction `process_csv_appli1`

<details>

<summary>
Dérouler pour récupérer le code pour mesurer les performances de la lecture en CSV
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_measure.qmd" >}}

</details>

* Exécuter `process_csv_appli1()` et `process_csv_appli1(return_output=True)`

* Sur le même modèle, construire une fonction `process_parquet_appli1` basée cette fois sur le fichier `data/RPindividus_24.parquet` chargé avec la fonction [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) de `Pandas`
* Comparer les performances (temps d'exécution et allocation mémoire) de ces deux méthodes grâce à la fonction.

<details>

<summary>
Correction complète
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_correction.qmd" >}}

</details>
:::

_❓️ Quelle semble être la limite de la fonction `read_parquet` ?_

On gagne déjà un temps conséquent en lecture mais on ne bénéficie pas vraiment de l'optimisation permise par `Parquet` car on transforme les données directement après la lecture en `DataFrame` `Pandas`. On n'utilise donc pas l'une des fonctionnalités principales du format `Parquet`, qui explique ses excellentes performances: le _predicate pushdown_ qui consiste à optimiser notre traitement pour faire remonter, le plus tôt possible, les filtres sur les colonnes pour ne garder que celles vraiment utilisées dans le traitement.
::::

:::: {.content-visible when-profile="en"}
::: {.callout-tip collapse="false"}
# Part 1: From `CSV` to `Parquet`

* Create a notebook named `benchmark_parquet.ipynb` to perform various performance comparisons throughout the application.  
* Define a custom decorator that will be used to benchmark the `Python` code by measuring execution time and memory usage.

<details>

<summary>
Click to expand and view the code for the performance-measuring decorator.
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_profiler.qmd" >}}

</details>

* Reuse this code to wrap the logic for constructing the age pyramid into a function named `process_csv_appli1`.

<details>

<summary>
Click to expand and view the code used to measure the performance of reading CSV files.
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_measure.qmd" >}}

</details>

* Run `process_csv_appli1()` and `process_csv_appli1(return_output=True)` to observe performance and optionally return the processed data.

* Using the same approach, define a new function named `process_parquet_appli1`, this time based on the `data/RPindividus_24.parquet` file, and load it using `Pandas`' [`read_parquet`](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) function.

* Compare the performance (execution time and memory usage) of the two methods using the benchmarking decorator.


<details>

<summary>
Full correction
</summary>

{{< include "/content/manipulation/05_parquet/_exercise1_correction.qmd" >}}

</details>
:::

_❓️ What seems to be the limitation of the `read_parquet` function?_

Although we already observe a significant speed improvement during file reading, we are not fully leveraging the optimizations provided by the `Parquet` format. This is because the data is immediately loaded into a `Pandas` `DataFrame`, where transformations are applied afterward.

As a result, we miss out on one of `Parquet`’s core performance features: **predicate pushdown**. This optimization allows filters to be applied as early as possible—at the file scan level—so that only the relevant columns and rows are read into memory. By bypassing this mechanism, we lose much of what makes `Parquet` so efficient in analytical workflows.

::::
