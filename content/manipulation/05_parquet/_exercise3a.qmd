:::: {.content-visible when-profile="fr"}
::: {.callout-tip collapse="false"}
## Partie 3a : Et si on filtrait sur les lignes ?

Ajoutez une étape de filtre sur les lignes dans nos requêtes:

* Avec `DuckDB`, vous devez modifier la requête avec un `WHERE DEPT IN ('18', '28', '36')`
* Avec `Arrow`, vous devez modifier l'étape `to_table` de cette manière: `dataset.to_table(filter=pc.field("DEPT").isin(['18', '28', '36']))`

{{< include "/content/manipulation/05_parquet/_exercise3a_correction.qmd" >}}
:::

*❓️ Pourquoi ne gagne-t-on pas de temps avec nos filtres sur les lignes (voire pourquoi en perdons nous?) comme c'est le cas avec les filtres sur les colonnes ?*

La donnée n'est pas organisée par blocs de lignes comme elle l'est par bloc de colonne. Heureusement, il existe pour cela un moyen: le partitionnement !
::::

:::: {.content-visible when-profile="en"}

::: {.callout-tip collapse="false"}
## Part 3a: What Happens When We Filter Rows?

Let us now add a row-level filtering step to our queries:

* With `DuckDB`, modify the SQL query to include a `WHERE` clause:  
  `WHERE DEPT IN ('18', '28', '36')`

* With `Arrow`, update the `to_table` call as follows:  
  `dataset.to_table(filter=pc.field("DEPT").isin(['18', '28', '36']))`


{{< include "/content/manipulation/05_parquet/_exercise3a_correction.qmd" >}}

:::

*❓️ Why do row filters not improve performance (and sometimes even slow things down), unlike column filters?*

This is because data is not stored in row blocks the way it is in column blocks. As a result, filtering rows does not allow the system to skip over large sections of the file as efficiently.

Fortunately, there is a solution: **partitioning**.

::::
