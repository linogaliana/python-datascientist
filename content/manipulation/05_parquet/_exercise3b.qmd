:::: {.content-visible when-profile="fr"}
::: {.callout-tip collapse="false"}
# Partie 3 : Le `Parquet` partitionné

La *lazy evaluation* et les optimisations d'`Arrow` apportent des gain de performance considérables. Mais on peut encore faire mieux ! Lorsqu'on sait qu'on va être amené à **filter régulièrement les données selon une variable d'intérêt**, on a tout intérêt à **partitionner** le fichier `Parquet` selon cette variable.

1. Parcourir la documentation de la fonction [`pyarrow.parquet.write_to_dataset`](https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets) pour comprendre comment spécifier une clé de partitionnement lors de l’écriture d’un fichier `Parquet`. Plusieurs méthodes sont possibles.

2. Importer la table complète des individus du recensement depuis `"data/RPindividus.parquet"` avec la fonction [`pyarrow.dataset.dataset`](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.dataset.html) et l'exporter en une table partitionnée `"data/RPindividus_partitionne.parquet"`, partitionnée par la région (`REGION`) et le département (`DEPT`).

3. Observer l’arborescence des fichiers de la table exportée pour voir comment la partition a été appliquée.

4. Modifier nos fonctions d'import, filtre et agrégations via `Arrow` ou `DuckDB` pour utiliser, cette fois, le `Parquet` partitionné. Comparer à l'utilisation du fichier non partitionné.

{{< include "/content/manipulation/05_parquet/_exercise3b_correction.qmd" >}}

:::

*❓️ Dans le cadre d'une mise à disposition de données en `Parquet`, comment bien choisir la/les clé(s) de partitionnement ? Quelle est la limite à garder en tête ?*
::::

:::: {.content-visible when-profile="en"}
::: {.callout-tip collapse="false"}
# Part 3: Partitioned `Parquet`

*Lazy evaluation* and the optimizations available through `Arrow` already provide significant performance improvements. But we can go even further. When you know in advance that your queries will frequently **filter data based on a specific variable**, it is highly advantageous to **partition** the `Parquet` file using that variable.

1. Review the documentation for [`pyarrow.parquet.write_to_dataset`](https://arrow.apache.org/docs/python/parquet.html#writing-to-partitioned-datasets) to understand how to define a partitioning key when writing a `Parquet` file. Several approaches are available.

2. Import the full individuals table from the census using `pyarrow.dataset.dataset("data/RPindividus.parquet")`, and export it as a partitioned dataset to `"data/RPindividus_partitionne.parquet"`, using both `REGION` and `DEPT` as partitioning keys.

3. Explore the resulting directory structure to examine how partitioning was applied—each partition key should create a subfolder representing a unique value.

4. Update your data loading, filtering, and aggregation functions (using either `Arrow` or `DuckDB`) to operate on the partitioned `Parquet` file. Then compare the performance with the non-partitioned version.

{{< include "/content/manipulation/05_parquet/_exercise3b_correction.qmd" >}}

:::

*❓️ When delivering data in `Parquet` format, how should you choose the partitioning key(s)? What limitations should you keep in mind?*

::::
