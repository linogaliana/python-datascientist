---
title: "Partie 1: manipuler des données"
title-en: "Part 1: data wrangling"
author: Lino Galiana
categories:
  - Manipulation
  - Introduction
description: |
  `Python` s'est imposé comme une alternative très crédible à `R` dans
  la manipulation de données. L'écosystème `Pandas` a permis de démocratiser
  l'utilisation des DataFrames dans `Python` et faciliter la manipulation
  de données structurées grâce à la philosophie `SQL`. `Python` reste également
  le langage le plus pratique pour récupérer et manipuler
  des données moins structurées (_webscraping_, API). `Python` tend à devenir,
  grâce au développement d'API vers d'autres langages (`C`, `Spark`, `Postgres`,
  `ElasticSearch`...),
  le langage *"one to rule them all"*
description-en: |
  Python has established itself as a credible alternative to R for data manipulation. The `Pandas` ecosystem has made it possible to democratize the use of DataFrames in `Python` and make it easier to manipulate of structured data thanks to the `SQL` philosophy. `Python` is also the most practical language for retrieving and manipulating unstructured data (_webscraping_, APIs). Python is tending to become, thanks to the development of APIs for other languages (`C`, `Spark`, `Postgres`, `ElasticSearch`...), the _"one to rule them all "_ language.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda.png
---

::: {.content-visible when-profile="fr"}

Si on associe souvent les _data scientists_ à la mise en oeuvre
de modèles d'intelligence artificielle, il est important
de ne pas oublier que l'entraînement et l'utilisation
de ces modèles ne représente pas
forcément le quotidien des _data scientists_.

En pratique,
la récupération de sources de données hétérogènes, la structuration
et harmonisation de celles-ci en vue d'une analyse exploratoire
préalable à la modélisation ou la visualisation
représente une part importante du travail des _data scientists_. 
Dans de nombreux environnements c'est même l'essence du travail
du _data scientist_. 
L'élaboration de modèles pertinents requiert en effet une réflexion approfondie sur les données ;
une étape que l'on ne saurait négliger.

Ce cours, 
comme de nombreuses ressources introductives sur
la _data science_ [@wickham2023r; @vanderplas2016python; @mckinney2012python],
proposera donc beaucoup d'éléments sur la manipulation de données, compétence
essentielle pour les _data scientists_. 

Les logiciels de programmation
orientés autour du concept de base de données
sont devenus les outils principaux des _data scientists_.
Le fait de pouvoir appliquer un certain nombre d'opérations
standards sur des bases de données, quelle que soit leur nature,
permet aux programmeurs d'être plus efficaces que s'ils devaient
répéter ces opérations à la main, comme dans `Excel`.

Tous les langages de programmation dominants dans l'écosystème
de la _data science_ reposent sur le principe du _dataframe_. 
Il s'agit même d'un objet central dans certains logiciels,
notamment `R`. 
La logique [`SQL`](https://fr.wikipedia.org/wiki/Structured_Query_Language),
un langage de déclaration d'opérations sur des données qui a déjà plus de cinquante ans, 
offre un cadre pertinent pour effectuer des opérations standardisées
sur les colonnes (création de nouvelles colonnes, sélection de sous-ensemble de lignes...).

Néanmoins, le _dataframe_ ne s'est imposé que récemment en `Python`,
grâce au package `Pandas` créé
par [Wes McKinney](https://fr.wikipedia.org/wiki/Wes_McKinney). 
L'essor de la librairie `Pandas` (téléchargée plus de 5 millions de fois
par jour en 2023) est pour beaucoup dans le succès de `Python`
dans l'écosystème de la _data science_ et a amené, en quelques années,
a un renouvellement complet de la manière de coder en `Python`, ce
langage si malléable, autour de l'analyse de données. 

Cette partie du cours est une introduction
générale à l'écosystème très riche de 
la manipulation de données avec `Python`.
Ces chapitres évoquent aussi bien la récupération de données
que la restructuration et la production d'analyse
à partir de celles-ci. 

:::

::: {.content-visible when-profile="en"}

Although data scientists are often associated with the implementation
of artificial intelligence models, it is important
not to forget that training and using
these models do not necessarily represent
the daily work of data scientists.

In practice,
gathering heterogeneous data sources, structuring
and harmonizing them for exploratory analysis
prior to modeling or visualization
represents a significant part of data scientists' work.
In many environments, this is even the essence of a data scientist's role.
Developing relevant models indeed requires deep reflection on the data;
an essential step that should not be overlooked.

This course,
like many introductory resources on
data science [@wickham2023r; @vanderplas2016python; @mckinney2012python],
will therefore offer a lot of content on data manipulation, an
essential skill for data scientists.

Programming software
based around the database concept
has become the main tool for data scientists.
Being able to apply a number of standard operations
on databases, regardless of their nature,
allows programmers to be more efficient than if they had to
repeat these operations manually, as in Excel.

All the dominant programming languages in the data science ecosystem
are based on the dataframe principle.
It is even a central object in some software,
notably R.
The logic of [`SQL`](https://fr.wikipedia.org/wiki/Structured_Query_Language),
a language for declaring data operations that has been around for over fifty years,
provides a relevant framework for performing standardized operations
on columns (creating new columns, selecting subsets of rows, etc.).

However, the dataframe only recently became established in Python,
thanks to the `Pandas` package created
by [Wes McKinney](https://fr.wikipedia.org/wiki/Wes_McKinney).
The rise of the `Pandas` library (downloaded over 5 million times
per day in 2023) is largely responsible for Python's success
in the data science ecosystem and has led, in just a few years,
to a complete renewal of how coding in Python, such
a flexible language, is approached for data analysis.

This part of the course is a general introduction
to the rich ecosystem of
data manipulation with Python.
These chapters cover both data retrieval
and the restructuring and analysis
of that data.

:::

::: {.content-visible when-profile="fr"}

## Résumé de cette partie {.unnumbered}

`Pandas` est devenu incontournable dans l'écosystème `Python` pour la *data science*. 
`Pandas` est lui-même construit à partir du package `Numpy`, qu'il est utile de comprendre
pour être à l'aise avec `Pandas`. `Numpy` est une librairie bas-niveau 
pour stocker et manipuler des données. 
`Numpy` est au coeur de l'écosystème de la *data science* car la plupart des librairies, même celles
qui manient des objets destructurés,
utilisent des objets construits à partir de `Numpy`[^1]. 

[^1]: Certaines librairies commencent, petit à petit, à s'émanciper
de `Numpy` qui n'est pas toujours le plus adapté pour la gestion
de certains types de données. Le _framework_ `Arrow` tend à devenir
la couche basse utilisée par de plus en plus de librairies de _data science_. 
[Ce _post_ de blog](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) approfondit
de manière très pédagogique ce sujet. 

L'approche `Pandas`, qui offre un point d'entrée harmonisé pour manipuler
des jeux de données de nature très différente,
a été étendue aux objets géographiques avec `Geopandas`.
Il est ainsi possible de manipuler des données géographiques comme s'il
s'agissait de données structurées classiques. Les données géographiques et
la représentation cartographique deviennent de plus en plus commun avec
la multiplication de données ouvertes localisées et de *big-data* géolocalisées.

Cependant, les données structurées, importées depuis des fichiers plats
ne représentent pas l'unique source de données. Les API et le *webscraping*
permettent de télécharger ou d'extraire 
des données de manière très flexible depuis des pages web ou des guichets
spécialisés. Ces données, notamment
celles obtenues par *webscraping* nécessitent souvent un peu plus de travail
de nettoyage de données, notamment des chaînes de caractère. 

L'écosystème `Pandas` représente donc un couteau-suisse
pour l'analyse de données. C'est pour cette raison que ce cours 
développera beaucoup de contenu dessus.
Avant d'essayer de mettre en oeuvre une solution _ad hoc_, il est
souvent utile de se poser la question suivante : _"ne pourrais-je pas le faire
avec les fonctionnalités de base de `Pandas` ?"_ Se poser cette question peut
éviter des chemins ardus et faire économiser beaucoup de temps. 

Néanmoins, `Pandas` n'est pas 
adapté à des données ayant une volumétrie
importante. Pour traiter de telles
données, il est plutôt recommandé 
de privilégier `Polars` ou `Dask` qui reprennent la logique de `Pandas` mais
optimisent son fonctionnement, `Spark` si on a une infrastructure adaptée, généralement dans
des environnements _big data_, ou
`DuckDB` si on est prêt à utiliser des requêtes SQL plutôt qu'une librairie haut-niveau.

:::

::: {.content-visible when-profile="en"}

## Summary of that section {.unnumbered}

`Pandas` has become essential in the `Python` ecosystem for *data science*. 
`Pandas` itself is built on top of the `Numpy` package, which is useful to understand
to be comfortable with `Pandas`. `Numpy` is a low-level library
for storing and manipulating data. 
`Numpy` is at the heart of the *data science* ecosystem because most libraries, even those
handling unstructured objects,
use objects built from `Numpy`[^1].

[^1]: Some libraries are gradually moving away
from `Numpy`, which is not always the most suitable for managing
certain types of data. The `Arrow` framework is becoming
the lower layer used by more and more data science libraries.
[This blog post](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) provides a detailed explanation of this topic.

The `Pandas` approach, which provides a unified entry point for manipulating
datasets of very different natures,
has been extended to geographic objects with `Geopandas`.
This allows for the manipulation of geographic data as if
it were classic structured data. Geographic data and
cartographic representation are becoming increasingly common with
the rise of open localized data and geolocated *big-data*.

However, structured data imported from flat files
is not the only data source. APIs and *web scraping*
allow for flexible downloading or extraction
of data from web pages or specialized portals. These data, particularly
those obtained through *web scraping*, often require a bit more data
cleaning work, especially with character strings.

The `Pandas` ecosystem thus represents a Swiss army knife
for data analysis. This is why this course
will cover it extensively.
Before trying to implement an _ad hoc_ solution, it is
often useful to ask the following question: _"Could I do this
with the basic functionalities of `Pandas`?"_ Asking this question can
prevent arduous paths and save a lot of time.

However, `Pandas` is not
suitable for handling large volumes of data.
To process such
data, it is recommended to use `Polars` or `Dask`, which adopt the logic of `Pandas` but
optimize its functionality, `Spark` if you have suitable infrastructure, generally in
big data environments, or
`DuckDB` if you are willing to use SQL queries rather than a high-level library.

:::


::: {.content-visible when-profile="fr"}
## Exercices {.unnumbered}

Cette partie présente à la fois des tutoriels détaillés
et des exercices guidés.

Il est
possible de les consulter sur ce site ou d'utiliser l'un des
badges présents en début de chapitre, par exemple
ceux-ci pour ouvrir
le [chapitre d'exercices sur `Pandas`](02b_pandas_TP/):

:::

::: {.content-visible when-profile="en"}

## Exercises {.unnumbered}

This section provides both detailed tutorials
and guided exercises.

You can view them on this site or use one of the
badges at the beginning of the chapter, for example
these to open
the [Pandas exercises chapter](02b_pandas_TP/):

:::


{{< badges
    fpath="/content/manipulation/index.qmd"
    printMessage="false"
>}}


::: {.content-visible when-profile="fr"}

## Pour aller plus loin {.unnumbered}

Ce cours n'aborde pas vraiment les questions de volumétrie ou de vitesse de 
calcul. 
`Pandas` peut montrer ses limites dans ce domaine sur des jeux de données
d'une volumétrie conséquente (plusieurs Gigas). 

Il est ainsi intéressant de porter attention à:

* Le livre [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html)
pour obtenir des éléments supplémentaires sur la question de la performance
avec `Pandas` ;
* La question des
[objets _sparse_](https://chrisalbon.com/machine_learning/vectors_matrices_and_arrays/create_a_sparse_matrix/) ;
* Les _packages_ [`Dask`](https://dask.org/) ou [`Polars`](https://ssphub.netlify.app/post/polars/) pour accélérer les calculs ;
* [`DuckDB`](https://duckdb.org/docs/api/python/overview.html) pour effectuer de manière très efficace des requêtes SQL ;
* [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html) pour des données très volumineuses.

:::

::: {.content-visible when-profile="en"}

## Going further {.unnumbered}

This course does not really address issues of volume or speed of 
computation. 
`Pandas` can show its limits in this area with large datasets
(several gigabytes).

It is therefore interesting to consider:

* The book [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html)
for additional insights on performance with `Pandas`;
* The question of
[sparse objects](https://chrisalbon.com/machine_learning/vectors_matrices_and_arrays/create_a_sparse_matrix/);
* The _packages_ [`Dask`](https://dask.org/) or [`Polars`](https://ssphub.netlify.app/post/polars/) to speed up computations;
* [`DuckDB`](https://duckdb.org/docs/api/python/overview.html) for very efficient SQL queries;
* [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html) for very large datasets.

:::


::: {.content-visible when-profile="fr"}

### Références {.unnumbered}

Voici une bibliographie sélective des ouvrages
intéressants en complément des chapitres de la partie "Manipulation" de ce cours :

:::

::: {.content-visible when-profile="en"}

### References {.unnumbered}

Here is a selective bibliography of interesting books
complementary to the chapters in the "Manipulation" section of this course:

:::

::: {#refs}
:::
