---
title: "Partie 1: manipuler des données"
date: 2023-07-16
categories:
  - Manipulation
  - Introduction
description: |
  `Python` s'est imposé comme une alternative très crédible à `R` dans
  la manipulation de données. L'écosystème `Pandas` a permis de démocratiser
  l'utilisation des DataFrames dans `Python` et faciliter la manipulation
  de données structurées grâce à la philosophie `SQL`. `Python` reste également
  le langage le plus pratique pour récupérer et manipuler
  des données moins structurées (_webscraping_, API). `Python` tend à devenir,
  grâce au développement d'API vers d'autres langages (`C`, `Spark`, `Postgres`,
  `ElasticSearch`...),
  le langage *"one to rule them all"*
slug: manipulation
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda.png
---

Si on associe souvent les _data scientists_ à la mise en oeuvre
de modèles d'intelligence artificielle, il est important
de ne pas oublier que l'entraînement et l'utilisation
de ces modèles ne représente pas
forcément le quotidien des _data scientists_.


En pratique,
la récupération de sources de données hétérogènes, la structuration
et harmonisation de celles-ci en vue d'une analyse exploratoire
préalable à la modélisation ou la visualisation
représente une part importante du travail des _data scientists_. 
Dans de nombreux environnements c'est même l'essence du travail
du _data scientist_. 
L'élaboration de modèles pertinents requiert en effet une réflexion approfondie sur les données ;
une étape que l'on ne saurait négliger.

Ce cours, 
comme de nombreuses ressources introductives sur
la _data science_ [@wickham2023r; @vanderplas2016python; @mckinney2012python],
proposera donc beaucoup d'éléments sur la manipulation de données, compétence
essentielle pour les _data scientists_. 

Les logiciels de programmation
orientés autour du concept de base de données
sont devenus les outils principaux des _data scientists_.
Le fait de pouvoir appliquer un certain nombre d'opérations
standards sur des bases de données, quelle que soit leur nature,
permet aux programmeurs d'être plus efficaces que s'ils devaient
répéter ces opérations à la main, comme dans `Excel`.

Tous les langages de programmation dominants dans l'écosystème
de la _data science_ reposent sur le principe du _dataframe_. 
Il s'agit même d'un objet central dans certains logiciels,
notamment `R`. 
La logique [`SQL`](https://fr.wikipedia.org/wiki/Structured_Query_Language),
un langage de déclaration d'opérations sur des données qui a déjà plus de cinquante ans, 
offre un cadre pertinent pour effectuer des opérations standardisées
sur les colonnes (création de nouvelles colonnes, sélection de sous-ensemble de lignes...).

Néanmoins, le _dataframe_ ne s'est imposé que récemment en `Python`,
grâce au package `Pandas` créé
par [Wes McKinney](https://fr.wikipedia.org/wiki/Wes_McKinney). 
L'essor de la librairie `Pandas` (téléchargée plus de 5 millions de fois
par jour en 2023) est pour beaucoup dans le succès de `Python`
dans l'écosystème de la _data science_ et a amené, en quelques années,
a un renouvellement complet de la manière de coder en `Python`, ce
langage si malléable, autour de l'analyse de données. 

Cette partie du cours est une introduction
générale à l'écosystème très riche de 
la manipulation de données avec `Python`.
Ces chapitres évoquent aussi bien la récupération de données
que la restructuration et la production d'analyse
à partir de celles-ci. 

## Résumé de cette partie {.unnumbered}

`Pandas` est devenu incontournable dans l'écosystème `Python` pour la *data science*. 
`Pandas` est lui-même construit à partir du package `Numpy`, qu'il est utile de comprendre
pour être à l'aise avec `Pandas`. `Numpy` est une librairie bas-niveau 
pour stocker et manipuler des données. 
`Numpy` est au coeur de l'écosystème de la *data science* car la plupart des librairies, même celles
qui manient des objets destructurés,
utilisent des objets construits à partir de `Numpy`[^1]. 

[^1]: Certaines librairies commencent, petit à petit, à s'émanciper
de `Numpy` qui n'est pas toujours le plus adapté pour la gestion
de certains types de données. Le _framework_ `Arrow` tend à devenir
la couche basse utilisée par de plus en plus de librairies de _data science_. 
[Ce _post_ de blog](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) approfondit
de manière très pédagogique ce sujet. 

L'approche `Pandas`, qui offre un point d'entrée harmonisé pour manipuler
des jeux de données de nature très différente,
a été étendue aux objets géographiques avec `Geopandas`.
Il est ainsi possible de manipuler des données géographiques comme s'il
s'agissait de données structurées classiques. Les données géographiques et
la représentation cartographique deviennent de plus en plus commun avec
la multiplication de données ouvertes localisées et de *big-data* géolocalisées.

Cependant, les données structurées, importées depuis des fichiers plats
ne représentent pas l'unique source de données. Les API et le *webscraping*
permettent de télécharger ou d'extraire 
des données de manière très flexible depuis des pages web ou des guichets
spécialisés. Ces données, notamment
celles obtenues par *webscraping* nécessitent souvent un peu plus de travail
de nettoyage de données, notamment des chaînes de caractère. 

L'écosystème `Pandas` représente donc un couteau-suisse
pour l'analyse de données. C'est pour cette raison que ce cours 
développera beaucoup de contenu dessus.
Avant d'essayer de mettre en oeuvre une solution _ad hoc_, il est
souvent utile de se poser la question suivante : _"ne pourrais-je pas le faire
avec les fonctionalités de base de `Pandas` ?"_ Se poser cette question peut
éviter des chemins ardus et faire économiser beaucoup de temps. 

Néanmoins, `Pandas` n'est pas 
adapté à des données ayant une volumétrie
importante. Pour traiter de telles
données, il est plutôt recommander 
de privilégier `Polars` ou `Dask` qui reprennent la logique de `Pandas` mais
optimisent son fonctionnement, `Spark` si on a une infrastructure adaptée, généralement dans
des environnements _big data_, ou
`DuckDB` si on est prêt à utiliser des requêtes SQL plutôt qu'une librairie haut-niveau.


## Exercices {.unnumbered}

Cette partie présente à la fois des tutoriels détaillés
et des exercices guidés.

Il est
possible de les consulter sur ce site ou d'utiliser l'un des
badges présents en début de chapitre, par exemple
ceux-ci pour ouvrir
le [chapitre d'exercices sur `Pandas`](02b_pandas_TP/):

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/manipulation/02b_pandas_TP.qmd")
```
:::


## Pour aller plus loin {.unnumbered}

Ce cours n'aborde pas vraiment les questions de volumétrie ou de vitesse de 
calcul. 
`Pandas` peut montrer ses limites dans ce domaine sur des jeux de données
d'une volumétrie conséquente (plusieurs Gigas). 

Il est ainsi intéressant de porter attention à:

* Le livre [Modern Pandas](https://tomaugspurger.github.io/modern-1-intro.html)
pour obtenir des éléments supplémentaires sur la question de la performance
avec `Pandas` ;
* La question des
[objets _sparse_](https://chrisalbon.com/machine_learning/vectors_matrices_and_arrays/create_a_sparse_matrix/) ;
* Les _packages_ [`Dask`](https://dask.org/) ou [`Polars`](https://ssphub.netlify.app/post/polars/) pour accélérer les calculs ;
* [`DuckDB`](https://duckdb.org/docs/api/python/overview.html) pour effectuer de manière très efficace des requêtes SQL ;
* [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html) pour des données très volumineuses.


### Références {.unnumbered}

Voici une bibliographie sélective des ouvrages
intéressants en complément des chapitres de la partie "Manipulation" de ce cours :

::: {#refs}
:::
