---
title: "Numpy, la brique de base de la data science"
title-en: "Numpy, the foundation of data science"
author: Lino Galiana
description: |
  `Numpy` constitue la brique de base de l'écosystème de la _data science_ en `Python`. Toutes les librairies de manipulation de données, de modélisation et de visualisation reposent, de manière plus ou moins directe, sur `Numpy`. Il est donc indispensable de revoir quelques notions sur ce package avant d'aller plus loin.
description-en: |
  `Numpy` is the cornerstone of the _data science_ ecosystem in `Python`. All data manipulation, modeling, and visualization libraries rely, directly or indirectly, on `Numpy`. It is therefore essential to review some concepts of this package before moving forward.
image: scatter_numpy.png
echo: false
---

{{< badges
    printMessage="true"
>}}

:::: {.content-visible when-format="ipynb"}
::: {.warninglang .callout-warning}
:::
::::

# Introduction

::: {.content-visible when-profile="fr"}

Ce chapitre constitue une introduction à _Numpy_ pour
s'assurer que les bases du calcul vectoriel avec `Python`
soient maîtrisées. La première partie du chapitre
présente des petits exercices pour pratiquer quelques fonctions basiques de `Numpy`. La fin du chapitre présente
des exercices pratiques d'utilisation de `Numpy` plus approfondis.

Il est recommandé de régulièrement se référer à
la [_cheatsheet numpy_](https://www.datacamp.com/community/blog/python-numpy-cheat-sheet) et à la
[doc officielle](https://numpy.org/doc/stable/) en cas de doute
sur une fonction. 

Dans ce chapitre, on ne dérogera pas à la convention qui s'est imposée
d'importer `Numpy` de la
manière suivante :

:::

::: {.content-visible when-profile="en"}
This chapter serves as an introduction to _Numpy_ to ensure that the basics of vector calculations with `Python` are mastered. The first part of the chapter presents small exercises to practice some basic functions of `Numpy`. The end of the chapter presents more in-depth practical exercises using `Numpy`.

It is recommended to regularly refer to the [_numpy cheatsheet_](https://www.datacamp.com/community/blog/python-numpy-cheat-sheet) and the [official documentation](https://numpy.org/doc/stable/) if you have any doubts about a function.

In this chapter, we will adhere to the convention of importing `Numpy` as follows:
:::


```{python}
#| label: import-np
#| echo: true
import numpy as np
```

::: {.content-visible when-profile="fr"}
Nous allons également fixer la racine du générateur aléatoire de nombres
afin d'avoir des résultats reproductibles :
:::

::: {.content-visible when-profile="en"}
We will also set the seed of the random number generator to obtain reproducible results:
:::

```{python}
#| echo: true
import numpy as np
rng = np.random.default_rng(seed=12345)
```

::: {.content-visible when-profile="fr"}

::: {.callout-caution}

Historiquement, la génération de nombres aléatoires se faisait pas le biais du package `numpy.random`. Néanmoins, les auteurs de `Numpy` [recommandent maintenant](https://numpy.org/doc/stable/reference/random/index.html) d'utiliser plutôt des générateurs pour cela. Les exemples de ce tutoriel adoptent donc cette pratique.

:::

:::

:::: {.content-visible when-profile="en"}

::: {.callout-caution}

Historically, random numbers were generated using the `numpy.random` package. However, the authors of `Numpy` [now recommend](https://numpy.org/doc/stable/reference/random/index.html) using generators instead. The examples in this tutorial adopt this practice.

:::

::::

::: {.content-visible when-profile="fr"}



# Le concept d'_array_

Dans le monde de la science des données, comme cela sera évoqué
plus en profondeur dans les prochains chapitres, 
l'objet central est le tableau à deux dimensions de données.
La première correspond aux lignes et la seconde aux colonnes. 
Si on ne se préoccupe que d'une dimension, on se rapporte 
à une variable (une colonne) de notre tableau de données. 
Il est donc naturel de faire le lien entre les
tableaux de données et l'objet mathématique
que sont les matrices et les vecteurs. 

`NumPy` (`Numerical Python`) est la brique de base
pour traiter des listes numériques ou des chaines
de textes comme des matrices.
`NumPy` intervient pour proposer
ce type d'objets, et
les opérations standardisées associées qui n'existent
pas dans le langage `Python` de base. 

L'objet central de `NumPy` est
l'**`array`** qui est un tableau de données multidimensionnel.
L'array `Numpy` peut être unidimensionnel et s'apparenter à un
vecteur (`1d-array`),
bidimensionnel et ainsi s'apparenter à une matrice (`2d-array`) ou,
de manière plus générale, 
prendre la forme d'un objet
multidimensionnel (`Nd-array`), sorte de tableau emboîté. 

Les tableaux simples (uni ou bi-dimensionnels) sont faciles à se représenter
et représentent la majorité des besoins liés à `Numpy`. 
Nous découvrirons lors du chapitre suivant, sur `Pandas`, qu'en pratique
on manipule rarement directement `Numpy` qui est une librairie
bas niveau. 
Un _DataFrame_ `Pandas` sera construit à partir d'une collection
d'array uni-dimensionnels (les variables de la table), ce qui permettra d'effectuer des opérations cohérentes
(et optimisées) avec le type de la variable.
Avoir quelques notions `Numpy` est utile pour comprendre
la logique de manipulation vectorielle
rendant les traitements sur des données plus lisibles,
plus efficaces et plus fiables.

Par rapport à une liste,

* un *array* ne peut contenir qu'un type de données (`integer`, `string`, etc.),
contrairement à une liste.
* les opérations implémentées par `Numpy` seront plus efficaces et demanderont moins
de mémoire

Les données géographiques constitueront une construction un peu plus complexe qu'un `DataFrame` traditionnel. 
La dimension géographique prend la forme d'un tableau plus profond, au moins bidimensionnel
(coordonnées d'un point). Néanmoins, les librairies de manipulation
de données géographiques permettront de ne pas se préoccuper de
cette complexité accrue.

:::

::: {.content-visible when-profile="en"}
# Concept of _array_

In the world of data science, as will be discussed in more depth in the upcoming chapters, the central object is the two-dimensional data table. The first dimension corresponds to rows and the second to columns. If we only consider one dimension, we refer to a variable (a column) of our data table. It is therefore natural to link data tables to the mathematical objects of matrices and vectors.

`NumPy` (`Numerical Python`) is the foundational brick for processing numerical lists or strings of text as matrices. `NumPy` comes into play to offer this type of object and the associated standardized operations that do not exist in the basic `Python` language.

The central object of `NumPy` is the **`array`**, which is a multidimensional data table. A `Numpy` array can be one-dimensional and considered as a vector (`1d-array`), two-dimensional and considered as a matrix (`2d-array`), or, more generally, take the form of a multidimensional object (`Nd-array`), a sort of nested table.

Simple arrays (one or two-dimensional) are easy to represent and cover most of the use-case related to `Numpy`. We will discover in the next chapter on `Pandas` that, in practice, we usually don't directly use  `Numpy` since it is a low-level library. A `Pandas` _DataFrame_ is constructed from a collection of one-dimensional arrays (the variables of the table), which allows performing coherent (and optimized) operations with the variable type. Having some `Numpy` knowledge is useful for understanding the logic of vector manipulation, making data processing more readable, efficient, and reliable.

Compared to a list,

* an *array* can only contain one type of data (`integer`, `string`, etc.), unlike a list.
* operations implemented by `Numpy` will be more efficient and require less memory.

Geographical data will constitute a slightly more complex construction than a traditional `DataFrame`. The geographical dimension takes the form of a deeper table, at least two-dimensional (coordinates of a point). However, geographical data manipulation libraries will handle this increased complexity.
:::


::: {.content-visible when-profile="fr"}

## Créer un array

On peut créer un _array_ de plusieurs manières. Pour créer un _array_ à partir d'une liste,
il suffit d'utiliser la méthode `array`:

:::

::: {.content-visible when-profile="en"}
## Creating an array

We can create an array in several ways. To create an array from a list, simply use the `array` method:

:::


```{python}
#| echo: true
np.array([1,2,5])
```

::: {.content-visible when-profile="fr"}

Il est possible d'ajouter un argument `dtype` pour contraindre le type du *array* :

:::

::: {.content-visible when-profile="en"}
It is possible to add a `dtype` argument to constrain the array type:
:::

```{python}
#| echo: true
np.array([["a","z","e"],["r","t"],["y"]], dtype="object")
```

::: {.content-visible when-profile="fr"}

Il existe aussi des méthodes pratiques pour créer des array:

* séquences logiques : `np.arange` (suite) ou `np.linspace` (interpolation linéaire entre deux bornes) ;
* séquences ordonnées : _array_ rempli de zéros, de 1 ou d'un nombre désiré : `np.zeros`, `np.ones` ou `np.full` ;
* séquences aléatoires : fonctions de génération de nombres aléatoires : `rng.uniform`, `rng.normal`, etc. où `rng` est un générateur de nombre aléatoires ;  
* tableau sous forme de matrice identité : `np.eye`.

Ceci donne ainsi, pour les séquences logiques:

:::

::: {.content-visible when-profile="en"}
There are also practical methods for creating arrays:

* Logical sequences: `np.arange` (sequence) or `np.linspace` (linear interpolation between two bounds);
* Ordered sequences: array filled with zeros, ones, or a desired number: `np.zeros`, `np.ones`, or `np.full`;
* Random sequences: random number generation functions: `rng.uniform`, `rng.normal`, etc. where `rng` is a random number generator;
* Matrix in the form of an identity matrix: `np.eye`.

This gives, for logical sequences:
:::



```{python}
#| echo: true
np.arange(0,10)
```

```{python}
#| echo: true
np.arange(0,10,3)
```

```{python}
#| echo: true
np.linspace(0, 1, 5)
```

::: {.content-visible when-profile="fr"}
Pour un _array_ initialisé à 0:
:::

::: {.content-visible when-profile="en"}
For an array initialized to 0:
:::

```{python}
#| echo: true
np.zeros(10, dtype=int)
```

::: {.content-visible when-profile="fr"}
ou initialisé à 1:
:::
::: {.content-visible when-profile="en"}
or initialized to 1:
:::

```{python}
#| echo: true
np.ones((3, 5), dtype=float)
```

::: {.content-visible when-profile="fr"}
ou encore initialisé à 3.14:
:::
::: {.content-visible when-profile="en"}
or even initialized to 3.14:
:::

```{python}
#| echo: true
np.full((3, 5), 3.14)
```

::: {.content-visible when-profile="fr"}
Enfin, pour créer la matrice $I_3$:
:::
::: {.content-visible when-profile="en"}
Finally, to create the matrix $I_3$:
:::

```{python}
#| echo: true
np.eye(3)
```

::: {.content-visible when-profile="fr"}
{{< include "01_numpy_exercises/_exo1_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo1_solution.qmd" >}}
:::

::: {.content-visible when-profile="en"}
{{< include "01_numpy_exercises/_exo1_en.qmd" >}}
{{< include "01_numpy_exercises/_exo1_solution.qmd" >}}
:::


::: {.content-visible when-profile="fr"}

# Indexation et _slicing_

## Logique dans le cas d'un array unidimensionnel

La structure la plus simple est l'_array_ unidimensionnel:
:::

::: {.content-visible when-profile="en"}

# Indexing and slicing

## Logic illustrated with a one-dimensional array

The simplest structure is the one-dimensional array:
:::

```{python}
#| echo: true
x = np.arange(10)
print(x)
```

::: {.content-visible when-profile="fr"}

L'indexation est dans ce cas similaire à celle d'une liste: 

* le premier élément est 0
* le énième élément est accessible à la position $n-1$

La logique d'accès aux éléments est ainsi la suivante :

```python
x[start:stop:step]
```

Avec un *array* unidimensionnel, l'opération de *slicing* (garder une coupe du *array*) est très simple. 
Par exemple, pour garder les *K* premiers éléments d'un *array*, on fera:

```python
x[:K]
```

En l'occurrence, on sélectionne le K$^{eme}$ élément en utilisant

```python
x[K-1]
```

Pour sélectionner uniquement un élément, on fera ainsi:

```{python}
#| echo: true
x = np.arange(10)
x[2]
```

Les syntaxes qui permettent de sélectionner des indices particuliers d'une liste fonctionnent également
avec les _arrays_.

:::

::: {.content-visible when-profile="en"}
Indexing in this case is similar to that of a list:

* The first element is 0
* The nth element is accessible at position $n-1$

The logic for accessing elements is as follows:

```python
x[start:stop:step]
```

With a one-dimensional array, the slicing operation (keeping a slice of the array) is very simple. For example, to keep the first *K* elements of an array, you would do:

```python
x[:K]
```

In this case, you select the K$^{th}$ element using:

```python
x[K-1]
```

To select only one element, you would do:

```{python}
#| echo: true
x = np.arange(10)
x[2]
```

The syntax for selecting particular indices from a list also works with arrays.
:::

::: {.content-visible when-profile="fr"}
{{< include "01_numpy_exercises/_exo2_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo2_solution.qmd" >}}
:::

::: {.content-visible when-profile="en"}
{{< include "01_numpy_exercises/_exo2_en.qmd" >}}
{{< include "01_numpy_exercises/_exo2_solution.qmd" >}}
:::

::: {.content-visible when-profile="fr"}

La logique se généralise pour les _array_ multidimensionnels. L'indexation se fait alors à plusieurs niveaux.  Prenons par exemple un array à 2 dimensions (une matrice en quelques sortes):

```{python}
#| echo: true
x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
```

Si on veut sélectionner la 2e ligne, 3e colonne (l'élément de valeur 6), on fait

```{python}
#| echo: true
x[1, 2]
```

Maintenant, pour sélectionner une colonne complète (par exemple la 2e), on peut utiliser le 2e index pour spécifier celle-ci (index 1 en Python puisque l'indexation part de 0) puis `:` sur la première dimension (version raccourcie de `0:N`) pour ne pas discriminer selon cette dimension: 

```{python}
#| echo: true
x[:,1]
```

Le principe se généralise, mais se complexifie, pour des _array_ imbriqués. Heureusement, ce sont des objets qu'on manipule assez rarement directement, la plupart de nos données numériques étant des tableaux plats (une valeur - l'observation - est le croisement d'une ligne - l'individu - et d'une colonne - la variable). 

:::

::: {.content-visible when-profile="en"}
The same logic applies to multidimensional _arrays_. Indexing then takes place at several levels.  Take, for example, a 2-dimensional array (a matrix of sorts):

```{python}
#| echo: true
x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
```

If we want to select the 2nd row, 3rd column (the element with value 6), we do

```{python}
#| echo: true
x[1, 2]
```

Now, to select a complete column (e.g. the 2nd), we can use the 2nd index to specify it (index 1 in Python since indexing starts from 0) and then `:` on the first dimension (shortened version of `0:N`) to avoid discriminating according to this dimension: 

```{python}
#| echo: true
x[:,1]
```

The principle is generalized, but becomes more complex, for nested _arrays_. Fortunately, these are objects we rarely manipulate directly, as most of our numerical data are flat arrays (a value - the observation - is the intersection of a row - the individual - and a column - the variable). 


:::


::: {.content-visible when-profile="fr"}

## Sur la performance

Un élément déterminant dans la performance de `Numpy` par rapport aux listes,
lorsqu'il est question de 
*slicing* est qu'un array ne renvoie pas une
copie de l'élément en question (copie qui coûte de la mémoire et du temps)
mais simplement une vue de celui-ci.

Lorsqu'il est nécessaire d'effectuer une copie,
par exemple pour ne pas altérer l'_array_ sous-jacent, on peut 
utiliser la méthode `copy`:

:::

::: {.content-visible when-profile="en"}
## Regarding performance

A key element in the performance of `Numpy` compared to lists, when it comes to slicing, is that an array does not return a copy of the element in question (a copy that costs memory and time) but simply a view of it.

When it is necessary to make a copy, for example to avoid altering the underlying array, you can use the `copy` method:

:::

```python
x_sub_copy = x[:2, :2].copy()
```

::: {.content-visible when-profile="fr"}

## Filtres logiques


Il est également possible, et plus pratique, de sélectionner des données à partir de conditions logiques
(opération qu'on appelle un __*boolean mask*__).
Cette fonctionalité servira principalement à 
effectuer des opérations de filtre sur les données.

Pour des opérations de comparaison simples, les comparateurs logiques peuvent être suffisants. 
Ces comparaisons fonctionnent aussi sur les tableaux multidimensionnels grâce au
*broadcasting* sur lequel nous reviendrons :

:::

::: {.content-visible when-profile="en"}
It is also possible, and more practical, to select data based on logical conditions (an operation called a __*boolean mask*__). This functionality will mainly be used to perform data filtering operations.

For simple comparison operations, logical comparators may be sufficient. These comparisons also work on multidimensional arrays thanks to broadcasting, which we will discuss later:
:::


```{python}
#| echo: true
x = np.arange(10)
x2 = np.array([[-1,1,-2],[-3,2,0]])
print(x)
print(x2)
```

```{python}
#| echo: true
x==2
x2<0
```


::: {.content-visible when-profile="fr"}
Pour sélectionner les observations relatives à la condition logique,
il suffit d'utiliser la logique de *slicing* de `numpy` qui fonctionne avec les conditions logiques

{{< include "01_numpy_exercises/_exo3_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo3_solution.qmd" >}}
:::

::: {.content-visible when-profile="en"}
To select the observations related to the logical condition, just use the `numpy` slicing logic that works with logical conditions.

{{< include "01_numpy_exercises/_exo3_en.qmd" >}}
{{< include "01_numpy_exercises/_exo3_solution.qmd" >}}
:::


::: {.content-visible when-profile="fr"}
Lorsque c'est possible, il est recommandé d'utiliser les fonctions logiques de `numpy` (optimisées et 
qui gèrent bien la dimension).
Parmi elles, on peut retrouver:

* `count_nonzero` ;
* `isnan` ;
* `any` ou `all`, notamment avec l'argument `axis` ;
* `np.array_equal` pour vérifier, élément par élément, l'égalité.


Soient `x` un *array* multidimensionnel et `y` un *array* unidimensionnel présentant une valeur manquante. 
:::

::: {.content-visible when-profile="en"}
Whenever possible, it is recommended to use `numpy`'s logical functions (optimized and well-handling dimensions). Among them are:

* `count_nonzero` ;
* `isnan` ;
* `any` or `all` especially with the `axis` argument ;
* `np.array_equal` to check element-by-element equality.

Let's create `x` a multidimensional array and `y` a one-dimensional array with a missing value.

:::

```{python}
#| echo: true
# Assuming rng has been created beforehand
x = rng.normal(0, size=(3, 4))
y = np.array([np.nan, 0, 1])
```

::: {.content-visible when-profile="fr"}
{{< include "01_numpy_exercises/_exo4_fr.qmd" >}}
:::

::: {.content-visible when-profile="en"}
{{< include "01_numpy_exercises/_exo4_en.qmd" >}}
:::

{{< include "01_numpy_exercises/_exo4_solution.qmd" >}}


::: {.content-visible when-profile="fr"}

# Manipuler un _array_

## Fonctions de manipulation

`Numpy` propose des méthodes ou des fonctions standardisées pour
modifier un array, voici un tableau en présentant quelques-unes:

| Opération | Implémentation |
|-----------|----------------|
| Aplatir un array | `x.flatten()` (méthode) |
| Transposer un array | `x.T` (méthode) ou `np.transpose(x)` (fonction) |
| Ajouter des éléments à la fin | `np.append(x, [1,2])` |
| Ajouter des éléments à un endroit donné (aux positions 1 et 2) | `np.insert(x, [1,2], 3)` |
| Supprimer des éléments (aux positions 0 et 3) | `np.delete(x, [0,3])` |

Pour combiner des array, on peut utiliser, selon les cas, 
les fonctions `np.concatenate`, `np.vstack` ou la méthode `.r_` (concaténation *rowwise*). 
`np.hstack` ou la méthode `.column_stack` ou `.c_` (concaténation *column-wise*)

:::

::: {.content-visible when-profile="en"}
# Manipulating an array


## Manipulation functions

`Numpy` provides standardized methods or functions for modifying
here's a table showing some of them:


Here are some functions to modify an array:

| Operation | Implementation |
|-----------|----------------|
| Flatten an array | `x.flatten()` (method) |
| Transpose an array | `x.T` (method) or `np.transpose(x)` (function) |
| Append elements to the end | `np.append(x, [1,2])` |
| Insert elements at a given position (at positions 1 and 2) | `np.insert(x, [1,2], 3)` |
| Delete elements (at positions 0 and 3) | `np.delete(x, [0,3])` |

To combine arrays, you can use, depending on the case, the functions `np.concatenate`, `np.vstack` or the method `.r_` (row-wise concatenation). `np.hstack` or the method `.column_stack` or `.c_` (column-wise concatenation).

:::

```{python}
#| echo: true
x = rng.normal(size = 10)
```

::: {.content-visible when-profile="fr"}
Pour ordonner un array, on utilise `np.sort`
:::
::: {.content-visible when-profile="en"}
To sort an array, use `np.sort`
:::

```{python}
#| echo: true
x = np.array([7, 2, 3, 1, 6, 5, 4])

np.sort(x)
```

::: {.content-visible when-profile="fr"}
Si on désire faire un ré-ordonnement partiel pour trouver les _k_ valeurs les plus petites d'un `array` sans les ordonner, on utilise `partition`:
:::
::: {.content-visible when-profile="en"}
If you want to perform a partial reordering to find the _k_ smallest values in an `array` without sorting them, use `partition`:
:::

```{python}
#| echo: true
np.partition(x, 3)
```

::: {.content-visible when-profile="fr"}

## Statistiques sur un _array_

Pour les statistiques descriptives classiques,
`Numpy` propose un certain nombre de fonctions déjà implémentées,
qui peuvent être combinées avec l'argument `axis`

:::

::: {.content-visible when-profile="en"}
For classical descriptive statistics, `Numpy` offers a number of already implemented functions, which can be combined with the `axis` argument.
:::

```{python}
#| echo: true
x = rng.normal(0, size=(3, 4))
```


::: {.content-visible when-profile="fr"}
{{< include "01_numpy_exercises/_exo5_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo5_solution.qmd" >}}
:::
::: {.content-visible when-profile="en"}
{{< include "01_numpy_exercises/_exo5_en.qmd" >}}
{{< include "01_numpy_exercises/_exo5_solution.qmd" >}}
:::

# Simulations numériques

`Numpy` est incontournable dès lors qu'on effectue des simulations aléatoires, ce qui est très commun en statistiques computationnelles avec un ensemble de méthodes dites de Monte-Carlo. Le principe général est de remplacer un calcul théorique difficile (une intégrale, une probabilité, une espérance) par une approximation numérique obtenue en répétant un grand nombre de tirages aléatoires et en moyennant la quantité d’intérêt, en s’appuyant sur la loi des grands nombres et le théorème central limite pour quantifier la précision de l’estimation.

Illustrons empiriquement quelques théorèmes incontournables de la statistique par une série d'exercices. Cela permettra d'explorer : 

* La loi des grands nombres (@tip-exo2fr);
* Le théorème central limite et sa version particulière dans le cas de lancer de pièce, le [théorème de Moivre-Laplace](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Moivre-Laplace) ;
* Les intervalles de confiance théoriques et leur contrepartie empirique à travers le _bootstrap_ ;
* Le principe des méthodes de Monte Carlo avec l'algorithme du rejet.  

Nous allons avoir besoin des éléments suivants pour initialiser notre processus générateur de données.

```{python}
#| echo: true
import numpy as np

def generate_grid(size_max=1000):
    n_small = np.arange(3, 200, 2)
    n_log = np.unique(np.round(np.logspace(np.log10(200), np.log10(size_max), 120)).astype(int))
    return np.unique(np.concatenate([n_small, n_log]))

N_max = 100_000
rng = np.random.default_rng(seed=123)
grid = generate_grid(size_max=N_max)
```

Ces éléments nous permettent de générer une série aléatoire d’observations jusqu’à une taille `N_max`, puis d’observer l’évolution des estimateurs empiriques de moments (moyenne et variance) lorsque l’on ne conserve que les `n` premières observations, pour différents `n` donnés par `grid`. L’objectif est d’illustrer la convergence des estimateurs empiriques vers leurs valeurs théoriques lorsque `n` augmente.

::: {#tip-exo2-fr .callout-tip}
## Exercice 6, partie 1. La loi des grands nombres

On considère une suite i.i.d. $(X_i)_{i \ge 1}$ telle que $X_i \sim \mathcal{U}\bigl([0,1]\bigr)$.

1. Générer un vecteur `X` de taille `N_max` suivant la loi uniforme $\mathcal{U}\bigl([0,1]\bigr)$.
2. Représenter l'histogramme des valeurs observées $X_i$.
3. Pour chaque valeur $n$ de `grid`, calculer les grandeurs empiriques associées au préfixe $(X_1,\dots,X_n)$:
   1. la moyenne empirique 
      $$
      \bar X_n = \frac{1}{n}\sum_{i=1}^n X_i
      $$
   2. la variance empirique (avec diviseur $n$) :
      $$
      s_n^2 = \frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2,
      $$
      puis la variance d’échantillon (version non biaisée, avec diviseur $n-1$) :
      $$
      \hat s_n^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X_n)^2.
      $$
4. Rappeler les valeurs théoriques de la moyenne $\mu$ et de la variance $\sigma^2$ de $\mathcal{U}\bigl([0,1]\bigr)$, puis comparer graphiquement les quantités empiriques à ces valeurs.
5. À quelle vitesse la moyenne empirique $\bar X_n$ converge-t-elle vers $\mu$ ? Proposer une illustration graphique (par exemple en traçant $|\bar X_n-\mu|$ en fonction de $n$ et en comparant à une décroissance en $1/\sqrt{n}$, ou en traçant $\sqrt{n}\,|\bar X_n-\mu|$).

:::


```{python}
# RNG function to make it easier to change distribution afterwards
def flexible_rng(
    dist="uniform",
    N_max=10000,
    seed=123,
    rng=None,
    **kwargs
):
    if rng is None:
        rng = np.random.default_rng(seed)

    d = dist.lower()

    correspondance = {
        # Discrètes
        "bernoulli": rng.binomial,
        "poisson":   rng.poisson,

        # Continues
        "uniform":     rng.uniform,
        "exponential": rng.exponential,
    }

    rng_func = correspondance.get(d, None)
    if rng_func is None:
        raise ValueError("dist arg should be: bernoulli, poisson, exponential or uniform")

    return rng_func(size=N_max, **kwargs)
```

```{python}
import matplotlib.pyplot as plt

X = flexible_rng("uniform", N_max=N_max, seed=123)

mu_theory = 0.5
var_theory = 1/12

# 5) Calcul rapide des moments empiriques sur les préfixes X[:n] (sans boucles sur les slices)
cs1 = np.cumsum(X)
cs2 = np.cumsum(X**2)

mean_n = cs1[grid - 1] / grid
var_emp_n = cs2[grid - 1] / grid - mean_n**2                    # variance "empirique" (ddof=0)
var_unb_n = (cs2[grid - 1] - grid * mean_n**2) / (grid - 1)     # variance corrigée (ddof=1), ok car grid>=3

bias_n = np.abs(mean_n - mu_theory)

moments = {
    "n": grid,
    "mean": mean_n,
    "var_empirique": var_emp_n,
    "var_corrigee": var_unb_n,
    "bias": bias_n,
    "sqrt_n": 1 / np.sqrt(grid),
}
```

```{python}
#| output: false
# Figures using matplotlib
plt.figure()
plt.hist(X, bins=50, density=True, edgecolor="white")
plt.xlabel("Valeurs de $X$")
plt.ylabel("Densité")
plt.title("Histogramme des valeurs réalisées de $X \sim \\mathcal{U}([0,1])$")
plt.tight_layout()
plt.show()
```

La densité empirique sur notre tirage de grande taille montre bien une quasi-équirépartition des valeurs (@fig-distrib-emp-q2-fr). Notre loi empirique n'est pas parfaitement uniforme mais n'est pas loin de l'être à vue d'oeil. Avant d'étudier plus amplement la distribution, objet des prochains exercices, on peut déjà vérifier les moments de notre tirage que sont la moyenne et la variance empirique.   

```{python}
#| output: false

# Figures using plotnine (see visualisation chapters)
import pandas as pd
from plotnine import *

p = (
    ggplot(pd.DataFrame(X, columns = ["x"]), aes(x = "x")) +
    geom_histogram(aes(y=after_stat("density")),
                   bins=50,
                   boundary=0,
                   closed="left",
                   color="white",
                   fill = "#991124") +
    labs(
        title="Histogramme des valeurs réalisées de $X \sim \\mathcal{U}([0,1])$",
        x="Valeurs de $X$",
        y="Densité"
    ) +
    theme_minimal()
)
```

::: {.content-visible when-profile="fr"}

```{python}
#| fig-cap: "Distribution empirique de notre tirage aléatoire de loi uniforme (question 2, @tip-exo2-fr)"
#| label: fig-distrib-emp-q2-fr
p
```

:::

::: {.content-visible when-profile="en"}

```{python}
#| fig-cap: "Empirical distribution from random draws with uniform law (question 2)"
#| label: fig-distrib-emp-q2-en
p
```

:::



```{python}
#| output: false
# Figures using matplotlib
plt.figure()
plt.plot(grid, moments["mean"], label=r"$\bar X_n$")
plt.axhline(mu_theory, linestyle="--", label=r"$\mu=1/2$")
plt.xscale("log")
plt.title("LGN sur $\\mathcal{U}([0,1])$ : convergence de la moyenne")
plt.xlabel("n")
plt.ylabel("valeur")
plt.legend()
plt.tight_layout()
```

```{python}
# Figures using plotnine (see visualisation chapters)
df_hline = pd.DataFrame({"mu": [mu_theory]})

breaks = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]

breaks = [b for b in breaks if b <= int(np.max(moments["n"]))]

p = (
    ggplot(pd.DataFrame(moments), aes(x="n", y="mean")) +
    geom_line() +
    geom_hline(df_hline, aes(yintercept="mu"), linetype="dashed", color="red") +
    labs(
        title=r"Loi des grands nombres avec $\mathcal{U}([0,1])$ :",
        subtitle="Convergence de la moyenne",
        x="n",
        y="valeur"
    ) +
    theme_minimal()
)

p_log = p + scale_x_log10(
        breaks=breaks,
        labels=[str(b) for b in breaks]   
    )
```

Avec la @fig-convergence-mean-fr, on voit la convergence de la moyenne empirique vers sa valeur théorique (l'espérance). Ceci illustre la loi des grands nombres, l'un des théorèmes, avec le [théorème fondamental de la statistique](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_Glivenko-Cantelli), qui fait le lien entre la statistique et les probabilités.

```{python}
#| label: fig-convergence-mean-fr
#| layout-ncol: 2
#| fig-cap: "Convergence de la moyenne vers l'espérance théorique (question 3, @tip-exo2-fr)"
#| fig-subcap: 
#|   - "Echelle normale"
#|   - "Echelle logarithmique"

p.show()
p_log.show()
```

```{python}
# 6.2 Convergence de la variance (ddof=0) et de la variance corrigée (ddof=1)
plt.figure()
plt.plot(grid, moments["var_empirique"], label=r"$s_n^2$ (diviseur $n$)")
plt.plot(grid, moments["var_corrigee"], label=r"$\hat s_n^2$ (diviseur $n-1$)", alpha=0.9)
plt.axhline(var_theory, linestyle="--", label=r"$\sigma^2=1/12$")
plt.xscale("log")
plt.title("LGN sur $\\mathcal{U}([0,1])$ : convergence de la variance")
plt.xlabel("n")
plt.ylabel("valeur")
plt.legend()
plt.tight_layout()

# 6.3 Vitesse ~ 1/sqrt(n) : comparer |Xbar - mu| à c/sqrt(n) en log-log
c = moments["bias"][0] * np.sqrt(grid[0])  # constante calibrée sur le 1er point
ref = c / np.sqrt(grid)

plt.figure()
plt.loglog(grid, moments["bias"], label=r"$|\bar X_n - \mu|$")
plt.loglog(grid, ref, linestyle="--", label=r"référence $\propto 1/\sqrt{n}$")
plt.title("Vitesse de convergence typique : ordre $1/\\sqrt{n}$")
plt.xlabel("n")
plt.ylabel(r"erreur")
plt.legend()
plt.tight_layout()

# 6.4 Option : tester la stabilisation de sqrt(n)*|Xbar - mu|
plt.figure()
plt.plot(grid, np.sqrt(grid) * moments["bias"])
plt.xscale("log")
plt.title(r"Test : $\sqrt{n}\,|\bar X_n-\mu|$ (ordre constant)")
plt.xlabel("n")
plt.ylabel(r"$\sqrt{n}\,|\bar X_n-\mu|$")
plt.tight_layout()

plt.show()
```



::: {.callout-tip}
## Exercice 6, partie 2. Moivre-Laplace (binomiale)

On considère une suite i.i.d. $(X_i)_{i \ge 1}$ telle que $X_i \sim \mathcal{B}(100,1/2)$ (nombre de piles sur 100 lancers d’une pièce équilibrée).

1. Représenter graphiquement la loi de $X_1$ (fonction de masse) : $k \mapsto \mathbb{P}(X_1=k)$ pour $k \in \{0,\dots,100\}$.
2. Calculer la moyenne $\mu=\mathbb{E}[X_1]$ et la variance $\sigma^2=\mathrm{Var}(X_1)$. Donner leurs valeurs numériques.
3. Pour une valeur $N$ (par exemple $N \in \{50,200,1000,5000\}$), générer $X_1,\dots,X_N$ i.i.d. et calculer $\bar X_N = \frac{1}{N}\sum_{i=1}^N X_i$.
4. En répétant l’expérience un grand nombre de fois, représenter la distribution empirique de
   $$
   Y_N = \sqrt{N}\,(\bar X_N-\mu).
   $$
5. Sur la même figure, superposer la densité de la loi normale $\mathcal{N}(0,\sigma^2)$. Dans ce cas, on doit obtenir $\sigma^2=25$.

:::

::: {.callout-tip}
## Exercice 6, partie 3. TCL pour d’autres lois (variance finie)

Choisir une loi parmi : Poisson, exponentielle, uniforme (au choix), en précisant ses paramètres. On considère une suite i.i.d. $(X_i)_{i \ge 1}$ suivant cette loi, de moyenne $\mu$ et variance $\sigma^2$ (finies).

1. Donner (ou calculer) $\mu$ et $\sigma^2$ pour la loi choisie.
2. Pour plusieurs valeurs de $N$, en répétant l’expérience un grand nombre de fois, représenter la distribution empirique de
   $$
   Y_N = \sqrt{N}\,(\bar X_N-\mu)
   $$
   et la comparer à la densité de la loi normale $\mathcal{N}(0,\sigma^2)$.
3. Option : étudier aussi la version standardisée
   $$
   Z_N = \sqrt{N}\,\frac{\bar X_N-\mu}{\sigma}
   $$
   et la comparer à $\mathcal{N}(0,1)$.

:::

::: {.callout-tip}
## Exercice 6, partie 4. Contre-exemple : loi de Cauchy

On considère maintenant une suite i.i.d. $(X_i)_{i \ge 1}$ suivant une loi de Cauchy standard.

1. Générer un vecteur `X` de taille `N_max` suivant une loi de Cauchy standard.
2. Pour chaque valeur $n$ de `grid`, calculer la moyenne empirique $\bar X_n = \frac{1}{n}\sum_{i=1}^n X_i$ et tracer $\bar X_n$ en fonction de $n$.
3. Le comportement observé ressemble-t-il à une convergence vers une constante ? Comparer qualitativement avec la partie 1.
4. La LGN et le TCL "classiques" s’appliquent-ils ici ? Justifier en discutant l’existence (ou non) de $\mu$ et de $\sigma^2$.

:::


::: {.content-visible when-profile="fr"}

# _Broadcasting_

Le *broadcasting* désigne un ensemble de règles permettant
d'appliquer des opérations sur des tableaux de dimensions différentes. En pratique, 
cela consiste généralement à appliquer une seule opération à l'ensemble des membres d'un tableau `numpy`. 

La différence peut être comprise à partir de l'exemple suivant. Le *broadcasting* permet
de transformer le scalaire `5` en *array* de dimension 3:

```{python}
#| echo: true
a = np.array([0, 1, 2])
b = np.array([5, 5, 5])

a + b
a + 5
```

Le *broadcasting* peut être très pratique pour effectuer de manière efficace des opérations sur des données à
la structure complexe. Pour plus de détails, se rendre
[ici](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) ou [ici](https://stackoverflow.com/questions/47435526/what-is-the-meaning-of-axis-1-in-keras-argmax).
:::

::: {.content-visible when-profile="en"}
# Broadcasting

Broadcasting refers to a set of rules for applying operations to arrays of different dimensions. In practice, it generally consists of applying a single operation to all members of a `numpy` array.

The difference can be understood from the following example. Broadcasting allows the scalar `5` to be transformed into a 3-dimensional array:

```{python}
#| echo: true
a = np.array([0, 1, 2])
b = np.array([5, 5, 5])

a + b
a + 5
```

Broadcasting can be very practical for efficiently performing operations on data with a complex structure. For more details, visit [here](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) or [here](https://stackoverflow.com/questions/47435526/what-is-the-meaning-of-axis-1-in-keras-argmax).

:::

::: {.content-visible when-profile="fr"}

## Une application: programmer ses propres k-nearest neighbors

```{python}
#| echo: false
lang = "fr"
```


{{< include "01_numpy_exercises/_exo6_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo6_solution.qmd" >}}


Ai-je inventé cet exercice corsé ? Pas du tout, il vient de l'ouvrage [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/02.08-sorting.html#Example:-k-Nearest-Neighbors). Mais, si je vous l'avais indiqué immédiatement, auriez-vous cherché à répondre aux questions ?

Par ailleurs, il ne serait pas une bonne idée de généraliser cet algorithme à de grosses données. La complexité de notre approche est $O(N^2)$. L'algorithme implémenté par `Scikit-Learn` est
en $O[NlogN]$.

De plus, le calcul de distances matricielles en utilisant la puissance des cartes graphiques serait plus rapide. A cet égard, la librairie [faiss](https://github.com/facebookresearch/faiss) ou les _frameworks_ spécialisés dans le calcul de distance entre des vecteurs à haute dimension comme [ChromaDB](https://www.trychroma.com/) 
offrent des performances beaucoup plus satisfaisantes que celles que permettraient `Numpy` sur ce problème précis.

:::

::: {.content-visible when-profile="en"}
## Application: programming your own k-nearest neighbors

```{python}
lang = "en"
```


{{< include "01_numpy_exercises/_exo6_en.qmd" >}}
{{< include "01_numpy_exercises/_exo6_solution.qmd" >}}


Did I invent this challenging exercise? Not at all, it comes from the book [_Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/02.08-sorting.html#Example:-k-Nearest-Neighbors). But if I had told you this immediately, would you have tried to answer the questions?

Moreover, it would not be a good idea to generalize this algorithm to large datasets. The complexity of our approach is $O(N^2)$. The algorithm implemented by `Scikit-Learn` is $O[NlogN]$.

Additionally, computing matrix distances using the power of GPU (graphics cards) would be faster. In this regard, the library [faiss](https://github.com/facebookresearch/faiss), or the dedicated frameworks for computing distance between high-dimensional vectors like [ChromaDB](https://www.trychroma.com/) offer much more satisfactory performance than `Numpy` for this specific problem.

:::

::: {.content-visible when-profile="fr"}

# Exercices supplémentaires

{{< include "01_numpy_exercises/_exo7_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo7_solution.qmd" >}}


Le site 1 est assez central car il est référencé 2 fois. Le site
5 est lui également central puisqu'il est référencé par le site 1.

```{python}
v
```

:::

::: {.content-visible when-profile="en"}
# Additional Exercises


`Google` became famous thanks to its `PageRank` algorithm. This algorithm allows, from links between websites, to give an importance score to a website which will be used to evaluate its centrality in a network. The objective of this exercise is to use `Numpy` to implement such an algorithm from an adjacency matrix that links the sites together.

{{< include "01_numpy_exercises/_exo7_fr.qmd" >}}
{{< include "01_numpy_exercises/_exo7_solution.qmd" >}}

Site 1 is quite central because it is referenced twice. Site 5 is also central since it is referenced by site 1.

```{python}
v
```
:::



