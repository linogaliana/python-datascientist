---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Premier pas vers l'industrialisation avec les pipelines scikit"
date: 2020-10-20T13:00:00Z
draft: false
weight: 60
slug: pipeline-scikit
type: book
tags:
  - scikit
  - machine learning
  - pipeline
categories:
  - Tutorial
summary: |
  Les pipelines scikit permettent d'intégrer de manière très flexible
  un ensemble d'opérations de pre-processing et d'entraînement de modèles
  dans une chaîne d'opérations. Il s'agit d'une approche particulièrement
  appropriée pour réduire la difficulté à changer d'algorithme ou pour
  faciliter la ré-application d'un code à de nouvelles données
---


```{r setup, include=FALSE}
dir_path <- gsub(here::here(), "..", here::here("course","modelisation"))
knitr::knit_hooks$set(
  plot = function(x, options) modif_plot(x, options, dir_path = dir_path)
)
```

```{r, echo = FALSE, results = 'asis', include = TRUE, eval = TRUE}
print_badges()
```

# Pourquoi utiliser les pipelines ?

Les chapitres précédents ont permis de montrer des bouts de code 
épars pour entraîner des modèles ou faire du _preprocessing_.
Cette démarche est intéressante pour tâtonner mais risque d'être coûteuse
ultérieurement s'il est nécessaire d'ajouter une étape de preprocessing 
ou de changer d'algorithme.

Heureusement, `scikit` propose un excellent outil pour proposer un cadre
général pour créer une chaîne de production *machine learning*. Il
s'agit des
[_pipelines_](https://scikit-learn.org/stable/modules/compose.html). 
Ils présentent de nombreux intérêts, parmi lesquels:

* Ils sont très __pratiques__ et __lisibles__. On rentre des données en entrée, on n'appelle qu'une seule fois les méthodes `fit` et `predict` ce qui permet de s'assurer une gestion cohérente des transformations de variables, par exemple après l'appel d'un `StandardScaler`
* La __modularité__ rend aisée la mise à jour d'un pipeline et renforce la capacité à le réutiliser
* Ils permettent de facilement chercher les hyperparamètres d'un modèle. Sans *pipeline*, écrire un code qui fait du *tuning* d'hyperparamètres peut être pénible. Avec les *pipelines*, c'est une ligne de code. 
* La __sécurité__ d'être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l'estimation. 


{{% panel status="hint" title="Hint" icon="fa fa-lightbulb" %}}
Un des intérêts des *pipelines* scikit est qu'ils fonctionnent aussi avec
des méthodes qui ne sont pas issues de `scikit`. Il est très 
facile d'introduire un modèle de réseau de neurone `Keras` dans
un pipeline `scikit`. Pour introduire un modèle économétrique `statsmodels`
c'est un peu plus coûteux mais nous allons proposer des exemples
qui peuvent servir de modèle et qui montrent que c'est faisable 
sans trop de difficulté.
{{% /panel %}}

# Comment créer un *pipeline*

Un *pipeline* est un enchaînement d'opérations qu'on code en enchainant
des pairs *(clé, valeur)*:

* la clé est le nom du pipeline, cela peut être utile lorsqu'on va
représenter le *pipeline* sous forme de diagramme acyclique (visualisation DAG)
ou qu'on veut afficher des informations sur une étape
* la valeur représente la transformation à mettre en oeuvre dans le *pipeline*
(c'est-à-dire, à l'exception de la dernière étape, 
mettre en oeuvre une méthode `transform` et éventuellement une
transformation inverse).


```{python}
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA

estimators = [('reduce_dim', PCA()), ('clf', SVC())]
pipe = Pipeline(estimators)
```

```{python, echo = FALSE}
from sklearn.utils import estimator_html_repr
```


{{% panel status="hint" title="Hint" icon="fa fa-lightbulb" %}}

Il est pratique de visualiser un *pipeline* sous forme de DAG.
Pour cela, dans un notebook, on utilise la configuration
suivante:

```{python, eval = FALSE}
from sklearn import set_config
set_config(display='diagram') 
```

{{% /panel %}}

```{python}
pipe
```

{{< rawhtml >}}

```{python, echo = FALSE}
print(estimator_html_repr(pipe))
```

{{< /rawhtml >}}

Au sein d'une étape de *pipeline*, les paramètres d'un estimateur
sont accessibles avec la notation `<estimator>__<parameter>`.
Cela permet de fixer des valeurs pour les arguments des fonctions `scikit`
qui sont appelées au sein d'un *pipeline*. 
C'est cela qui rendra l'approche des pipelines particulièrement utile
pour la *grid search*:

```{python}
from sklearn.model_selection import GridSearchCV
param_grid = {"reduce_dim__n_components":[2, 5, 10], "clf__C":[0.1, 10, 100]}
grid_search = GridSearchCV(pipe, param_grid=param_grid)
```

{{< rawhtml >}}

```{python, echo = FALSE}
print(estimator_html_repr(grid_search))
```

{{< /rawhtml >}}


## Données utilisées

Nous allons utiliser les données de transactions immobilières DVF pour chercher
la meilleure manière de prédire, sachant les caractéristiques d'un bien, son
prix.

