---
title: "Introduction √† pandas"
date: 2020-07-28T13:00:00Z
draft: false
weight: 20
tags:
  - pandas
  - Pollution
  - Ademe
  - Tutoriel
  - Manipulation
categories:
  - Tutoriel
  - Manipulation
slug: pandas
type: book
summary: |
  `pandas` est l'√©l√©ment central de l'√©cosyst√®me `Python` pour la _data-science_. 
  Le succ√®s r√©cent de `Python` dans l'analyse de donn√©es tient beaucoup √† `pandas` qui a permis d'importer la
  logique `SQL` dans le langage `Python`. `pandas` embarque √©norm√©ment de
  fonctionalit√©s qui permettent d'avoir des _pipelines_ efficaces pour
  traiter des donn√©es de volum√©trie moyenne (jusqu'√† quelques Gigas). Au-del√†
  de cette volum√©trie, il faudra se tourner vers d'autres solutions
  (`PostgresQL`, `Dask`, `Spark`...).
bibliography: ../../../../reference.bib
---



Pour essayer les exemples pr√©sents dans ce tutoriel : 

::: {.cell .markdown}
```{python}
#| echo: false
#| output: asis
#| include: true
#| eval: true
import sys
sys.path.insert(1, '../../../../') #insert the utils module
from utils import print_badges
#print_badges(__file__)
print_badges("content/course/manipulation/02a_pandas_tutorial.qmd")
```
:::

Le _package_ `pandas` est l'une des briques centrales de l'√©cosyst√®me de
la data-science. Son cr√©ateur, Wes McKinney, l'a pens√© comme 
une surcouche √† la librairie `numpy` pour introduire le _DataFrame_
qui est un objet central dans des langages comme `R` 
ou `Stata` mais qui √©tait absent dans `Python`. `pandas` est rapidement
devenu un incontournable de la _data-science_. L'ouvrage
de r√©f√©rence de @mckinney2012python pr√©sente de mani√®re plus 
ample ce _package_.

Ce tutoriel vise √† introduire aux concepts
de base de ce package par l'exemple et √† introduire √† certaines
des t√¢ches les plus fr√©quentes de (re)structuration
des donn√©es du _data-scientist_. Il ne s'agit pas d'un ensemble
exhaustif de commandes: `pandas` est un package tentaculaire 
qui permet de r√©aliser la m√™me op√©ration de nombreuses mani√®res. 
Nous nous concentrerons ainsi sur les √©l√©ments les plus pertinents
dans le cadre d'une introduction √† la _data-science_ et laisserons
les utilisateurs int√©ress√©s approfondir leurs connaissances
dans les ressources foisonnantes qu'il existe sur le sujet. 

Dans ce tutoriel `pandas`, nous allons utiliser:

* Les √©missions de gaz √† effet de serre estim√©es au niveau communal par l'ADEME. Le jeu de donn√©es est 
disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)
et requ√™table directement dans `Python` avec
[cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert)

Le [chapitre suivant](#pandasTP) permettra de mettre en application des √©l√©ments pr√©sents dans ce chapitre avec
les donn√©es ci-dessus associ√©es √† des donn√©es de contexte au niveau communal[^1].


::: {.cell .markdown}
[^1]: Id√©alement, on utiliserait les donn√©es
[disponibles sur le site de l'Insee](https://www.insee.fr/fr/statistiques/3560121) mais celles-ci n√©cessitent un peu de travail
de nettoyage qui n'entre pas dans le cadre de ce TP. 
Pour faciliter l'import de donn√©es `Insee`, il est recommand√© d'utiliser le package
[`pynsee`](https://github.com/InseeFrLab/Py-Insee-Data) qui simplifie l'acc√®s aux donn√©es
de l'Insee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)
ou via des API. 
:::

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Le package `pynsee` est relativement jeune et n'est disponible que sur
[`Github`](https://github.com/InseeFrLab/Py-Insee-Data), pas sur `PyPi`.
Id√©alement, on utilise les commandes suivantes,
pour installer le package et quelques d√©pendances n√©cessaires :

```{python}
#| eval: false
!pip install pathlib2
!pip install python-Levenshtein
!pip install --upgrade xlrd
!pip install git+https://github.com/InseeFrLab/Py-Insee-Data.git
```

Cette approche, qui est la plus simple,
implique que `Jupyter` et `Git` sont capables de communiquer. Si
`Jupyter` ne sait pas o√π trouver `Git`, comme sur des ordinateurs
o√π `Git` a √©t√© install√© en version portable sans ajout au `PATH`, 
il est possible de rencontrer une erreur.

Dans ce cas,
il faut t√©l√©charger le package compress√© et l'installer localement : 

```{python}
#| eval: false
import requests
url = 'https://github.com/InseeFrLab/Py-Insee-Data/archive/refs/heads/master.zip'
r = requests.get(url)
with open("pynsee.zip" , 'wb') as zipfile:
    zipfile.write(r.content)
```

L'installation se fait ensuite de la mani√®re suivante:

```python
!pip install --ignore-installed pynsee.zip
!pip install python-Levenshtein
```

Si le fait de ne pas avoir de barre de progr√®s lors du t√©l√©chargement
vous trouble, vous pouvez vous rendre √† la
[Section Annexe](#annexe) pour d√©couvrir un bout de code
qui effectue les m√™mes t√¢ches mais avec des barres de progr√®s

```{=html}
</div>
```
:::


‚ö†Ô∏è `pandas` offre la possibilit√© d'importer des donn√©es
directement depuis un url. C'est l'option prise dans ce tutoriel.
Si vous pr√©f√®rez, pour des
raisons d'acc√®s au r√©seau ou de performance, importer depuis un poste local,
vous pouvez t√©l√©charger les donn√©es et changer
les commandes d'import avec le chemin ad√©quat plut√¥t que l'url. 


Nous suivrons les conventions habituelles dans l'import des packages

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pynsee.download
```

Pour obtenir des r√©sultats reproductibles, on peut fixer la racine du g√©n√©rateur
pseudo-al√©atoire. 

```{python}
np.random.seed(123)
```

Au cours de cette d√©monstration des principales fonctionalit√©s de `pandas`, et
lors du chapitre suivant,
je recommande de se r√©f√©rer r√©guli√®rement aux ressources suivantes:

* L'[aide officielle de pandas](https://pandas.pydata.org/docs/user_guide/index.html).
Notamment, la
[page de comparaison des langages](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/index.html)
est tr√®s utile
* La _cheatsheet suivante_, [issue de ce post](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463)

![Cheasheet pandas](https://cdn-images-1.medium.com/max/2000/1*YhTbz8b8Svi22wNVvqzneg.jpeg)

# Logique de pandas

L'objet central dans la logique `pandas` est le `DataFrame`.
Il s'agit d'une structure particuli√®re de donn√©es
√† deux dimensions, structur√©es en alignant des lignes et colonnes. Les colonnes
peuvent √™tre de type diff√©rent.

Un `DataFrame` est compos√© des √©l√©ments suivants:

* l'indice de la ligne ;
* le nom de la colonne ;
* la valeur de la donn√©e ;

Structuration d'un DataFrame pandas,
emprunt√©e √† <https://medium.com/epfl-extension-school/selecting-data-from-a-pandas-dataframe-53917dc39953>:

```{python}
#| include: false

import shutil
import requests

url = 'https://miro.medium.com/max/700/1*6p6nF4_5XpHgcrYRrLYVAw.png'
response = requests.get(url, stream=True)
with open('featured.png', 'wb') as out_file:
    shutil.copyfileobj(response.raw, out_file)
```

![](featured.png)

Le concept de *tidy* data, popularis√© par Hadley Wickham via ses packages `R`,
est parfaitement pertinent pour d√©crire la structure d'un `DataFrame pandas`.
Les trois r√®gles sont les suivantes:

* Chaque variable poss√®de sa propre colonne ;
* Chaque observation poss√®de sa propre ligne ;
* Une valeur, mat√©rialisant une observation d'une variable,
se trouve sur une unique cellule.


![Concept de tidy data (emprunt√© √† H. Wickham)](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)


::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```

Les `DataFrames` sont assez rapides en `Python`[^2] et permettent de traiter en local de mani√®re efficace des tables de
donn√©es comportant plusieurs millions d'observations (en fonction de la configuration de l'ordinateur)
et dont la volum√©trie peut √™tre cons√©quente (plusieurs centaines
de Mo). N√©anmoins,  pass√© un certain seuil, qui d√©pend de la puissance de la machine mais aussi de la complexit√©
de l'op√©ration effectu√©e, le DataFrame `pandas` peut montrer certaines limites. Dans ce cas, il existe diff√©rentes
solutions: `Dask` (dataframe aux op√©rations parall√©lis√©s), `SQL` (notamment `Postgres`),
`Spark` (solution _big data_). Un chapitre sp√©cial de ce cours est consacr√© √† `Dask`.

```{=html}
</div>
```
:::



::: {.cell .markdown}
[^2]:  En `R`, les deux formes de dataframes qui se sont impos√©es r√©cemment sont les `tibbles` (package `dplyr`)
et les `data.tables` (package `data.table`). `dplyr` reprend la syntaxe `SQL` de mani√®re relativement
transparente ce qui rend la syntaxe tr√®s proche de celle de `pandas`. Cependant,
alors que `dplyr` supporte tr√®s mal les donn√©es dont la volum√©trie d√©passe 1Go, `pandas` s'en
accomode bien. Les performances de `pandas` sont plus proches de celles de `data.table`, qui est
connu pour √™tre une approche efficace avec des donn√©es de taille importante.
:::

Concernant la syntaxe, une partie des commandes python est inspir√©e par la logique `SQL`. On retrouvera ainsi
des instructions relativement transparentes.

Il est vivement recommand√©, avant de se lancer dans l'√©criture d'une
fonction, de se poser la question de son impl√©mentation native dans `numpy`, `pandas`, etc.
En particulier, la plupart du temps, s'il existe une solution impl√©ment√©e dans une librairie, il convient
de l'utiliser.

# Les Series

En fait, un DataFrame est une collection d'objets appel√©s `pandas.Series`.
Ces `Series` sont des objets d'une dimension qui sont des extensions des
array-unidimensionnels `numpy`. En particulier, pour faciliter le traitement
de donn√©es cat√©gorielles ou temporelles, des types de variables
suppl√©mentaires sont disponibles dans `pandas` par rapport √†
`numpy` (`categorical`, `datetime64` et `timedelta64`). Ces
types sont associ√©s √† des m√©thodes optimis√©es pour faciliter le traitement
de ces donn√©es.

Il ne faut pas n√©gliger l'attribut `dtype` d'un objet
`pandas.Series` car cela a une influence d√©terminante sur les m√©thodes
et fonctions pouvant √™tre utilis√©es (on ne fait pas les m√™mes op√©rations
sur une donn√©e temporelle et une donn√©e cat√©gorielle) et le volume en
m√©moire d'une variable (le type de la variable d√©termine le volume
d'information stock√© pour chaque √©l√©ment ; √™tre trop pr√©cis est parfois
n√©faste).

Il existe plusieurs types possibles pour un `pandas.Series`.
Le type `object` correspond aux types Python `str` ou `mixed`.
Il existe un type particulier pour les variables dont le nombre de valeurs
est une liste finie et relativement courte, le type `category`.
Il faut bien examiner les types de son DataFrame, et convertir √©ventuellement
les types lors de l'√©tape de `data cleaning`.

## Indexation

La diff√©rence essentielle entre une `Series` et un objet `numpy` est l'indexation.
Dans `numpy`,
l'indexation est implicite ; elle permet d'acc√©der √† une donn√©e (celle √†
l'index situ√© √† la position *i*).
Avec une `Series`, on peut bien-s√ªr utiliser un indice de position mais on peut
surtout faire appel √† des indices plus explicites.
Par exemple,

```{python}
taille = pd.Series(
    [1.,1.5,1],
    index = ['chat', 'chien', 'koala']
)

taille.head()
```


Cette indexation permet d'acc√©der √† des valeurs de la `Series`
via une valeur de l'indice. Par
exemple, `taille['koala']`:

```{python}
taille['koala']
```

L'existence d'indice rend le *subsetting* particuli√®rement ais√©, ce que vous
pouvez exp√©rimenter dans les TP

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/course/manipulation/02b_pandas_TP.qmd")
```
:::



Pour transformer un objet `pandas.Series` en array `numpy`,
on utilise la m√©thode `values`. Par exemple, `taille.values`:

```{python}
taille.values
```

Un avantage des `Series` par rapport √† un *array* `numpy` est que
les op√©rations sur les `Series` alignent
automatiquement les donn√©es √† partir des labels.
Avec des `Series` lab√©lis√©es, il n'est ainsi pas n√©cessaire
de se poser la question de l'ordre des lignes.
L'exemple dans la partie suivante permettra de s'en assurer.


## Valeurs manquantes

Par d√©faut, les valeurs manquantes sont affich√©es `NaN` et sont de type `np.nan` (pour
les valeurs temporelles, i.e. de type `datatime64`, les valeurs manquantes sont
`NaT`).

On a un comportement coh√©rent d'agr√©gation lorsqu'on combine deux `DataFrames` (ou deux colonnes).
Par exemple,


```{python}
x = pd.DataFrame(
    {'prix': np.random.uniform(size = 5),
     'quantite': [i+1 for i in range(5)]
    },
    index = ['yaourt','pates','riz','tomates','gateaux']
)
x
```

```{python}
y = pd.DataFrame(
    {'prix': [np.nan, 0, 1, 2, 3],
     'quantite': [i+1 for i in range(5)]
    },
    index = ['tomates','yaourt','gateaux','pates','riz']
)
y
```

```{python}
x + y
```

donne bien une valeur manquante pour la ligne `tomates`. Au passage, on peut remarquer que l'agr√©gation
a tenu compte des index.

Il est possible de supprimer les valeurs manquantes gr√¢ce √† `dropna()`.
Cette m√©thode va supprimer toutes les lignes o√π il y a au moins une valeur manquante.
Il est aussi possible de supprimer seulement les colonnes o√π il y a des valeurs manquantes
dans un DataFrame avec `dropna()` avec le param√®tre `axis=1` (par d√©faut √©gal √† 0).

Il est √©galement possible de remplir les valeurs manquantes gr√¢ce √† la m√©thode `fillna()`.

# Le DataFrame pandas

Le `DataFrame` est l'objet central de la librairie `pandas`.
Il s'agit d'une collection de `pandas.Series` (colonnes) align√©es par les index.
Les types des variables peuvent diff√©rer.

Un `DataFrame` non-index√© a la structure suivante:

```{python}
#| echo: false
df = pd.DataFrame(
    {'taille': [1.,1.5,1],
    'poids' : [3, 5, 2.5]
    },
    index = ['chat', 'chien', 'koala']
)
df.reset_index()
```

Alors que le m√™me `DataFrame` index√© aura la structure suivante:

```{python}
#| echo: false
df = pd.DataFrame(
    {'taille': [1.,1.5,1],
    'poids' : [3, 5, 2.5]
    },
    index = ['chat', 'chien', 'koala']
)
df.head()
```


## Les attributs et m√©thodes utiles

Pour pr√©senter les m√©thodes les plus pratiques pour l'analyse de donn√©es,
on peut partir de l'exemple des consommations de CO2 communales issues
des donn√©es de l'Ademe. Cette base de donn√©es est exploit√©e plus intens√©ment
dans le TP.

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/course/manipulation/02b_pandas_TP.qmd")
```
:::

L'import de donn√©es depuis un fichier plat se fait avec la fonction `read_csv`:

```{python}
df = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
df
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```


Dans un processus de production, o√π normalement on connait les types des variables du `DataFrame` qu'on va importer,
il convient de pr√©ciser les types avec lesquels on souhaite importer les donn√©es
(argument `dtype`, sous la forme d'un dictionnaire).
Cela est particuli√®rement important lorsqu'on d√©sire utiliser une colonne
comme une variable textuelle mais qu'elle comporte des attributs proches d'un nombre
qui vont inciter `pandas` √† l'importer sous forme de variable num√©rique.

Par exemple, une colonne `[00001,00002,...] ` risque d'√™tre import√©e comme une variable num√©rique, ignorant l'information des premiers 0 (qui peuvent pourtant la distinguer de la s√©quence 1, 2, etc.). Pour s'assurer que `pandas` importe sous forme textuelle la variable, on peut utiliser `dtype = {"code": "str"}`
Sinon, on peut importer le csv, et modifier les types avec `astype()`.
Avec `astype`, on peut g√©rer les erreurs de conversion avec le param√®tre `errors`.

```{=html}
</div>
```
:::

L'affichage des DataFrames est tr√®s ergonomique. On obtiendrait le m√™me *output*
avec `display(df)`[^3]. Les premi√®res et derni√®res lignes s'affichent
automatiquement. Autrement, on peut aussi faire:

* `head` qui permet, comme son
nom l'indique, de n'afficher que les premi√®res lignes ;
* `tail` qui permet, comme son
nom l'indique, de n'afficher que les derni√®res lignes
* `sample` qui permet d'afficher un √©chantillon al√©atoire de *n* lignes.
Cette m√©thode propose de nombreuses options.

::: {.cell .markdown}
[^3]: Il est pr√©f√©rable d'utiliser la fonction `display` (ou tout simplement
taper le nom du `DataFrame`) qu'utiliser la fonction `print`. Le
`display` des objets `pandas` est assez esth√©tique, contrairement √† `print`
qui renvoie du texte brut.
:::

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```

Il faut faire attention au `display` et aux
commandes qui r√©v√®lent des donn√©es (`head`, `tail`, etc.)
dans un `Notebook` ou un `Markdown` qui exploite
des donn√©es confidentielles lorsqu'on utilise `Git`.

En effet, on peut se
retrouver √† partager des donn√©es, involontairement, dans l'historique
`Git`. Avec un `R Markdown`, il suffit d'ajouter les sorties au fichier
`.gitignore` (par exemple avec une balise de type `*.html`). Avec un
`Notebook Jupyter`, la d√©marche est plus compliqu√©e car les fichiers
`.ipynb` int√®grent dans le m√™me document, texte, sorties et mise en forme.

Techniquement, il est possible d'appliquer des filtres avec `Git`
(voir
[ici](http://timstaley.co.uk/posts/making-git-and-jupyter-notebooks-play-nice/))
mais c'est une d√©marche tr√®s complexe. 
Ce post de l'√©quipe √† l'origine de [nbdev2](https://www.fast.ai/posts/2022-08-25-jupyter-git.html)
r√©sume bien le probl√®me du contr√¥le de version avec `Git` et des solutions qui
peuvent y √™tre apport√©es.

Une solution est d'utiliser [`Quarto`](https://quarto.org/) qui permet de g√©n√©rer les
`.ipynb` en _output_ d'un document texte, ce qui facilite le contr√¥le sur les 
√©l√©ments pr√©sents dans le document.

```{=html}
</div>
```
:::



### Dimensions et structure du DataFrame

Les premi√®res m√©thodes utiles permettent d'afficher quelques
attributs d'un `DataFrame`.

```{python}
df.axes
```

```{python}
df.columns
```

```{python}
df.index
```

Pour conna√Ætre les dimensions d'un DataFrame, on peut utiliser quelques m√©thodes
pratiques:

```{python}
df.ndim
```

```{python}
df.shape
```

```{python}
df.size
```

Pour d√©terminer le nombre de valeurs uniques d'une variable, plut√¥t que chercher √† √©crire soi-m√™me une fonction,
on utilise la
m√©thode `nunique`. Par exemple,

```{python}
df['Commune'].nunique()
```

`pandas` propose √©norm√©ment de m√©thodes utiles. 
Voici un premier r√©sum√©, accompagn√© d'un comparatif avec `R`

| Op√©ration                     | pandas       | dplyr (`R`)    | data.table (`R`)           |
|-------------------------------|--------------|----------------|----------------------------|
| R√©cup√©rer le nom des colonnes | `df.columns` | `colnames(df)` | `colnames(df)`             |
| R√©cup√©rer les indices[^4]     | `df.index`   |                |`unique(df[,get(key(df))])` |
| R√©cup√©rer les dimensions      | `df.shape` | `c(nrow(df), ncol(df))` | `c(nrow(df), ncol(df))` |
| R√©cup√©rer le nombre de valeurs uniques d'une variable | `df['myvar'].nunique()` | `df %>%  summarise(distinct(myvar))` | `df[,uniqueN(myvar)]` |

::: {.cell .markdown}
[^4]: Le principe d'indice n'existe pas dans `dplyr`. Ce qui s'approche le plus des indices, au sens de
`pandas`, sont les *cl√©s* en `data.table`.
:::

### Statistiques agr√©g√©es

`pandas` propose une s√©rie de m√©thodes pour faire des statistiques
agr√©g√©es de mani√®re efficace.

On peut, par exemple, appliquer des m√©thodes pour compter le nombre de lignes,
faire une moyenne ou une somme de l'ensemble des lignes

```{python}
df.count()
```

```{python}
df.mean()
```


```{python}
df.sum()
```

```{python}
df.nunique()
```

```{python}
df.quantile(q = [0.1,0.25,0.5,0.75,0.9])
```


Il faut toujours regarder les options de ces fonctions en termes de valeurs manquantes, car
ces options sont d√©terminantes dans le r√©sultat obtenu.

Les exercices de TD visent √† d√©montrer l'int√©r√™t de ces m√©thodes dans quelques cas pr√©cis.

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/course/manipulation/02b_pandas_TP.qmd")
```
:::



Le tableau suivant r√©capitule le code √©quivalent pour avoir des
statistiques sur toutes les colonnes d'un dataframe en `R`.


| Op√©ration                     | pandas       | dplyr (`R`)    | data.table (`R`)           |
|-------------------------------|--------------|----------------|----------------------------|
| Nombre de valeurs non manquantes | `df.count()`   | `df %>% summarise_each(funs(sum(!is.na(.))))` | `df[, lapply(.SD, function(x) sum(!is.na(x)))]`
| Moyenne de toutes les variables | `df.mean()` | `df %>% summarise_each(funs(mean((., na.rm = TRUE))))` | `df[,lapply(.SD, function(x) mean(x, na.rm = TRUE))]`| TO BE CONTINUED |

La m√©thode `describe` permet de sortir un tableau de statistiques
agr√©g√©es:

```{python}
df.describe()
```


### M√©thodes relatives aux valeurs manquantes

Les m√©thodes relatives aux valeurs manquantes peuvent √™tre mobilis√©es
en conjonction des m√©thodes de statistiques agr√©g√©es. C'est utiles lorsqu'on
d√©sire obtenir une id√©e de la part de valeurs manquantes dans un jeu de
donn√©es

```{python}
df.isnull().sum()
```

On trouvera aussi la r√©f√©rence √† `isna()` qui est la m√™me m√©thode que `isnull()`.

# Graphiques rapides

Les m√©thodes par d√©faut de graphique
(approfondies dans la [partie visualisation](#visualisation))
sont pratiques pour
produire rapidement un graphique, notamment apr√®s des op√©rations
complexes de maniement de donn√©es.

En effet, on peut appliquer la m√©thode `plot()` directement √† une `pandas.Series`:

```{python}
#| eval: false

df['D√©chets'].plot()
df['D√©chets'].hist()
df['D√©chets'].plot(kind = 'hist', logy = True)
```


```{python}
plt.figure()
fig = df['D√©chets'].plot()
fig
#plt.savefig('plot_base.png', bbox_inches='tight')

plt.figure()
fig = df['D√©chets'].hist()
fig
#plt.savefig('plot_hist.png', bbox_inches='tight')

plt.figure()
fig = df['D√©chets'].plot(kind = 'hist', logy = True)
fig
#plt.show()
#plt.savefig('plot_hist_log.png', bbox_inches='tight')
```


La sortie est un objet `matplotlib`. La *customisation* de ces
figures est ainsi
possible (et m√™me d√©sirable car les graphiques `matplotlib`
sont, par d√©faut, assez rudimentaires), nous en verrons quelques exemples.


# Acc√©der √† des √©l√©ments d'un DataFrame

## S√©lectionner des colonnes

En SQL, effectuer des op√©rations sur les colonnes se fait avec la commande
`SELECT`. Avec `pandas`,
pour acc√©der √† une colonne dans son ensemble on peut
utiliser plusieurs approches:

* `dataframe.variable`, par exemple `df.Energie`.
Cette m√©thode requiert n√©anmoins d'avoir des
noms de colonnes sans espace.
* `dataframe[['variable']]` pour renvoyer la variable sous
forme de `DataFrame` ou `dataframe['variable']` pour
la renvoyer sous forme de `Series`. Par exemple, `df[['Autres transports']]`
ou `df['Autres transports']`. C'est une mani√®re pr√©f√©rable de proc√©der.

## Acc√©der √† des lignes

Pour acc√©der √† une ou plusieurs valeurs d'un `DataFrame`,
il existe deux mani√®res conseill√©es de proc√©der, selon la
forme des indices de lignes ou colonnes utilis√©s:

* `df.loc`: utilise les labels
* `df.iloc`: utilise les indices

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```

Les bouts de code utilisant la structure `df.ix`
sont √† bannir car la fonction est *deprecated* et peut
ainsi dispara√Ætre √† tout moment.

```{=html}
</div>
```
:::

`iloc` va se r√©f√©rer √† l'indexation de 0 √† *N* o√π *N* est √©gal √† `df.shape[0]` d'un
`pandas.DataFrame`. `loc` va se r√©f√©rer aux valeurs de l'index
de `df`.

Par exemple, avec le `pandas.DataFrame` `df_example`:


```{python}
df_example = pd.DataFrame(
    {'year': [2012, 2014, 2013, 2014], 'sale': [55, 40, 84, 31]})
df_example
```

- `df_example.loc[1, :]` donnera la premi√®re ligne de `df` (ligne o√π l'indice `month` est √©gal √† 1) ;
- `df_example.iloc[1, :]` donnera la deuxi√®me ligne (puisque l'indexation en `Python` commence √† 0) ;
- `df_example.iloc[:, 1]` donnera la deuxi√®me colonne, suivant le m√™me principe.




# Principales manipulation de donn√©es

L'objectif du [TP pandas](#pandasTP) est de se familiariser plus avec ces
commandes √† travers l'exemple des donn√©es des √©missions de C02.

Les op√©rations les plus fr√©quentes en SQL `sont` r√©sum√©es par le tableau suivant.
Il est utile de les conna√Ætre (beaucoup de syntaxes de maniement de donn√©es
reprennent ces termes) car, d'une
mani√®re ou d'une autre, elles couvrent la plupart
des usages de manipulation des donn√©es

| Op√©ration | SQL | pandas | dplyr (`R`) | data.table (`R`) |
|-----|-----------|--------|-------------|------------------|
| S√©lectionner des variables par leur nom | `SELECT` | `df[['Autres transports','Energie']]` | `df %>% select(Autres transports, Energie)` | `df[, c('Autres transports','Energie')]` |
| S√©lectionner des observations selon une ou plusieurs conditions; | `FILTER` | `df[df['Agriculture']>2000]` | `df %>% filter(Agriculture>2000)` | `df[Agriculture>2000]` |
| Trier la table selon une ou plusieurs variables | `SORT BY` | `df.sort_values(['Commune','Agriculture'])` | `df %>% arrange(Commune, Agriculture)` | `df[order(Commune, Agriculture)]` |
| Ajouter des variables qui sont fonction d‚Äôautres variables; | `SELECT *, LOG(Agriculture) AS x FROM df` | `df['x'] = np.log(df['Agriculture'])`  |  `df %>% mutate(x = log(Agriculture))` | `df[,x := log(Agriculture)]` |
| Effectuer une op√©ration par groupe | `GROUP BY` | `df.groupby('Commune').mean()` | `df %>% group_by(Commune) %>% summarise(m = mean)` | `df[,mean(Commune), by = Commune]` |
| Joindre deux bases de donn√©es (*inner join*) | `SELECT * FROM table1 INNER JOIN table2 ON table1.id = table2.x` | `table1.merge(table2, left_on = 'id', right_on = 'x')` | `table1 %>% inner_join(table2, by = c('id'='x'))` | `merge(table1, table2, by.x = 'id', by.y = 'x')` |

## Op√©rations sur les colonnes: `select`, `mutate`, `drop`

Les `DataFrames` pandas sont des objets *mutables* en langage `Python`,
c'est-√†-dire qu'il est possible de faire √©voluer le `DataFrame` au gr√®s
des op√©rations. L'op√©ration la plus classique consiste √† ajouter ou retirer
des variables √† la table de donn√©es.

```{python}
df_new = df.copy()
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```

Attention au comportement de `pandas` lorsqu'on cr√©e une duplication
d'un `DataFrame`.

Par d√©faut, `pandas` effectue une copie par r√©f√©rence. Dans ce
cas, les deux objets (la copie et l'objet copi√©) restent reli√©s. Les colonnes
cr√©es sur l'un vont √™tre r√©percut√©es sur l'autre. Ce comportement permet de
limiter l'inflation en m√©moire de `Python`. En faisant √ßa, le deuxi√®me
objet prend le m√™me espace m√©moire que le premier. Le package `data.table`
en `R` adopte le m√™me comportement, contrairement √† `dplyr`.

Cela peut amener √† quelques surprises si ce comportement d'optimisation
n'est pas anticip√©. Si vous voulez, par s√©curit√©, conserver intact le
premier DataFrame, faites appel √† une copie profonde (*deep copy*) en
utilisant la m√©thode `copy`, comme ci-dessus.

Attention toutefois, cela a un co√ªt m√©moire.
Avec des donn√©es volumineuses, c'est une pratique √† utiliser avec pr√©caution.

```{=html}
</div>
```
:::



La mani√®re la plus simple d'op√©rer pour ajouter des colonnes est
d'utiliser la r√©assignation. Par exemple, pour cr√©er une variable
`x` qui est le `log` de la
variable `Agriculture`:

```{python}
df_new['x'] = np.log(df_new['Agriculture'])
```

Il est possible d'appliquer cette approche sur plusieurs colonnes. Un des
int√©r√™ts de cette approche est qu'elle permet de recycler le nom de colonnes.

```{python}
vars = ['Agriculture', 'D√©chets', 'Energie']

df_new[[v + "_log" for v in vars]] = np.log(df_new[vars])
df_new
```

Il est √©galement possible d'utiliser la m√©thode `assign`. Pour des op√©rations
vectoris√©es, comme le sont les op√©rateurs de `numpy`, cela n'a pas d'int√©r√™t.

Cela permet notamment d'enchainer les op√©rations sur un m√™me `DataFrame` (notamment gr√¢ce au `pipe` que
nous verrons plus loin).
Cette approche utilise g√©n√©ralement
des *lambda functions*. Par exemple le code pr√©c√©dent (celui concernant une
seule variable) prendrait la forme:

```{python}
df_new.assign(Energie_log = lambda x: np.log(x['Energie']))
```

Dans les m√©thodes suivantes, il est possible de modifier le `pandas.DataFrame`
*en place*, c'est √† dire en ne le r√©assignant pas, avec le param√®tre `inplace = True`.
Par d√©faut, `inplace` est √©gal √† `False` et pour modifier le `pandas.DataFrame`,
il convient de le r√©assigner.

On peut facilement renommer des variables avec la m√©thode `rename` qui
fonctionne bien avec des dictionnaires (pour renommer des colonnes il faut
pr√©ciser le param√®tre `axis = 1`):

```{python}
df_new = df_new.rename({"Energie": "eneg", "Agriculture": "agr"}, axis=1)
```

Enfin, pour effacer des colonnes, on utilise la m√©thode `drop` avec l'argument
`columns`:

```{python}
df_new = df_new.drop(columns = ["eneg", "agr"])
```

## R√©ordonner

La m√©thode `sort_values` permet de r√©ordonner un `DataFrame`. Par exemple,
si on d√©sire classer par ordre d√©croissant de consommation de CO2 du secteur
r√©sidentiel, on fera

```{python}
df = df.sort_values("R√©sidentiel", ascending = False)
```

Ainsi, en une ligne de code, on identifie les villes o√π le secteur
r√©sidentiel consomme le plus.


## Filtrer

L'op√©ration de s√©lection de lignes s'appelle `FILTER` en SQL. Elle s'utilise
en fonction d'une condition logique (clause `WHERE`). On s√©lectionne les
donn√©es sur une condition logique. Il existe plusieurs m√©thodes en `pandas`.

La plus simple est d'utiliser les *boolean mask*, d√©j√† vus dans le chapitre
[`numpy`](#numpy).

Par exemple, pour s√©lectionner les communes dans les Hauts-de-Seine, on
peut utiliser le r√©sultat de la m√©thode `str.startswith` (qui renvoie
`True` ou `False`) directement dans les crochets:

```{python}
df[df['INSEE commune'].str.startswith("92")].head(2)
```

Pour remplacer des valeurs sp√©cifiques, on utilise la m√©thode `where` ou une
r√©assignation coupl√©e √† la m√©thode pr√©c√©dente.

Par exemple, pour assigner des valeurs manquantes aux d√©partements du 92,
on peut faire cela

```{python}
df_copy = df.copy()
df_copy = df_copy.where(~df['INSEE commune'].str.startswith("92"))
```

et v√©rifier les r√©sultats:

```{python}
df_copy[df['INSEE commune'].str.startswith("92")].head(2)
df_copy[~df['INSEE commune'].str.startswith("92")].head(2)
```

ou alors utiliser une r√©assignation plus classique:

```{python}
df_copy = df.copy()
df_copy[df_copy['INSEE commune'].str.startswith("92")] = np.nan
```

Il est conseill√© de filtrer avec `loc` en utilisant un masque.
En effet, contrairement √† `df[mask]`, `df.loc[mask, :]` permet d'indiquer clairement
√† Python que l'on souhaite appliquer le masque aux labels de l'index.
Ce n'est pas le cas avec `df[mask]`.
D'ailleurs, lorsqu'on utilise la syntaxe `df[mask]`, `pandas` renvoie g√©n√©ralement un *warning*

## Op√©rations par groupe

En `SQL`, il est tr√®s simple de d√©couper des donn√©es pour
effectuer des op√©rations sur des blocs coh√©rents et recollecter des r√©sultats
dans la dimension appropri√©e.
La logique sous-jacente est celle du *split-apply-combine* qui est repris
par les langages de manipulation de donn√©es, auxquels `pandas`
[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).

L'image suivante, issue de
[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/)
repr√©sente bien la mani√®re dont fonctionne l'approche
`split`-`apply`-`combine`

![Split-apply-combine](https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png)


Ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet
est particuli√®rement utile.

Pour donner quelques exemples, on peut cr√©er une variable d√©partementale qui
servira de crit√®re de groupe.

```{python}
df['dep'] = df['INSEE commune'].str[:2]
```

En `pandas`, on utilise `groupby` pour d√©couper les donn√©es selon un ou
plusieurs axes. Techniquement, cette op√©ration consiste √† cr√©er une association
entre des labels (valeurs des variables de groupe) et des
observations.

Par exemple, pour compter le nombre de communes par d√©partement en SQL, on
utiliserait la requ√™te suivante:

```sql
SELECT dep, count(INSEE commune)
FROM df
GROUP BY dep;
```

Ce qui, en `pandas`, donne:

```{python}
df.groupby('dep')["INSEE commune"].count()
```

La syntaxe est quasiment transparente. On peut bien-s√ªr effectuer des op√©rations
par groupe sur plusieurs colonnes. Par exemple,

```{python}
df.groupby('dep').mean()
```

A noter que la variable de groupe, ici `dep`, devient, par d√©faut, l'index
du DataFrame de sortie. Si on avait utilis√© plusieurs variables de groupe,
on obtiendrait un objet multi-index√©. Sur la gestion des `multi-index`, on
pourra se r√©f√©rer √† l'ouvrage `Modern Pandas` dont la r√©f√©rence est
donn√©e en fin de cours.

Tant qu'on n'appelle pas une action sur un `DataFrame` par groupe, du type
`head` ou `display`, `pandas` n'effectue aucune op√©ration. On parle de
*lazy evaluation*. Par exemple, le r√©sultat de `df.groupby('dep')` est
une transformation qui n'est pas encore √©valu√©e:

```{python}
df.groupby('dep')
```

Il est possible d'appliquer plus d'une op√©ration √† la fois gr√¢ce √† la m√©thode
`agg`. Par exemple, pour obtenir √† la fois le minimum, la m√©diane et le maximum
de chaque d√©partement, on peut faire:

```{python}
df.groupby('dep').agg(['min',"median","max"])
```

## Appliquer des fonctions

`pandas` est, comme on a pu le voir, un package tr√®s flexible, qui
propose une grande vari√©t√© de m√©thodes optimis√©es. Cependant, il est fr√©quent
d'avoir besoin de m√©thodes non impl√©ment√©es.

Dans ce cas, on recourt souvent aux `lambda` functions. Par exemple, si
on d√©sire conna√Ætre les communes dont le nom fait plus de 40 caract√®res,
on peut appliquer la fonction `len` de mani√®re it√©rative:

```{python}
# Noms de communes superieurs √† 40 caracteres
df[df['Commune'].apply(lambda s: len(s)>40)]
```


Cependant, toutes les `lambda` functions ne se justifient pas.
Par exemple, prenons
le r√©sultat d'agr√©gation pr√©c√©dent. Imaginons qu'on d√©sire avoir les r√©sultats
en milliers de tonnes. Dans ce cas, le premier r√©flexe est d'utiliser
la `lambda` function suivante:

```{python}
df.groupby('dep').agg(['min',"median","max"]).apply(lambda s: s/1000)
```

En effet, cela effectue le r√©sultat d√©sir√©. Cependant, il y a mieux: utiliser
la m√©thode `div`:

```{python}
#| eval: false
import timeit
%timeit df.groupby('dep').agg(['min',"median","max"]).div(1000)
%timeit df.groupby('dep').agg(['min',"median","max"]).apply(lambda s: s/1000)
```

La m√©thode `div` est en moyenne plus rapide et a un temps d'ex√©cution
moins variable. Dans ce cas, on pourrait m√™me utiliser le principe
du *broadcasting* de numpy (cf. [chapitre numpy](numpy)) qui offre
des performances √©quivalentes:


```{python}
#| eval: false
%timeit df.groupby('dep').agg(['min',"median","max"])/1000
```

`apply` est plus rapide qu'une boucle (en interne, `apply` utilise `Cython`
pour it√©rer) mais reste moins rapide qu'une solution vectoris√©e quand
elle existe. Ce [site](https://realpython.com/fast-flexible-pandas/#pandas-apply)
propose des solutions, par exemple les m√©thodes `isin` ou `digitize`, pour
√©viter de manuellement cr√©er des boucles lentes.

En particulier, il faut noter que `apply` avec le param√®tre `axis=1` est en g√©n√©rale lente.

## Joindre


Il est commun de devoir combiner des donn√©es issues de sources diff√©rentes.
Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation
o√π une information permet d'apparier de mani√®re exacte deux bases de donn√©es (autrement nous
serions dans une situation, beaucoup plus complexe, d'appariement flou[^6]).

[^6]: Sur l'appariement flou, se reporter aux chapitres pr√©sentant `ElasticSearch`. 

La situation typique est l'appariement entre deux sources de donn√©es selon un identifiant
individuel. Ici, il s'agit d'un identifiant de code commune.

Il est recommand√© de lire [ce guide assez complet sur la question des jointures avec R](https://www.book.utilitr.org/jointures.html)
qui donne des recommandations √©galement utiles pour un utilisateur de `Python`.

![](https://pics.me.me/thumb_left-join-right-join-inner-join-full-outer-join-imgflip-com-66845242.png)

On utilise de mani√®re indiff√©rente les termes *merge* ou *join*.
Le deuxi√®me terme provient de la syntaxe SQL.
En `Pandas`, dans la plupart des cas, on peut utiliser indiff√©remment `df.join` et `df.merge`

![](pandas_join.png)

Il est aussi possible de r√©aliser un merge en utilisant la fonction `pandas.concat()` avec `axis=1`.
Se r√©f√©rer √† la documentation de `concat` pour voir les options possibles.

## Reshape

On pr√©sente g√©n√©ralement deux types de donn√©es:

* format __wide__: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu (ou groupe), dans des colonnes diff√©rentes
* format __long__: les donn√©es comportent des observations r√©p√©t√©es, pour un m√™me individu, dans des lignes diff√©rentes avec une colonne permettant de distinguer les niveaux d'observations

Un exemple de la distinction entre les deux peut √™tre emprunt√© √† l'ouvrage de r√©f√©rence d'Hadley Wickham, *R for Data Science*:

![](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png)


L'aide m√©moire suivante aidera √† se rappeler les fonctions √† appliquer si besoin:

![](reshape.png)

Le fait de passer d'un format *wide* au format *long* (ou vice-versa) peut √™tre extr√™mement pratique car
certaines fonctions sont plus ad√©quates sur une forme de donn√©es ou sur l'autre.
En r√®gle g√©n√©rale, avec `Python` comme avec `R`, les formats *long* sont souvent pr√©f√©rables.

Le chapitre suivant, qui fait office de TP, proposera des applications de ces principes

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/course/manipulation/02b_pandas_TP.qmd")
```
:::

## Les pipe

En g√©n√©ral, dans un projet, le nettoyage de donn√©es va consister en un ensemble de
m√©thodes appliqu√©es √† un `pandas.DataFrame`.
On a vu que `assign` permettait de cr√©er une variable dans un `DataFrame`.
Il est √©galement possible d'appliquer une fonction, appel√©e par exemple `my_udf` au
DataFrame gr√¢ce √† `pipe`:

```python
df = (pd.read_csv(path2data)
            .pipe(my_udf))
```

L'utilisation des `pipe` rend le code tr√®s lisible et peut √™tre tr√®s
pratique lorsqu'on enchaine des op√©rations sur le m√™me
_dataset_. 

# Quelques enjeux de performance

La librairie `Dask` int√®gre la structure de `numpy`, `pandas` et `sklearn`.
Elle a vocation √† traiter de donn√©es en grande dimension, ainsi elle ne sera pas
optimale pour des donn√©es qui tiennent tr√®s bien en RAM.
Il s'agit d'une librairie construite sur la parall√©lisation.
[Un chapitre dans ce cours](/dask.html) lui est consacr√©
Pour aller plus loin, se r√©f√©rer √† la [documentation de `Dask`](https://docs.dask.org/en/latest/).


# R√©f√©rences

* Le site
[pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html)
fait office de r√©f√©rence

* Le livre `Modern Pandas` de Tom Augspurger: https://tomaugspurger.github.io/modern-1-intro.html

::: {#refs}
:::

# Annexe {#annexe}

::: {.cell .markdown}
```{=html}
<details><summary>T√©l√©chargement de <code>pynsee</code> avec barre de progr√®s üëá</summary>
```

Ce code vient du [package `pynsee`](https://github.com/InseeFrLab/Py-Insee-Data/blob/master/pynsee/download/__init__.py)...

```{python}
#| eval: false
import warnings
import os
import requests
import zipfile
from pathlib import Path
from shutil import copyfile, copyfileobj

# import tqdm.auto as tqdma
from tqdm import tqdm
from tqdm.utils import CallbackIOWrapper


def download_pb(url: str, fname: str, total: int = None):
    """Useful function to get request with a progress bar
    Borrowed from https://gist.github.com/yanqd0/c13ed29e29432e3cf3e7c38467f42f51
    Arguments:
        url {str} -- URL for the source file
        fname {str} -- Destination where data will be written
    """
    resp = requests.get(url, stream=True)

    if total is None:
        total = int(resp.headers.get('content-length', 0))

    with open(fname, 'wb') as file, tqdm(
            desc='Downloading: ',
            total=total,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
    ) as bar:
        for data in resp.iter_content(chunk_size=1024):
            size = file.write(data)
            bar.update(size)


def unzip_pb(fzip, dest, desc="Extracting"):
    """
    Useful function to unzip with progress bar
    Args:
        fzip: Filename of the zipped file
        dest: Destination where data must be written
        desc: Argument inherited from zipfile.ZipFile
    Returns:
        zipfile.Zipfile(fzip).extractall(dest) with progress
    """

    dest = Path(dest).expanduser()
    Path(dest).mkdir(parents=True, exist_ok=True)

    with zipfile.ZipFile(fzip) as zipf, tqdm(
            desc=desc, unit="B", unit_scale=True, unit_divisor=1024,
            total=sum(getattr(i, "file_size", 0) for i in zipf.infolist()),
    ) as pbar:
        for i in zipf.infolist():
            if not getattr(i, "file_size", 0):  # directory
                zipf.extract(i, os.fspath(dest))
            else:
                with zipf.open(i) as fi, open(os.fspath(dest / i.filename), "wb") as fo:
                    copyfileobj(CallbackIOWrapper(pbar.update, fi), fo)


download_pb('https://github.com/InseeFrLab/Py-Insee-Data/archive/refs/heads/master.zip', 'pynsee.zip')

unzip_pb("pynsee.zip", "pynsee")
```

```{=html}
</details>
```
:::
