---
title: "Intégration continue avec Python"
date: 2020-07-22T12:00:00Z
draft: false
weight: 10
slug: pythonCI
type: book
---

L'un des apports principaux des innovations
récentes de la _data-science_ est la 
manière dont des projets, malgré
leur complexité, peuvent facilement
être converti en projets pérennes
à partir
d'un prototype bien construit. 
En s'inspirant de l'approche `devops` ,
méthode de travail qui consiste à adopter un certain
nombre de gestes pour 
automatiser la production de livrables ou de tests
dès la 
conception du produit, les _data-scientists_
ont adopté une méthode de travail très efficace
pour favoriser la réutilisation de leur travail
par d'autres équipes que celles à l'origine de
la conception du protype initial. 

Cette approche `devops` a été reprise et étendue
pour donner un autre buzz-word, le `MLops`. 
Il s'agit d'une approche qui vise à créer
et mettre à disposition des modèles de machine 
learning de manière fiable et automatisée 
à chaque nouvelle étape du projet, en parallèle
de la mise à jour du code ayant produit ces
output.

Ces nouvelles méthodes de travail permettent
des gains substantiels de productivité
pour les équipes développant des modèles
et réduit fortement le coût de reprise d'un 
code par une équipe en charge de sa
pérenisation. Ce coût est en effet le principal
frein à la mise en production de nouveaux
projets ce qui peut représenter un gâchis
non négligeable de temps et de ressources. 
Comme nous l'expliquons avec Romain Avouac
dans un cours de dernière année de l'ENSAE
(https://ensae-reproductibilite.netlify.app/),
l'adoption de certaines bonnes pratiques
de développement de code et d'une démarche
exploitant les dernières innovations de
la _data-science_ peut substantiellement
augmenter les chances d'un succès
d'un projet. Le nouveau paradigme, qui
consiste à intégrer en amont du projet
certaines contraintes de la production
et tester continuellement la manière dont les
livrables évoluent, évite que la mise
en production d'un projet, qui est coûteuse
en temps et en ressources, n'aboutisse qu'au
moment où le projet est déjà caduc
(car les données ou les besoins ont évolués...).

# L'intégration continue: une opportunité pour les _data-scientists_

On retrouve régulièrement l'acronyme CI/CD
pour illustrer cette
nouvelle méthode de travail dans le
monde du développement logiciel :

- l'intégration continue (CI pour
_continuous integration_)
est une pratique consistant, de manière automatique,
à fréquemment tester les effets d'une modification faite à un code ou à un
document faisant parti d'un projet informatique.

- le déploiement en continu (CD pour _continuous
delivery_) consiste à intégrer de manière automatisée
la production d'un ou plusieurs livrables (environnement
portable, application, site web, etc.) à chaque
modification du code associé à un projet informatique.

Cette pratique permet ainsi de détecter de manière précoce des possibilités
de *bug* ou l'introduction d'un changement non anticipé. Tout comme `Git`,
cette pratique devient un standard dans les domaines collaboratifs.

L'intégration continue permet de sécuriser le travail, puisqu'elle offre un
filet de sécurité (par exemple un test sur une machine à la configuration
arbitraire), mais permet aussi de déployer en temps réel certaines
évolutions. On parle parfois de déploiement en continu, complémentaire de
l'intégration continue. Cette approche réduit ainsi
la muraille de Chine entre un
analyste de données et une équipe de développeurs d'application. Elle offre donc
plus de contrôle, pour le producteur d'une analyse statistique, sur la
valorisation de celle-ci.


Cette approche consiste une excellente opportunité
pour les _data-scientists_ d'être en mesure
de valoriser leurs projets auprès de publics aux
exigences différentes. Pour des développeurs, le
_data-scientist_ pourra fournir une image `Docker`
(environnement portable où l'ensemble des dépendances
et des configurations systèmes pour faire tourner un code
sont contrôlés) permettant à d'autres d'exécuter
facilement le code d'un projet. Pour faciliter
la réutilisation d'un modèle par d'autres _data-scientists_,
il devient de plus en plus fréquent d'exposer
un modèle sous forme d'API: les personnes désirant
réutiliser le modèle peuvent directement l'appliquer
en accédant à une prédiction par le biais d'une API
ce qui évite d'avoir à fournir le jeu d'entraînement
si ce dernier est sensible. Pour toucher
des publics moins 
familiers du code, la mise à disposition de sites web
interactifs valorisant certains résultats d'un projet
peut être intéressante. Cette approche très exigeante
d'utiliser un même projet pour toucher des cibles 
très différentes est grandement facilitée par le
déploiement en continu et la mise à disposition 
de librairies ou d'infrastructures
dédiées dans le monde de l'_open-source_.

Tout en restant éco-responsable (voir partie XXX), cela
permet de mieux valoriser des projets pour réduire
les coûts à le maintenir et le faire évoluer.
Le cours de dernière année de l'ENSAE que je développe
avec Romain Avouac (https://ensae-reproductibilite.netlify.app/)
présente beaucoup plus de détails sur cette question.


# L'intégration continue en pratique

L'intégration continue fonctionne très bien sur `Gitlab` et sur `Github`.
A chaque interaction avec le dépôt distant (`push`), une série d'instruction
définie par l'utilisateur est exécutée. `Python` et `R` s'intègrent très bien dans ce paradigme grâce
à un certain nombre d'images de base (concept sur lequel nous allons revenir)
qui peuvent être customisées pour répondre à une certaine configuration
nécessaire pour exécuter des codes
([voir ici pour quelques éléments sur R](https://linogaliana.gitlab.io/collaboratif/package.html#utiliser-lint%C3%A9gration-continue-de-gitlab).
C'est une méthode idéale pour améliorer la reproductibilité d'un projet: les
instructions exécutées le sont dans un environnement isolé et contrôlé, ce qui
diffère d'une machine personnelle.


# Comment fonctionne l'intégration continue ?

L'intégration continue repose sur le système de la *dockerisation* ou *conteneurisation*.
La technologie sous jacente s'appelle `Docker`.
Il s'agit d'une technologie qui permet la construction
de machines autosuffisantes
(que l'on nomme **containeurs**) répliquant un environnement
contrôlé (que l'on nomme **image**).

On parle de *pipelines* pour désigner une suite de tâches pour partir de 0
(généralement une machine `Linux` à la configuration minimale) et aboutir
à l'issue d'une série d'instructions définies par l'utilisateur.

L'objectif est de trouver une image la plus
parcimonieuse possible, c'est-à-dire à la configuration minimale, qui permet
de faire tourner le code voulu. 
Les [Actions Github](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)
consistuent un modèle sur lequel il est facile
de s'appuyer lorsqu'on a des connaissances limitées
concernant `Docker. 
Il est également très simple de construire son image
de rien, ce qui est la démarche choisie dans
l'autre cours de l'ENSAE que nous donnons avec Romain
Avouac (https://ensae-reproductibilite.netlify.app/).

Quand on utilise un dépôt `Github` <i class="fa-brands fa-github"></i>
ou `Gitlab` <i class="fa-brands fa-gitlab"></i>,
des services automatiques
d'intégration continue peuvent être utilisés:

* `Gitlab CI`: solution pleinement intégrée à un dépôt `Gitlab`. Très généraliste
et permettant des *pipelines* très complexes
([voir l'intégration continue du projet utilitR, une documentation pour R](https://gitlab.com/linogaliana/documentationR/-/blob/master/.gitlab-ci.yml)).
Il est également possible de
l'utiliser avec un dépôt stocké sur `Github`. L'inconvénient de cette approche
est qu'elle est assez lente.
* `Github Actions`: c'est l'alternative (relativement récente) au service d'intégration continue de
Gitlab uniquement basée sur les technologies `Github`. La très forte
dynamique de développement a rendu ce service incontournable.
Un grand nombre de scripts pré-définis et paramétrables
facilitent l'entrée dans le monde de l'intégration
continue.

Historiquement, il existait d'autres services d'intégration continue, notamment
`Travis CI` ou `AppVeyor`[^1]

[^1]: Ces services d'intégration continue étaient utilisés lorsque `Github`
ne proposait pas encore de service intégré, comme le faisait `Gitlab`.
Ils sont de moins en moins fréquemment utilisés.

## Fonctionnement des actions Github

Les actions `Github` fonctionnent par couches successives au sein desquelles
on effectue un certain nombre d'instructions. 
La meilleure manière d'apprendre les actions `Github` est, certes, de [lire la
documentation officielle](https://docs.github.com/en/actions) mais surtout,
à mon avis, de regarder quelques *pipelines* pour comprendre la démarche. 

L'un des intérêts des `Github Actions` est la possibilité d'avoir un *pipeline*
proposant une intrication de langages différents pour avoir une chaine de 
production qui propose les outils les plus efficaces pour répondre à un
objectif en limitant les verrous techniques. 

Par exemple, le *pipeline* de ce cours, disponible
sur `Github` {{< githubrepo >}} propose une intrication des langages
`Python` et `R` avec des technologies `Anaconda` (pour contrôler
l'environnement `Python` comme expliqué dans les chapitres précédents)
et `Javascript` (pour le déploiement d'un site web avec le service tiers
`Netlify`)[^2]. Cette chaîne de production multi-langage permet que
les mêmes fichiers sources génèrent un site web et des notebooks disponibles
sur plusieurs environnements.

```{python class.output = "python"}
#| echo: false
#| code-fold: true
#| code-summary: "Voir l'action Github générant ce site web"

import requests
prod = requests.get("https://raw.githubusercontent.com/linogaliana/python-datascientist/master/.github/workflows/prod.yml")
print(prod.text)
```

[^2]: Pour réduire le temps nécessaire pour construire le site *web*, ce
*pipeline* s'appuie sur un environnement `Docker` construit sur un autre dépôt
disponible également sur `Github`
<a href="https://github.com/linogaliana/python-datascientist-docker/blob/master/.github/workflows/prod.yml" class="github"><i class="fab fa-github"></i></a>.
Celui-ci part d'une configuration système `Linux` et construit un environnement
`Anaconda` à partir d'un fichier `environment.yml` qui liste toutes les dépendances
nécessaires pour exécuter les morceaux de code du site *web*. 
Cet environnement `Anaconda` est construit grâce à l'outil `mamba` qui permet
d'aller beaucoup plus vite dans la constitution d'environnements que ne le
permet `conda`. 

Les couches qui constituent les étapes du *pipeline*
portent ainsi le nom de `steps`. Un *step* peut comporter un certain 
nombre d'instructions ou exécuter des instructions pré-définies.
L'une de ces instructions prédéfinies est, par exemple,
l'[installation de Python](https://github.com/actions/setup-python)
ou l'[initialisation d'un environnement conda](https://github.com/marketplace/actions/setup-miniconda). 
La documentation officielle de `Github` propose un
[fichier qui peut servir de modèle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-nodejs-or-python?langId=py)
pour tester un script `Python` voire l'uploader de manière automatique
sur `Pypi`.

## Intégration continue avec `Python`: tester un notebook

Cette section n'est absolument pas exhaustive. Au contraire, elle ne fournit
qu'un exemple minimal pour expliquer la logique de l'intégration continue. Il
ne s'agit ainsi pas d'une garantie absolue de reproductibilité d'un *notebook*.

`Github` propose une action officielle pour utiliser `Python` dans un
*pipeline* d'intégration continue. Elle est disponible sur le
[MarketPlace Github](https://github.com/marketplace/actions/setup-python).
Il s'agit d'un bon point de départ, à enrichir.

Le fichier qui contrôle les instructions exécutées dans l'environnement `Actions`
doit se trouver dans le dossier `.github/workflows/`
(:warning: ne pas oublier le point au début du
nom du dossier). Il doit être au format `YAML` avec une extension `.yml`
ou `.yaml`.
Il peut avoir n'importe quel nom néanmoins il 
vaut mieux lui donner un nom signifiant,
par exemple `prod.yml` pour un fichier contrôlant une chaîne de production. 

### Lister les dépendances

Avant d'écrire les instructions à exécuter par `Github`, il faut définir un
environnement d'exécution car `Github` ne connaît pas la configuration `Python`
dont vous avez besoin.

Il convient ainsi de lister les dépendances nécessaires dans un fichier
`requirements.txt` (si on utilise un environnement virtuel)
ou un fichier `environment.yml` (si on préfère
utiliser un environnement conda).
Bien que le principe sous-jacent soit légèrement différent,
ces fichiers ont la même fonction: 
permettre la création d'un environnement _ex-nihilo_
avec un certain nombre de dépendances pré-installées[^4].

[^4]: Sur la différence entre les environnements virtuels
et les environnements conda, voir
[cette partie](https://ensae-reproductibilite.netlify.app/portability/#les-environnements-virtuels-) de cours 
plus avancé que nous donnons
avec Romain Avouac sur la mise en production
de projets _data-science_.

Si on fait le choix de l'option `environment.yml`,
le fichier prendra ainsi la forme
suivante, à enrichir en fonction de la
richesse de l'environnement souhaité. :

```yaml
channels:
  - conda-forge

dependencies:
  - python>=3.10
  - jupyter
  - jupytext
  - matplotlib
  - nbconvert
  - numpy
  - pandas
  - scipy
  - seaborn
```

Le même fichier sous le format `requirements.txt` aura
la forme suivante:

```python
jupyter
jupytext
matplotlib
nbconvert
numpy
pandas
scipy
seaborn
```

Sous leur apparente équivalence, au-delà de
la question du formatage, ces fichiers ont
deux différences principales :

- la version minimale de `Python` est définie dans
le fichier `environment.yml` alors qu'elle ne l'est
pas dans un fichier `requirements.txt`. C'est
parce que le second installe les dépendances dans
un environnement déjà existant par ailleurs alors
que le premier peut servir à créer l'environnement
avec une certaine configuration de `Python` ;
- le mode d'installation des _packages_ n'est pas le
même. Avec un `environment.yml` on installera des
packages via conda alors qu'avec un `requirements.txt`
on privilégiera plutôt `pip`[^5].

[^5]: Il est possible d'installer une partie des _packages_
avec `pip` en définissant un champ `pip` dans le
fichier `environment.yml`. Néanmoins, les concepteurs
d'Anaconda recommandent d'être prudent avec cette méthode 
qui présente certes l'avantage d'accélérer le temps de 
création de l'environnement mais peut créer des 
difficultés avec des librairies nécessitant d'autres
langages système comme le `C`. 

Dans le cas de l'environnement `conda`,
le choix du *channel* `conda-forge` vise à contrôler le dépôt utilisé par
`Anaconda`. 

{{% box status="hint" title="Conseil" icon="fa fa-lightbulb" %}}
La `conda forge` est un dépôt de _package_ alternatif
au canal par défaut d'Anaconda qui est maintenu par
l'équipe de développeurs officiels d'Anaconda. 
Comme cette dernière cherche en priorité à 
assurer la stabilité de l'écosystème `Anaconda`, 
les versions de _package_ évoluent moins vite
que le rythme voulu par les développeurs de
_packages_. Pour cette raison, un dépôt
alternatif, où les montées de version sont
plus simples parce qu'elles dépendent des
développeurs de chaque package, a émergé. 
Il s'agit de la `conda forge`. Lorsqu'on
désire utiliser des fonctionalités récentes
de l'écosystème de la _data-science_,
il est conseillé de l'utiliser. 
{{% /box %}}


Ne pas oublier de mettre ce fichier sous contrôle de version et de l'envoyer
sur le dépôt par un `push`.

### Créer un environnement reproductible dans Github Actions

Deux approches sont possibles à ce niveau, selon le degré
de reproductibilité désiré[^6]:

- Créer l'environnement via une action existante. L'action
[`conda-incubator/setup-miniconda@v2`](https://github.com/conda-incubator/setup-miniconda)
est un bon point de départ.
- Créer l'environnement dans une image `Docker`. 

La deuxième solution permet de contrôler de manière
beaucoup plus fine l'environnement dans lequel
`Python` s'éxécutera ainsi que la manière dont
l'environnement sera créé[^7]. Néanmoins, elle nécessite
des connaissances plus poussées dans la principe
de la conteneurisation qui peuvent être coûteuses
à acquérir. Selon l'ambition du projet, notamment
les réutilisation qu'il désire, 
un _data-scientist_ pourra privilégier
telle ou telle option. Les deux solutions sont présentées
dans l'exemple fil-rouge du cours que nous
donnons avec Romain Avouac
(https://ensae-reproductibilite.netlify.app/application/).

[^6]: Le point de vue que nous défendons avec
Romain Avouac dans notre cours sur la reproductibilité
est qu'il s'agit d'un _continuum_ dans lequel on investit
plus ou moins en fonction de ses contraintes, de ses
besoins, de ses compétences, du temps humain qu'on
peut dédier à développer des output reproductibles
et le temps gagné en développant une telle approche.
Selon où on se trouve sur ce cursus, en fonction 
des solutions déjà existantes qu'on peut trouver
sur internet, on va plus ou moins raffiner
notre intégration et nos déploiements
continus. 

[^7]: Par exemple, si l'environnement est de
taille conséquente, cette approche permettra
de privilégier [`mamba`](https://mamba.readthedocs.io/en/latest/),
qui est beaucoup plus
rapide que `conda` pour la création de l'environnement

### Tester un notebook `myfile.ipynb`

Dans cette partie, on va supposer que le *notebook* à tester s'appelle `myfile.ipynb`
et se trouve à la racine du dépôt. Les
dépendances pour l'exécuter sont
listées dans un fichier `requirements.txt`.

Le modèle suivant, expliqué en dessous, fournit un modèle de recette pour
tester un notebook. Supposons que ce fichier soit présent
dans un chemin `.github/workflows/test-notebook.yml`

{{< tabs tabTotal="2" >}}
{{% tab tabName="Environnement virtuel" %}}

```yaml
name: Test notebook execution using Github Actions

on: [push]

jobs:
  build-linux:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
      - shell: bash
      run: |
        python --version
    - name: Install dependencies
      run:
        pip install -r requirements.txt
        pip install jupyter nbconvert
    - name: Test jupyter from command line
      run:
        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb
    - uses: actions/upload-artifact@v3
      with:
        name: Notebook
        path: myfile.ipynb
        retention-days: 5
```

{{% /tab %}}
{{% tab tabName="Environnement conda" %}}

```yaml
name: Test notebook execution using Github Actions

on: [push]

jobs:
  build-linux:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
    - name: Add conda to system path
      run: |
        # $CONDA is an environment variable pointing to the root of the miniconda directory
        echo $CONDA/bin >> $GITHUB_PATH
    - name: Install dependencies
      run: |
        conda env update --file environment.yml --name base
        conda install jupyter nbconvert
    - name: Test jupyter from command line
      run:
        jupyter nbconvert --execute --to notebook --inplace myfile.ipynb
    - uses: actions/upload-artifact@v3
      with:
        name: Notebook
        path: myfile.ipynb
        retention-days: 5
```
{{% /tab %}}
{{< /tabs >}}

Dans les deux cas, la démarche est la même:

- on récupère les fichiers présents dans le dépôt
(action `checkout`) ;
- on installe `Python` ;
- on installe les dépendances pour exécuter le code.
Dans l'approche `conda`, il est également nécessaire
de faire quelques configurations supplémentaires (notamment
ajouter `conda` aux logiciels reconnus par la ligne
de commande) ;
- on teste le notebook en ligne de commande et remplace
celui existant, sur la machine temporaire, par la version
produite sur cet environnement neutre.
- on rend possible le téléchargement du
notebook produit automatiquement pendant 5 jours[^7]. Ceci
repose sur les _artefacts_ qui sont un élément récupéré
des machines temporaires qui n'existent plus dès que le
code a fini d'être exécuté.  

[^7]: Il est recommandé de ne pas garder la période de rétention
des artefacts par défaut car celle-ci est assez longue (90 jours).
Les _output_ pouvant être assez volumineux et expirant rapidement
(en général ce qui nous intéresse est la dernière ou l'avant
dernière version de l'_output), pour des raisons écologiques,
il est recommandé de fixer des périodes courtes. Cela peut être
fait directement dans le fichier configurant l'intégration
continue comme ici ou dans les paramètres par défaut
du dépôt pour que cette règle s'applique à toutes les 
productions faites par intégration continue. 

Ces actions sont exécutées à chaque interaction avec
le dépôt distant (`push`), quelle que soit la 
branche. A partir de ce modèle, il est possible de 
raffiner pour, par exemple, automatiquement
faire un commit du notebook validé et le pusher
via le robot `Github`[^8]

[^8]: Il s'agit de la méthode adaptée par ce cours
pour proposer des notebooks toujours en adéquation
avec le contenu de ce site web. Pour plus de détails,
consulter le fichier
[netlify-test.yaml](https://github.com/linogaliana/python-datascientist/blob/master/.github/workflows/netlify-test.yaml)
sur le dépôt `Github` {{< githubrepo >}} du cours.

## Annexe: la même approche avec `Travis`

`Travis` est un système d'intégration continue
qui était populaire avant l'émergence des
`Github Actions`. La manière équivalente de procéder
sur `Travis` par rapport à celle
ci-dessus serait:

```yaml
# Modèle de fichier .travis.yml
language: python
python:
  - "3.10"

install:
  - sudo apt-get update
  # We do this conditionally because it saves us some downloading if the
  # version is the same.
  - if [[ "$TRAVIS_PYTHON_VERSION" == "2.7" ]]; then
      wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
    else
      wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
    fi
  - bash miniconda.sh -b -p $HOME/miniconda
  - export PATH="$HOME/miniconda/bin:$PATH"
  - hash -r
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  # Useful for debugging any issues with conda
  - conda info -a
  - conda env create -n test-environment python=$TRAVIS_PYTHON_VERSION -f environment.yml
  - source activate test-environment

script:
  - jupytext --to py --execute myfile.ipynb
```


# Références

- https://ensae-reproductibilite.netlify.app/
- https://towardsdatascience.com/from-jupyter-to-kubernetes-refactoring-and-deploying-notebooks-using-open-source-tools-19f99585e923
