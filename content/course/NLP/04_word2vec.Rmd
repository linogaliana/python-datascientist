---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
title: "Prédiction avec Word2Vec et Keras"
date: 2020-10-29T13:00:00Z
draft: false
weight: 40
slug: word2vec
type: book
tags:
  - NLP
  - Littérature
  - Topics Modelling
  - Word2Vec
  - Keras
categories:
  - Tutoriel
summary: |
  La partie précédente nous a amené à explorer les structures latentes
  d'un texte. Le modèle Word2Vec permet maintenant d'explorer le
  word embedding (prolongement de mots).
  Word2Vec est une représentation d'un corpus sous forme vectorielle
  permettant d'entraîner des réseaux de neurone pour
  reconstruire le contexte linguistique des mots et ainsi pouvoir,
  dans un contexte donné, déterminer un certain champ lexical.
---

```{r setup, include=FALSE}
dir_path <- gsub(here::here(), "..", here::here("course","NLP"))
knitr::knit_hooks$set(
  plot = function(x, options) modif_plot(x, options, dir_path = dir_path)
)
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo = FALSE, results = 'asis', include = TRUE, eval = TRUE}
print_badges()
```


Cette page approfondit certains aspects présentés dans la
[partie introductive](#nlp). Après avoir travaillé sur le
*Comte de Monte Cristo*, on va continuer notre exploration de la littérature
avec cette fois des auteurs anglophones:

* Edgar Allan Poe, (EAP) ;
* HP Lovecraft (HPL) ;
* Mary Wollstonecraft Shelley (MWS).

Les données sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent être requétées via l'url 
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`.

Les chapitres précédents permettaient de mieux comprendre les enjeux
du nettoyage de documents textuels et la modélisation de documents
par une approche fréquentiste. On va ici aller plus loin dans la modélisation
en utilisant un réseau de neurone grâce au package `Keras`.


Ce notebook librement inspiré de  : 

* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras
* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg
* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook

## Librairies nécessaires

Cette page évoquera, les principales librairies pour faire du NLP, notamment: 

* [WordCloud](https://github.com/amueller/word_cloud)
* [nltk](https://www.nltk.org/)
* [spacy](https://spacy.io/)
* [Keras](https://keras.io/)
* [TensorFlow](https://www.tensorflow.org/)


{{% panel status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments pour que `NTLK` puisse fonctionner correctement. Pour cela, faire:

~~~python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('genesis')
nltk.download('wordnet')
~~~

Comme nous allons utiliser également `spacy`, il convient de télécharger
le corpus Anglais. Pour cela, on peut se référer à
[la documentation de `spacy`](https://spacy.io/usage/models),
extrèmement bien faite:

- Idéalement, il faut installer le module via la ligne de commande. Dans
une cellule de notebook `Jupyter`, faire:

~~~python
!python -m spacy download en_core_web_sm
~~~

- Sans accès à la ligne de commande (depuis une instance `Docker` par exemple),
faire

```{python, eval = TRUE, results = "hide"}
import spacy
spacy.cli.download("en_core_web_sm")
```

- Sinon, il est également possible d'installer le module en faisant pointer
`pip install` depuis le fichier adéquat sur
[`Github`](https://github.com/explosion/spacy-models). Pour cela, taper

~~~python
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl
~~~

{{% /panel %}}


La liste des modules à importer est assez longue, la voici:

```{python}
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
#from IPython.display import display
import base64
import string
import re
import nltk

from collections import Counter
from time import time
# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords
from sklearn.metrics import log_loss
import matplotlib.pyplot as plt

from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
```

## Prédiction - Modélisation en utilisant Word2vec et Keras

Jusqu'à présent, nous avons utilisé principalement `nltk` pour le 
*preprocessing* de données textuelles. Cette fois, nous proposons
d'utiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de
*pipeline* de *preprocessing*. 



```{python, echo = TRUE}
import spacy
nlp = spacy.load('en_core_web_sm')
```

On va utiliser une fonction standardisée pour nettoyer les champs textuels
avec `spacy`. Idéalement, on pourrait utiliser un `pipe` `spacy`. Mais là
on va privilégier une fonction:

```{python}
punctuations = string.punctuation
from nltk.corpus import stopwords  
stop_words = spacy.lang.en.STOP_WORDS

def preprocess_text(text):
    doc = text
    doc = nlp(doc, disable=['parser', 'ner','tagger'])
    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']
    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]
    tokens = ' '.join(tokens)
    return(tokens)

train['text_clean'] = train['text'].apply(preprocess_text)
```

