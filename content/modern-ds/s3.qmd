---
title: "Les nouveaux modes d'acc√®s aux donn√©es : le format Parquet et les donn√©es sur le cloud"
tags:
  - S3
  - boto3
categories:
  - Tutoriel
  - Avanc√©
abstract: |
  Dans les entreprises et administrations, un nombre croissant 
  d'infrastructure se basent sur des _clouds_, qui sont des sessions non persistentes o√π les donn√©es ne sont pas stock√©es dans les m√™mes serveurs que les machines qui ex√©cutent du code. L'une des technologies dominantes dans le domaine est un syst√®me de stockage nomm√© `S3`, d√©velopp√© par [Amazon](https://docs.aws.amazon.com/fr_fr/AmazonS3/latest/userguide/Welcome.html). 
  
  `Python`, √† travers plusieurs _packages_ (notamment `boto3`, `s3fs` ou `pyarrow`),
  permet d'utiliser ce syst√®me de stockage distant comme si on
  acc√©dait √† des fichiers depuis son poste personnel. Cette r√©volution est 
  √©troitement associ√©e √† l'√©mergence du format de
  donn√©es [`Apache Parquet`](https://parquet.apache.org/), format utilisable en
  `Python` par le biais du package [`pyarrow`](https://arrow.apache.org/docs/python/index.html)
  ou avec [`Spark`](https://spark.apache.org/) et pr√©sentant
  de nombreux avantages pour l'analyse de donn√©es (vitesse d'import, possibilit√© de traiter
  des donn√©es plus volumineuses que la RAM...)
eval: false
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/python_cloud.png
---

{{< badges >}}

Ce chapitre est une introduction √† la question
du stockage des donn√©es et aux innovations 
r√©centes dans ce domaine. L'objectif
est d'abord de pr√©senter les avantages
du format `Parquet` et la mani√®re dont 
on peut utiliser les
librairies [`pyarrow`](https://arrow.apache.org/docs/python/index.html)
ou [`duckdb`](https://duckdb.org/docs/api/python/overview.html) pour traiter
de mani√®re efficace des donn√©es volumineuses
au format `Parquet`. Ensuite, on pr√©sentera
la mani√®re dont ce format `parquet` s'int√®gre
bien avec des syst√®mes de stockage _cloud_,
qui tendent √† devenir la norme dans le monde
de la _data science_. 

# Elements de contexte

## Principe du stockage de la donn√©e

Pour comprendre les apports du format `Parquet`, il est n√©cessaire
de faire un d√©tour pour comprendre la mani√®re dont une information
est stock√©e et accessible √† un langage de traitement de la donn√©e.

Il existe deux approches dans le monde du stockage de la donn√©e. 
La premi√®re est celle de la __base de donn√©es relationnelle__. La seconde est le
principe du __fichier__. 
La diff√©rence entre les deux est dans la mani√®re dont l'acc√®s aux
donn√©es est organis√©. 

## Les fichiers

Dans un fichier, les donn√©es sont organis√©es selon un certain format et
le logiciel de traitement de la donn√©e va aller chercher et structurer
l'information en fonction de ce format. Par exemple, dans un fichier 
`.csv`, les diff√©rentes informations seront stock√©es au m√™me niveau
avec un caract√®re pour les s√©parer (la virgule `,` dans les `.csv` anglosaxons, le point virgule dans les `.csv` fran√ßais, la tabulation dans les `.tsv`). Le fichier suivant

```raw
nom ; profession 
Ast√©rix ; 
Ob√©lix ; Tailleur de menhir ;
Assurancetourix ; Barde
```

sera ainsi organis√© naturellement sous forme tabul√©e par `Python`

```{python}
#| echo: false
#| eval: true

import pandas as pd
from io import StringIO
pd.read_csv(
    StringIO(
        """
        nom ; profession
        Ast√©rix ; 
        Ob√©lix ; Tailleur de menhir
        Assurancetourix ; Barde
        """
    ),
    sep = ";"
)
```

A propos des fichiers de ce type, on parle de __fichiers plats__ car
les enregistrements relatifs √† une observation sont stock√©s ensemble,
sans hi√©rarchie.  

Certains formats de donn√©es vont permettre d'organiser les informations
de mani√®re diff√©rente. Par exemple, le format `JSON` va
hi√©rarchiser diff√©remment la m√™me information [^1]:

```raw
[
  {
    "nom": "Ast√©rix"
  },
  {
    "nom": "Ob√©lix",
    "profession": "Tailleur de menhir"
  },
  {
    "nom": "Assurancetourix",
    "profession": "Barde"
  }
]
```

::: {.caution}

La diff√©rence entre le CSV et le format `JSON` va au-del√† d'un simple "formattage" des donn√©es.

Par sa nature non tabulaire, le format JSON permet des mises √† jour beaucoup plus facile de la donn√©e dans les entrep√¥ts de donn√©es.

Par exemple, un site web qui collecte de nouvelles donn√©es n'aura pas √† mettre √† jour l'ensemble de ses enregistrements ant√©rieurs
pour stocker la nouvelle donn√©e (par exemple pour indiquer que pour tel ou tel client cette donn√©e n'a pas √©t√© collect√©e)
mais pourra la stocker dans
un nouvel item. Ce sera √† l'outil de requ√™te (`Python` ou un autre outil)
de cr√©er une relation entre les enregistrements stock√©s √† des endroits
diff√©rents.

Ce type d'approche flexible est l'un des fondements de l'approche `NoSQL`,
sur laquelle nous allons revenir, qui a permis l'√©mergence de technologies au coeur de l'√©cosyst√®me actuel du _big-data_ comme `Hadoop` ou `ElasticSearch`. 

:::


Cette fois, quand on n'a pas d'information, on ne se retrouve pas avec nos deux s√©parateurs accol√©s (cf. la ligne _"Ast√©rix"_) mais l'information
n'est tout simplement pas collect√©e. 

::: {.note}

Il se peut tr√®s bien que l'information sur une observation soit diss√©min√©e
dans plusieurs fichiers dont les formats diff√®rent.

Par exemple, dans le domaine des donn√©es g√©ographiques,
lorsqu'une donn√©e est disponible sous format de fichier(s), elle peut l'√™tre de deux mani√®res!

- Soit la donn√©e est stock√©e dans un seul fichier qui m√©lange contours g√©ographiques et valeurs attributaires
(la valeur associ√©e √† cette observation g√©ographique, par exemple le taux d'abstention). Ce principe est celui du `geojson`.
- Soit la donn√©e est stock√©e dans plusieurs fichiers qui sont sp√©cialis√©s : un fichier va stocker les contours g√©ographiques,
l'autre les donn√©es attributaires et d'autres fichiers des informations annexes (comme le syst√®me de projection). Ce principe est celui du `shapefile`.
C'est alors le logiciel qui requ√™te
les donn√©es (`Python` par exemple) qui saura o√π aller chercher l'information
dans les diff√©rents fichiers et associer celle-ci de mani√®re coh√©rente.

:::



Un concept suppl√©mentaire dans le monde du fichier est celui du __file system__. Le  _file system_ est
le syst√®me de localisation et de nommage des fichiers. 
Pour simplifier, le _file system_ est la mani√®re dont votre ordinateur saura
retrouver, dans son syst√®me de stockage, les bits pr√©sents dans tel ou tel fichier
appartenant √† tel ou tel dossier.  

## Les bases de donn√©es


La logique des bases de donn√©es est diff√©rente. Elle est plus syst√©mique. 
Un syst√®me de gestion de base de donn√©es (_Database Management System_)
est un logiciel qui g√®re √† la fois le stockage d'un ensemble de donn√©es reli√©e,
permet de mettre √† jour celle-ci (ajout ou suppression d'informations, modification
des caract√©ristiques d'une table...)
et qui g√®re √©galement
les modalit√©s d'acc√®s √† la donn√©e (type de requ√™te, utilisateurs
ayant les droits en lecture ou en √©criture...). 

La relation entre les entit√©s pr√©sentes dans une base de donn√©es
prend g√©n√©ralement la forme d'un __sch√©ma en √©toile__. Une base va centraliser
les informations disponibles qui seront ensuite d√©taill√©es dans des tables
d√©di√©es. 

![](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png)
Source: [La documentation `Databricks` sur le sch√©ma en √©toile](https://www.databricks.com/fr/glossary/star-schema)

Le logiciel associ√© √† la base de donn√©es fera ensuite le lien
entre ces tables √† partir de requ√™tes `SQL`. L'un des logiciels les plus efficaces dans ce domaine
est [`PostgreSQL`](https://www.postgresql.org/). `Python` est tout √† fait
utilisable pour passer une requ√™te SQL √† un gestionnaire de base de donn√©es. 
Les packages [`sqlalchemy`](https://www.sqlalchemy.org/) et [`psycopg2`](https://www.psycopg.org/docs/)
peuvent servir √† utiliser `PostgreSQL` pour requ√™ter une
base de donn√©e ou la mettre √† jour. 

La logique de la base de donn√©es est donc tr√®s diff√©rente de celle du fichier.
Ces derniers sont beaucoup plus l√©gers pour plusieurs raisons. 
D'abord, parce qu'ils sont moins adh√©rents √† 
un logiciel gestionnaire. L√† o√π le fichier ne n√©cessite, pour la gestion,
qu'un _file system_, install√© par d√©faut sur
tout syst√®me d'exploitation, une base de donn√©es va n√©cessiter un
logiciel sp√©cialis√©. L'inconv√©nient de l'approche fichier, sous sa forme
standard, est qu'elle
ne permet pas une gestion fine des droits d'acc√®s et am√®ne g√©n√©ralement √† une 
duplication de la donn√©e pour √©viter que la source initiale soit
r√©-√©crite (involontairement ou de mani√®re intentionnelle par un utilisateur malveillant).
R√©soudre ce probl√®me est l'une des
innovations des syst√®mes _cloud_, sur lesquelles nous reviendrons en √©voquant le
syst√®me `S3`.
Un deuxi√®me inconv√©nient de l'approche base de donn√©es par
rapport √† l'approche fichier, pour un utilisateur de `Python`,
est que les premiers n√©cessitent l'interm√©diation du logiciel de gestion 
de base de donn√©es l√† o√π, dans le second cas, on va se contenter d'une
librairie, donc un syst√®me beaucoup plus l√©ger, 
qui sait comment transformer la donn√©e brute en `DataFrame`. 
Pour ces raisons, entre autres, les bases de donn√©es sont donc moins √† la 
mode dans l'√©cosyst√®me r√©cent de la _data science_ que les fichiers.

# Le format `Parquet`


Le format `CSV` a rencontr√© un grand succ√®s par sa simplicit√© : il 
est lisible par un humain (un bloc-note suffit pour l'ouvrir et
apercevoir les premi√®res lignes), sa nature plate lui permet
de bien correspondre au concept de donn√©es tabul√©es sans hi√©rarchie 
qui peuvent √™tre rapidement valoris√©es, il est universel (il n'est
pas adh√©rent √† un logiciel). Cependant, le CSV pr√©sente
plusieurs inconv√©nients qui justifient l'√©mergence d'un format
concurrent :

- le CSV est un format __lourd__ car les informations ne sont pas compress√©es 
(ce qui le rend lisible facilement depuis un bloc-note) mais aussi
parce que toutes les donn√©es sont stock√©es de la m√™me mani√®re.
C'est la
librairie faisant l'import qui va essayer d'optimiser le typage des donn√©es
pour trouver le typage qui utilise le moins de m√©moire possible sans
alt√©ration de l'information. En effet, si `pandas` d√©termine qu'une colonne
pr√©sente les valeurs `6 ; 5 ; 0`, il va privil√©gier l'utilisation du type
`int` au type `double` qui sera lui m√™me pr√©f√©r√© au type `object` (objets
de type donn√©es textuelles). Cependant, pour faire cela, `pandas` va devoir
scanner un nombre suffisant de valeurs, ce qui demande du temps et expose 
√† des erreurs (en se fondant sur trop peu de valeurs, on peut se tromper
de typage) ;
- le stockage √©tant __orient√© ligne__, 
acc√©der √† une information donn√©e dans un `CSV` implique
de le lire le fichier en entier, s√©lectionner la ou les colonnes
d'int√©r√™t et ensuite les lignes d√©sir√©es. Par exemple, si on d√©sire
conna√Ætre uniquement la profession de la deuxi√®me ligne dans l'exemple
plus haut :point_up:, un algorithme de recherche devra:
prendre le fichier, d√©terminer que la profession est la deuxi√®me colonne,
et ensuite aller chercher la deuxi√®me ligne dans cette colonne. Si
on d√©sire acc√©der √† un sous-ensemble de lignes dont les indices
sont connus, le `CSV` est int√©ressant. Cependant,
si on d√©sire acc√©der √† un sous-ensemble
de colonnes dans un fichier (ce qui est un cas d'usage plus fr√©quent
pour les _data scientists_), alors le `CSV` n'est pas le format le plus
appropri√© ;
- mettre √† jour la donn√©e est co√ªteux car cela implique de r√©√©crire
l'ensemble du fichier. Par exemple, si apr√®s une premi√®re
analyse de la donn√©e,
on d√©sire ajouter une colonne, on ne peut accoler ces nouvelles informations
√† celles d√©j√† existantes, il est n√©cessaire de r√©√©crire l'ensemble 
du fichier. Pour reprendre l'exemple de nos gaulois pr√©f√©r√©s, si on veut
ajouter une colonne `cheveux` entre les deux d√©j√† existantes,
il faudra changer totalement le fichier: 

```raw
"""
nom ; cheveux ; profession
Ast√©rix; blond; ; 
Ob√©lix; roux; Tailleur de menhir
Assurancetourix; blond; Barde
"""
```

::: {.note}

La plupart des logiciels d'analyse de donn√©es proposent 
un format de fichier pour sauvegarder des bases de donn√©es. On
peut citer le `.pickle` (`Python`), le `.rda` ou `.RData` (`R`),
le `.dta` (`Stata`) ou le `.sas7bdat` (`SAS`). L'utilisation
de ces formats est probl√©matique car cela revient √† se lier
les mains pour l'analyse ult√©rieure des donn√©es, surtout
lorsqu'il s'agit d'un format propri√©taire (comme avec
`SAS` ou `Stata`). 

Par exemple, `Python` ne
sait pas nativement lire un `.sas7bdat`. Il existe des librairies
pour le faire (notamment `Pandas`) mais le format
√©tant propri√©taire, les d√©veloppeurs de la librairie ont d√ª t√¢tonner et
on n'est ainsi jamais assur√© qu'il n'y ait pas d'alt√©ration de la donn√©e. 

Malgr√© tous les inconv√©nients du `.csv` list√©s plus haut, il pr√©sente 
l'immense avantage, par rapport √† ces formats, de l'universalit√©. 
Il vaut ainsi mieux privil√©gier un `.csv` √† ces formats pour le stockage
de la donn√©e. Ceci dit, comme vise √† le montrer ce chapitre, il vaut
mieux privil√©gier le format `parquet` au `CSV`.

:::


Pour r√©pondre √† ces limites du `CSV`, le format `parquet`, 
qui est un [projet open-source `Apache`](https://apache.org/), a √©merg√©.
La premi√®re diff√©rence entre le format `parquet` et le `CSV` est
que le premier repose sur un __stockage orient√© colonne__ l√† o√π
le second est orient√© ligne. Pour comprendre la diff√©rence, voici un
exemple issu du [blog d'upsolver](https://www.upsolver.com/blog/apache-parquet-why-use): 

![](https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.52.58.png)

Dans notre exemple pr√©c√©dent, cela donnera une information prenant 
la forme suivante (ignorez l'√©l√©ment `pyarrow.Table`, nous
reviendrons dessus) :

```{python}
#| echo: false
#| eval: true
from pyarrow import csv
import io

s = """
nom ;profession
Ast√©rix ;
Ob√©lix ;Tailleur de menhir
Assurancetourix ;Barde
"""

source = io.BytesIO(s.encode())

df = csv.read_csv(source, parse_options = csv.ParseOptions(delimiter=";"))
df
```

Pour reprendre l'exemple fil rouge :point_up:, il sera ainsi beaucoup plus
facile de r√©cup√©rer la deuxi√®me ligne de la colonne `profession`:
on ne consid√®re que le vecteur `profession` et on r√©cup√®re la deuxi√®me
valeur. 
Le requ√™tage d'√©chantillon de donn√©es ne n√©cessite donc pas l'import de 
l'ensemble des donn√©es. A cela s'ajoute des fonctionnalit√©s suppl√©mentaires
des librairies d'import de donn√©es parquet (par exemple `pyarrow` ou `spark`)
qui vont faciliter des recherches complexes bas√©es, par exemple, sur des
requ√™tes de type `SQL`, ou permettant l'utilisation de donn√©es plus volumineuses que la RAM. 

Le format `parquet` pr√©sente d'autres avantages par rapport au
`CSV`: 

- Le format `parquet` est (tr√®s) compress√©, ce qui r√©duit la volum√©trie
des donn√©es sur disque ;
- Des m√©tadonn√©es, notamment le typage des variables, sont stock√©es en compl√©ment dans le fichier.
Cette partie, nomm√©e le _footer_ du fichier `parquet`, permet que l'import des donn√©es soit 
optimis√© sans risque d'alt√©ration de celle-ci. Pour un producteur de donn√©es, c'est une mani√®re
d'assurer la qualit√© des donn√©es. Par exemple, un fournisseur de
donn√©es de type code-barre sera
certain que les donn√©es `000012` ne seront pas consid√©r√©es identiques √† un code-barre `12`.
- Il est possible de partitionner un jeu de donn√©es en fonction de diff√©rents niveaux (par
exemple des niveaux g√©ographiques) en une arborescence de fichiers `parquet`. Cela
permet de travailler sur un √©chantillon pour facilement passer √† l'√©chelle ensuite.
Par exemple, une structure partitionn√©e, emprunt√©e
√† la [documentation `Spark`](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery)
peut prendre la forme suivante :

```raw
path
‚îî‚îÄ‚îÄ to
    ‚îî‚îÄ‚îÄ table
        ‚îú‚îÄ‚îÄ gender=male
        ‚îÇ   ‚îú‚îÄ‚îÄ ...
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ country=US
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
        ‚îÇ   ‚îú‚îÄ‚îÄ country=CN
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îî‚îÄ‚îÄ gender=female
            ‚îú‚îÄ‚îÄ ...
            ‚îÇ
            ‚îú‚îÄ‚îÄ country=US
            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
            ‚îú‚îÄ‚îÄ country=CN
            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
            ‚îî‚îÄ‚îÄ ...
```

Qu'on lise un ou plusieurs fichiers, on finira avec le sch√©ma suivant :

```raw
root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
```

Ces diff√©rents avantages expliquent le succ√®s du format `parquet` dans le monde du
_big-data_. Le paragraphe suivant, extrait du [post d'upsolver]() d√©j√† cit√©, 
r√©sume bien l'int√©r√™t:

> Complex data such as logs and event streams would need to be represented as a table with hundreds or thousands of columns, and many millions of rows. Storing this table in a row based format such as CSV would mean:
> 
> - Queries will take longer to run since more data needs to be scanned, rather than only querying the subset of columns we need to answer a query (which typically requires aggregating based on dimension or category)
> - Storage will be more costly since CSVs are not compressed as efficiently as Parquet

Cependant, **`Parquet` ne devrait pas int√©resser que les producteurs ou utilisateurs de donn√©es _big-data_**.
C'est l'ensemble
des producteurs de donn√©es qui b√©n√©ficient des fonctionalit√©s
de `Parquet`. 

Pour en savoir plus sur `Arrow`,
des √©l√©ments suppl√©mentaires sur `Parquet` sont disponibles sur ce tr√®s bon
post de blog d'[upsolver](https://www.upsolver.com/blog/apache-parquet-why-use)
et [sur la page officielle du projet `Parquet`](https://parquet.apache.org/).

## Lire un `parquet` en `Python`: la librairie `pyarrow`

La librairie `pyarrow` permet la lecture et l'√©criture
de fichiers `parquet` avec `Python`[^3]. Elle repose
sur un type particulier de _dataframe_, le `pyarrow.Table`
qui peut √™tre utilis√© en substitut ou en compl√©ment
du `DataFrame` 
de `pandas`. Il est recommand√© de r√©guli√®rement
consulter la documentation officielle de `pyarrow` 
concernant [la lecture et √©criture de fichiers](https://arrow.apache.org/docs/python/parquet.html) et celle relative
aux [manipulations de donn√©es](https://arrow.apache.org/cookbook/py/data.html).

[^3]: Elle permet aussi la lecture et l'√©criture 
de `.csv`.

Pour illustrer les fonctionalit√©s de `pyarrow`,
repartons de notre CSV initial que nous allons
enrichir d'une nouvelle variable num√©rique
et que nous
allons 
convertir en objet `pyarrow` avant de l'√©crire au format `parquet`:

```{python}
#| eval: true
import pandas as pd
from io import StringIO 
import pyarrow as pa
import pyarrow.parquet as pq

s = """
nom;cheveux;profession
Ast√©rix;blond;
Ob√©lix;roux;Tailleur de menhir
Assurancetourix;blond;Barde
"""

source = StringIO(s)

df = pd.read_csv(source, sep = ";", index_col=False)
df["taille"] = [155, 190, 175]
table = pa.Table.from_pandas(df)

table

pq.write_table(table, 'example.parquet')
```


::: {.tip}
L'utilisation des noms `pa` pour `pyarrow` et `pq` pour
`pyarrow.parquet` est une convention communautaire
qu'il est recommand√© de suivre.

:::


Pour importer et traiter ces donn√©es, on peut conserver
les donn√©es sous le format `pyarrow.Table`
ou transformer en `pandas.DataFrame`. La deuxi√®me
option est plus lente mais pr√©sente l'avantage
de permettre ensuite d'appliquer toutes les
manipulations offertes par l'√©cosyst√®me
`pandas` qui est g√©n√©ralement mieux connu que
celui d'`Arrow`. 

Supposons qu'on ne s'int√©resse qu'√† la taille et √† la couleur
de cheveux de nos gaulois. 
Il n'est pas n√©cessaire d'importer l'ensemble de la base, cela
ferait perdre du temps pour rien. On appelle
cette approche le __`column pruning`__ qui consiste √† 
ne parcourir, dans le fichier, que les colonnes qui nous
int√©ressent. Du fait du stockage orient√© colonne du `parquet`,
il suffit de ne consid√©rer que les blocs qui nous 
int√©ressent (alors qu'avec un CSV il faudrait scanner tout
le fichier avant de pouvoir √©liminer certaines colonnes).
Ce principe du `column pruning` se mat√©rialise avec
l'argument `columns` dans `parquet`.

Ensuite, avec `pyarrow`, on pourra utiliser `pyarrow.compute` pour
effectuer des op√©rations directement sur une table
`Arrow` :

```{python}
#| eval: false
import pyarrow.compute as pc

table = pq.read_table('example.parquet', columns=['taille', 'cheveux'])

(
  table
  .group_by("cheveux")
  .aggregate([("taille", "mean")])
)
```

La mani√®re √©quivalente de proc√©der en passant
par l'interm√©diaire de `pandas` est

```{python}
#| eval: true
table = pq.read_table('example.parquet', columns=['taille', 'cheveux'])

(
  table
  .to_pandas()
  .groupby("cheveux")["taille"]
  .mean()
)
```

Ici, comme les donn√©es sont peu volumineuses, deux des
avantages du `parquet` par rapport
au `CSV` (donn√©es moins
volumineuses et vitesse de l'import)
ne s'appliquent pas vraiment. 


::: {.note}

Un autre principe d'optimisation de la performance qui est
au coeur de la librairie `Arrow` est le `filter pushdown`
(ou `predicate pushdown`). 

Quand on ex√©cute un filtre de s√©lection de ligne 
juste apr√®s avoir charg√© un jeu de donn√©es,
`Arrow` va essayer de le mettre en oeuvre lors de l'√©tape de lecture
et non apr√®s. Autrement dit, `Arrow` va modifier le plan 
d'ex√©cution pour pousser le filtre en amont de la s√©quence d'ex√©cution
afin de ne pas essayer de lire les lignes inutiles. 

:::






# Les donn√©es sur le _cloud_

Si les fichiers `parquet` sont une 
solution avantageuse pour 
les _data scientists_, ils ne r√©solvent
pas tous les inconv√©nients de
l'approche fichier. 
En particulier, la question de la
duplication des donn√©es pour la mise
√† disposition s√©curis√©e des sources
n'est pas r√©solue. Pour que 
l'utilisateur `B` n'alt√®re pas les
donn√©es de l'utilisateur `A`, il est n√©cessaire
qu'ils travaillent sur deux fichiers
diff√©rents, dont l'un peut √™tre une copie
de l'autre. 

La mise √† disposition de donn√©es dans
les syst√®mes de stockage _cloud_ est
une r√©ponse √† ce probl√®me.
Les _data lake_ qui se sont d√©velopp√©s dans les 
institutions et entreprises utilisatrices de donn√©es


Le principe d'un stockage cloud
est le m√™me que celui d'une
`Dropbox` ou d'un `Drive` mais adapt√© √†
l'analyse de donn√©es. Un utilisateur de donn√©es
acc√®de √† un fichier stock√© sur un serveur distant
_comme s'il_ √©tait dans son _file system_ local[^4]. 
Donc, du point de vue de l'utilisateur `Python`, 
il n'y a pas de diff√©rence fondamentale. Cependant,
les donn√©es ne sont pas heberg√©es dans un dossier
local (par exemple `Mes Documents/monsuperfichier`)
mais sur un serveur distant auquel l'utilisateur
de `Python` acc√®de √† travers un √©change r√©seau.

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/featured.png)

Dans l'univers du _cloud_, la hi√©rarchisation des donn√©es
dans des dossiers et des fichiers bien rang√©s 
est d'ailleurs moins
importante que dans le monde du _file system_ local. 
Lorsque vous essayez de retrouver un fichier dans
votre arborescence de fichiers, vous utilisez parfois
la barre de recherche de votre explorateur de fichiers,
avec des r√©sultats mitig√©s[^4]. Dans le monde du _cloud_,
les fichiers sont parfois accumul√©s de mani√®re plus 
chaotique car les outils de recherche sont plus 
efficaces[^4]. 

[^4]: D'ailleurs, les g√©n√©rations n'ayant connu nativement
que ce type de stockage ne sont pas familiaris√©es
au concept de _file system_ et pr√©f√®rent 
payer le temps de recherche. Voir
[cet article](https://futurism.com/the-byte/gen-z-kids-file-systems)
sur le sujet. 

En ce qui concerne la s√©curit√© des donn√©es,
la gestion des droits de lecture et √©criture peut √™tre
fine: on peut autoriser certains utilisateurs uniquement 
√† la lecture, d'autres peuvent avoir les droits
d'√©criture pour modifier les donn√©es. Cela permet
de concilier les avantages des bases de donn√©es (la s√©curisation
des donn√©es) avec ceux des fichiers. 

## Qu'est-ce que le syst√®me de stockage `S3` ?

Dans les entreprises et administrations,
un nombre croissant de donn√©es sont
disponibles depuis un syst√®me de stockage
nomm√© `S3`. 
Le syst√®me `S3` (*Simple Storage System*) est un syst√®me de stockage d√©velopp√©
par Amazon et qui est maintenant devenu une r√©f√©rence pour le stockage en ligne.
Il s'agit d'une architecture √† la fois
s√©curis√©e (donn√©es crypt√©es, acc√®s restreints) et performante.

Le concept central du syst√®me S3 est le __*bucket*__.
Un *bucket* est un espace (priv√© ou partag√©) o√π on peut stocker une
arborescence de fichiers. Pour acc√©der aux fichiers figurant
dans un *bucket* priv√©, il faut des jetons d'acc√®s (l'√©quivalent d'un mot de passe)
reconnus par le serveur de stockage. On peut alors lire et √©crire dans le *bucket*.


::: {.note}
Les exemples suivants seront r√©plicables pour les utilisateurs de la plateforme
SSP Cloud

{{< badges >}}

Ils peuvent √©galement l'√™tre pour des utilisateurs ayant un 
acc√®s √† AWS, il suffit de changer l'URL du `endpoint` 
pr√©sent√© ci-dessous. 

:::


## Comment faire avec Python ?

### Les librairies principales

L'interaction entre ce syst√®me distant de fichiers et une session locale de Python
est possible gr√¢ce √† des API. Les deux principales librairies sont les suivantes :

* [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), une librairie cr√©√©e et maintenue par Amazon ;
* [s3fs](https://s3fs.readthedocs.io/en/latest/), une librairie qui permet d'interagir avec les fichiers stock√©s √† l'instar d'un filesystem classique.

La librairie `pyarrow` que nous avons d√©j√† pr√©sent√©e permet √©galement
de traiter des donn√©es stock√©es sur le _cloud_ comme si elles
√©taient sur le serveur local. C'est extr√™mement pratique 
et permet de fiabiliser la lecture ou l'√©criture de fichiers
dans une architecture _cloud_. 
Un exemple, assez court, est disponible 
[dans la documentation officielle](https://arrow.apache.org/docs/python/filesystems.html#s3)

Il existe √©galement d'autres librairies permettant de g√©rer
des _pipelines_ de donn√©es (chapitre √† venir) de mani√®re
quasi indiff√©rente entre une architecture locale et une architecture
_cloud_. Parmi celles-ci, nous pr√©senterons quelques exemples 
avec `snakemake`. 
En arri√®re-plan, `snakemake`
va utiliser `boto3` pour communiquer avec le syst√®me
de stockage.


Enfin, selon le m√™me principe du _comme si_ les donn√©es
√©taient en local, il existe l'outil en ligne de commande
`mc` ([`Minio Client`](https://docs.min.io/docs/minio-client-complete-guide.html)) qui permet de g√©rer par des lignes
de commande Linux les d√©p√¥ts distants comme s'ils √©taient
locaux. 

Toutes ces librairies offrent la possibilit√© de se connecter depuis `Python`,
√† un d√©p√¥t de fichiers distant, de lister les fichiers disponibles dans un
*bucket*, d'en t√©l√©charger un ou plusieurs ou de faire de l'*upload*
Nous allons pr√©senter quelques-unes des op√©rations les plus fr√©quentes,
en mode _cheatsheet_. 




### Connexion √† un bucket

Par la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent
√† s'authentifier. 

```python
key_id = 'MY_KEY_ID'
access_key = 'MY_ACCESS_KEY'
token = "MY_TOKEN"
```

Ces valeurs peuvent √™tre √©galement disponibles dans 
les variables d'environnement de `Python`. Comme il s'agit d'une information
d'authentification personnelle, il ne faut pas stocker les vraies valeurs de ces
variables dans un projet, sous peine de partager des traits d'identit√© sans le
vouloir lors d'un partage de code. 

::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Avec `boto3`, on cr√©√© d'abord un client puis on ex√©cute des requ√™tes dessus.
Pour initialiser un client, il suffit, en supposant que l'url du d√©p√¥t S3 est
`"https://minio.lab.sspcloud.fr"`, de faire:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

La logique est identique avec `s3fs`. 

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```


La logique d'`Arrow` est proche de celle de `s3fs`. Seuls les noms
d'arguments changent

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override="http://"+"minio.lab.sspcloud.fr")
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

La logique de `Snakemake` est, quant √† elle,
plus proche de celle de `boto3`. Seuls les noms
d'arguments changent

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
```

```{=html}
</details>
```
:::




Il se peut que la connexion √† ce stade soit refus√©e (`HTTP error 403`).
Cela peut provenir 
d'une erreur dans l'URL utilis√©. Cependant, cela refl√®te plus g√©n√©ralement
des param√®tres d'authentification erron√©s.



::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Les param√®tres d'authentification sont des arguments suppl√©mentaires:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr",
                  aws_access_key_id=key_id, 
                  aws_secret_access_key=access_key, 
                  aws_session_token = token)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

La logique est la m√™me, seuls les noms d'arguments diff√®rent

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},
  key = key_id, secret = access_key,
  token = token)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

Tout est en argument cette fois:

```python
from pyarrow import fs

s3 = fs.S3FileSystem(
    access_key = key_id,
    secret_key = access_key,
    session_token = token,
    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',
    scheme = "https"
    )
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

La logique est la m√™me, seuls les noms d'arguments diff√®rent

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)
```

```{=html}
</details>
```
:::




::: {.note}

Sur le SSP Cloud, les jetons d'acc√®s au stockage S3 sont inject√©s automatiquement dans les services lors de leur cr√©ation. Ils sont ensuite valides pour une dur√©e de 7 jours. Si l'ic√¥ne du service passe du vert au rouge, cela signifie que ces jetons sont p√©rim√©s, il faut donc sauvegarder son code / ses donn√©es et reprendre depuis un nouveau service.

:::


### Lister les fichiers

S'il n'y a pas d'erreur √† ce stade, c'est que la connexion est bien effective.
Pour le v√©rifier, on peut essayer de faire la liste des fichiers disponibles
dans un `bucket` auquel on d√©sire acc√©der.

Par exemple, on peut vouloir
tester l'acc√®s aux bases `FILOSOFI` (donn√©es de revenu localis√©es disponibles
sur <https://www.insee.fr>) au sein du bucket `donnees-insee`. 



::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Pour cela,
la m√©thode `list_objects` offre toutes les options n√©cessaires:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
for key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:
    print(key['Key'])
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

Pour lister les fichiers, c'est la m√©thode `ls` (celle-ci ne liste pas par
d√©faut les fichiers de mani√®re r√©cursive comme `boto3`):

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
fs.ls("donnees-insee/diffusion")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')
s3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```


```shell
mc ls -r
```

```{=html}
</details>
```
:::





### T√©l√©charger un fichier depuis `S3` pour l'enregistrer en local

Cette m√©thode n'est en g√©n√©ral pas recommand√©e car, comme on va le voir
par la suite, il est possible de lire √† la vol√©e des fichiers. Cependant,
t√©l√©charger un fichier depuis le _cloud_ pour l'√©crire sur le disque
local peut parfois √™tre utile (par exemple, lorsqu'il est n√©cessaire
de d√©zipper un fichier). 


::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

On utilise cette fois la m√©thode `download_file`

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
s3.download_file('donnees-insee', "diffusion/FILOSOFI/2014/FILOSOFI_COM.csv", 'data.csv')
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
fs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier = S3.remote(f'{bucket}/moninput.csv')
    output:
        fichier='mon_dossier_local/monoutput.csv'
    run:
        shell("cp {input[0]} {output[0]}")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```

```python
mc cp "donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv" 'data.csv'
```

```{=html}
</details>
```
:::





### Lire un fichier directement

La m√©thode pr√©c√©dente n'est pas optimale. En effet, l'un des int√©r√™ts des API
est qu'on peut traiter un fichier sur `S3` comme s'il s'agissait d'un fichier
sur son PC. Cela est d'ailleurs une mani√®re plus s√©curis√©e de proc√©der puisqu'on
lit les donn√©es √† la vol√©e, sans les √©crire dans un filesystem local. 


::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```


```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
obj = s3.get_object(Bucket='donnees-insee', Key="diffusion/FILOSOFI/2014/FILOSOFI_COM.csv")
df = pd.read_csv(obj['Body'], sep = ";")
df.head(2)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

Le code suivant devrait permettre d'effectuer la m√™me op√©ration avec `s3fs`

```python
import pandas as pd
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
df = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', "diffusion/FILOSOFI/2014/FILOSOFI_COM.csv"),
                         mode='rb'), sep = ";"
                 )

df.head(2)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier = S3.remote(f'{bucket}/moninput.csv')
    run:
        import pandas as pd
        df = pd.read_csv(input.fichier)
        # PLUS D'OPERATIONS
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

`Arrow` est une librairie qui permet de lire des `CSV`.
Il est n√©anmoins
beaucoup plus pratique d'utiliser le format `parquet` avec `arrow`. 
Dans un premier temps, on configure le _filesystem_ avec les 
fonctionalit√©s d'`Arrow` (cf. pr√©c√©demment). 

```{python}
from pyarrow import fs

s3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')
```

Pour lire un csv, on fera:

```python
from pyarrow import fs
from pyarrow import csv

s3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')

with s3.open_input_file("donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv") as file:
    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=";")).to_pandas()
```

Pour un fichier au format parquet, la d√©marche est plus simple gr√¢ce √† l'argument
`filesystem` dans `pyarrow.parquet.ParquetDataset` :

```python
import pyarrow.parquet as pq

#bucket = ""
#parquet_file=""
df = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()
```

```{=html}
</details>
```
:::




### Uploader un fichier

::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

```python
s3.upload_file(file_name, bucket, object_name)
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```


```python
fs.put(filepath, f"{bucket}/{object_name}", recursive=True)
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```


Supposons que `df` soit un `pd.DataFrame` 
Dans un syst√®me local, on convertirait
en table `Arrow` puis on √©crirait en `parquet`
([voir la documentation officielle](https://arrow.apache.org/docs/python/parquet.html#reading-and-writing-single-files)).
Quand on est sur un syst√®me `S3`, il s'agit seulement d'ajouter
notre connexion √† `S3` dans l'argument `filesystem`
([voir la page sur ce sujet dans la documentation Arrow](https://arrow.apache.org/docs/python/filesystems.html#filesystem-s3))

```python
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.Table.from_pandas(df)
pq.write_table(table, f"{bucket}/{path}", filesystem=s3)
```

```{=html}
</details>
```
:::



::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier='mon_dossier_local/moninput.csv'
    output:
        fichier=S3.remote(f'{bucket}/monoutput.csv')
    run:
        shell("cp output.fichier input.fichier")
```


```{=html}
</details>
```
:::



::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```


```python
mc cp 'data.csv' "MONBUCKET/monoutput.csv"
```

```{=html}
</details>
```
:::



## Cas pratique : stocker les donn√©es de son projet sur le SSP Cloud

Une composante essentielle de l'√©valuation des projets `Python` est la __reproductibilit√©__, i.e. la possibilit√© de retrouver les m√™mes r√©sultats √† partir des m√™mes donn√©es d'entr√©e et du m√™me code. Dans la mesure du possible, il faut donc que votre rendu final parte des donn√©es brutes utilis√©es comme source dans votre projet. Si les fichiers de donn√©es source sont accessibles via une URL publique par exemple, il est id√©al de les importer directement √† partir de cette URL au d√©but de votre projet (voir le [TP Pandas](/content/manipulation/02_pandas_suite.qmd) pour un exemple d'un tel import via `Pandas`).

En pratique, cela n'est pas toujours possible. Peut-√™tre que vos donn√©es ne sont pas directement publiquement accessibles, ou bien sont disponibles sous des formats complexes qui demandent des pr√©-traitements avant d'√™tre exploitables dans un format de donn√©e standard. Peut-√™tre que vos donn√©es r√©sultent d'une phase de r√©cup√©ration automatis√©e via une [API](/content/manipulation/04c_API_TP.qmd) ou du [webscraping](/content/manipulation/04a_webscraping_TP.qmd), auquel cas l'√©tape de r√©cup√©ration peut prendre du temps √† reproduire. Par ailleurs, les sites internet √©voluent fr√©quemment dans le temps, il est donc pr√©f√©rable de "figer" les donn√©es une fois l'√©tape de r√©cup√©ration effectu√©e. De la m√™me fa√ßon, m√™me s'il ne s'agit pas de donn√©es source, vous pouvez vouloir entra√Æner des mod√®les et stocker leur version entra√Æn√©e, car cette √©tape peut √©galement √™tre chronophage.

Dans toutes ces situations, il est n√©cessaire de pouvoir stocker des donn√©es (ou des mod√®les). **Votre d√©p√¥t `Git` n'est pas le lieu adapt√© pour le stockage de fichiers volumineux**. Un projet `Python` bien construit est modulaire: il s√©pare le stockage du code (`Git`), d'√©l√©ments de configuration (par exemple des jetons d'API qui ne doivent pas √™tre dans le code) et du stockage des donn√©es. Cette s√©paration conceptuelle entre code et donn√©es permet de meilleurs projets. 

![](environment.png)

L√† o√π `Git` est fait pour stocker du code, on utilise des solutions adapt√©es pour le stockage de fichiers. De nombreuses solutions existent pour ce faire. Sur le SSP Cloud, on propose `MinIO`, une impl√©mentation open-source du stockage `S3` pr√©sent√© plus haut. Ce court tutoriel vise √† pr√©senter une utilisation standard dans le cadre de vos projets.

::: {.warning}
Quelle que soit la solution de stockage retenue pour vos donn√©es/mod√®les, **le code ayant servir √† produire ces objets doit imp√©rativement figurer dans votre d√©p√¥t de projet**.
:::

### Partager des fichiers sur le SSP Cloud

Comme expliqu√© plus haut, on stocke les fichiers sur `S3` dans un bucket. Sur le SSP Cloud, un bucket est cr√©√© automatiquement lors de votre cr√©ation de compte, avec le m√™me nom que votre compte SSP Cloud. L'interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) vous permet d'y acc√©der de mani√®re visuelle, d'y importer des fichiers, de les t√©l√©charger, etc. 

Dans ce tutoriel, nous allons plut√¥t y acc√©der de mani√®re programmatique, via du code `Python`. Le package `s3fs` permet de requ√™ter votre bucket √† la mani√®re d'un filesystem classique. Par exemple, vous pouvez lister les fichiers disponibles sur votre *bucket* avec la commande suivante : 

```{python}
#| eval: false
import s3fs

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

MY_BUCKET = "mon_nom_utilisateur_sspcloud"
fs.ls(MY_BUCKET)
```

Si vous n'avez jamais ajout√© de fichier sur MinIO, votre *bucket* est vide, cette commande devrait donc renvoyer une liste vide. On va donc ajouter un premier dossier pour voir la diff√©rence.

Par d√©faut, un bucket vous est personnel, c'est √† dire que les donn√©es qui s'y trouvent ne peuvent √™tre lues ou modifi√©es que par vous. Dans le cadre de votre projet, vous aurez envie de partager ces fichiers avec les membres de votre groupe pour d√©velopper de mani√®re collaborative. Mais pas seulement ! Il faudra √©galement que vos correcteurs puissent acc√©der √† ces fichiers pour reproduire vos analyses. 

Il existe diff√©rentes possibilit√©s de rendre des fichiers plus ou moins publics sur `MinIO`. La plus simple, et celle que nous vous recommandons, est de cr√©er un dossier `diffusion` √† la racine de votre bucket. Sur le SSP Cloud, tous les fichiers qui se situent dans un dossier `diffusion` sont accessibles **en lecture** √† l'ensemble des utilisateurs authentifi√©s. Utilisez l'interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) pour cr√©er un dossier `diffusion` √† la racine de votre *bucket*. Si tout a bien fonctionn√©, la commande `Python` ci-dessus devrait d√©sormais afficher le chemin `mon_nom_utilisateur_sspcloud/diffusion`.

::: {.note}
Le stockage *cloud* favorise le travail collaboratif ! 

Plut√¥t que chaque membre du projet travaille avec ses propres fichiers sur son ordinateur, ce qui implique une synchronisation fr√©quente entre membres du groupe et limite la reproductibilit√© du fait des risques d'erreur, les fichiers sont mis sur un d√©p√¥t central, que chaque membre du groupe peut ensuite requ√™ter. 

Pour cela, il faut simplement s'accorder au sein du groupe pour utiliser le bucket d'un des membres du projet, et s'assurer que les autres membres du groupe peuvent acc√©der aux donn√©es, en les mettant dans le dossier `diffusion` du bucket choisi.
:::

### R√©cup√©ration et stockage de donn√©es

Maintenant que nous savons o√π mettre nos donn√©es sur `MinIO`, regardons comment le faire en pratique depuis `Python`.

#### Cas d'un Dataframe

Reprenons un exemple issu du cours sur les [API](/content/manipulation/04c_API_TP.qmd#illustration-avec-une-api-de-lademe-pour-obtenir-des-diagnostics-√©nerg√©tiques) pour simuler une √©tape de r√©cup√©ration de donn√©es co√ªteuse en temps. 

```{python}
import requests
import pandas as pd

api_query <- "https://koumoul.com/data-fair/api/v1/datasets/dpe-france/lines?format=json&q_mode=simple&qs=code_insee_commune_actualise%3A%2201450%22&size=100&select=%2A&sampling=neighbors"
response_json = requests.get(url_api).json()
df_dpe = pd.json_normalize(response_json["results"])

df_dpe.head(2)
```

Cette requ√™te nous permet de r√©cup√©rer un _DataFrame_ `Pandas`, dont les deux premi√®res lignes sont imprim√©es ci-dessus. Dans notre cas le processus est volontairement simpliste, mais on peut imaginer que de nombreuses √©tapes de requ√™tage / pr√©paration de la donn√©es sont n√©cessaires pour aboutir √† un dataframe exploitable dans la suite du projet, et que ce processus est co√ªteux en temps. On va donc stocker ces donn√©es "interm√©diaires" sur `MinIO` afin de pouvoir ex√©cuter la suite du projet sans devoir refaire tourner tout le code qui les a produites.

On peut utiliser les fonctions d'export de `Pandas`, qui permettent d'exporter dans diff√©rents formats de donn√©es. Vu qu'on est dans le cloud, une √©tape suppl√©mentaire est n√©cessaire : on ouvre une connexion vers `MinIO`, puis on exporte notre dataframe.

```{python}
#| eval: false
MY_BUCKET = "mon_nom_utilisateur_sspcloud"
FILE_PATH_OUT_S3 = f"{MY_BUCKET}/diffusion/df_dpe.csv"

with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:
    df_dpe.to_csv(file_out)
```

On peut v√©rifier que notre fichier a bien √©t√© upload√© via l'interface [Mes Fichiers](https://datalab.sspcloud.fr/my-files) ou bien directement en `Python` en interrogeant le contenu du dossier `diffusion` de notre *bucket* :

```{python}
#| eval: false
fs.ls(f"{MY_BUCKET}/diffusion")
```

On pourrait tout aussi simplement exporter notre _dataset_ en `Parquet`, pour limiter l'espace de stockage et maximiser les performances √† la lecture. Attention : vu que `Parquet` est un format compress√©, il faut pr√©ciser qu'on √©crit un fichier binaire : le mode d'ouverture du fichier pass√© √† la fonction `fs.open` passe de `w` (`write`) √† `wb` (`write binary`).

```{python}
#| eval: false
FILE_PATH_OUT_S3 = f"{MY_BUCKET}/diffusion/df_dpe.parquet"

with fs.open(FILE_PATH_OUT_S3, 'wb') as file_out:
    df_dpe.to_parquet(file_out)
```

#### Cas de fichiers

Dans la partie pr√©c√©dente, on √©tait dans le cas "simple" d'un dataframe, ce qui nous permettait d'utiliser directement les fonctions d'export de `Pandas`. Maintenant, imaginons qu'on ait plusieurs fichiers d'entr√©e, pouvant chacun avoir des formats diff√©rents. Un cas typique de tels fichiers sont les fichiers `ShapeFile`, qui sont des fichiers de donn√©es g√©ographiques, et se pr√©sentent sous forme d'une combinaison de fichiers (cf. [chapitre sur GeoPandas](/content/manipulation/03_geopandas_intro.qmd#le-format-shapefile-.shp-et-le-geopackage-.gpkg)). Commen√ßons par r√©cup√©rer un fichier `.shp` pour voir sa structure.

On r√©cup√®re ci-dessous les [contours des d√©partements fran√ßais](https://www.data.gouv.fr/fr/datasets/contours-des-departements-francais-issus-d-openstreetmap/), sous la forme d'une archive `.zip` qu'on va d√©compresser en local dans un dossier `departements_fr`.

```{python}
import io
import os
import requests
import zipfile

# Import et d√©compression
contours_url = "https://www.data.gouv.fr/fr/datasets/r/eb36371a-761d-44a8-93ec-3d728bec17ce"
response = requests.get(contours_url, stream=True)
zipfile = zipfile.ZipFile(io.BytesIO(response.content))
zipfile.extractall("departements_fr")

# V√©rification du dossier (local, pas sur S3)
os.listdir("departements_fr")
```

Vu qu'il s'agit cette fois de fichiers locaux et non d'un *dataframe* `Pandas`, on doit utiliser le package `s3fs` pour transf√©rer les fichiers du filesystem local au filesystem distant (`MinIO`). Gr√¢ce √† la commande `put`, on peut copier en une seule commande le dossier sur `MinIO`. Attention √† bien sp√©cifier le param√®tre `recursive=True`, qui permet de copier √† la fois un dossier et son contenu.

```{python}
#| eval: false
fs.put("departements_fr/", f"{MY_BUCKET}/diffusion/departements_fr/", recursive=True)
```

V√©rifions que le dossier a bien √©t√© copi√© :

```{python}
#| eval: false
fs.ls(f"{MY_BUCKET}/diffusion/departements_fr")
```

Si tout a bien fonctionn√©, la commande ci-dessus devrait renvoyer une liste contenant les chemins sur `MinIO` des diff√©rents fichiers (`.shp`, `.shx`, `.prj`, etc.) constitutifs du `ShapeFile` des d√©partements.

### Utilisation des donn√©es

En sens inverse, pour r√©cup√©rer les fichiers depuis `MinIO` dans une session `Python`, les commandes sont sym√©triques.

#### Cas d'un dataframe

Attention √† bien passer cette fois le param√®tre `r` (`read`, pour lecture) et non plus `w` (`write`, pour √©criture) √† la fonction `fs.open` afin de ne pas √©craser le fichier !

```{python}
#| eval: false
MY_BUCKET = "mon_nom_utilisateur_sspcloud"
FILE_PATH_S3 = f"{MY_BUCKET}/diffusion/df_dpe.csv"

# Import
with fs.open(FILE_PATH_S3, 'r') as file_in:
    df_dpe = pd.read_csv(file_in)

# V√©rification
df_dpe.head(2)
```

De m√™me, si le fichier est en `Parquet` (en n'oubliant pas de passer de `r` √† `rb` pour tenir compte de la compression) : 

```{python}
#| eval: false
MY_BUCKET = "mon_nom_utilisateur_sspcloud"
FILE_PATH_S3 = f"{MY_BUCKET}/diffusion/df_dpe.parquet"

# Import
with fs.open(FILE_PATH_S3, 'rb') as file_in:
    df_dpe = pd.read_parquet(file_in)

# V√©rification
df_dpe.head(2)
```

#### Cas de fichiers

Dans le cas de fichiers, on va devoir dans un premier temps rapatrier les fichiers de `MinIO` vers la machine local (en l'occurence, le service ouvert sur le SSP Cloud).

```{python}
#| eval: false

# R√©cup√©ration des fichiers depuis MinIO vers la machine locale
fs.get(f"{MY_BUCKET}/diffusion/departements_fr/", "departements_fr/", recursive=True)
```

Puis on les importe classiquement depuis `Python` avec le *package* appropri√©. Dans le cas des `ShapeFile`, o√π les diff√©rents fichiers sont en fait des parties d'un seul et m√™me fichier, une seule commande permet de les importer apr√®s les avoir rappatri√©s.

```{python}
#| eval: false
import geopandas as gpd

df_dep = gpd.read_file("departements_fr")
df_dep.head(2)
```

## Pour aller plus loin

- [La documentation sur MinIO du SSPCloud](https://docs.sspcloud.fr/content/storage.html)
