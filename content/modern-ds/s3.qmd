---
title: "Les nouveaux modes d'acc√®s aux donn√©es : le format parquet et les donn√©es sur le cloud"
date: 2022-08-28T15:00:00Z
draft: false
weight: 20
slug: readS3
type: book
tags:
  - S3
  - boto3
categories:
  - Tutoriel
  - Avanc√©
summary: |
  Dans les entreprises et administrations, un nombre croissant 
  d'infrastructure se basent sur des _clouds_, qui sont des sessions 
  non persistentes o√π les donn√©es ne sont pas stock√©es dans les m√™mes
  serveurs que les machines qui ex√©cutent du code. L'une des technologies
  dominantes dans le domaine est un syst√®me de stockage nomm√© `S3`,
  d√©velopp√© par [Amazon](https://docs.aws.amazon.com/fr_fr/AmazonS3/latest/userguide/Welcome.html). 
  
  `Python`, √† travers plusieurs _packages_ (notamment `boto3`, `s3fs` ou `pyarrow`),
  permet d'utiliser ce syst√®me de stockage distant comme si on
  acc√©dait √† des fichiers depuis son poste personnel. Cette r√©volution est 
  √©troitement associ√©e √† l'√©mergence du format de
  donn√©es [`Apache Parquet`](https://parquet.apache.org/), format utilisable en
  `Python` par le biais du package [`pyarrow`](https://arrow.apache.org/docs/python/index.html)
  ou avec [`Spark`](https://spark.apache.org/) et pr√©sentant
  de nombreux avantages pour l'analyse de donn√©es (vitesse d'import, possibilit√© de traiter
  des donn√©es plus volumineuses que la RAM...)
eval: false
image: python_cloud.png
---

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/NLP/05a_s3.qmd")
```
:::

Ce chapitre est une introduction √† la question
du stockage des donn√©es et aux innovations 
r√©centes dans ce domaine. L'objectif
est d'abord de pr√©senter les avantages
du format `Parquet` et la mani√®re dont 
on peut utiliser les
librairies [`pyarrow`](https://arrow.apache.org/docs/python/index.html)
ou [`duckdb`](https://duckdb.org/docs/api/python/overview.html) pour traiter
de mani√®re efficace des donn√©es volumineuses
au format `Parquet`. Ensuite, on pr√©sentera
la mani√®re dont ce format `parquet` s'int√®gre
bien avec des syst√®mes de stockage _cloud_,
qui tendent √† devenir la norme dans le monde
de la _data science_. 

# Elements de contexte

## Principe du stockage de la donn√©e

Pour comprendre les apports du format `Parquet`, il est n√©cessaire
de faire un d√©tour pour comprendre la mani√®re dont une information
est stock√©e et accessible √† un langage de traitement de la donn√©e.

Il existe deux approches dans le monde du stockage de la donn√©e. 
La premi√®re est celle de la __base de donn√©es relationnelle__. La seconde est le
principe du __fichier__. 
La diff√©rence entre les deux est dans la mani√®re dont l'acc√®s aux
donn√©es est organis√©. 

## Les fichiers

Dans un fichier, les donn√©es sont organis√©es selon un certain format et
le logiciel de traitement de la donn√©e va aller chercher et structurer
l'information en fonction de ce format. Par exemple, dans un fichier 
`.csv`, les diff√©rentes informations seront stock√©es au m√™me niveau
avec un caract√®re pour les s√©parer (la virgule `,` dans les `.csv` anglosaxons, le point virgule dans les `.csv` fran√ßais, la tabulation dans les `.tsv`). Le fichier suivant

```raw
nom ; profession 
Ast√©rix ; 
Ob√©lix ; Tailleur de menhir ;
Assurancetourix ; Barde
```

sera ainsi organis√© naturellement sous forme tabul√©e par `Python`

```{python}
#| echo: false
#| eval: true

import pandas as pd
from io import StringIO
pd.read_csv(
    StringIO(
        """
        nom ; profession
        Ast√©rix ; 
        Ob√©lix ; Tailleur de menhir
        Assurancetourix ; Barde
        """
    ),
    sep = ";"
)
```

A propos des fichiers de ce type, on parle de __fichiers plats__ car
les enregistrements relatifs √† une observation sont stock√©s ensemble,
sans hi√©rarchie.  

Certains formats de donn√©es vont permettre d'organiser les informations
de mani√®re diff√©rente. Par exemple, le format `JSON` va
hi√©rarchiser diff√©remment la m√™me information [^1]:

```raw
[
  {
    "nom": "Ast√©rix"
  },
  {
    "nom": "Ob√©lix",
    "profession": "Tailleur de menhir"
  },
  {
    "nom": "Assurancetourix",
    "profession": "Barde"
  }
]
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint </h3>
```
La diff√©rence entre le CSV et le format `JSON` va au-del√† d'un simple "formattage" des donn√©es.

Par sa nature non tabulaire, le format JSON permet des mises √† jour beaucoup plus facile de la donn√©e dans les entrep√¥ts de donn√©es.

Par exemple, un site web qui collecte de nouvelles donn√©es n'aura pas √† mettre √† jour l'ensemble de ses enregistrements ant√©rieurs
pour stocker la nouvelle donn√©e (par exemple pour indiquer que pour tel ou tel client cette donn√©e n'a pas √©t√© collect√©e)
mais pourra la stocker dans
un nouvel item. Ce sera √† l'outil de requ√™te (`Python` ou un autre outil)
de cr√©er une relation entre les enregistrements stock√©s √† des endroits
diff√©rents.

Ce type d'approche flexible est l'un des fondements de l'approche `NoSQL`,
sur laquelle nous allons revenir, qui a permis l'√©mergence de technologies au coeur de l'√©cosyst√®me actuel du _big-data_ comme `Hadoop` ou `ElasticSearch`. 

```{=html}
</div>
```
:::


Cette fois, quand on n'a pas d'information, on ne se retrouve pas avec nos deux s√©parateurs accol√©s (cf. la ligne _"Ast√©rix"_) mais l'information
n'est tout simplement pas collect√©e. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```
Il se peut tr√®s bien que l'information sur une observation soit diss√©min√©e
dans plusieurs fichiers dont les formats diff√®rent.

Par exemple, dans le domaine des donn√©es g√©ographiques,
lorsqu'une donn√©e est disponible sous format de fichier(s), elle peut l'√™tre de deux mani√®res!

- Soit la donn√©e est stock√©e dans un seul fichier qui m√©lange contours g√©ographiques et valeurs attributaires
(la valeur associ√©e √† cette observation g√©ographique, par exemple le taux d'abstention). Ce principe est celui du `geojson`.
- Soit la donn√©e est stock√©e dans plusieurs fichiers qui sont sp√©cialis√©s: un fichier va stocker les contours g√©ographiques,
l'autre les donn√©es attributaires et d'autres fichiers des informations annexes (comme le syst√®me de projection). Ce principe est celui du `shapefile`.
C'est alors le logiciel qui requ√™te
les donn√©es (`Python` par exemple) qui saura o√π aller chercher l'information
dans les diff√©rents fichiers et associer celle-ci de mani√®re coh√©rente.

```{=html}
</div>
```
:::



Un concept suppl√©mentaire dans le monde du fichier est celui du __file system__. Le  _file system_ est
le syst√®me de localisation et de nommage des fichiers. 
Pour simplifier, le _file system_ est la mani√®re dont votre ordinateur saura
retrouver, dans son syst√®me de stockage, les bits pr√©sents dans tel ou tel fichier
appartenant √† tel ou tel dossier.  

## Les bases de donn√©es


La logique des bases de donn√©es est diff√©rente. Elle est plus syst√©mique. 
Un syst√®me de gestion de base de donn√©es (_Database Management System_)
est un logiciel qui g√®re √† la fois le stockage d'un ensemble de donn√©es reli√©e,
permet de mettre √† jour celle-ci (ajout ou suppression d'informations, modification
des caract√©ristiques d'une table...)
et qui g√®re √©galement
les modalit√©s d'acc√®s √† la donn√©e (type de requ√™te, utilisateurs
ayant les droits en lecture ou en √©criture...). 

La relation entre les entit√©s pr√©sentes dans une base de donn√©es
prend g√©n√©ralement la forme d'un __sch√©ma en √©toile__. Une base va centraliser
les informations disponibles qui seront ensuite d√©taill√©es dans des tables
d√©di√©es. 

![](https://www.databricks.com/wp-content/uploads/2022/04/star-schema-erd.png)
Source: [La documentation `Databricks` sur le sch√©ma en √©toile](https://www.databricks.com/fr/glossary/star-schema)

Le logiciel associ√© √† la base de donn√©es fera ensuite le lien
entre ces tables √† partir de requ√™tes `SQL`. L'un des logiciels les plus efficaces dans ce domaine
est [`PostgreSQL`](https://www.postgresql.org/). `Python` est tout √† fait
utilisable pour passer une requ√™te SQL √† un gestionnaire de base de donn√©es. 
Les packages [`sqlalchemy`](https://www.sqlalchemy.org/) et [`psycopg2`](https://www.psycopg.org/docs/)
peuvent servir √† utiliser `PostgreSQL` pour requ√™ter une
base de donn√©e ou la mettre √† jour. 

La logique de la base de donn√©es est donc tr√®s diff√©rente de celle du fichier.
Ces derniers sont beaucoup plus l√©gers pour plusieurs raisons. 
D'abord, parce qu'ils sont moins adh√©rents √† 
un logiciel gestionnaire. L√† o√π le fichier ne n√©cessite, pour la gestion,
qu'un _file system_, install√© par d√©faut sur
tout syst√®me d'exploitation, une base de donn√©es va n√©cessiter un
logiciel sp√©cialis√©. L'inconv√©nient de l'approche fichier, sous sa forme
standard, est qu'elle
ne permet pas une gestion fine des droits d'acc√®s et am√®ne g√©n√©ralement √† une 
duplication de la donn√©e pour √©viter que la source initiale soit
r√©-√©crite (involontairement ou de mani√®re intentionnelle par un utilisateur malveillant).
R√©soudre ce probl√®me est l'une des
innovations des syst√®mes _cloud_, sur lesquelles nous reviendrons en √©voquant le
syst√®me `S3`.
Un deuxi√®me inconv√©nient de l'approche base de donn√©es par
rapport √† l'approche fichier, pour un utilisateur de `Python`,
est que les premiers n√©cessitent l'interm√©diation du logiciel de gestion 
de base de donn√©es l√† o√π, dans le second cas, on va se contenter d'une
librairie, donc un syst√®me beaucoup plus l√©ger, 
qui sait comment transformer la donn√©e brute en `DataFrame`. 
Pour ces raisons, entre autres, les bases de donn√©es sont donc moins √† la 
mode dans l'√©cosyst√®me r√©cent de la _data science_ que les fichiers.

# Le format `parquet`


Le format `CSV` a rencontr√© un grand succ√®s par sa simplicit√©: il 
est lisible par un humain (un bloc-note suffit pour l'ouvrir et
apercevoir les premi√®res lignes), sa nature plate lui permet
de bien correspondre au concept de donn√©es tabul√©es sans hi√©rarchie 
qui peuvent √™tre rapidement valoris√©es, il est universel (il n'est
pas adh√©rent √† un logiciel). Cependant, le CSV pr√©sente
plusieurs inconv√©nients qui justifient l'√©mergence d'un format
concurrent:

- le CSV est un format __lourd__ car les informations ne sont pas compress√©es 
(ce qui le rend lisible facilement depuis un bloc-note) mais aussi
parce que toutes les donn√©es sont stock√©es de la m√™me mani√®re.
C'est la
librairie faisant l'import qui va essayer d'optimiser le typage des donn√©es
pour trouver le typage qui utilise le moins de m√©moire possible sans
alt√©ration de l'information. En effet, si `pandas` d√©termine qu'une colonne
pr√©sente les valeurs `6 ; 5 ; 0`, il va privil√©gier l'utilisation du type
`int` au type `double` qui sera lui m√™me pr√©f√©r√© au type `object` (objets
de type donn√©es textuelles). Cependant, pour faire cela, `pandas` va devoir
scanner un nombre suffisant de valeurs, ce qui demande du temps et expose 
√† des erreurs (en se fondant sur trop peu de valeurs, on peut se tromper
de typage) ;
- le stockage √©tant __orient√© ligne__, 
acc√©der √† une information donn√©e dans un `CSV` implique
de le lire le fichier en entier, s√©lectionner la ou les colonnes
d'int√©r√™t et ensuite les lignes d√©sir√©es. Par exemple, si on d√©sire
conna√Ætre uniquement la profession de la deuxi√®me ligne dans l'exemple
plus haut :point_up:, un algorithme de recherche devra:
prendre le fichier, d√©terminer que la profession est la deuxi√®me colonne,
et ensuite aller chercher la deuxi√®me ligne dans cette colonne. Si
on d√©sire acc√©der √† un sous-ensemble de lignes dont les indices
sont connus, le `CSV` est int√©ressant. Cependant,
si on d√©sire acc√©der √† un sous-ensemble
de colonnes dans un fichier (ce qui est un cas d'usage plus fr√©quent
pour les _data scientists_), alors le `CSV` n'est pas le format le plus
appropri√© ;
- mettre √† jour la donn√©e est co√ªteux car cela implique de r√©√©crire
l'ensemble du fichier. Par exemple, si apr√®s une premi√®re
analyse de la donn√©e,
on d√©sire ajouter une colonne, on ne peut accoler ces nouvelles informations
√† celles d√©j√† existantes, il est n√©cessaire de r√©√©crire l'ensemble 
du fichier. Pour reprendre l'exemple de nos gaulois pr√©f√©r√©s, si on veut
ajouter une colonne `cheveux` entre les deux d√©j√† existantes,
il faudra changer totalement le fichier: 

```raw
"""
nom ; cheveux ; profession
Ast√©rix; blond; ; 
Ob√©lix; roux; Tailleur de menhir
Assurancetourix; blond; Barde
"""
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```
La plupart des logiciels d'analyse de donn√©es proposent 
un format de fichier pour sauvegarder des bases de donn√©es. On
peut citer le `.pickle` (`Python`), le `.rda` ou `.RData` (`R`),
le `.dta` (`Stata`) ou le `.sas7bdat` (`SAS`). L'utilisation
de ces formats est probl√©matique car cela revient √† se lier
les mains pour l'analyse ult√©rieure des donn√©es, surtout
lorsqu'il s'agit d'un format propri√©taire (comme avec
`SAS` ou `Stata`). Par exemple, `Python` ne
sait pas nativement lire un `.sas7bdat`. Il existe des librairies
pour le faire (notamment `Pandas`) mais le format
√©tant propri√©taire, les d√©veloppeurs de la librairie ont d√ª t√¢tonner et
on n'est ainsi jamais assur√© qu'il n'y ait pas d'alt√©ration de la donn√©e. 

Malgr√© tous les inconv√©nients du `.csv` list√©s plus haut, il pr√©sente 
l'immense avantage, par rapport √† ces formats, de l'universalit√©. 
Il vaut ainsi mieux privil√©gier un `.csv` √† ces formats pour le stockage
de la donn√©e. Ceci dit, comme vise √† le montrer ce chapitre, il vaut
mieux privil√©gier le format `parquet` au `CSV`.
```{=html}
</div>
```
:::


Pour r√©pondre √† ces limites du `CSV`, le format `parquet`, 
qui est un [projet open-source `Apache`](https://apache.org/), a √©merg√©.
La premi√®re diff√©rence entre le format `parquet` et le `CSV` est
que le premier repose sur un __stockage orient√© colonne__ l√† o√π
le second est orient√© ligne. Pour comprendre la diff√©rence, voici un
exemple issu du [blog d'upsolver](https://www.upsolver.com/blog/apache-parquet-why-use): 

![](https://www.upsolver.com/wp-content/uploads/2020/05/Screen-Shot-2020-05-26-at-17.52.58.png)

Dans notre exemple pr√©c√©dent, cela donnera une information prenant 
la forme suivante (ignorez l'√©l√©ment `pyarrow.Table`, nous
reviendrons dessus) :

```{python}
#| echo: false
#| eval: true
from pyarrow import csv
import io

s = """
nom ;profession
Ast√©rix ;
Ob√©lix ;Tailleur de menhir
Assurancetourix ;Barde
"""

source = io.BytesIO(s.encode())

df = csv.read_csv(source, parse_options = csv.ParseOptions(delimiter=";"))
df
```

Pour reprendre l'exemple fil rouge :point_up:, il sera ainsi beaucoup plus
facile de r√©cup√©rer la deuxi√®me ligne de la colonne `profession`:
on ne consid√®re que le vecteur `profession` et on r√©cup√®re la deuxi√®me
valeur. 
Le requ√™tage d'√©chantillon de donn√©es ne n√©cessite donc pas l'import de 
l'ensemble des donn√©es. A cela s'ajoute des fonctionnalit√©s suppl√©mentaires
des librairies d'import de donn√©es parquet (par exemple `pyarrow` ou `spark`)
qui vont faciliter des recherches complexes bas√©es, par exemple, sur des
requ√™tes de type `SQL`, ou permettant l'utilisation de donn√©es plus volumineuses que la RAM. 

Le format `parquet` pr√©sente d'autres avantages par rapport au
`CSV`: 

- Le format `parquet` est (tr√®s) compress√©, ce qui r√©duit la volum√©trie
des donn√©es sur disque ;
- Des m√©tadonn√©es, notamment le typage des variables, sont stock√©es en compl√©ment dans le fichier.
Cette partie, nomm√©e le _footer_ du fichier `parquet`, permet que l'import des donn√©es soit 
optimis√© sans risque d'alt√©ration de celle-ci. Pour un producteur de donn√©es, c'est une mani√®re
d'assurer la qualit√© des donn√©es. Par exemple, un fournisseur de
donn√©es de type code-barre sera
certain que les donn√©es `000012` ne seront pas consid√©r√©es identiques √† un code-barre `12`.
- Il est possible de partitionner un jeu de donn√©es en fonction de diff√©rents niveaux (par
exemple des niveaux g√©ographiques) en une arborescence de fichiers `parquet`. Cela
permet de travailler sur un √©chantillon pour facilement passer √† l'√©chelle ensuite.
Par exemple, une structure partitionn√©e, emprunt√©e
√† la [documentation `Spark`](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery)
peut prendre la forme suivante :

```raw
path
‚îî‚îÄ‚îÄ to
    ‚îî‚îÄ‚îÄ table
        ‚îú‚îÄ‚îÄ gender=male
        ‚îÇ   ‚îú‚îÄ‚îÄ ...
        ‚îÇ   ‚îÇ
        ‚îÇ   ‚îú‚îÄ‚îÄ country=US
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
        ‚îÇ   ‚îú‚îÄ‚îÄ country=CN
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îî‚îÄ‚îÄ gender=female
            ‚îú‚îÄ‚îÄ ...
            ‚îÇ
            ‚îú‚îÄ‚îÄ country=US
            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
            ‚îú‚îÄ‚îÄ country=CN
            ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet
            ‚îî‚îÄ‚îÄ ...
```

Qu'on lise un ou plusieurs fichiers, on finira avec le sch√©ma suivant:

```raw
root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
```

Ces diff√©rents avantages expliquent le succ√®s du format `parquet` dans le monde du
_big-data_. Le paragraphe suivant, extrait du [post d'upsolver]() d√©j√† cit√©, 
r√©sume bien l'int√©r√™t:

> Complex data such as logs and event streams would need to be represented as a table with hundreds or thousands of columns, and many millions of rows. Storing this table in a row based format such as CSV would mean:
> 
> - Queries will take longer to run since more data needs to be scanned, rather than only querying the subset of columns we need to answer a query (which typically requires aggregating based on dimension or category)
> - Storage will be more costly since CSVs are not compressed as efficiently as Parquet

Cependant, **`Parquet` ne devrait pas int√©resser que les producteurs ou utilisateurs de donn√©es _big-data_**.
C'est l'ensemble
des producteurs de donn√©es qui b√©n√©ficient des fonctionalit√©s
de `Parquet`. 

Pour en savoir plus sur `Arrow`,
des √©l√©ments suppl√©mentaires sur `Parquet` sont disponibles sur ce tr√®s bon
post de blog d'[upsolver](https://www.upsolver.com/blog/apache-parquet-why-use)
et [sur la page officielle du projet `Parquet`](https://parquet.apache.org/).

## Lire un `parquet` en `Python`: la librairie `pyarrow`

La librairie `pyarrow` permet la lecture et l'√©criture
de fichiers `parquet` avec `Python`[^3]. Elle repose
sur un type particulier de _dataframe_, le `pyarrow.Table`
qui peut √™tre utilis√© en substitut ou en compl√©ment
du `DataFrame` 
de `pandas`. Il est recommand√© de r√©guli√®rement
consulter la documentation officielle de `pyarrow` 
concernant [la lecture et √©criture de fichiers](https://arrow.apache.org/docs/python/parquet.html) et celle relative
aux [manipulations de donn√©es](https://arrow.apache.org/cookbook/py/data.html).

[^3]: Elle permet aussi la lecture et l'√©criture 
de `.csv`.

Pour illustrer les fonctionalit√©s de `pyarrow`,
repartons de notre CSV initial que nous allons
enrichir d'une nouvelle variable num√©rique
et que nous
allons 
convertir en objet `pyarrow` avant de l'√©crire au format `parquet`:

```{python}
#| eval: true
import pandas as pd
from io import StringIO 
import pyarrow as pa
import pyarrow.parquet as pq

s = """
nom;cheveux;profession
Ast√©rix;blond;
Ob√©lix;roux;Tailleur de menhir
Assurancetourix;blond;Barde
"""

source = StringIO(s)

df = pd.read_csv(source, sep = ";", index_col=False)
df["taille"] = [155, 190, 175]
table = pa.Table.from_pandas(df)

table

pq.write_table(table, 'example.parquet')
```


::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint </h3>
```
L'utilisation des noms `pa` pour `pyarrow` et `pq` pour
`pyarrow.parquet` est une convention communautaire
qu'il est recommand√© de suivre.
```{=html}
</div>
```
:::


Pour importer et traiter ces donn√©es, on peut conserver
les donn√©es sous le format `pyarrow.Table`
ou transformer en `pandas.DataFrame`. La deuxi√®me
option est plus lente mais pr√©sente l'avantage
de permettre ensuite d'appliquer toutes les
manipulations offertes par l'√©cosyst√®me
`pandas` qui est g√©n√©ralement mieux connu que
celui d'`Arrow`. 

Supposons qu'on ne s'int√©resse qu'√† la taille et √† la couleur
de cheveux de nos gaulois. 
Il n'est pas n√©cessaire d'importer l'ensemble de la base, cela
ferait perdre du temps pour rien. On appelle
cette approche le __`column pruning`__ qui consiste √† 
ne parcourir, dans le fichier, que les colonnes qui nous
int√©ressent. Du fait du stockage orient√© colonne du `parquet`,
il suffit de ne consid√©rer que les blocs qui nous 
int√©ressent (alors qu'avec un CSV il faudrait scanner tout
le fichier avant de pouvoir √©liminer certaines colonnes).
Ce principe du `column pruning` se mat√©rialise avec
l'argument `columns` dans `parquet`.

Ensuite, avec `pyarrow`, on pourra utiliser `pyarrow.compute` pour
effectuer des op√©rations directement sur une table
`Arrow` :

```{python}
#| eval: false
import pyarrow.compute as pc

table = pq.read_table('example.parquet', columns=['taille', 'cheveux'])

table.group_by("cheveux").aggregate([("taille", "mean")])
```

La mani√®re √©quivalente de proc√©der en passant
par l'interm√©diaire de `pandas` est

```{python}
#| eval: true
table = pq.read_table('example.parquet', columns=['taille', 'cheveux'])

table.to_pandas().groupby("cheveux")["taille"].mean()
```

Ici, comme les donn√©es sont peu volumineuses, deux des
avantages du `parquet` par rapport
au `CSV` (donn√©es moins
volumineuses et vitesse de l'import)
ne s'appliquent pas vraiment. 


::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Un autre principe d'optimisation de la performance qui est
au coeur de la librairie `Arrow` est le `filter pushdown`
(ou `predicate pushdown`). 

Quand on ex√©cute un filtre de s√©lection de ligne 
juste apr√®s avoir charg√© un jeu de donn√©es,
`Arrow` va essayer de le mettre en oeuvre lors de l'√©tape de lecture
et non apr√®s. Autrement dit, `Arrow` va modifier le plan 
d'ex√©cution pour pousser le filtre en amont de la s√©quence d'ex√©cution
afin de ne pas essayer de lire les lignes inutiles. 

```{=html}
</div>
```
:::




# Le syst√®me de stockage `S3`

Si les fichiers `parquet` sont une 
solution avantageuse pour 
les _data scientists_, ils ne r√©solvent
pas tous les inconv√©nients de
l'approche fichier. 
En particulier, la question de la
duplication des donn√©es pour la mise
√† disposition s√©curis√©e des sources
n'est pas r√©solue. Pour que 
l'utilisateur `B` n'alt√®re pas les
donn√©es de l'utilisateur `A`, il est n√©cessaire
qu'ils travaillent sur deux fichiers
diff√©rents, dont l'un peut √™tre une copie
de l'autre. 

# Les donn√©es sur le _cloud_

La mise √† disposition de donn√©es dans
les syst√®mes de stockage _cloud_ est
une r√©ponse √† ce probl√®me.
Les _data lake_ qui se sont d√©velopp√©s dans les 
institutions et entreprises utilisatrices de donn√©es


Le principe d'un stockage cloud
est le m√™me que celui d'une
`Dropbox` ou d'un `Drive` mais adapt√© √†
l'analyse de donn√©es. Un utilisateur de donn√©es
acc√®de √† un fichier stock√© sur un serveur distant
_comme s'il_ √©tait dans son _file system_ local[^4]. 
Donc, du point de vue de l'utilisateur `Python`, 
il n'y a pas de diff√©rence fondamentale. Cependant,
les donn√©es ne sont pas heberg√©es dans un dossier
local (par exemple `Mes Documents/monsuperfichier`)
mais sur un serveur distant auquel l'utilisateur
de `Python` acc√®de √† travers un √©change r√©seau.

![](featured.png)

Dans l'univers du _cloud_, la hi√©rarchisation des donn√©es
dans des dossiers et des fichiers bien rang√©s 
est d'ailleurs moins
importante que dans le monde du _file system_ local. 
Lorsque vous essayez de retrouver un fichier dans
votre arborescence de fichiers, vous utilisez parfois
la barre de recherche de votre explorateur de fichiers,
avec des r√©sultats mitig√©s[^4]. Dans le monde du _cloud_,
les fichiers sont parfois accumul√©s de mani√®re plus 
chaotique car les outils de recherche sont plus 
efficaces[^4]. 

[^4]: D'ailleurs, les g√©n√©rations n'ayant connu nativement
que ce type de stockage ne sont pas familiaris√©es
au concept de _file system_ et pr√©f√®rent 
payer le temps de recherche. Voir
[cet article](https://futurism.com/the-byte/gen-z-kids-file-systems)
sur le sujet. 

En ce qui concerne la s√©curit√© des donn√©es,
la gestion des droits de lecture et √©criture peut √™tre
fine: on peut autoriser certains utilisateurs uniquement 
√† la lecture, d'autres peuvent avoir les droits
d'√©criture pour modifier les donn√©es. Cela permet
de concilier les avantages des bases de donn√©es (la s√©curisation
des donn√©es) avec ceux des fichiers. 

## Qu'est-ce que le syst√®me de stockage `S3` ?

Dans les entreprises et administrations,
un nombre croissant de donn√©es sont
disponibles depuis un syst√®me de stockage
nomm√© `S3`. 
Le syst√®me `S3` (*Simple Storage System*) est un syst√®me de stockage d√©velopp√©
par Amazon et qui est maintenant devenu une r√©f√©rence pour le stockage en ligne.
Il s'agit d'une architecture √† la fois
s√©curis√©e (donn√©es crypt√©es, acc√®s restreints) et performante.

Le concept central du syst√®me S3 est le __*bucket*__.
Un *bucket* est un espace (priv√© ou partag√©) o√π on peut stocker une
arborescence de fichiers. Pour acc√©der aux fichiers figurant
dans un *bucket* priv√©, il faut des jetons d'acc√®s (l'√©quivalent d'un mot de passe)
reconnus par le serveur de stockage. On peut alors lire et √©crire dans le *bucket*.


::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Les exemples suivants seront r√©plicables pour les utilisateurs de la plateforme
SSP Cloud

```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true
print_badges("content/NLP/05a_s3.qmd", onyxia_only=True)
```

Ils peuvent √©galement l'√™tre pour des utilisateurs ayant un 
acc√®s √† AWS, il suffit de changer l'URL du `endpoint` 
pr√©sent√© ci-dessous. 

```{=html}
</div>
```
:::


## Comment faire avec Python ?

### Les librairies principales

L'interaction entre ce syst√®me distant de fichiers et une session locale de Python
est possible gr√¢ce √† des API. Les deux principales librairies sont les suivantes :

* [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), une librairie cr√©√©e et maintenue par Amazon ;
* [s3fs](https://s3fs.readthedocs.io/en/latest/), une librairie qui permet d'interagir avec les fichiers stock√©s √† l'instar d'un filesystem classique.

La librairie `pyarrow` que nous avons d√©j√† pr√©sent√©e permet √©galement
de traiter des donn√©es stock√©es sur le _cloud_ comme si elles
√©taient sur le serveur local. C'est extr√™mement pratique 
et permet de fiabiliser la lecture ou l'√©criture de fichiers
dans une architecture _cloud_. 
Un exemple, assez court, est disponible 
[dans la documentation officielle](https://arrow.apache.org/docs/python/filesystems.html#s3)

Il existe √©galement d'autres librairies permettant de g√©rer
des _pipelines_ de donn√©es (chapitre √† venir) de mani√®re
quasi indiff√©rente entre une architecture locale et une architecture
_cloud_. Parmi celles-ci, nous pr√©senterons quelques exemples 
avec `snakemake`. 
En arri√®re-plan, `snakemake`
va utiliser `boto3` pour communiquer avec le syst√®me
de stockage.


Enfin, selon le m√™me principe du _comme si_ les donn√©es
√©taient en local, il existe l'outil en ligne de commande
`mc` ([`Minio Client`](https://docs.min.io/docs/minio-client-complete-guide.html)) qui permet de g√©rer par des lignes
de commande Linux les d√©p√¥ts distants comme s'ils √©taient
locaux. 

Toutes ces librairies offrent la possibilit√© de se connecter depuis `Python`,
√† un d√©p√¥t de fichiers distant, de lister les fichiers disponibles dans un
*bucket*, d'en t√©l√©charger un ou plusieurs ou de faire de l'*upload*
Nous allons pr√©senter quelques unes des op√©rations les plus fr√©quentes,
en mode _cheatsheet_. 




## Connexion √† un bucket

Par la suite, on va utiliser des alias pour les trois valeurs suivantes, qui servent
√† s'authentifier. 

```python
key_id = 'MY_KEY_ID'
access_key = 'MY_ACCESS_KEY'
token = "MY_TOKEN"
```

Ces valeurs peuvent √™tre √©galement disponibles dans 
les variables d'environnement de `Python`. Comme il s'agit d'une information
d'authentification personnelle, il ne faut pas stocker les vraies valeurs de ces
variables dans un projet, sous peine de partager des traits d'identit√© sans le
vouloir lors d'un partage de code. 

::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Avec `boto3`, on cr√©√© d'abord un client puis on ex√©cute des requ√™tes dessus.
Pour initialiser un client, il suffit, en supposant que l'url du d√©p√¥t S3 est
`"https://minio.lab.sspcloud.fr"`, de faire:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

La logique est identique avec `s3fs`. 

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```


La logique d'`Arrow` est proche de celle de `s3fs`. Seuls les noms
d'arguments changent

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override="http://"+"minio.lab.sspcloud.fr")
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

La logique de `Snakemake` est, quant √† elle,
plus proche de celle de `boto3`. Seuls les noms
d'arguments changent

Si on a des jetons d'acc√®s √† jour et dans les variables d'environnement
ad√©quates:

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
```

```{=html}
</details>
```
:::




Il se peut que la connexion √† ce stade soit refus√©e (`HTTP error 403`).
Cela peut provenir 
d'une erreur dans l'URL utilis√©. Cependant, cela refl√®te plus g√©n√©ralement
des param√®tres d'authentification erron√©s.



::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Les param√®tres d'authentification sont des arguments suppl√©mentaires:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr",
                  aws_access_key_id=key_id, 
                  aws_secret_access_key=access_key, 
                  aws_session_token = token)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

La logique est la m√™me, seuls les noms d'arguments diff√®rent

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},
  key = key_id, secret = access_key,
  token = token)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

Tout est en argument cette fois:

```python
from pyarrow import fs

s3 = fs.S3FileSystem(
    access_key = key_id,
    secret_key = access_key,
    session_token = token,
    endpoint_override = 'https://'+'minio.lab.sspcloud.fr',
    scheme = "https"
    )
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

La logique est la m√™me, seuls les noms d'arguments diff√®rent

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'), access_key_id=key_id, secret_access_key=access_key)
```

```{=html}
</details>
```
:::




::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Dans le SSP Cloud, 
lorsque l'initialisation du service `Jupyter` du SSP Cloud est r√©cente
(moins de 12 heures), il est possible d'utiliser
automatiquement les jetons stock√©s automatiquement √† la cr√©ation du d√©p√¥t. 

Si on d√©sire acc√©der aux donn√©es du SSP Cloud depuis une session Python du
datalab (service VSCode, Jupyter...),
il faut remplacer l'url par `http://minio.lab.sspcloud.fr`
```{=html}
</div>
```
:::


## Lister les fichiers

S'il n'y a pas d'erreur √† ce stade, c'est que la connexion est bien effective.
Pour le v√©rifier, on peut essayer de faire la liste des fichiers disponibles
dans un `bucket` auquel on d√©sire acc√©der.

Par exemple, on peut vouloir
tester l'acc√®s aux bases `FILOSOFI` (donn√©es de revenu localis√©es disponibles
sur <https://www.insee.fr>) au sein du bucket `donnees-insee`. 



::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

Pour cela,
la m√©thode `list_objects` offre toutes les options n√©cessaires:

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
for key in s3.list_objects(Bucket='donnees-insee', Prefix='diffusion/FILOSOFI')['Contents']:
    print(key['Key'])
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

Pour lister les fichiers, c'est la m√©thode `ls` (celle-ci ne liste pas par
d√©faut les fichiers de mani√®re r√©cursive comme `boto3`):

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
fs.ls("donnees-insee/diffusion")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

```python
from pyarrow import fs
s3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')
s3.get_file_info(fs.FileSelector('donnees-insee/diffusion', recursive=True))
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```


```shell
mc ls -r
```

```{=html}
</details>
```
:::





## T√©l√©charger un fichier depuis `S3` pour l'enregistrer en local

Cette m√©thode n'est en g√©n√©ral pas recommand√©e car, comme on va le voir
par la suite, il est possible de lire √† la vol√©e des fichiers. Cependant,
t√©l√©charger un fichier depuis le _cloud_ pour l'√©crire sur le disque
local peut parfois √™tre utile (par exemple, lorsqu'il est n√©cessaire
de d√©zipper un fichier). 


::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

On utilise cette fois la m√©thode `download_file`

```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
s3.download_file('donnees-insee', "diffusion/FILOSOFI/2014/FILOSOFI_COM.csv", 'data.csv')
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

```python
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
fs.download('donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv','test.csv')
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier = S3.remote(f'{bucket}/moninput.csv')
    output:
        fichier='mon_dossier_local/monoutput.csv'
    run:
        shell("cp {input[0]} {output[0]}")
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```

```python
mc cp "donnees-insee/FILOSOFI/2014/FILOSOFI_COM.csv" 'data.csv'
```

```{=html}
</details>
```
:::





## Lire un fichier directement

La m√©thode pr√©c√©dente n'est pas optimale. En effet, l'un des int√©r√™ts des API
est qu'on peut traiter un fichier sur `S3` comme s'il s'agissait d'un fichier
sur son PC. Cela est d'ailleurs une mani√®re plus s√©curis√©e de proc√©der puisqu'on
lit les donn√©es √† la vol√©e, sans les √©crire dans un filesystem local. 


::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```


```python
import boto3
s3 = boto3.client("s3",endpoint_url = "https://minio.lab.sspcloud.fr")
obj = s3.get_object(Bucket='donnees-insee', Key="diffusion/FILOSOFI/2014/FILOSOFI_COM.csv")
df = pd.read_csv(obj['Body'], sep = ";")
df.head(2)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```

Le code suivant devrait permettre d'effectuer la m√™me op√©ration avec `s3fs`

```python
import pandas as pd
import s3fs
fs = s3fs.S3FileSystem(
  client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})
df = pd.read_csv(fs.open('{}/{}'.format('donnees-insee', "diffusion/FILOSOFI/2014/FILOSOFI_COM.csv"),
                         mode='rb'), sep = ";"
                 )

df.head(2)
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```

```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier = S3.remote(f'{bucket}/moninput.csv')
    run:
        import pandas as pd
        df = pd.read_csv(input.fichier)
        # PLUS D'OPERATIONS
```

```{=html}
</details>
```
:::


::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```

`Arrow` est une librairie qui permet de lire des `CSV`.
Il est n√©anmoins
beaucoup plus pratique d'utiliser le format `parquet` avec `arrow`. 
Dans un premier temps, on configure le _filesystem_ avec les 
fonctionalit√©s d'`Arrow` (cf. pr√©c√©demment). 

```{python}
from pyarrow import fs

s3 = fs.S3FileSystem(endpoint_override='http://'+'minio.lab.sspcloud.fr')
```

Pour lire un csv, on fera:

```python
from pyarrow import fs
from pyarrow import csv

s3 = fs.S3FileSystem(endpoint_override='https://'+'minio.lab.sspcloud.fr')

with s3.open_input_file("donnees-insee/diffusion/FILOSOFI/2014/FILOSOFI_COM.csv") as file:
    df = csv.read_csv(file, parse_options=csv.ParseOptions(delimiter=";")).to_pandas()
```

Pour un fichier au format parquet, la d√©marche est plus simple gr√¢ce √† l'argument
`filesystem` dans `pyarrow.parquet.ParquetDataset` :

```python
import pyarrow.parquet as pq

#bucket = ""
#parquet_file=""
df = pq.ParquetDataset(f'{bucket}/{parquet_file}', filesystem=s3).read_pandas().to_pandas()
```

```{=html}
</details>
```
:::




## Uploader un fichier

::: {.cell .markdown}
```{=html}
<details><summary><code>boto3</code> üëá</summary>
```

```python
s3.upload_file(file_name, bucket, object_name)
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>S3FS</code> üëá</summary>
```


```python
fs.put(filepath, f"{bucket}/{object_name}", recursive=True)
```

```{=html}
</details>
```
:::

::: {.cell .markdown}
```{=html}
<details><summary><code>Arrow</code> üëá</summary>
```


Supposons que `df` soit un `pd.DataFrame` 
Dans un syst√®me local, on convertirait
en table `Arrow` puis on √©crirait en `parquet`
([voir la documentation officielle](https://arrow.apache.org/docs/python/parquet.html#reading-and-writing-single-files)).
Quand on est sur un syst√®me `S3`, il s'agit seulement d'ajouter
notre connexion √† `S3` dans l'argument `filesystem`
([voir la page sur ce sujet dans la documentation Arrow](https://arrow.apache.org/docs/python/filesystems.html#filesystem-s3))

```python
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.Table.from_pandas(df)
pq.write_table(table, f"{bucket}/{path}", filesystem=s3)
```

```{=html}
</details>
```
:::



::: {.cell .markdown}
```{=html}
<details><summary><code>Snakemake</code> üëá</summary>
```


```python
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider(host = "https://" + os.getenv('AWS_S3_ENDPOINT'))
bucket = "mon-bucket"

rule ma_super_regle_s3:
    input:
        fichier='mon_dossier_local/moninput.csv'
    output:
        fichier=S3.remote(f'{bucket}/monoutput.csv')
    run:
        shell("cp output.fichier input.fichier")
```


```{=html}
</details>
```
:::



::: {.cell .markdown}
```{=html}
<details><summary><code>mc</code> üëá</summary>
```


```python
mc cp 'data.csv' "MONBUCKET/monoutput.csv"
```

```{=html}
</details>
```
:::





## Pour aller plus loin

- [La documentation sur MinIO du SSPCloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees)
