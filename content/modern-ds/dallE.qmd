---
title: "Génération d'images avec Python, DALL-E et StableDiffusion"
date: 2022-08-26T13:00:00Z
draft: false
weight: 60
slug: dalle
tags:
  - Deep Learning
  - DallE
  - Stable Diffusion
  - Tutoriel
  - Image
  - Thèmes avancés
categories:
  - Tutoriel
  - Avancé
type: book
description: |
  La _hype_ autour du
  modèle de génération d'image `Dall-E` a amené 
  une grande attention sur les modèles
  autogénératifs de contenu. `Dall-E` est, à l'heure
  actuelle, le modèle le plus célèbre de génération
  d'image à partir de texte. Il est maintenant
  possible de créer, depuis `Python` grâce à 
  l'implémentation de [`StableDiffusion`](https://huggingface.co/CompVis/stable-diffusion-v1-4),
  soit-même
  ses propres images rigolotes.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/featured_dallE.png
---

Pour tester les exemples présentés dans ce chapitre:

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/modern-ds/dallE.qmd", GPU = True)
```
:::

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

L'utilisation de ce tutoriel est assez exigeante en termes d'infrastructure
car il est nécessaire de disposer de __GPU__.
Les GPU sont des __ressources rares et assez chères__, elle ne sont donc pas mises à disposition de façon
aussi aisées que les CPU dans les _cloud providers_. Il s'agit de plus
de ressources __plus polluantes__ que les CPU.

Des GPU sont disponibles sur `Google Colab`, la procédure pour les activer
est expliquée ci-dessous. Des GPU sont également disponibles sur le `SSPCloud`
mais sont à utiliser avec parcimonie. Elles ne sont pas mises à disposition
par défaut car il s'agit d'une ressource rare. Ce chapitre, lancé depuis le 
bouton en début de chapitre, active cette option pour permettre la réplication
des exemples. 

```{=html}
</div>
```
:::

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```

Par défaut, `Colab` n'utilise pas de GPU mais de la CPU. Il est donc nécessaire
d'éditer les paramètres d'exécution du Notebook

- Dans le menu `Exécution`, cliquer sur `Modifier le type d'exécution` ;
- Sélectionner `GPU` sous `Accélérateur matériel`.

```{=html}
</div>
```
:::

# Contexte

La publication en avril 2022 par l'organisation [Open AI](https://openai.com/) de
son modèle de génération de contenu créatif [`Dall-E-2`](https://openai.com/dall-e-2/)
(un jeu de mot mélangeant Dali et Wall-E) a créé un bruit inédit dans
le monde de la _data science_.
A propos de `Dall-E`, le bloggeur _tech_ Casey Newton a pu parler d'une 
[révolution créative dans le monde de l'IA](https://www.platformer.news/p/how-dall-e-could-power-a-creative).
_[The Economist](https://www.economist.com/news/2022/06/11/how-a-computer-designed-this-weeks-cover)_ a par consacré
un numéro au sujet de l'émergence de ces intelligences artificielles
créatrices de contenu. 

Ce bruit sur la capacité des
intelligences artificielle à générer du contenu créatif
a d'ailleurs été amplifié plus récemment
avec la publication du _chatbot_ `chatGPT`
(voir [cet éditorial du _Guardian_](https://www.theguardian.com/commentisfree/2022/dec/10/i-wrote-this-column-myself-but-how-long-before-a-chatbot-could-do-it-for-me)).


L'inconvénient principal de `Dall-E`
pour générer facilement du contenu
est que le nombre de contenu pouvant être créé
avec un accès gratuit est limité (50 crédits gratuits par mois).
Depuis le 22 Août 2022, un générateur de contenu 
similaire est disponible gratuitement,
avec une licence plus permissive[^3]. Ce générateur, développé
par une équipe de chercheurs [@Rombach_2022_CVPR], 
s'appelle `Stable Diffusion` ([dépôt `Github` pour le code source](https://github.com/CompVis/stable-diffusion) et
[dépôt `HuggingFace` pour le modèle mis à disposition](https://huggingface.co/CompVis/stable-diffusion-v1-4)[^4]).
Un [excellent article de blog](https://huggingface.co/blog/stable_diffusion) décrit la démarche de `Stable Diffusion`. La plupart des exemples originaux
dans cette partie seront basés sur `Stable Diffusion`.


[^3]: Il est notamment possible de réutiliser l'image générée à des fins commerciales. En revanche, il est interdit de chercher à nuire à une personne. Pour cette raison, il est fréquent que les visages de personnes célèbres soient floutés pour éviter la création de contenu nuisant à leur réputation.

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> <code>HuggingFace</code></h3>
```

`Huggingface` est une plateforme de partage de modèles de type réseau de neurone. Les utilisateurs de réseaux de neurones peuvent
ainsi mettre à disposition le résultat de leurs travaux sous forme d'API pour faciliter la réutilisation de leurs
modèles ou réutiliser facilement des modèles, ce qui évite de les ré-entraîner (ce qui aurait un coût écologique non 
négligeable comme expliqué dans le chapitre introductif).

```{=html}
</div>
```
:::


`Dall-E-2` et `StableDiffusion`
sont des modèles généralistes. 
D'autres modèles, plus spécialisés, 
existent également. 
Le modèle [`Midjourney`](https://en.wikipedia.org/wiki/Midjourney) 
(produit propriétaire de la société du même nom)
permet la production de contenu
artistique, [DreamBooth](https://dreambooth.github.io/) (développé par Google)
est spécialisé dans la génération de contenu dans un nouveau
contexte. 

Le principe de tous ces modèles est le même: un utilisateur
donne une instruction (une ou plusieurs phrases) et l'intelligence
artificielle l'interprète et génère une image censée être
cohérente avec l'instruction. 

Voici par exemple l'une des productions possibles de `DALL-E-2`

![](https://upload.wikimedia.org/wikipedia/commons/2/2b/A_Shiba_Inu_dog_wearing_a_beret_and_black_turtleneck_DALLE2.jpg)

_"A Shiba Inu dog wearing a beret and black turtleneck"_


`Midjourney`, spécialisé dans le contenu esthétique,
génèrera l'image suivante avec l'instruction  _"mechanical dove"_ :

![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/A_mechanical_dove_8274822e-52fe-40fa-ac4d-f3cde5a332ae.png/250px-A_mechanical_dove_8274822e-52fe-40fa-ac4d-f3cde5a332ae.png)

`StableDiffusion`, modèle généraliste comme `Dall-E`,
crééra le contenu suivant avec
l'instruction _"A photograph of an astronaut riding a horse"_ :

!["A photograph of an astronaut riding a horse"](https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/A_photograph_of_an_astronaut_riding_a_horse_2022-08-28.png/250px-A_photograph_of_an_astronaut_riding_a_horse_2022-08-28.png)

Enfin, `DreamBooth` pourra lui introduire un chien dans une grande variété
de contextes:

![](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)


Un compte _Twitter_ ([Weird AI Generations](https://twitter.com/weirddalle))
propose de nombreuses générations de contenu drôles ou incongrues. 
Voici un premier exemple de production humoristique faite à partir de Mini Dall-E, la version
publique: 

::: {.cell .markdown}
```{=html}
<blockquote class="twitter-tweet"><p lang="zxx" dir="ltr"><a href="https://t.co/DIerJPtXGE">pic.twitter.com/DIerJPtXGE</a></p>&mdash; Weird Ai Generations (@weirddalle) <a href="https://twitter.com/weirddalle/status/1556027692163760130?ref_src=twsrc%5Etfw">August 6, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
```
:::

Ainsi qu'un deuxième:

::: {.cell .markdown}
```{=html}
<blockquote class="twitter-tweet"><p lang="zxx" dir="ltr"><a href="https://t.co/Ju0Pdcokth">pic.twitter.com/Ju0Pdcokth</a></p>&mdash; Weird Ai Generations (@weirddalle) <a href="https://twitter.com/weirddalle/status/1556573904600268801?ref_src=twsrc%5Etfw">August 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
```
:::

Les modèles `Dall-E-2` et `Stable Diffusion`
s'appuient sur des réseaux de neurones à différents niveaux :

- le contenu de la phrase est analysé par un réseau de neurones similaire (mais bien sûr plus évolué) que 
ceux que nous avons présenté dans la partie [NLP](#nlp)
- les éléments importants de la phrase (recontextualisés) sont ensuite transformés en image à partir de
modèles entraînés à reconnaître des images

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png)

Illustration du fonctionnement de ce type de générateur d'image (ici à partir de `Stable Diffusion`)

# Générer du contenu avec Dall-E via l'API d'OpenAI

::: {.cell .markdown}
```{=html}
<div class="alert alert-danger" role="alert">
<i class="fa-solid fa-triangle-exclamation"></i> Warning</h3>
```

Les services d'`OpenAI` ne sont gratuits que dans une certaine
limite. Votre clé d'API est donc assez précieuse car si elle
est usurpée, elle peut permettre à certaines personnes
d'épuiser vos crédits gratuits voire d'utiliser des crédits
payants à votre place. 

Si vous êtes enregistrés récemment dans le service d'API 
d'`OpenAI`, vous avez accès à des crédits gratuits. Ne les
utilisez néanmoins pas avec trop de légèreté en ne contrôlant
pas les paramètres de vos appels aux API car ces crédits
sont pour l'ensemble des services d'`OpenAI`(`chatGPT`,
`Dall-E`, `DaVinci`...)


```{=html}
</div>
```
:::


Le contenu de cette partie s'appuie sur 
le tutoriel du site [realpython](https://realpython.com/generate-images-with-dalle-openai-api/)
L'utilisation de `Dall-E` sera faite via le package `openai` qui donne
accès à l'[API d'OpenAI](https://beta.openai.com/docs/api-reference?lang=python).
Pour l'installer depuis la cellule d'un `Notebook`:

```{python}
#| eval: false
!pip install openai
```

Après avoir obtenu votre [clé d'API](https://realpython.com/generate-images-with-dalle-openai-api/#get-your-openai-api-key), on va supposer que celle-ci
est stockée dans une variable `key`:

```{python}
key = "sk-XXXXXXXXXX" #remplacer avec votre clé
```

Ensuite, l'utilisation de l'API est assez directe:

```{python}
#| eval: false
openai.api_key = key
openai.Image.create(
  prompt="Teddy bears working on new AI research underwater with 1990s technology",
  n=2,
  size="1024x1024"
)
```

L'_output_ est un JSON avec les URL des images générées.
Voici les deux images générées :

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/dallE.png)

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/dallE2.png)

Pour aller plus loin, vous pouvez consulter
le [tutoriel de `realpython`](https://realpython.com/generate-images-with-dalle-openai-api/#get-your-openai-api-key)

# Comment utiliser `Stable Diffusion` ?

[`Stable Diffusion`](https://github.com/CompVis/stable-diffusion) est
une intelligence artificielle créatrice de contenu qui permet de 
générer du contenu à partir d'une phrase - ce pour quoi nous allons
l'utiliser - mais aussi modifier des images à partir d'instructions. 

`Stable Diffusion` est un modèle plus pratique à utiliser depuis `Python`
que `Dall-E`. Celui-ci
est _open-source_ et peut être téléchargé et réutilisé directement depuis `Python`. 
La méthode la plus pratique est d'utiliser le modèle mis
à disposition sur `HuggingFace`. Le modèle est implémenté
à travers le _framework_ [`PyTorch`](https://pytorch.org/). 

[`PyTorch`](https://pytorch.org/), librairie développée
par `Meta`, n'est pas implementé directement en `Python`
pour des raisons de performance mais en `C++` - `Python` étant un
langage lent, le revers de la médaille de sa facilité 
d'usage. A travers `Python`, on va utiliser une API haut niveau
qui va contrôler la structure des réseaux de neurones ou 
créer une interface entre des
données (sous forme d'array `Numpy`) et le modèle. 
Pour ce type de _packages_ qui utilisent un langage compilé, 
l'installation via `Pandas` 

::: {.cell .markdown}
```{=html}
<details><summary>Configuration spécifique à <code>Colab</code> 👇</summary>
```


Sur `Colab`, `conda` n'est pas disponible par défaut.
Pour pouvoir
installer un package en utilisant `conda` sur `Colab`,
on utilise donc l'astuce
suivante :

```python
!pip install -q condacolab
import condacolab
condacolab.install()
```

```{=html}
</details>
```
:::


## Installation de `PyTorch`

Pour installer `PyTorch`, la librairie de _Deep Learning_
développée par `Meta`, il suffit de suivre les recommandations
sur le [site web officiel](https://pytorch.org/).
Dans un `Notebook`, cela prendra la forme suivante :

```{python}
#| eval: false
!conda install mamba
!mamba install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Je propose ici d'utiliser `mamba` pour accélérer l'installation.
Des éléments sur `mamba` sont disponibles dans l'introduction de ce cours.

```{=html}
</div>
```
:::


## Accès à `HuggingFace`

La question - non négligeable - de l'accès à
de la GPU mise à part,
la réutilisation des modèles de `Stable Diffusion` est
très facile car la documentation mise à disposition sur 
`HuggingFace` est très bien faite.

La première étape est de se créer un compte sur `HuggingFace`
et se créer un _token_[^4]. Ce _token_ sera donné à l'API
de `HuggingFace` pour s'authentifier. 

[^4]: Comme les autres plateformes du monde de la _data science_,
`HuggingFace` a adopté l'utilisation standardisée des
jetons (_token_) comme méthode d'authentification. Le jeton est
comme un mot de passe sauf qu'il n'est pas inventé par l'utilisateur
(ce qui permet qu'il ne soit pas partagé avec d'autres sites web potentiellement
moins sécurisés), est révocable (date d'expiration ou choix de l'utilisateur)
et dispose de droits moins importants qu'un
mot de passe qui vous permet, potentiellement,
de changer tous les paramètres de votre compte. Je recommande vivement l'utilisation
d'un gestionnaire de mot de passe pour
stocker vos _token_ (si vous utilisez `Git`, `Docker`, etc.
vous en avez potentiellement beaucoup) plutôt que
stocker ces jetons dans des fichiers non sécurisés. 

L'API d'`HuggingFace` nécessite l'installation du
package [`diffusers`](https://huggingface.co/docs/transformers/installation).
Dans un `Notebook`, le code suivant permet d'installer la librairie
requise:

```{python}
#| eval: false
!pip install --upgrade diffusers transformers scipy accelerate
```


::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

On va supposer que le _token_ est stocké dans une variable
d'environnement `HF_PAT`. Cela évite d'écrire le _token_
dans un _Notebook_ qu'on va
potentiellement partager, alors que le _token_
est un élément à garder secret. Pour l'importer
dans la session `Python`:

Si vous n'avez pas la possibilité de rentrer le token dans les variables
d'environnement, créez une cellule qui crée la variable
`HF_TOKEN` et supprimez là de suite pour ne pas l'oublier avant
de partager votre token. 

```{=html}
</div>
```
:::

```{python}
#| eval: false
import os
HF_TOKEN = os.getenv('HF_PAT')
```



# Générer une image

On va créer l'image suivante : 

> __"Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene"__

Pas mal comme scénario, non ?! 

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```


Pour que les résultats soient reproductibles entre différentes
sessions, 
nous allons fixer
la racine du générateur aléatoire. 

```{=html}
</div>
```
:::


```{python}
#| eval: false
import torch
generator = torch.Generator("cuda").manual_seed(123)
```

Si vous voulez vous amuser à explorer différents résultats
pour un même texte, vous pouvez ne pas fixer de racine aléatoire.
Dans ce cas, retirer l'argument `generator` des codes présentés
ultérieurement. 

Nous allons donc utiliser l'instruction suivante :

```{python}
prompt = "Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene"
```

L'initialisation du modèle se fait de la manière
suivante :

```{python}
#| eval: false
import torch
from torch import autocast
from diffusers import StableDiffusionPipeline

model_id = "CompVis/stable-diffusion-v1-4"
device = "cuda"

generator = torch.Generator("cuda").manual_seed(1024)
```

Enfin, pour générer l'image: 

```{python}
#| eval: false

pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)
pipe = pipe.to(device)

with autocast("cuda"):
    image = pipe(prompt, guidance_scale=7.5, generator = generator)["images"][0]  

   
image.save("featured.png")
```

Qui peut être visualisé avec le code suivant, dans un `notebook`:

```{python}
#| eval: false
from IPython.display import Image 
pil_img = Image(filename="featured.png")
display(pil_img)
```

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/featured.png)

C'est une représentation assez fidèle du
pitch _"Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene"_ :boom:.
Y a un petit côté [_Les Dix Commandements_](https://fr.wikipedia.org/wiki/Les_Dix_Commandements_(film,_1956)#/media/Fichier:Charlton_Heston_in_The_Ten_Commandments_film_trailer.jpg) que j'aime bien. 


En voici une autre que j'aime bien (mais malheureusement je ne peux la reproduire car je n'ai pas
gardé en mémoire la racine l'ayant généré :sob:)

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/chuck.png)

Il est également possible de générer plusieurs images du même texte (voir
la [note de blog](https://huggingface.co/blog/stable_diffusion) de l'équipe
à l'origine de `Stable Diffusion`). Cependant, c'est assez exigeant en
mémoire et cela risque d'être impossible sur `Colab` (y compris
en réduisant le poids des vecteurs numériques comme proposé dans le _post_)


# Bonus

Pour le plaisir, voici `PuppyMan`, le dernier né du Marvel Universe:

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/puppyman.png)


```{python}
prompt = "In a new Marvel film we discover puppyman a new super hero that is half man half bulldog"
```

```{python}
#| eval: false
import torch
from torch import autocast
from diffusers import StableDiffusionPipeline

model_id = "CompVis/stable-diffusion-v1-4"
device = "cuda"

generator = torch.Generator("cuda").manual_seed(1024)

pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=HF_TOKEN, generator=generator)
pipe = pipe.to(device)

with autocast("cuda"):
    image = pipe(prompt, guidance_scale=7.5, generator = generator)["images"][0]  

   
image.save("puppyman.png")
```

La moitié humain semble être son costume de super-héros, pas la bipédie.
Mais le rendu 
est quand même épatant !

A vous de jouer :hugging_face:

# Références

::: {#refs}
:::
