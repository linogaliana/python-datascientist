---
title: "Les embeddings, ou comment synthétiser l'information textuelle"
title-en: "Synthetizing textual information with embeddings"
author: Lino Galiana
description: |
  Pour pouvoir utiliser des données textuelles dans des algorithmes de _machine learning_, il faut les vectoriser, c'est à dire transformer le texte en données numériques. Dans ce TP, nous allons comparer différentes méthodes de vectorisation, à travers une tâche de prédiction : _peut-on prédire un auteur littéraire à partir d'extraits de ses textes ?_ Parmi ces méthodes, on va notamment explorer le modèle `Word2Vec`, qui permet d'exploiter les structures latentes d'un texte en construisant des _word embeddings_ (plongements de mots).
description-en: |
  To be able to use textual data in machine learning algorithms, we need to vectorize text, i.e. transform that object into numerical data. In this tutorial, we will compare different vectorization methods, using a prediction task: _can we predict a literary author from extracts of his texts?_ Among these methods, we will explore the `Word2Vec` model, which allows us to exploit the latent structures of a text by building word embeddings.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/librarian.png
echo: false
bibliography: ../../reference.bib
---

::: {.callout-warning}
Ce chapitre va évoluer prochainement.
:::

{{< badges
    printMessage="true"
    sspCloudService="pytorch"
>}}

:::: {.content-visible when-profile="fr"}

# Introduction

Cette page approfondit certains aspects présentés dans la
[partie introductive](/content/NLP/02_exoclean.qmd).
Nous allons avancer dans notre compréhension des enjeux du NLP grâce à la
modélisation du langage.

Nous partirons de la conclusion, évoquée à la fin du précédent chapitre, que les approches fréquentistes présentent plusieurs
inconvénients : représentation du langage sur des régularités statistiques indépendantes des proximités entre des mots ou phrases, difficulté à prendre en compte le contexte.

L'objectif de ce chapitre est d'évoquer le premier point. Il s'agira d'une introduction au sujet des _embeddings_, ces représentations du langage qui sont au fondement des modèles de langage actuels utilisés par des outils entrés dans notre quotidien (`DeepL`, `ChatGPT`...).

## Données utilisées

Nous allons continuer notre exploration de la littérature
avec, à nouveau, les trois auteurs anglophones :

* Edgar Allan Poe, (EAP) ;
* HP Lovecraft (HPL) ;
* Mary Wollstonecraft Shelley (MWS).

Les données sont disponibles sur un CSV mis à disposition sur [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). L'URL pour les récupérer directement est
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Pour rentrer dans le sujet des _embeddings_, nous allons faire de la modélisation du langage en essayant de prédire les auteurs ayant écrit tel ou tel texte. On parle de modèle de langage pour désigner la représentation d'un texte ou d'une langue sous la forme d'une distribution de probabilités de termes (généralement les mots).

::: {.callout-note}
## Sources d'inspiration
Ce chapitre s'inspire de plusieurs ressources disponibles en ligne:

* Un [premier _notebook_ sur `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)
et un [deuxième](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook
) ;
* Un [dépôt `Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg) ;
:::

## Packages à installer

Comme dans la [partie précédente](/content/NLP/02_exoclean.qmd), il faut télécharger des librairies
spécialiséees pour le NLP, ainsi que certaines de leurs dépendances. Ce TD utilisera plusieurs librairies dont certaines dépendent de `PyTorch` qui est une librairie volumineuse.

::: {.callout-important}
## `PyTorch` sur le `SSPCloud`

__La prochaine remarque ne concerne que les utilisateurs.trices du `SSPCloud`.__

Les services `Python` standards sur le  `SSPCloud` (les services `vscode-python` et `jupyter-python`) ne proposent pas `PyTorch` préinstallé. Cette librairie est en effet assez volumineuse (de l'ordre de 600Mo) et nécessite un certain nombre de configurations _ad hoc_ pour fonctionner de manière transparente quelle que soit la configuration logicielle derrière. Pour des raisons de frugalité écologique, cet environnement _boosté_ n'est pas proposé par défaut. Néanmoins, si besoin, un tel environnement où `Pytorch` est pré à l'emploi est disponible.

Pour cela, il suffit de démarrer un service `vscode-pytorch` ou `jupyter-pytorch`. Si vous avez utilisé l'un des boutons disponibles ci-dessus, c'est ce service standardisé qui a automatiquement été mis à disposition pour vous.

:::

::::

::: {.content-visible when-profile="en"}

# Introduction

This page builds on certain aspects presented in the [introductory section](/content/NLP/02_exoclean.qmd).
We will advance our understanding of NLP issues through language modeling.

We start from the conclusion noted at the end of the previous chapter: frequentist approaches have several shortcomings, such as modeling language based on statistical regularities without considering word or phrase proximity, and difficulty incorporating context.

The aim of this chapter is to address the first of those points. This will serve as an introduction to _embeddings_, the language representations at the core of modern language models used in everyday tools like `DeepL` or `ChatGPT`.

## Data Used

We will continue our exploration of literature using the same three English-language authors:

* Edgar Allan Poe (EAP);
* HP Lovecraft (HPL);
* Mary Wollstonecraft Shelley (MWS).

The dataset is available in a CSV file hosted on [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv), and can be directly downloaded from:
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

To explore the topic of _embeddings_, we will use a language modeling task: predicting the author of a given text. A language model represents a text or language as a probability distribution over terms (usually words).

:::: {.callout-note}
## Sources of Inspiration
This chapter is inspired by several online resources:

* A [first notebook on `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)
and [a second one](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook);
* A [Github repository](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg);
:::

## Required Packages

As in the [previous section](/content/NLP/02_exoclean.qmd), we need to install specialized NLP libraries along with their dependencies. This tutorial will use several libraries, including some that depend on `PyTorch`, which is a large framework.

::: {.callout-important}
## `PyTorch` on `SSPCloud`

__The following note is only relevant for users of `SSPCloud`.__

The standard `Python` services on `SSPCloud` (such as `vscode-python` and `jupyter-python`) do not include `PyTorch` by default. This library is quite large (around 600MB) and requires specific configuration to work seamlessly across different software environments. For ecological sustainability, this enhanced environment is not provided by default. However, when needed, an environment with `PyTorch` preinstalled is available.

To access it, simply start a `vscode-pytorch` or `jupyter-pytorch` service. If you used one of the buttons above, this pre-configured service was automatically launched for you.

:::

::::

```{python}
#| eval: false
#| echo: true
!pip install numpy pandas spacy transformers scikit-learn langchain_community
```

::: {.content-visible when-profile="fr"}

Ensuite, comme nous allons utiliser la librairie `SpaCy` avec un corpus de textes
en Anglais, il convient de télécharger le modèle NLP pour l'Anglais. Pour cela,
on peut se référer à [la documentation de `SpaCy`](https://spacy.io/usage/models),
extrêmement bien faite.

:::

::: {.content-visible when-profile="en"}

Next, since we will be using the `SpaCy` library with a corpus of English texts,
we need to download the English NLP model. For this, you can refer to
[the official `SpaCy` documentation](https://spacy.io/usage/models),
which is extremely well-designed.

:::


```{python}
#| echo: true
#| output: false
!python -m spacy download en_core_web_sm
```


::: {.content-visible when-profile="fr"}

# Préparation des données

Nous allons à nouveau utiliser le jeu de données `spooky` :

:::

::: {.content-visible when-profile="en"}

# Data Preparation

We will once again use the `spooky` dataset:

:::

```{python}
#| echo: true
import pandas as pd

data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'
spooky_df = pd.read_csv(data_url)
```

::: {.content-visible when-profile="fr"}

Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite :

:::

::: {.content-visible when-profile="en"}

The dataset pairs each author with a sentence they wrote:

:::

```{python}
#| echo: true
spooky_df.head()
```

::: {.content-visible when-profile="fr"}

## Preprocessing

Comme nous l'avons évoqué dans le chapitre précédent, la première étape de tout travail sur données textuelles est souvent celle du *preprocessing*, qui inclut notamment les étapes de tokenization et de nettoyage du texte.

On se contentera ici d'un *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les méthodes de vectorisation basées sur des comptages).

Pour initialiser le processus de nettoyage,
on va utiliser le corpus `en_core_web_sm` de `Spacy`

:::

::: {.content-visible when-profile="en"}

## Preprocessing

As discussed in the previous chapter, the first step in any work with textual data is often *preprocessing*, which typically includes tokenization and text cleaning.

Here, we will stick to minimal preprocessing: removing punctuation and stop words (for visualization and count-based vectorization methods).

To begin the cleaning process,
we will use the `en_core_web_sm` model from `Spacy`

:::


```{python}
#| echo: true
import spacy
nlp = spacy.load('en_core_web_sm')
```

::: {.content-visible when-profile="fr"}

On va utiliser un `pipe` `spacy` qui permet d'automatiser, et de paralléliser,
un certain nombre d'opérations. Les *pipes* sont l'équivalent, en NLP, de
nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s'agit donc d'un outil
très approprié pour industrialiser un certain nombre d'opérations de
*preprocessing* :

:::

::: {.content-visible when-profile="en"}

We will use a `spacy` `pipe` that automates and parallelizes
a number of operations. *Pipes* in NLP are similar to
`scikit` pipelines or `pandas` pipes. They are well-suited tools
for industrializing various *preprocessing* tasks:

:::

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Preprocessing de la base textuelle"
from typing import List
import spacy

def clean_docs(
    texts: List[str],
    remove_stopwords: bool = False,
    n_process: int = 4,
    remove_punctuation: bool = True
) -> List[str]:
    """
    Cleans a list of text documents by tokenizing, optionally removing stopwords, and optionally removing punctuation.

    Parameters:
        texts (List[str]): List of text documents to clean.
        remove_stopwords (bool): Whether to remove stopwords. Default is False.
        n_process (int): Number of processes to use for processing. Default is 4.
        remove_punctuation (bool): Whether to remove punctuation. Default is True.

    Returns:
        List[str]: List of cleaned text documents.
    """
    # Load spacy's nlp model
    docs = nlp.pipe(
        texts,
        n_process=n_process,
        disable=['parser', 'ner', 'lemmatizer', 'textcat']
    )

    # Pre-load stopwords for faster checking
    stopwords = set(nlp.Defaults.stop_words)

    # Process documents
    docs_cleaned = (
        ' '.join(
            tok.text.lower().strip()
            for tok in doc
            if (not remove_punctuation or not tok.is_punct) and
               (not remove_stopwords or tok.text.lower() not in stopwords)
        )
        for doc in docs
    )

    return list(docs_cleaned)
```

::: {.content-visible when-profile="fr"}

On applique la fonction `clean_docs` à notre colonne `pandas`.
Les `pandas.Series` étant itérables, elles se comportent comme des listes et
fonctionnent ainsi très bien avec notre `pipe` `spacy`.

:::

::: {.content-visible when-profile="en"}

We apply the `clean_docs` function to our `pandas` column.
Since `pandas.Series` are iterable, they behave like lists and
work very well with our `spacy` pipe.

:::

```{python}
#| echo: true
spooky_df['text_clean'] = clean_docs(spooky_df['text'])
```

```{python}
#| echo: true
spooky_df.head()
```

::: {.content-visible when-profile="fr"}

## Encodage de la variable à prédire

On réalise un simple encodage de la variable à prédire :
il y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.
Pour cela, on utilise le `LabelEncoder` de `Scikit` déjà présenté
dans la [partie modélisation](/content/modelisation/0_preprocessing.qmd). On va utiliser la méthode
`fit_transform` qui permet, en un tour de main, d'appliquer à la fois
l'entraînement (`fit`), à savoir la création d'une correspondance entre valeurs
numériques et _labels_, et l'appliquer (`transform`) à la même colonne.

:::

::: {.content-visible when-profile="en"}

## Encoding the Target Variable

We perform a simple encoding of the target variable:
there are three categories (authors), represented by integers 0, 1, and 2.
For this, we use `Scikit`'s `LabelEncoder`, previously introduced
in the [modeling section](/content/modelisation/0_preprocessing.qmd). We will use the `fit_transform` method, which conveniently combines
fitting (i.e., creating a mapping between numerical values and labels)
and transforming the same column in one step.

:::

```{python}
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])
```

::: {.content-visible when-profile="fr"}

On peut vérifier les classes de notre `LabelEncoder` :

:::

::: {.content-visible when-profile="en"}

We can check the classes of our `LabelEncoder`:

:::

```{python}
le.classes_
```


::: {.content-visible when-profile="fr"}

## Construction des bases d'entraînement et de test

On met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).
Cela permettra d'évaluer nos différents modèles toute à la fin de manière très rigoureuse,
puisque ces données n'auront jamais utilisées pendant l'entraînement.

Notre échantillon initial n'est pas équilibré (*balanced*) : on retrouve plus d'oeuvres de
certains auteurs que d'autres. Afin d'obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d'auteurs dans nos
ensembles d'entraînement et de test.

:::

::: {.content-visible when-profile="en"}

## Creating the Training and Test Sets

We set aside a test sample (20%) before performing any analysis (even descriptive).
This ensures a rigorous evaluation of our models at the end, since these data will never have been seen during training.

Our initial dataset is not balanced—some authors have more texts than others.
To ensure fair evaluation of our model, we will stratify the sampling so that the training and test sets contain a similar distribution of authors.

:::


```{python}
#| echo: true
from sklearn.model_selection import train_test_split

y = spooky_df["author"]
X = spooky_df['text_clean']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```


::: {.content-visible when-profile="fr"}

Aperçu du premier élément de `X_train` :

:::

::: {.content-visible when-profile="en"}

Preview of the first element in `X_train`:

:::

```{python}
X_train[0]
```


::: {.content-visible when-profile="fr"}

# Vectorisation par l'approche _bag of words_

La représentation de nos textes sous forme de sac de mot permet de vectoriser notre corpus et ainsi d'avoir une représentation numérique de chaque texte. On peut à partir de là effectuer plusieurs types de tâches de modélisation.

Définissons notre représentation vectorielle par TF-IDF grâce à `Scikit`:

:::

::: {.content-visible when-profile="en"}

# Vectorization Using the _Bag of Words_ Approach

Representing our texts as a bag of words allows us to vectorize the corpus and thus obtain a numerical representation of each text. From there, we can perform various types of modeling tasks.

Let's define our vector representation using TF-IDF with `Scikit`:

:::

```{python}
#| echo: true
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

pipeline_tfidf = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000)),
])
pipeline_tfidf
```

::: {.content-visible when-profile="fr"}

Entraînons d'ores et déjà notre modèle à vectoriser le texte à partir de la méthode TF-IDF. Pour le moment il n'est pas encore question de faire de l'évaluation, faisons donc un entraînement sur l'ensemble de notre base et pas seulement sur `X_train`.

:::

::: {.content-visible when-profile="en"}

Let's go ahead and train our model to vectorize the text using the TF-IDF method. At this stage, we are not yet concerned with evaluation, so we will train on the entire dataset, not just `X_train`.

:::

```{python}
#| echo: true
pipeline_tfidf.fit(spooky_df['text_clean'])
```

::: {.content-visible when-profile="fr"}

## Trouver le texte le plus similaire

En premier lieu, on peut chercher le texte le plus proche, au sens de TF-IDF, d'une phrase donnée. Prenons cet exemple:

:::

::: {.content-visible when-profile="en"}

## Finding the Most Similar Text

First, we can look for the text that is closest—according to TF-IDF similarity—to a given sentence. Let's take the following example:

:::

```{python}
#| echo: true
text = "He was afraid by Frankenstein monster"
```

::: {.content-visible when-profile="fr"}

Comment retrouver le texte le plus proche de celui-ci ? Il faudrait transformer notre texte dans cette même représentation vectorielle et rapprocher ensuite celui-ci des autres textes représentés sous cette même forme.

Cela revient à effectuer une tâche de recherche d'information, cas d'usage classique du NLP, mis en oeuvre par exemple par les moteurs de recherche. Le terme Frankenstein étant assez discrminant, nous devrions, grâce à TF-IDF, retrouver des similarités entre notre texte et d'autres textes écrits par Mary Shelley.

Une métrique régulièrement utilisée pour comparer des vecteurs est la similarité cosinus. Il s'agit d'ailleurs d'une mesure centrale dans l'approche moderne du NLP. Celle-ci a plus de sens avec des vecteurs dense, que nous verrons prochainement, qu'avec des vecteurs comportant beaucoup de 0 comme le sont les vecteurs _sparse_ des mesures _bag-of-words_. Néanmoins c'est déjà un exercice intéressant pour comprendre la similarité entre deux vecteurs.

Si chaque dimension d'un vecteur correspond à une direction, l'idée derrière la similarité cosinus est de mesurer l'angle entre deux vecteurs. L'angle sera réduit si les vecteurs sont proches.

![](https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png)

:::

::: {.content-visible when-profile="en"}

How can we find the text most similar to this one? We need to transform our sentence into the same vector representation, then compare it to the other texts using that same form.

This is essentially an information retrieval task—a classic NLP use case—implemented, for example, by search engines. Since the term "Frankenstein" is quite distinctive, we should be able to identify similarities with other texts written by Mary Shelley using TF-IDF.

A metric commonly used to compare vectors is cosine similarity. This is a central measure in modern NLP. While it is more meaningful with dense vectors (which we'll explore soon), it still provides a useful exercise for understanding similarity between two vectors, even when those vectors are sparse, as in the bag-of-words approach.

If each dimension of a vector represents a direction, cosine similarity measures the angle between two vectors. The smaller the angle, the closer the vectors.

![](https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png)

:::

::: {.content-visible when-profile="fr"}
### Avec `Scikit-Learn`
:::

::: {.content-visible when-profile="en"}
### With `Scikit-Learn`
:::

{{< include "03_embedding/exercise1.qmd" >}}


::: {.content-visible when-profile="fr"}
### Avec `Langchain`
:::

::: {.content-visible when-profile="en"}
### With `Langchain`
:::

::: {.content-visible when-profile="fr"}

Cette approche de calcul de similarité textuelle est assez laborieuse avec `Scikit`. Avec le développement continu d'applications `Python` utilisant des modèles de langage, un écosystème très complet s'est développé pour pouvoir faire ces tâches en quelques lignes de code avec `Python`.

Parmi les outils les plus précieux, nous trouvons [`Langchain`](https://www.langchain.com/), un écosystème `Python` haut-niveau permettant de construire des chaînes de production utilisant des données textuelles.

Nous allons ici procéder en 2 étapes:

- Créer un _retriever_, c'est-à-dire vectoriser avec TF-IDF notre corpus (les textes de nos trois auteurs) et les stocker sous forme de base de données vectorielle.
- Vectoriser à la volée notre texte de recherche (l'objet `text` créé précédemment) et rechercher sa contrepartie la plus proche dans la base de données vectorielle.

La vectorisation de notre corpus se fait très simplement grâce à `Langchain`
puisque le `TfidfVectorizer` de `Scikit` est encapsulé dans un module _ad hoc_ de `Langchain`

:::

::: {.content-visible when-profile="en"}

This approach to computing text similarity is rather tedious with `Scikit`. With the rapid development of `Python` applications leveraging language models, a rich ecosystem has emerged to make these tasks achievable in just a few lines of code.

Among the most valuable tools is [`Langchain`](https://www.langchain.com/), a high-level `Python` ecosystem for building production-ready pipelines using textual data.

We will proceed here in two steps:

- Create a _retriever_, which involves vectorizing our corpus (texts from the three authors) using TF-IDF and storing it in a vector database.
- Vectorize our search query (`text`, created earlier) on the fly and retrieve its closest match from the vector database.

Vectorizing our corpus is very straightforward using `Langchain`,
as `Scikit`'s `TfidfVectorizer` is wrapped in a dedicated module provided by `Langchain`.

:::

```{python}
#| echo: true
from langchain_community.retrievers import TFIDFRetriever
from langchain_community.document_loaders import DataFrameLoader

loader = DataFrameLoader(spooky_df, page_content_column="text_clean")

retriever = TFIDFRetriever.from_documents(
    loader.load()
)
```

::: {.content-visible when-profile="fr"}

Cet objet `retriever` est un point d'entrée sur notre corpus. `Langchain` présente l'intérêt de fournir plusieurs points d'entrées standardisés, forts utiles dans les projets de NLP puisqu'il suffit de changer les vectoriseurs en entrée sans avoir à changer leur usage en fin de chaîne.

La méthode `invoke` permet de rechercher les vecteurs les plus similaires à notre texte de recherche:

:::

::: {.content-visible when-profile="en"}

This `retriever` object serves as an entry point to our corpus. `Langchain` is particularly valuable in NLP projects because it provides standardized entry points, allowing you to easily switch out vectorizers without needing to change how the results are used at the end of the pipeline.

The `invoke` method is used to find the most similar vectors to our search query:

:::

```{python}
#| echo: true
retriever.invoke(text)
```

::: {.content-visible when-profile="fr"}

La sortie est un objet `Langchain`, ce qui n'est pas pratique pour nous dans notre situation. On se ramène à un _DataFrame_:

:::

::: {.content-visible when-profile="en"}

The output is a `Langchain` object, which is not convenient for our purposes here. We convert it into a _DataFrame_:

:::

```{python}
#| echo: true
documents = []
for best_echoes in retriever.invoke(text):
    documents += [{**best_echoes.metadata, **{"text_clean": best_echoes.page_content}}]

documents = pd.DataFrame(documents)
```

::: {.content-visible when-profile="fr"}

On peut ajouter à ce _DataFrame_ la colonne de score:

:::

::: {.content-visible when-profile="en"}

We can add the similarity score column to this _DataFrame_:

:::

```{python}
from sklearn.metrics.pairwise import cosine_similarity

documents['score'] = cosine_similarity(
    pipeline_tfidf.transform(documents['text_clean']),
    pipeline_tfidf.transform([text])
)
```

::: {.content-visible when-profile="fr"}

On retrouve bien les mêmes documents:

:::

::: {.content-visible when-profile="en"}

We do indeed retrieve the same documents:

:::

```{python}
documents
```

:::: {.content-visible when-profile="fr"}

::: {.callout-note}
## La métrique BM25

BM25 est un modèle de récupération d'informations basé sur la pertinence probabiliste, au même titre que TF-IDF. BM25 est souvent utilisée dans les moteurs de recherche pour classer les documents par rapport à une requête.

BM25 repose sur une combinaison de la fréquence des termes (TF), la fréquence inverse des documents (IDF), et une normalisation basée sur la longueur des documents. Autrement dit, il s'agit de tenir compte d'améliorer TF-IDF tout en normalisant les mesures en fonction de la taille des _strings_ afin de ne pas surpondérer les grands documents.

BM25 est donc particulièrement performant dans des environnements où les documents varient en longueur et en contenu. C'est pour cette raison que des moteurs de recherche comme `Elasticsearch` en ont fait une pierre angulaire du mécanisme de recherche.
:::

Pourquoi ne sont-ils pas tous pertinents ? On peut anticiper plusieurs raisons à cela.

La première hypothèse vient du fait qu'on entraîne notre vectoriseur sur un corpus biaisé. Certes Frankestein est un terme rare mais il est beaucoup plus fréquent dans notre corpus que dans la langue anglaise. L'_inverse document frequency_ est donc biaisée en défaveur de ce terme: son apparition devrait être un signe beaucoup plus fort que le texte recherché correspond à Mary Shelley. Si cela peut améliorer un peu la pertinence des résultats renvoyés, ce n'est néanmoins pas là que le bât blesse.

L'approche fréquentiste suppose que les termes sont aussi dissemblables les uns que les autres. Une phrase où apparaît le terme _"créature"_ ne bénéficiera pas d'un score positif si on recherche _"monstre"_. De plus, là encore, nous avons pris notre corpus comme un sac où les mots sont indépendants: on n'a pas plus de chance de tirer _"Frankenstein"_ après _"docteur"_. Ces limites vont nous amener vers le sujet des _embeddings_. Néanmoins, si l'approche fréquentiste est un peu _old school_, elle n'est néanmoins pas inutile et représente souvent une _"tough to beat baseline"_. Dans les domaines de l'extraction d'information avec des textes courts, où chaque terme est porteur d'un signal fort, cette approche est souvent judicieuse.

::::

:::: {.content-visible when-profile="en"}

::: {.callout-note}
## The BM25 Metric

BM25 is a probabilistic relevance-based information retrieval model, similar to TF-IDF. It is commonly used in search engines to rank documents relative to a query.

BM25 combines term frequency (TF), inverse document frequency (IDF), and a normalization based on document length. In other words, it improves on TF-IDF by adjusting scores based on string length to avoid overemphasizing longer documents.

BM25 performs particularly well in environments where documents vary in length and content. This is why search engines such as `Elasticsearch` have made it a cornerstone of their search mechanisms.
:::

Why aren't all results relevant? We can anticipate several reasons.

The first hypothesis is that we're training our vectorizer on a biased corpus. While “Frankenstein” is a rare term, it appears more frequently in our dataset than in general English usage. The inverse document frequency is thus biased against the term: its appearance should be a much stronger indicator that the text belongs to Mary Shelley. While addressing this might slightly improve relevance, it's not the core issue.

The frequentist approach assumes all terms are equally distinct. A sentence containing the word _“creature”_ won't get a higher score when searching for _“monster”_. Again, we've treated our corpus as a bag where words are independent—there's no increased likelihood of encountering _“Frankenstein”_ after _“doctor”_. These limitations point us toward the topic of _embeddings_. Even though the frequentist method may seem a bit _old school_, it's not useless and often provides a “tough to beat baseline.” In fields like information extraction from short texts, where every term carries strong signal, this approach is often effective.

::::



::: {.content-visible when-profile="fr"}

## Trouver l'auteur le plus proche: une introduction au classifieur naif Bayes

Avant d'explorer les _embeddings_, nous pouvons essayer d'avoir un cas d'usage un petit peu différent dans notre cadre probabiliste. Supposons qu'on désire maintenant faire de la prédiction d'auteur. Si l'intuition précédente est vraie - certains mots sont plus probables dans les textes de certains auteurs - cela veut dire qu'on peut entraîner un algorithme de classification automatique à prédire un auteur à partir d'un texte.

La méthode la plus naturelle pour se lancer dans cette approche est d'utiliser le classifieur naif de Bayes. Ce dernier est parfaitement adapté à l'approche fréquentiste que nous avons adoptée jusqu'à présent puisqu'il exploite les probabilités d'occurrence de mots par auteur.

Le classifieur naif de Bayes consiste à appliquer une règle de décision, à savoir sélectionner la classe la plus probable sachant la structure observée du document, c'est-à-dire les mots apparaissant dans celui-ci.

Autrement dit, on sélectionne la classe $\widehat{c}$ qui est la plus probable, sachant les termes dans le document $d$.

:::

::: {.content-visible when-profile="en"}

## Finding the Closest Author: An Introduction to the Naive Bayes Classifier

Before diving into _embeddings_, let's explore a slightly different use case within our probabilistic framework. Suppose we want to predict the author of a given text. If our previous intuition holds—certain words are more likely to appear in texts by specific authors—then we can train an automatic classification algorithm to predict the author based on the text.

The most natural method for this task is the Naive Bayes classifier. This model is a perfect fit for the frequentist approach we've used so far, as it relies on the probabilities of word occurrences per author.

The Naive Bayes classifier applies a decision rule: it selects the most probable class given the observed structure of the document—i.e., the words that appear in it.

In other words, we choose the class $\widehat{c}$ that is most probable given the terms in document $d$.

:::

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(c|d) =  \arg \max_{c \in \mathcal{C}} \frac{ \mathbb{P}(d|c)\mathbb{P}(c)}{\mathbb{P}(d)}
$$ {#eq-definition-bayes}

::: {.content-visible when-profile="fr"}

Comme ceci est classique en estimation bayésienne, on peut se passer de certains termes constants, à savoir $\mathbb{P}(d)$. La définition de la classe estimée peut ainsi être reformulée de cette manière:

:::

::: {.content-visible when-profile="en"}

As is common in Bayesian estimation, we can ignore constant terms such as $\mathbb{P}(d)$. The definition of the predicted class can thus be reformulated as follows:

:::

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(d|c)\mathbb{P}(c)
$$ {#eq-rewriting-bayes}

::: {.content-visible when-profile="fr"}

L'hypothèse du sac de mot intervient à ce niveau. Un document $d$ est une collection de mots $w_i$ dont l'ordre n'a pas d'intérêt. Autrement dit, on peut se contenter de faire un modèle sur les mots, sans faire intervenir des probabilités conditionnelles sur l'ordre d'occurrence.
La seconde hypothèse forte est l'hypothèse naive à laquelle la méthode doit son nom: la probabilité de tirer un mot ne dépend que de la catégorie $c$ d'appartenance du document. Autrement dit, on peut considérer qu'un document est une suite de tirage indépendants de mots dont la probabilité ne dépend que de l'auteur.

Comme cela est expliqué dans la boite dédiée, en faisant ces hypothèses, on peut réécrire ce classifieur sous la forme

:::

::: {.content-visible when-profile="en"}

The bag-of-words assumption comes into play here. A document $d$ is considered a collection of words $w_i$, where word order is irrelevant. In other words, we can build a model based on individual words without involving conditional probabilities related to their order.
The second strong assumption is the naive assumption from which the method gets its name: the probability of drawing a word depends only on the category $c$ to which the document belongs. In other words, a document is treated as a sequence of independent word draws, where the probability depends solely on the author.

As explained in the dedicated box, under these assumptions, the classifier can be rewritten in the following form

:::

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(c)\prod_{w \in \mathcal{W}}{\mathbb{P}(w|c)}
$$

::: {.content-visible when-profile="fr"}

avec $\mathcal{W}$ l'ensemble des mots dans le corpus (notre vocabulaire).

Empiriquement, nous sommes dans une tâche d'apprentissage supervisé où le _label_ est la classe du document et les _features_ sont nos mots vectorisés. Empiriquement, les probabilités sont estimées à partir du dénombrement des mots dans le corpus et des types de documents dans le corpus.

Il est bien-sûr possible de calculer toutes ces grandeurs à la main mais `Scikit` permet d'implémenter un estimateur naif de Bayes après avoir vectorisé son corpus comme le montre le prochain exercice. Cela peut néanmoins poser un problème pratique: en principe, le corpus de test ne doit pas comporter de nouveaux mots car ces "nouvelles" dimensions n'étaient pas présentes dans le corpus d'entraînement. En pratique, la solution la plus simple est souvent celle choisie: ces mots sont ignorés.

:::

::: {.content-visible when-profile="en"}

where $\mathcal{W}$ is the set of words in the corpus (our vocabulary).

Empirically, this is a supervised learning task where the _label_ is the document class and the _features_ are our vectorized words. In practice, the probabilities are estimated from word counts in the corpus and the distribution of document types.

While it is possible to compute all these quantities manually, `Scikit` makes it easy to implement a Naive Bayes estimator after vectorizing the corpus, as shown in the next exercise. However, this may introduce a practical issue: ideally, the test set should not contain new words that were not in the training set, since these new dimensions did not exist during training. In practice, the most common solution is the one adopted here: these words are ignored.

:::


{{< include "03_embedding/exercise2.qmd" >}}


:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
## Comprendre la logique du classifieur naif de Bayes

Supposons que nous sommes dans un problème de classification avec des classes $(c_1,...,c_K) (ensemble noté $\mathcal{C}$). Nous plaçant dans le cadre de pensée du sac de mot, nous pouvons ne pas nous préoccuper des positions des mots dans les documents, qui complexifieraient beaucoup l'écriture de nos équations.

L'équation @eq-rewriting-bayes peut être réécrite

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(w_1, ..., w_n|c)\mathbb{P}(c)
$$

Dans le monde bayésien, on nomme $\mathbb{P}(w_1, ..., w_n|c)$ la vraisemblance (_likelihood_) et $\mathbb{P}(c)$ l'_a priori_ (_prior_).

L'hypothèse Bayes naive permet de traiter un document comme une suite de tirages aléatoires dont les probabilités ne dépendent que de la catégorie. Dans ce cas, le tirage d'une phrase est une suite de tirages de mots et la probabilité composée est donc


$$
\mathbb{P}(w_1, ..., w_n|c) = \prod_{i=1}^n \mathbb{P}(w_i|c)
$$

Par exemple, en simplifiant en deux classes, si les probabilités sont celles du @tbl-fake-proba, la phrase _"afraid by Doctor Frankenstein"_ aura un peu moins de 1% de chance (0.8%) d'être écrite si l'autrice est Mary Shelley mais sera encore moins vraisemblable chez Lovecraft (0.006%) car si _"afraid"_ est très probable chez lui, Frankenstein est un événement rare qui rend peu vraisemblable cette composition de mots.

| Mot ($w_i$) | Probabilité chez Mary Shelley| Probabilité chez Lovecraft |
|-----------|--------|--------|
| Afraid | 0.1 | 0.6 |
| By | 0.2 | 0.2 |
| Doctor | 0.2 | 0.05 |
| Frankenstein | 0.2 | 0.01 |

: Exemple fictif de probabilités de tirage {#tbl-fake-proba}

En composant ces différentes équations, on obtient

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(c)\prod_{w \in \mathcal{W}}{\mathbb{P}(w|c)}
$$

La contrepartie empirique de $\mathbb{P}(c)$ est assez évidente: la fréquence observée de chaque catégorie (les auteurs) dans notre corpus. Autrement dit,

$$
\widehat{\mathbb{P}(c)} = \frac{n_c}{n_{doc}}
$$

Quelle est la contrepartie empirique de $\mathbb{P}(w_i|c)$ ? C'est la fréquence d'apparition du mot en question chez l'auteur. Pour le calculer, il suffit de compter le nombre de fois qu'il apparaît chez l'auteur et de diviser par le nombre de mots de l'auteur.

:::

::::

:::: {.content-visible when-profile="en"}

::: {.callout-tip}
## Understanding the logic of the naive Bayes classifier

Suppose we are in a classification problem with classes $(c_1,...,c_K)$ (set denoted $\mathcal{C}$). Placing ourselves within the bag-of-words framework, we can ignore the positions of words in documents, which would greatly complicate the writing of our equations.

The equation @eq-rewriting-bayes can be rewritten

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(w_1, ..., w_n|c)\mathbb{P}(c)
$$

In the Bayesian world, we call $\mathbb{P}(w_1, ..., w_n|c)$ the likelihood and $\mathbb{P}(c)$ the prior.

The naive Bayes assumption allows us to treat a document as a sequence of random draws whose probabilities depend only on the category. In this case, drawing a sentence is a sequence of word draws and the compound probability is therefore


$$
\mathbb{P}(w_1, ..., w_n|c) = \prod_{i=1}^n \mathbb{P}(w_i|c)
$$

For example, simplifying to two classes, if the probabilities are those from @tbl-fake-proba, the sentence _"afraid by Doctor Frankenstein"_ will have a little less than 1% chance (0.8%) of being written if the author is Mary Shelley but will be even less likely with Lovecraft (0.006%) because while _"afraid"_ is very probable with him, Frankenstein is a rare event that makes this word composition unlikely.

| Word ($w_i$) | Probability for Mary Shelley| Probability for Lovecraft |
|-----------|--------|--------|
| Afraid | 0.1 | 0.6 |
| By | 0.2 | 0.2 |
| Doctor | 0.2 | 0.05 |
| Frankenstein | 0.2 | 0.01 |

: Fictional example of drawing probabilities {#tbl-fake-proba}

By combining these different equations, we get

$$
\widehat{c} = \arg \max_{c \in \mathcal{C}} \mathbb{P}(c)\prod_{w \in \mathcal{W}}{\mathbb{P}(w|c)}
$$

The empirical counterpart of $\mathbb{P}(c)$ is quite obvious: the observed frequency of each category (the authors) in our corpus. In other words,

$$
\widehat{\mathbb{P}(c)} = \frac{n_c}{n_{doc}}
$$

What is the empirical counterpart of $\mathbb{P}(w_i|c)$? It is the frequency of appearance of the word in question for the author. To calculate it, we simply count the number of times it appears for the author and divide by the number of words by the author.

:::

::::

:::: {.content-visible when-profile="fr"}

# Le modèle Word2Vec, une représentation plus synthétique

## Vers une représentation plus synthétique du langage

La représentation vectorielle issue de l'approche _bag of words_ n'est pas très synthétique ni stable et surtout est assez frustre.

Si on a un petit corpus, on va avoir des problèmes à extrapoler puisque de nouveaux textes ont toutes les chances d'apporter de nouveaux mots, qui sont de nouvelles dimensions de _features_ qui n'étaient pas présentes dans le corpus d'entraînement, ce qui conceptuellement est un problème puisque les algorithmes de _machine learning_ n'ont pas vocation à prédire sur des caractéristiques sur lesquelles ils n'ont pas été entraîné[^genAI].

A l'inverse, plus on a de texte dans un corpus, plus notre représentation vectorielle sera importante. Par exemple, si votre sac de mot a vu tout le vocabulaire français, soit 60 000 mots selon l'[Académie Française](https://www.dictionnaire-academie.fr/article/QDL056) (les estimations étant de 200 000 pour la langue anglaise), cela fait des vecteurs de taille conséquente. Cependant, la diversité des textes est, en pratique, bien moindre: l'usage courant du Français nécessite plutôt autour de 3000 mots et la plupart des textes, notamment s'ils sont courts, n'utilisent pas un vocabulaire si complet. Ceci implique donc des vecteurs très peu denses, avec beaucoup de 0.

[^genAI]: Cette remarque peut apparaître étonnante alors que les IA génératives occupent une place importante dans nos usages. Néanmoins, il faut garder à l'esprit que certes vous posez de nouvelles questions à des IA mais vous les posez dans des termes qu'elles connaissent: du langage naturel dans une langue présente dans leur corpus d'entraînement, des images numériques qui sont donc interprétables par une machine, etc. Autrement dit, votre _prompt_ n'est pas, en soi, inconnu pour l'IA, elle peut l'interpréter même si son contenu est nouveau et original.

La vectorisation selon cette approche est donc peu efficace; le signal est peu compressé. Des représentations denses, c'est-à-dire de dimension plus faible mais portant toutes une information, semblent plus adéquate pour pouvoir généraliser notre modélisation du langage.
L'algorithme qui a rendu célèbre cette approche est le modèle `Word2Vec`, en quelques sortes le premier ancêtre commun des LLM modernes. La représentation vectorielle de `Word2Vec` est assez synthétique: la dimension de ces _embeddings_ est entre 100 et 300.

## Des relations sémantique entre les termes

Cette représentation dense va représenter une solution à une limite de l'approche _bag of words_ que nous avons évoquée à de multiples reprises. Chacune de ces dimensions va représenter un facteur latent,
c'est à dire une variable inobservée,
de la même manière que les composantes principales produites par une ACP. Ces dimensions latentes peuvent être interprétées comme des dimensions "fondamentales" du langage

![Illustration du principe de la représentation de Word2Vec (source: [Jay Alammar](https://jalammar.github.io/illustrated-word2vec/))](https://jalammar.github.io/images/word2vec/word2vec.png)


Par exemple, un humain sait qu'un document contenant le mot _"Roi"_
et un autre document contenant le mot _"Reine"_ ont beaucoup de chance
d'aborder des sujets semblables. Un modèle `Word2Vec` bien entraîné va capter
qu'il existe un facteur latent de type _"royauté"_
et la similarité entre les vecteurs associés aux deux mots sera forte.

La magie va même plus loin : le modèle captera aussi qu'il existe un
facteur latent de type _"genre"_,
et va permettre de construire un espace sémantique dans lequel les
relations arithmétiques entre vecteurs ont du sens. Par exemple,

::::

:::: {.content-visible when-profile="en"}

# The Word2Vec model, a more synthetic representation

## Towards a more synthetic representation of language

The vector representation resulting from the _bag of words_ approach is not very synthetic or stable and is quite crude.

If we have a small corpus, we will have problems extrapolating since new texts are very likely to bring new words, which are new feature dimensions that were not present in the training corpus. This is conceptually a problem since _machine learning_ algorithms are not intended to predict on characteristics they have not been trained on[^genAI].

Conversely, the more text we have in a corpus, the larger our vector representation will be. For example, if your bag of words has seen the entire French vocabulary, which is 60,000 words according to the [French Academy](https://www.dictionnaire-academie.fr/article/QDL056) (estimates being 200,000 for the English language), this results in vectors of considerable size. However, the diversity of texts is, in practice, much lower: common use of French requires around 3,000 words and most texts, especially if they are short, do not use such a complete vocabulary. This therefore implies very sparse vectors, with many 0s.

[^genAI]: This remark may seem surprising while generative AIs occupy an important place in our usage. Nevertheless, we must keep in mind that while you ask new questions to AIs, you ask them in terms they know: natural language in a language present in their training corpus, digital images that are therefore interpretable by a machine, etc. In other words, your _prompt_ is not, in itself, unknown to the AI, it can interpret it even if its content is new and original.

Vectorization according to this approach is therefore inefficient; the signal is poorly compressed. Dense representations, that is, of smaller dimension but all carrying information, seem more adequate to be able to generalize our language modeling.
The algorithm that made this approach famous is the `Word2Vec` model, in some ways the first common ancestor of modern LLMs. The vector representation of `Word2Vec` is quite synthetic: the dimension of these _embeddings_ is between 100 and 300.

## Semantic relationships between terms

This dense representation will represent a solution to a limitation of the _bag of words_ approach that we have mentioned multiple times. Each of these dimensions will represent a latent factor,
that is, an unobserved variable,
in the same way as principal components produced by a PCA. These latent dimensions can be interpreted as "fundamental" dimensions of language.

![Illustration of the principle of Word2Vec representation (source: [Jay Alammar](https://jalammar.github.io/illustrated-word2vec/))](https://jalammar.github.io/images/word2vec/word2vec.png)


For example, a human knows that a document containing the word _"King"_
and another document containing the word _"Queen"_ are very likely
to address similar subjects. A well-trained `Word2Vec` model will capture
that there exists a latent factor of type _"royalty"_
and the similarity between the vectors associated with the two words will be strong.

The magic goes even further: the model will also capture that there exists a
latent factor of type _"gender"_,
and will allow the construction of a semantic space in which
arithmetic relationships between vectors make sense. For example,

::::

$$
\text{king} - \text{man} + \text{woman} ≈ \text{queen}
$$

::: {.content-visible when-profile="fr"}

ou, pour reprendre, l'exemple issu du papier originel `Word2Vec` [@mikolov2013efficient],

:::

::: {.content-visible when-profile="en"}

or, to revisit the example from the original `Word2Vec` paper [@mikolov2013efficient],

:::

$$
\text{Paris} - \text{France} + \text{Italy} ≈ \text{Rome}
$$

::: {.content-visible when-profile="fr"}

![Illustration du plongement lexical. Source : Post de blog [Word Embedding : Basics](https://medium.com/@hari4om/word-embedding-d816f643140)](https://ssphub.netlify.app/post/embedding/word_embedding.png)

Un autre "miracle" de cette approche est qu'on obtient une forme de transfert entre les langues. Les relations sémantiques pouvant être similaires entre les langues, pour de nombreux mots usuels, on peut voir translater certaines langues les unes avec les autres si elles ont un socle commun (par exemple les langues occidentales). Ce concept est le point de départ des traducteurs automatiques et des IA multilingues

![Exemple de translation entre deux représentations vectorielles. Source: [Meta](https://engineering.fb.com/2018/01/24/ml-applications/under-the-hood-multilingual-embeddings/)](https://engineering.fb.com/wp-content/uploads/2018/01/GJ_9lgFMnVaR0ZYAAAAAAABV9MkQbj0JAAAC.gif)

:::

::: {.content-visible when-profile="en"}

![Illustration of lexical embedding. Source: Blog post [Word Embedding: Basics](https://medium.com/@hari4om/word-embedding-d816f643140)](https://ssphub.netlify.app/post/embedding/word_embedding.png)

Another "miracle" of this approach is that it allows a form of transfer between languages. Since semantic relationships can be similar across languages, many common words can be mapped between languages if they share a common base (such as Western languages). This concept is the foundation of automatic translators and multilingual AI systems.

![Example of translation between two vector representations. Source: [Meta](https://engineering.fb.com/2018/01/24/ml-applications/under-the-hood-multilingual-embeddings/)](https://engineering.fb.com/wp-content/uploads/2018/01/GJ_9lgFMnVaR0ZYAAAAAAABV9MkQbj0JAAAC.gif)

:::


:::: {.content-visible when-profile="fr"}

## Comment ces modèles sont-ils entraînés ?

Ces modèles sont entraînés à partir d'une tâche de prédiction résolue par un réseau de neurones simple, généralement avec une approche par renforcement.

L'idée fondamentale est que la signification d'un mot se comprend en regardant les mots qui apparaissent fréquemment dans son voisinage. Pour un mot donné, on va donc essayer de prédire les mots qui apparaissent dans une fenêtre autour du mot cible.

En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,
on obtient finalement des *embeddings* pour chaque mot du vocabulaire,
qui présentent les propriétés discutées précédemment. L'ensemble des articles `Wikipedia` est un des corpus de prédilection des personnes ayant construit des plongements
lexicaux. Il comporte en effet des phrases complètes, contrairement à des informations issues de commentaires de réseaux sociaux,
et propose des rapprochements intéressants entre des personnes, des lieux, etc.

Le contexte d'un mot est défini par une fenêtre de taille fixe autour de ce mot. La taille de la fenêtre est un paramètre de la construction de l'_embedding_. Le corpus fournit un grand ensemble d'exemples mots-contexte, qui peuvent servir à entraîner un réseau de neurones.

Plus précisément, il existe deux approches, dont nous ne développerons pas les détails :

*   _Continuous bag of words_ (CBOW), où le modèle est entraîné à prédire un mot à partir de son contexte ;
*   _Skip-gram_, où le modèle tente de prédire le contexte à partir d'un seul mot.

![Illustration de la différence entre les approches CBOW et Skip-gram](https://ssphub.netlify.app/post/embedding/CBOW_Skipgram_training.png)

::::

:::: {.content-visible when-profile="en"}

## How are these models trained?

These models are trained from a prediction task solved by a simple neural network, generally with a reinforcement approach.

The fundamental idea is that the meaning of a word is understood by looking at words that frequently appear in its neighborhood. For a given word, we will therefore try to predict the words that appear in a window around the target word.

By repeating this task many times and on a sufficiently varied corpus,
we finally obtain *embeddings* for each word in the vocabulary,
which present the properties discussed previously. The collection of `Wikipedia` articles is one of the preferred corpora for people who have built lexical
embeddings. It indeed contains complete sentences, unlike information from social media comments,
and proposes interesting connections between people, places, etc.

The context of a word is defined by a fixed-size window around this word. The window size is a parameter of the _embedding_ construction. The corpus provides a large set of word-context examples, which can be used to train a neural network.

More precisely, there are two approaches, whose details we will not develop:

*   _Continuous bag of words_ (CBOW), where the model is trained to predict a word from its context;
*   _Skip-gram_, where the model attempts to predict the context from a single word.

![Illustration of the difference between CBOW and Skip-gram approaches](https://ssphub.netlify.app/post/embedding/CBOW_Skipgram_training.png)

::::

:::: {.content-visible when-profile="fr"}

## Modèles liés

Plusieurs modèles ont une filiation directe avec le modèle `Word2Vec` quoiqu'ils s'en distinguent par la nature de l'architecture utilisée.

C'est le cas, par exemple, du modèle modèle [`GloVe`](https://nlp.stanford.edu/projects/glove/), développé en 2014 à Stanford,
qui ne repose pas sur des réseaux de neurones mais sur la construction d'une grande matrice de co-occurrences de mots. Pour chaque mot, il s'agit de calculer les fréquences d'apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières.

Le modèle [`FastText`](https://fasttext.cc/), développé en 2016 par une équipe de `Facebook`, fonctionne de façon similaire à `Word2Vec` mais se distingue particulièrement sur deux points :

*   En plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille $n$, par exemple _« tar »_, _« art »_ et _« rte »_ sont les trigrammes du mot _« tarte »_), ce qui le rend notamment robuste aux variations d'orthographe ;
*   Le modèle a été optimisé pour que son entraînement soit particulièrement rapide.

Le modèle [`FastText`](https://fasttext.cc/) est particulièrement performant pour les problématiques de classification automatique. L'Insee l'utilise par exemple pour plusieurs modèles de classification de libellés textuels dans des nomenclatures.

![Illustration du modèle fastText](https://ssphub.netlify.app/post/embedding/fasttext.png)

Voici un exemple sur un projet de classification automatisée des professions dans la typologie
des nomenclatures d'activités (les PCS) à partir d'un modèle entraîné par la librairie `Fasttext` :

::::

:::: {.content-visible when-profile="en"}

## Related models

Several models have a direct lineage with the `Word2Vec` model although they distinguish themselves by the nature of the architecture used.

This is the case, for example, of the [`GloVe`](https://nlp.stanford.edu/projects/glove/) model, developed in 2014 at Stanford,
which does not rely on neural networks but on the construction of a large word co-occurrence matrix. For each word, the task is to calculate the frequencies of appearance of other words in a fixed-size window around it. The obtained co-occurrence matrix is then factorized by a singular value decomposition.

The [`FastText`](https://fasttext.cc/) model, developed in 2016 by a `Facebook` team, works similarly to `Word2Vec` but particularly distinguishes itself on two points:

*   In addition to the words themselves, the model learns representations for character n-grams (character subsequences of size $n$, for example _"tar"_, _"art"_ and _"rte"_ are the trigrams of the word _"tarte"_), which makes it particularly robust to spelling variations;
*   The model has been optimized so that its training is particularly fast.

The [`FastText`](https://fasttext.cc/) model is particularly effective for automatic classification problems. INSEE uses it for example for several models of classification of textual labels in nomenclatures.

![Illustration of the fastText model](https://ssphub.netlify.app/post/embedding/fasttext.png)

Here is an example of an automated profession classification project in the typology
of activity nomenclatures (PCS) based on a model trained by the `Fasttext` library:

::::


::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
viewof activite = Inputs.text(
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Prédiction de l'activité
    </caption>
    <tr>
      <th style="text-align:center;">Libellé (NA2008)</th>
      <th>Probabilité</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

:::

::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = (
    "https://codification-ape-test.lab.sspcloud.fr/"
    f"predict?nb_echos_max=3&prob_min=0&text_feature={activite}"
)

try:
    # requête
    resp = requests.get(urlApe, timeout=10)
    resp.raise_for_status()  # lève une erreur si code HTTP != 200
    data = resp.json()

    # récupération de IC
    IC = data.pop("IC", None)

    # transformation en DataFrame
    df = pd.DataFrame(data.values())
    df["indice_confiance"] = IC

    print(df)

except requests.exceptions.RequestException as e:
    print("Erreur lors de l'appel API :", e)
    df = pd.DataFrame()  # DataFrame vide en cas d'échec

except (ValueError, KeyError) as e:
    print("Erreur lors du parsing des données :", e)
    df = pd.DataFrame()

df
```

:::

:::: {.content-visible when-profile="fr"}

Ces modèles sont héritiers de `Word2Vec` dans le sens où ils reprennent une représentation vectorielle dense de faible dimension de documents textuels. `Word2Vec` reste un modèle héritier de la logique sac de mot. La représentation d'une phrase ou d'un document est une forme de moyenne des représentations des mots qui les composent.

Depuis 2013, plusieurs révolutions ont amené à enrichir les modèles de langage pour aller au-delà d'une représentation par mot de ceux-ci. Des architectures beaucoup plus complexes pour représenter non seulement les mots sous forme d'_embeddings_ mais aussi les phrases et les documents sont aujourd'hui à l'oeuvre et peuvent être reliées à la révolution des architectures _transformers_.

# Les _transformers_: une représentation plus riche du langage

Si le modèle `Word2Vec` est entraîné de manière contextuelle, sa vocation est de donner une représentation vectorielle d'un mot de manière absolue, indépendamment du contexte. Par exemple, le terme _"banc"_ aura exactement la même représentation vectorielle qu'il se trouve dans la phrase _"Elle court vers le banc de sable"_ ou "Il t'attend sur un banc au parc"_. C'est une limite majeure de ce type d'approche et on se doute bien de l'importance du contexte pour l'interprétation du langage.

L'objectif des architectures _transformers_ est de permettre des représentations vectorielles contextuelles. Autrement dit, un mot aura plusieurs représentations vectorielles, selon son contexte d'occurrence. Ces modèles s'appuient sur le mécanisme d'attention [@vaswani2017attention]. Avant cette approche, lorsqu'un modèle apprenait à vectoriser un texte et qu'il arrivait au énième mot, la seule mémoire qu'il gardait était celle du mot précédent. Par récurrence, cela signifiait qu'il gardait une mémoire des mots précédents mais celle-ci tendait à se dissiper. Par conséquent, pour un mot arrivant loin dans la phrase, il était probable que le contexte de début de phrase était oublié. Autrement dit, dans la phrase _"à la plage, il allait explorer le banc"_, il était fort probable qu'arrivé au mot _"banc"_, le modèle ait oublié le début de phrase qui avait pourtant de l'importance pour l'interprétation.

L'objectif du mécanisme d'attention est de créer une mémoire interne au modèle permettant, pour tout mot d'un texte, de pouvoir garder trace des autres mots. Bien-sûr tous ne sont pas pertinents pour interpréter le texte mais cela évite d'oublier ceux qui sont importants. L'innovation principale des dernières années en NLP a été de parvenir à créer des mécanismes d'attention à grande échelle sans pour autant rendre intractables les modèles. Les fenêtres de contexte des modèles les plus performants deviennent immenses. Par exemple le modèle Llama 3.1 (rendu public par Meta en Juillet 2024) propose une fenêtre de contexte de 128 000 _tokens_, soit environ 96 000 mots, l'équivalent du _Hobbit_ de Tolkien. Autrement dit, pour déduire la subtilité du sens d'un mot, ce modèle peut parcourir un contexte aussi long qu'un roman d'environ 300 pages.

Les deux modèles qui ont marqué leur époque dans le domaine sont les modèles `BERT` développé en 2018 par _Google_ (qui était déjà à l'origine de `Word2Vec`) et la première version du bien-connu `GPT` d'`OpenAI`, qui, en 2017, était le premier modèle préentraîné basé sur l'architecture _transformer_. Ces deux familles de _transformer_ diffèrent dans la manière dont ils intègrent le contexte pour faire une prédiction. `GPT` est un modèle autorégressif, donc ne considère que les _tokens_ avant celui dont on désire faire une prédiction. `BERT` utilise les _tokens_ à gauche et à droite pour inférer le contexte. Ces deux grands modèles de langage entraînés sont entraînés par auto-renforcement, principalement sur des tâches de prédiction du prochain _token_ [@huggingfacecourse]. Depuis le succès de `ChatGPT`, les nouveaux modèles GPT (à partir de la version 3) ne sont plus _open source_. Pour les utiliser, il faut donc passer par les API d'OpenAI. Il existe néanmoins de nombreuses alternatives dont les poids sont ouverts, à défaut d'être _open source_[^diff-open], qui permettent d'utiliser ces LLM par le biais de `Python`, par le biais, notamment, de la librairie `transformers` développée par _Hugging Face_.

[^diff-open]: Certaines organisations, comme Meta pour Llama, mettent à disposition les poids après entraînement de leur modèle sur la plateforme _Hugging Face_, permettant une réutilisation de ces modèles si la licence le permet. Néanmoins, il ne s'agit pas pour autant de modèles _open source_ puisque le code utilisé pour entraîner les modèles et constituer les corpus d'apprentissage, issus de collectes massives de données par _webscraping_, et les éventuelles annotations supplémentaires pour en faire des versions spécialisées, ne sont pas partagés.

Quand on travaille avec des corpus de taille restreinte,
c'est généralement une mauvaise idée d'entraîner son propre modèle _from scratch_. Heureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles. Ils permettent de réaliser du *transfer learning*, c'est-à-dire de bénéficier de la performance d'un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.

::::

:::: {.content-visible when-profile="en"}

These models are inheritors of `Word2Vec` in the sense that they adopt a dense, low-dimensional vector representation of textual documents. `Word2Vec` remains a model that inherits from the bag-of-words logic. The representation of a sentence or document is a form of average of the representations of the words that compose them.

Since 2013, several revolutions have led to enriching language models to go beyond a word-by-word representation of these. Much more complex architectures to represent not only words in the form of _embeddings_ but also sentences and documents are now at work and can be linked to the revolution of _transformer_ architectures.

# _Transformers_: a richer representation of language

While the `Word2Vec` model is trained contextually, its purpose is to give a vector representation of a word in an absolute manner, independent of context. For example, the term _"bank"_ will have exactly the same vector representation whether it appears in the sentence _"She runs towards the sandbank"_ or _"He's waiting for you on a bench in the park"_. This is a major limitation of this type of approach and we can well imagine the importance of context for language interpretation.

The objective of _transformer_ architectures is to enable contextual vector representations. In other words, a word will have several vector representations, depending on its context of occurrence. These models rely on the attention mechanism [@vaswani2017attention]. Before this approach, when a model learned to vectorize a text and reached the nth word, the only memory it kept was that of the previous word. By recurrence, this meant it kept a memory of previous words but this tended to dissipate. Consequently, for a word appearing far in the sentence, it was likely that the context from the beginning of the sentence was forgotten. In other words, in the sentence _"at the beach, he was going to explore the bank"_, it was very likely that upon reaching the word _"bank"_, the model had forgotten the beginning of the sentence which was nevertheless important for interpretation.

The objective of the attention mechanism is to create an internal memory in the model allowing, for any word in a text, to keep track of other words. Of course, not all are relevant for interpreting the text but this avoids forgetting those that are important. The main innovation of recent years in NLP has been to manage to create large-scale attention mechanisms without making the models intractable. The context windows of the most performant models are becoming immense. For example, the Llama 3.1 model (made public by Meta in July 2024) offers a context window of 128,000 _tokens_, or about 96,000 words, the equivalent of Tolkien's _Hobbit_. In other words, to deduce the subtlety of a word's meaning, this model can browse through a context as long as a novel of about 300 pages.

The two models that marked their era in the field are the `BERT` models developed in 2018 by _Google_ (which was already behind `Word2Vec`) and the first version of the well-known `GPT` from `OpenAI`, which, in 2017, was the first pre-trained model based on the _transformer_ architecture. These two _transformer_ families differ in how they integrate context to make a prediction. `GPT` is an autoregressive model, therefore only considers the _tokens_ before the one we want to predict. `BERT` uses the _tokens_ to the left and right to infer context. These two major trained language models are trained by self-reinforcement, mainly on next _token_ prediction tasks [@huggingfacecourse]. Since the success of `ChatGPT`, the new GPT models (from version 3 onwards) are no longer _open source_. To use them, one must therefore go through OpenAI's APIs. There are nevertheless many alternatives whose weights are open, if not _open source_[^diff-open], which allow using these LLMs through `Python`, notably through the `transformers` library developed by _Hugging Face_.

[^diff-open]: Some organizations, like Meta for Llama, make available the post-training weights of their model on the _Hugging Face_ platform, allowing reuse of these models if the license permits. Nevertheless, these are not _open source_ models since the code used to train the models and constitute the learning corpora, derived from massive data collection by _webscraping_, and any additional annotations to make specialized versions, are not shared.

When working with small-sized corpora,
it's generally a bad idea to train your own model _from scratch_. Fortunately, models pre-trained on very large corpora are available. They allow for *transfer learning*, that is, to benefit from the performance of a model that has been trained on another task or on another corpus.

::::

:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
## Exercice 3

1. Refaire un train/test split avec 500 lignes aléatoires
2. Importer le modèle `all-MiniLM-L6-v2` avec le package `sentence transformers`. Encoder `X_train` et `X_test`
3. Faire une classification avec une méthode simple, par exemple des SVC, s'appuyant sur les _embeddings_ produits à la question précédente. Comme le jeu d'entraînement est réduit, vous pouvez faire de la validation croisée.
4. Comprendre pourquoi les performances sont détériorées par rapport au classifieur naif de Bayes.
:::

Réponse à la question 1:

::::


:::: {.content-visible when-profile="en"}

::: {.callout-tip}
## Exercise 3

1. Repeat a train/test split with 500 random lines
2. Import the `all-MiniLM-L6-v2` model with the `sentence transformers` package. Encode `X_train` and `X_test`.
3. Perform a classification using a simple method, such as CVS, based on the _embeddings_ produced in the previous question. As the training set is small, you can perform cross-validation.
4. Understand why the performance is worse than that of Bayes' naive classifier.
:::

Answer to question 1:

::::



```{.python}
random_rows = spooky_df.sample(500)
y = random_rows["author"]
X = random_rows['text']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

Réponse à la question 2:

```{.python}
from sentence_transformers import SentenceTransformer
from sklearn.svm import LinearSVC

model = SentenceTransformer(
    "all-MiniLM-L6-v2", model_kwargs={"torch_dtype": "float16"}
)

X_train_vectors = model.encode(X_train.values)
X_test_vectors = model.encode(X_test.values)
```
:::: {.content-visible when-profile="fr"}

Réponse à la question 3:

::::


:::: {.content-visible when-profile="en"}

Answer to question 3:

::::

```{.python}
from sklearn.model_selection import cross_val_score

clf = LinearSVC(max_iter=10000, C=0.1, dual="auto")

scores = cross_val_score(
    clf, X_train_vectors, y_train,
    cv=4, scoring='f1_micro', n_jobs=4
)

print(f"CV scores {scores}")
print(f"Mean F1 {np.mean(scores)}")
```

:::: {.content-visible when-profile="fr"}

__Mais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?__

On peut avancer plusieurs raisons :

- le `TF-IDF` est un modèle simple, mais toujours très performant
(on parle de _"tough-to-beat baseline"_).
- la classification d'auteurs est une tâche très particulière et très ardue,
qui ne fait pas justice aux *embeddings*. Comme on l'a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu'il est question de similarité sémantique entre des textes (_clustering_, etc.).

Dans le cas de notre tâche de classification, il est probable que
certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,
ce que ne permettent pas de capter les *embeddings* qui accordent à tous les mots la même importance.

::::
:::: {.content-visible when-profile="en"}

__But why, with a very complicated method, can't we beat a very simple one?__

There are several possible reasons:

- the TF-IDF is a simple model, but it still performs very well
(this is known as a 'tough-to-beat baseline').
- the classification of authors is a very specific and arduous task,
which does not do justice to the *embeddings*. As we said earlier, the latter are particularly relevant when it comes to semantic similarity between texts (_clustering_, etc.).

In the case of our classification task, it is likely that
certain words (character names, place names) are sufficient to classify in a relevant way,
This is not captured by *embeddings*, which give all words the same importance.

::::
