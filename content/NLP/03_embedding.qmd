---
title: "Méthodes de vectorisation : comptages et word embeddings"
type: book
tags:
  - NLP
  - Littérature
  - Topics Modelling
  - Word2Vec
categories:
  - Tutoriel
  - NLP
description: |
  Pour pouvoir utiliser des données textuelles dans des algorithmes
  de _machine learning_, il faut les vectoriser, c'est à dire transformer
  le texte en données numériques. Dans ce TP, nous allons comparer
  différentes méthodes de vectorisation, à travers une tâche de prédiction :
  _peut-on prédire un auteur littéraire à partir d'extraits de ses textes ?_
  Parmi ces méthodes, on va notamment explorer le modèle `Word2Vec`, qui
  permet d'exploiter les structures latentes d'un texte en construisant
  des _word embeddings_ (plongements de mots).
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/word_embedding.png
---

::: {.warning}
Ce chapitre va évoluer prochainement.
:::

{{< badges
    printMessage="true"
>}}

# Introduction

Cette page approfondit certains aspects présentés dans la
[partie introductive](/content/NLP/02_exoclean.qmd). 
Nous allons avancer dans notre compréhension des enjeux du NLP grâce à la
modélisation du langage. 

Nous partirons de la conclusion, évoquée à la fin du précédent chapitre, que les approches fréquentistes présentent plusieurs
inconvénients : représentation du langage sur des régularités statistiques indépendantes des proximités entre des mots ou phrases, difficulté à prendre en compte le contexte.

L'objectif de ce chapitre est d'évoquer le premier point. Il s'agira d'une introduction au sujet des _embeddings_, ces représentations du langage qui sont au fondement des modèles de langage actuels utilisés par des outils entrés dans notre quotidien (`DeepL`, `ChatGPT`...). 


## Données utilisées

Nous allons continuer notre exploration de la littérature
avec, à nouveau, les trois auteurs anglophones :

* Edgar Allan Poe, (EAP) ;
* HP Lovecraft (HPL) ;
* Mary Wollstonecraft Shelley (MWS).

Les données sont disponibles sur un CSV mis à disposition sur [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). L'URL pour les récupérer directement est 
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Pour rentrer dans le sujet des _embeddings_, nous allons faire de la modélisation du langage en essayant de prédire les auteurs ayant écrit tel ou tel texte. On parle de modèle de langage pour désigner la représentation d'un texte ou d'une langue sous la forme d'une distribution de probabilités de termes (généralement les mots). 

::: {.note}
## Sources d'inspiration
Ce chapitre s'inspire de plusieurs ressources disponibles en ligne:

* Un [premier _notebook_ sur `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)
et un [deuxième](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook
) ;
* Un [dépôt `Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg) ;
:::

## Packages à installer

Comme dans la [partie précédente](/content/NLP/02_exoclean.qmd), il faut télécharger des librairies 
spécialiséees pour le NLP, ainsi que certaines de leurs dépendances.

```{python}
#| eval: false
!pip install scipy==1.12 gensim sentence_transformers pandas matplotlib seaborn
```

Ensuite, comme nous allons utiliser la librairie `SpaCy` avec un corpus de textes
en Anglais, il convient de télécharger le modèle NLP pour l'Anglais. Pour cela, 
on peut se référer à [la documentation de `SpaCy`](https://spacy.io/usage/models),
extrêmement bien faite.

```{python}
#| eval: false
#| echo: true
!python -m spacy download en_core_web_sm
```

```{python}
#| eval: false
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gensim

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV, cross_val_score

from gensim.models.word2vec import Word2Vec
import gensim.downloader
from sentence_transformers import SentenceTransformer
```


# Préparation des données

Nous allons ainsi à nouveau utiliser le jeu de données `spooky` :

```{python}
#| echo: true
import pandas as pd

data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'
spooky_df = pd.read_csv(data_url)
```

Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite :

```{python}
#| echo: true
spooky_df.head()
```

Comme nous l'avons évoqué dans le chapitre précédent, la première étape de tout travail sur données textuelles est souvent celle du *preprocessing*, qui inclut notamment les étapes de tokenization et de nettoyage du texte. 

On se contentera ici d'un *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les méthodes de vectorisation basées sur des comptages).

Pour initialiser le processus de nettoyage,
on va utiliser le corpus `en_core_web_sm` de `Spacy`


```{python}
#| echo: true
import spacy
nlp = spacy.load('en_core_web_sm')
```

On va utiliser un `pipe` `spacy` qui permet d'automatiser, et de paralléliser,
un certain nombre d'opérations. Les *pipes* sont l'équivalent, en NLP, de
nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s'agit donc d'un outil
très approprié pour industrialiser un certain nombre d'opérations de
*preprocessing* :

```{python}
#| echo: true
from typing import List
import spacy

def clean_docs(
    texts: List[str], 
    remove_stopwords: bool = False, 
    n_process: int = 4, 
    remove_punctuation: bool = True
) -> List[str]:
    """
    Cleans a list of text documents by tokenizing, optionally removing stopwords, and optionally removing punctuation.

    Parameters:
        texts (List[str]): List of text documents to clean.
        remove_stopwords (bool): Whether to remove stopwords. Default is False.
        n_process (int): Number of processes to use for processing. Default is 4.
        remove_punctuation (bool): Whether to remove punctuation. Default is True.

    Returns:
        List[str]: List of cleaned text documents.
    """
    # Load spacy's nlp model
    docs = nlp.pipe(
        texts, 
        n_process=n_process, 
        disable=['parser', 'ner', 'lemmatizer', 'textcat']
    )
    
    # Pre-load stopwords for faster checking
    stopwords = set(nlp.Defaults.stop_words)

    # Process documents
    docs_cleaned = (
        ' '.join(
            tok.text.lower().strip()
            for tok in doc
            if (not remove_punctuation or not tok.is_punct) and 
               (not remove_stopwords or tok.text.lower() not in stopwords)
        )
        for doc in docs
    )
    
    return list(docs_cleaned)
```

On applique la fonction `clean_docs` à notre colonne `pandas`.
Les `pandas.Series` étant itérables, elles se comportent comme des listes et
fonctionnent ainsi très bien avec notre `pipe` `spacy`.

```{python}
#| echo: true
spooky_df['text_clean'] = clean_docs(spooky_df['text'])
```

```{python}
#| echo: true
spooky_df.head()
```


## Encodage de la variable à prédire

On réalise un simple encodage de la variable à prédire :
il y a trois catégories (auteurs), représentées par des entiers 0, 1 et 2.

Pour cela, on utilise le `LabelEncoder` de `Scikit` déjà présenté 
dans la [partie modélisation](/content/modelisation/0_preprocessing.qmd). On va utiliser la méthode
`fit_transform` qui permet, en un tour de main, d'appliquer à la fois
l'entraînement (`fit`), à savoir la création d'une correspondance entre valeurs
numériques et _labels_, et l'appliquer (`transform`) à la même colonne.

```{python}
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])
```

On peut vérifier les classes de notre `LabelEncoder` :

```{python}
le.classes_
```

## Construction des bases d'entraînement et de test

On met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).
Cela permettra d'évaluer nos différents modèles toute à la fin de manière très rigoureuse,
puisque ces données n'auront jamais utilisées pendant l'entraînement.

```{python}
from sklearn.model_selection import train_test_split

y = spooky_df["author_encoded"]
X = spooky_df['text_clean']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

Notre échantillon initial n'est pas équilibré (*balanced*) : on retrouve plus d'oeuvres de
certains auteurs que d'autres. Afin d'obtenir un modèle qui soit évalué au mieux, nous allons donc stratifier notre échantillon de manière à obtenir une répartition similaire d'auteurs dans nos
ensembles d'entraînement et de test.

Aperçu du premier élément de `X_train` : 

```{python}
X_train[0]
```

On peut aussi vérifier qu'on est capable de retrouver
la correspondance entre nos auteurs initiaux avec
la méthode `inverse_transform` :

```{python}
print(y_train[0], le.inverse_transform([y_train[0]])[0])
```

Nous avons vu précédemment que notre corpus n'est pas vraiment équilibré et qu'il existe des différences notables entre les auteurs. 
Il semble donc qu'il y ait des particularités propres à chacun des auteurs
en termes de vocabulaire,
ce qui laisse penser qu'il est envisageable de prédire les auteurs à partir
de leurs textes dans une certaine mesure.

# Vectorisation par l'approche _bag of words_

La représentation de nos textes sous forme de sac de mot permet de vectoriser notre corpus et ainsi d'avoir une représentation numérique de chaque texte. On peut à partir de là effectuer plusieurs types de tâches de modélisation.

Définissons notre représentation vectorielle par TF-IDF grâce à `Scikit`:

```{python}
#| echo: true
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

pipeline_tfidf = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000)),
])
pipeline_tfidf
```

Entraînons d'ores et déjà notre modèle à vectoriser le texte à partir de la méthode TF-IDF. Pour le moment il n'est pas encore question de faire de l'évaluation, faisons donc un entraînement sur l'ensemble de notre base et pas seulement sur `X_train`. 

```{python}
pipeline_tfidf.fit(spooky_df['text_clean'])
```

## Trouver le texte le plus similaire

En premier lieu, on peut chercher le texte le plus proche, au sens de TF-IDF, d'une phrase donnée. Prenons cet exemple:

```{python}
text = "He was afraid by Frankenstein monster"
```

Comment retrouver le texte le plus proche de celui-ci ? Il faudrait transformer notre texte dans cette même représentation vectorielle et rapprocher ensuite celui-ci des autres textes représentés sous cette même forme.  

Cela revient à effectuer une tâche de recherche d'information, cas d'usage classique du NLP, mis en oeuvre par exemple par les moteurs de recherche. Le terme Frankenstein étant assez discrminant, nous devrions, grâce à TF-IDF, retrouver des similarités entre notre texte et d'autres textes écrits par Mary Shelley. 

Une métrique régulièrement utilisée pour comparer des vecteurs est la similarité cosinus. Il s'agit d'ailleurs d'une mesure centrale dans l'approche moderne du NLP. Celle-ci a plus de sens avec des vecteurs dense, que nous verrons prochainement, qu'avec des vecteurs comportant beaucoup de 0 comme le sont les vecteurs _sparse_ des mesures _bag-of-words_. Néanmoins c'est déjà un exercice intéressant pour comprendre la similarité entre deux vecteurs. 

Si chaque dimension d'un vecteur correspond à une direction, l'idée derrière la similarité cosinus est de mesurer l'angle entre deux vecteurs. L'angle sera réduit si les vecteurs sont proches.

![](https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png)


### Avec `Scikit-Learn`


::: {.exercise}
## Exercice 1: recherche de similarité avec TF-IDF

1. Utiliser la méthode `transform` pour vectoriser tout notre corpus d'entraînement. 

2. En supposant que votre jeu d'entraînement vectorisé s'appelle `X_train_tfidf`, vous pouvez le transformer en _DataFrame_ avec la commande suivante:

```{.python}
X_train_tfidf=pd.DataFrame(
    X_train_tfidf.todense(),columns=pipeline_tfidf.get_feature_names_out()
)
```

3. Utiliser la méthode `cosine_similarity` de `Scikit` pour calculer la similarité cosinus entre notre texte vectorisé et l'ensemble du corpus d'entraînement grâce au code suivant:

```{.python}
from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities = cosine_similarity(
    X_train_tfidf,
    pipeline_tfidf.transform([text])
).flatten()

top_4_indices = np.argsort(cosine_similarities)[-4:][::-1]  # Sort and reverse for descending order
top_4_similarities = cosine_similarities[top_4_indices]
```

4. Retrouver les documents concernés. Êtes-vous satisfait du résultat ? Comprenez-vous ce qu'il s'est passé ?

:::


```{python}
#| label: exo1-q1-q2
X_train_tfidf = (
    pipeline_tfidf.transform(spooky_df['text_clean'])
)
X_train_tfidf=pd.DataFrame(
    X_train_tfidf.todense(),columns=pipeline_tfidf.get_feature_names_out()
)
```

```{python}
#| label: exo1-q3
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities = cosine_similarity(
    X_train_tfidf,
    pipeline_tfidf.transform([text])
).flatten()

top_4_indices = np.argsort(cosine_similarities)[-4:][::-1]  # Sort and reverse for descending order
top_4_similarities = cosine_similarities[top_4_indices]
```

A l'issue de l'exercice, les 4 textes les plus similaires sont:

```{python}
documents_plus_proches = spooky_df.iloc[top_4_indices].loc[:, ["text", "author"]]
documents_plus_proches['score'] = top_4_similarities

documents_plus_proches
```


### Avec `Langchain` 

Cette approche de calcul de similarité textuelle est assez laborieuse avec `Scikit`. Avec le développement continu d'applications `Python` utilisant des modèles de langage, un écosystème très complet s'est développé pour pouvoir faire ces tâches en quelques lignes de code avec `Python`.

Parmi les outils les plus précieux, nous trouvons [`Langchain`](https://www.langchain.com/) un écosystème `Python` haut-niveau permettant de construire des chaînes de production utilisant des données textuelles. 

Nous allons ici procéder en 2 étapes: 

- Créer un _retriever_, c'est-à-dire vectoriser avec TF-IDF notre corpus (les textes de nos trois auteurs) et les stocker sous forme de base de données vectorielle. 
- Vectoriser à la volée notre texte de recherche (l'object `text` créé précédemment) et rechercher sa contrepartie la plus proche dans la base de données vectorielle. 

La vectorisation de notre corpus se fait très simplement grâce à `Langchain` 
puisque le TFIDFVectoriser de `Scikit` est encapsulé dans un module _ad hoc_ de `Lanchain`

```{python}
#| echo: true
from langchain_community.retrievers import TFIDFRetriever
from langchain_community.document_loaders import DataFrameLoader

loader = DataFrameLoader(spooky_df, page_content_column="text_clean")

retriever = TFIDFRetriever.from_documents(
    loader.load()
)
```

Cet objet `retriever` est un point d'entrée sur notre corpus. `Langchain` présente l'intérêt de fournir plusieurs points d'entrées standardisés, forts utiles dans les projets de NLP puisqu'il suffit de changer les vectoriseurs en entrée sans avoir à changer leur usage en fin de chaîne. 

La méthode `invoke` permet de rechercher les vecteurs les plus similaires à notre texte de recherche:

```{python}
#| echo: true
retriever.invoke(text)
```

La sortie est un objet `Lanchain`, ce qui n'est pas pratique pour nous dans notre situation. On se ramène à un _DataFrame_:

```{python}
#| echo: true
documents = []
for best_echoes in retriever.invoke(text):
    documents += [{**best_echoes.metadata, **{"text_clean": best_echoes.page_content}}]

documents = pd.DataFrame(documents)
```

On peut ajouter à ce _DataFrame_ la colonne de score:

```{python}
from sklearn.metrics.pairwise import cosine_similarity

documents['score'] = cosine_similarity(
    pipeline_tfidf.transform(documents['text_clean']),
    pipeline_tfidf.transform([text])
)
```

On retrouve bien les mêmes documents:

```{python}
documents
```

::: {.note}
## La métrique BM25

BM25 est un modèle de récupération d'informations basé sur la pertinence probabiliste, au même titre que TF-IDF. BM25 est souvent utilisée dans les moteurs de recherche pour classer les documents par rapport à une requête.

BM25 repose sur une combinaison de la fréquence des termes (TF), la fréquence inverse des documents (IDF), et une normalisation basée sur la longueur des documents. Autrement dit, il s'agit de tenir compte d'améliorer TF-IDF tout en normalisant les mesures en fonction de la taille des _strings_ afin de ne pas surpondérer les grands documents. 

BM25 est donc particulièrement performant dans des environnements où les documents varient en longueur et en contenu. C'est pour cette raison que des moteurs de recherche comme `Elasticsearch` en ont fait une pierre angulaire du mécanisme de recherche.
:::

Pourquoi ne sont-ils pas tous pertinents ? On peut anticiper plusieurs raisons à cela.

La première hypothèse vient du fait qu'on entraîne notre vectoriseur sur un corpus biaisé. Certes Frankestein est un terme rare mais il est beaucoup plus fréquent dans notre corpus que dans la langue anglaise. L'_inverse document frequency_ est donc biaisée en défaveur de ce terme: son apparition devrait être un signe beaucoup plus fort que le texte recherché correspond à Mary Shelley. Si cela peut améliorer un peu la pertinence des résultats renvoyés, ce n'est néanmoins pas là que le bât blesse. 

L'approche fréquentiste suppose que les termes sont aussi dissemblables les uns que les autres. Une phrase où apparaît le terme _"créature"_ ne bénéficiera pas d'un score positif si on recherche _"monstre"_. De plus, là encore, nous avons pris notre corpus comme un sac où les mots sont indépendants: on n'a pas plus de chance de tirer _"Frankenstein"_ après _"docteur"_. Ces limites vont nous amener vers le sujet des _embeddings_. Néanmoins, si l'approche fréquentiste est un peu _old school_, elle n'est néanmoins pas inutile et représente souvent une _"tough to beat baseline"_. Dans les domaines de l'extraction d'information avec des textes courts, où chaque terme est porteur d'un signal fort, cette approche est souvent judicieuse.



## Trouver l'auteur le plus proche: une introduction au classifieur Bayes

Avant d'explorer les _embeddings_, nous pouvons essayer d'avoir un cas d'usage un petit peu différent dans notre cadre probabiliste. Supposons qu'on désire maintenant faire de la prédiction d'auteur. Si l'intuition précédente est vraie - certains mots sont plus probables dans les textes de certains auteurs - cela veut dire qu'on peut entraîner un algorithme de classification automatique à prédire un auteur à partir d'un texte.

La méthode la plus naturelle pour se lancer dans cette approche est d'utiliser le classifieur naif de Bayes. Ce dernier est parfaitement adapté à l'approche fréquentiste que nous avons adoptée jusqu'à présent puisqu'il exploite les probabilités d'occurrence de mots par auteur. 


OLD -----------------


# Prédiction sur le set d'entraînement

Nous allons à présent vérifier cette conjecture en comparant
plusieurs modèles de vectorisation,
_i.e._ de transformation du texte en objets numériques
pour que l'information contenue soit exploitable dans un modèle de classification.

## Démarche

Comme nous nous intéressons plus à l'effet de la vectorisation qu'à la tâche de classification en elle-même,
nous allons utiliser un algorithme de classification simple (un SVM linéaire), avec des paramètres non fine-tunés (c'est-à-dire des paramètres pas nécessairement choisis pour être les meilleurs de tous).

```{python}
#| eval: false
clf = LinearSVC(max_iter=10000, C=0.1, dual="auto")
```

Ce modèle est connu pour être très performant sur les tâches de classification de texte, et nous fournira donc un bon modèle de référence (*baseline*). Cela nous permettra également de comparer de manière objective l'impact des méthodes de vectorisation sur la performance finale.

Pour les deux premières méthodes de vectorisation
(basées sur des fréquences et fréquences relatives des mots),
on va simplement normaliser les données d'entrée, ce qui va permettre au SVM de converger plus rapidement, ces modèles étant sensibles aux différences d'échelle dans les données.

On va également _fine-tuner_ via _grid-search_
certains hyperparamètres liés à ces méthodes de vectorisation : 

- on teste différents _ranges_ de `n-grams` (unigrammes et unigrammes + bigrammes)
- on teste avec et sans _stop-words_

Afin d'éviter le surapprentissage,
on va évaluer les différents modèles via validation croisée, calculée sur 4 blocs.

On récupère à la fin le meilleur modèle selon une métrique spécifiée.
On choisit le `score F1`,
moyenne harmonique de la précision et du rappel,
qui donne un poids équilibré aux deux métriques, tout en pénalisant fortement le cas où l'une des deux est faible.
Précisément, on retient le `score F1 *micro-averaged*` :
les contributions des différentes classes à prédire sont agrégées,
puis on calcule le `score F1` sur ces données agrégées.
L'avantage de ce choix est qu'il permet de tenir compte des différences
de fréquences des différentes classes.

## Pipeline de prédiction

On va utiliser un *pipeline* `scikit` ce qui va nous permettre d'avoir
un code très concis pour effectuer cet ensemble de tâches cohérentes. 
De plus, cela va nous assurer de gérer de manière cohérente nos différentes
transformations (cf. [partie sur les pipelines](#pipelines))

Pour se faciliter la vie, on définit une fonction `fit_vectorizers` qui
intègre dans un *pipeline* générique une méthode d'estimation `scikit`
et fait de la validation croisée en cherchant le meilleur modèle
(en excluant/incluant les *stop words* et avec unigrammes/bigrammes)

```{python}
#| eval: false
def fit_vectorizers(vectorizer):
    pipeline = Pipeline(
    [
        ("vect", vectorizer()),
        ("scaling", StandardScaler(with_mean=False)),
        ("clf", clf),
    ]
    )

    parameters = {
        "vect__ngram_range": ((1, 1), (1, 2)),  # unigrams or bigrams
        "vect__stop_words": ("english", None)
    }

    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',
                               cv=4, n_jobs=4, verbose=1)
    grid_search.fit(X_train, y_train)

    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

    print(f"CV scores {grid_search.cv_results_['mean_test_score']}")
    print(f"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}")
    
    return grid_search
```

# Approche _bag-of-words_

On commence par une approche __"bag-of-words"__, 
i.e. qui revient simplement à représenter chaque document par un vecteur
qui compte le nombre d'apparitions de chaque mot du vocabulaire dans le document.

Illustrons d'abord le principe à l'aide d'un exemple simple.

```{python}
#| eval: false
corpus = [
    'Un premier document à propos des chats.',
    'Un second document qui parle des chiens.'
]

vectorizer = CountVectorizer()
vectorizer.fit(corpus)
```

L'objet `vectorizer` a été "entraîné" (*fit*) sur notre corpus d'exemple contenant deux documents. Il a notamment appris le vocabulaire complet du corpus, dont on peut afficher l'ordre.

```{python}
#| eval: false
vectorizer.get_feature_names_out()
```

L'objet `vectorizer` entraîné peut maintenant vectoriser le corpus initial, selon l'ordre du vocabulaire affiché ci-dessus.

```{python}
#| eval: false
X = vectorizer.transform(corpus)
print(X.toarray())
```

Quel score `F1` obtient-on finalement avec cette méthode de vectorisation sur notre problème de classification d'auteurs ?

```{python}
#| eval: false
cv_bow = fit_vectorizers(CountVectorizer)
```

# TF-IDF

On s'intéresse ensuite à l'approche __TF-IDF__,
qui permet de tenir compte des fréquences *relatives* des mots.

Ainsi, pour un mot donné, on va multiplier la fréquence d'apparition du mot dans le document (calculé comme dans la méthode précédente) par un terme qui pénalise une fréquence élevée du mot dans le corpus. L'image ci-dessous, empruntée à Chris Albon, illustre cette mesure:

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/tfidf.png)

*Source: [Towards Data Science](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)*

La vectorisation `TF-IDF` permet donc de limiter l'influence des *stop-words*
et donc de donner plus de poids aux mots les plus salients d'un document.
Illustrons cela à nouveau avec notre corpus d'exemple de deux documents.

```{python}
#| eval: false
corpus = [
    'Un premier document à propos des chats.',
    'Un second document qui parle des chiens.'
]

vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
```

Là encore, le vectoriseur a "appris" le vocabulaire du corpus.

```{python}
#| eval: false
vectorizer.get_feature_names_out()
```

Et peut être utilisé pour calculer les scores TF-IDF de chacun des termes des documents.

```{python}
#| eval: false
X = vectorizer.transform(corpus)
print(X.toarray())
```

On remarque que "chats" et "chiens" possèdent les scores les plus élevés, ce sont bien les termes les plus distinctifs. A l'inverse, les termes qui reviennent dans les deux documents ("un", "document", "des") ont un score inférieur, car ils sont beaucoup présents dans le corpus relativement.

Quel score `F1` obtient-on avec cette méthode de vectorisation sur notre problème de classification d'auteurs ?

```{python}
#| eval: false
cv_tfidf = fit_vectorizers(TfidfVectorizer)
```

On observe clairement que la performance de classification est bien supérieure,
ce qui montre la pertinence de cette technique.

# Word2vec avec averaging

On va maintenant explorer les techniques de vectorisation basées sur les
*embeddings* de mots, et notamment la plus populaire : `Word2Vec`.

L'idée derrière est simple, mais a révolutionné le NLP :
au lieu de représenter les documents par des
vecteurs *sparse* de très grande dimension (la taille du vocabulaire)
comme on l'a fait jusqu'à présent,
on va les représenter par des vecteurs *dense* (continus)
de dimension réduite (en général, autour de 100-300).

Chacune de ces dimensions va représenter un facteur latent,
c'est à dire une variable inobservée,
de la même manière que les composantes principales produites par une ACP.

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/w2v_vecto.png)

*Source: [Medium](https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d)*


__Pourquoi est-ce intéressant ?__
Pour de nombreuses raisons, mais pour résumer :
cela permet de beaucoup mieux capturer la similarité sémantique entre les documents.

Par exemple, un humain sait qu'un document contenant le mot _"Roi"_
et un autre document contenant le mot _"Reine"_ ont beaucoup de chance
d'aborder des sujets semblables.

Pourtant, une vectorisation de type comptage ou TF-IDF
ne permet pas de saisir cette similarité :
le calcul d'une mesure de similarité (norme euclidienne ou similarité cosinus)
entre les deux vecteurs ne prendra en compte la similarité des deux concepts, puisque les mots utilisés sont différents.

A l'inverse, un modèle `word2vec` bien entraîné va capter
qu'il existe un facteur latent de type _"royauté"_,
et la similarité entre les vecteurs associés aux deux mots sera forte.

La magie va même plus loin : le modèle captera aussi qu'il existe un
facteur latent de type _"genre"_,
et va permettre de construire un espace sémantique dans lequel les
relations arithmétiques entre vecteurs ont du sens ;
par exemple :
$$\text{king} - \text{man} + \text{woman} ≈ \text{queen}$$

__Comment ces modèles sont-ils entraînés ?__
Via une tâche de prédiction résolue par un réseau de neurones simple.

L'idée fondamentale est que la signification d'un mot se comprend
en regardant les mots qui apparaissent fréquemment dans son voisinage.

Pour un mot donné, on va donc essayer de prédire les mots
qui apparaissent dans une fenêtre autour du mot cible.

En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,
on obtient finalement des *embeddings* pour chaque mot du vocabulaire,
qui présentent les propriétés discutées précédemment.


```{python}
#| eval: false
X_train_tokens = [text.split() for text in X_train]
w2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, 
                     min_count=1, workers=4)
```


```{python}
#| eval: false
w2v_model.wv.most_similar("mother")
```

On voit que les mots les plus similaires à _"mother"_
sont souvent des mots liés à la famille, mais pas toujours.

C'est lié à la taille très restreinte du corpus sur lequel on entraîne le modèle,
qui ne permet pas de réaliser des associations toujours pertinentes.


L'*embedding* (la représentation vectorielle) de chaque document correspond à la moyenne des *word-embeddings* des mots qui le composent : 


```{python}
#| eval: false
def get_mean_vector(w2v_vectors, words):
    words = [word for word in words if word in w2v_vectors]
    if words:
        avg_vector = np.mean(w2v_vectors[words], axis=0)
    else:
        avg_vector = np.zeros_like(w2v_vectors['hi'])
    return avg_vector

def fit_w2v_avg(w2v_vectors):
    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)
                                for words in X_train_tokens])
    
    scores = cross_val_score(clf, X_train_vectors, y_train, 
                         cv=4, scoring='f1_micro', n_jobs=4)

    print(f"CV scores {scores}")
    print(f"Mean F1 {np.mean(scores)}")
    return scores
```


```{python}
#| eval: false
cv_w2vec = fit_w2v_avg(w2v_model.wv)
```

La performance chute fortement ;
la faute à la taille très restreinte du corpus, comme annoncé précédemment.

# Word2vec pré-entraîné + averaging

Quand on travaille avec des corpus de taille restreinte,
c'est généralement une mauvaise idée d'entraîner son propre modèle `word2vec`.

Heureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles.
Ils permettent de réaliser du *transfer learning*,
c'est-à-dire de bénéficier de la performance d'un modèle qui a été entraîné sur une autre tâche ou bien sur un autre corpus.

L'un des modèles les plus connus pour démarrer est le `glove_model` de
`Gensim` (Glove pour _Global Vectors for Word Representation_)[^1]:

> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. 
>
> _Source_ : https://nlp.stanford.edu/projects/glove/

[^1]: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. _GloVe: Global Vectors for Word Representation_. 

On peut le charger directement grâce à l'instruction suivante : 

```{python}
#| output: hide
#| eval: false
glove_model = gensim.downloader.load('glove-wiki-gigaword-200')
```

Par exemple, la représentation vectorielle de roi est l'objet
multidimensionnel suivant :

```{python}
#| eval: false
glove_model['king']
```

Comme elle est peu intelligible, on va plutôt rechercher les termes les
plus similaires. Par exemple,

```{python}
#| eval: false
glove_model.most_similar('mother')
```

On peut retrouver notre formule précédente

$$\text{king} - \text{man} + \text{woman} ≈ \text{queen}$$
dans ce plongement de mots:

```{python}
#| eval: false
glove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])
```

Vous pouvez vous référer à [ce tutoriel](https://jalammar.github.io/illustrated-word2vec/)
pour en découvrir plus sur `Word2Vec`.

Faisons notre apprentissage par transfert :

```{python}
#| eval: false
cv_w2vec_transfert = fit_w2v_avg(glove_model)
```

La performance remonte substantiellement.
Cela étant, on ne parvient pas à faire mieux que les approches basiques,
on arrive à peine aux performances de la vectorisation par comptage.

En effet, pour rappel, les performances sont les suivantes :

```{python}
#| eval: false
perfs = pd.DataFrame(
    [np.mean(cv_bow.cv_results_['mean_test_score']),
     np.mean(cv_tfidf.cv_results_['mean_test_score']),
    np.mean(cv_w2vec),
    np.mean(cv_w2vec_transfert)],
    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pré-entraîné', 'Word2Vec pré-entraîné'],
    columns = ["Mean F1 score"]
).sort_values("Mean F1 score",ascending = False)
perfs
```

Les performences limitées du modèle *Word2Vec* sont cette fois certainement dues à la manière dont
les *word-embeddings* sont exploités : ils sont moyennés pour décrire chaque document. 

Cela a plusieurs limites : 

- on ne tient pas compte de l'ordre et donc du contexte des mots
- lorsque les documents sont longs, la moyennisation peut créer
des représentation bruitées.

# Contextual embeddings

Les *embeddings* contextuels visent à pallier les limites des *embeddings*
traditionnels évoquées précédemment.

Cette fois, les mots n'ont plus de représentation vectorielle fixe,
celle-ci est calculée dynamiquement en fonction des mots du voisinage, et ainsi de suite.
Cela permet de tenir compte de la structure des phrases
et de tenir compte du fait que le sens d'un mot est fortement dépendant des mots
qui l'entourent. 
Par exemple, dans les expressions "le président Macron" et "le camembert Président" le mot président n'a pas du tout le même rôle.

Ces *embeddings* sont produits par des architectures très complexes,
de type Transformer (`BERT`, etc.).


_TODO: approfondir le sujet_

```{python}
#| eval: false
#| output: hide
model = SentenceTransformer('all-mpnet-base-v2')
```


```{python}
#| eval: false
#| echo: false
X_train_vectors = model.encode(X_train)
```


```{python}
#| eval: false
#| echo: false
scores = cross_val_score(clf, X_train_vectors, y_train, 
                         cv=4, scoring='f1_micro', n_jobs=4)

print(f"CV scores {scores}")
print(f"Mean F1 {np.mean(scores)}")
```

```{python}
#| echo: false
#| eval: false
perfs = pd.concat(
  [perfs,
  pd.DataFrame(
    [np.mean(scores)],
    index = ['Contextual Embedding'],
    columns = ["Mean F1 score"])]
).sort_values("Mean F1 score",ascending = False)
perfs
```


Verdict : on fait très légèrement mieux que la vectorisation TF-IDF.
On voit donc l'importance de tenir compte du contexte.

__Mais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?__

On peut avancer plusieurs raisons : 

- le `TF-IDF` est un modèle simple, mais toujours très performant
(on parle de _"tough-to-beat baseline"_).
- la classification d'auteurs est une tâche très particulière et très ardue,
qui ne fait pas justice aux *embeddings*. Comme on l'a dit précédemment, ces derniers se révèlent particulièrement pertinents lorsqu'il est question de similarité sémantique entre des textes (_clustering_, etc.).

Dans le cas de notre tâche de classification, il est probable que
certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,
ce que ne permettent pas de capter les *embeddings* qui accordent à tous les mots la même importance.

# Aller plus loin

- Nous avons entraîné différents modèles sur l'échantillon d'entraînement par validation croisée, mais nous n'avons toujours pas utilisé l'échantillon test que nous avons mis de côté au début. Réaliser la prédiction sur les données de test, et vérifier si l'on obtient le même classement des méthodes de vectorisation.
- Faire un *vrai* split train/test : faire l'entraînement avec des textes de certains auteurs, et faire la prédiction avec des textes d'auteurs différents. Cela permettrait de neutraliser la présence de noms de lieux, de personnages, etc.
- Comparer avec d'autres algorithmes de classification qu'un SVM
- (Avancé) : fine-tuner le modèle d'embeddings contextuels sur la tâche de classification
