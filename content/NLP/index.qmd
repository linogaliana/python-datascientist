---
title: "Partie 4 : Natural Language Processing (NLP)"
date: 2020-10-14T13:00:00Z
slug: "nlp"
categories:
  - Introduction
  - NLP
description: |
  L'un des grands avantages comparatifs de {{< fa brands python >}} par rapport aux
  langages concurrents ({{< fa brands r-project >}} notamment) est dans
  la richesse des librairies de traitement du langage naturel (mieux
  connu sous son acronyme anglais : NLP pour _natural langage processing_).
  Cette partie vise √† illustrer la richesse de cet √©cosyst√®me √† partir
  de quelques exemples litt√©raires :  Dumas, Poe, Shelley, Lovecraft.
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/nlp.png
---

Les parties pr√©c√©dentes √©taient consacr√©es √† l'acquisition de comp√©tences
transversales pour la valorisation des donn√©es. De mani√®re naturelle, 
nous nous sommes jusqu'√† pr√©sent plut√¥t consacr√©s
√† la valorisation de donn√©es structur√©es, d'une
dimension certes modeste mais qui ouvraient d√©j√† √©norm√©ment de
probl√©matiques √† creuser. Cette partie propose maintenant de se
consacrer √† un sujet dont il n'est pas √©vident _a priori_ que
les ordinateurs s'emparent: le __langage humain__ et sa richesse. 

En effet, si la linguistique propose certes de repr√©senter
de mani√®re conceptuelle le langage sous une forme de donn√©es, comment
des ordinateurs, qui au fond ne connaissent que le _0_ et le _1_, peuvent-ils
s'approprier cet objet √©minemment complexe qu'est le langage et qu'un 
humain met lui-m√™me des ann√©es √† comprendre et s'approprier ?[^blague]

[^blague]: En √©crivant ces lignes j'ai demand√© √† `ChatGPT` de me faire une blague. 

    > Pourquoi les programmeurs d√©testent-ils la plage ?
    > Parce que le sable change tout le temps de paradigme !

    Ne l'ayant pas comprise du fait de mon esprit obtus, je lui
    ai demand√© de me l'expliquer. Voici sa r√©ponse

    > Bien s√ªr, je serais ravi de l'expliquer ! Cette blague joue sur le double sens du mot "paradigme" et fait un jeu de mots avec les notions li√©es √† la programmation et √† la plage.
    >
    > Dans le domaine de la programmation, un "paradigme" d√©signe un mod√®le ou une m√©thode de programmation, comme la programmation orient√©e objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec diff√©rents paradigmes de programmation.
    >
    > D'autre part, en contexte de plage, "paradigme" est utilis√© pour signifier "mar√©e qui monte et descend", se r√©f√©rant √† l'action naturelle du sable et de l'eau sur la plage.
    >
    > La blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu'ils doivent s'adapter √† de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les mar√©es. En associant les deux sens du mot "paradigme", la blague cr√©e une situation absurde et amusante, faisant sourire ceux qui comprennent les r√©f√©rences √† la programmation et √† la plage.
   
    Finalement, j'ai trouv√© son explication plus dr√¥le que sa blague.  
  

Le traitement automatique du langage - traduction fran√ßaise du concept de
_natural language processing_ (NLP) - est l'ensemble des techniques
permettant aux ordinateurs de comprendre, analyser synth√©tiser et
g√©n√©rer le langage humain[^1]. 

[^1]: Le concept de _natural language processing_ tend √† remplacer celui
de _text mining_ dont le p√©rim√®tre √©tait plus sp√©cifique. Le _text mining_
renvoie √† la recherche d'information - la fouille - dans un ensemble de 
textes. C'est l'un des champs d'applications
du _natural language processing_ mais ce n'est pas l'unique.

Il s'agit d'un champ disciplinaire √† l'intersection de la statistique
et de la linguistique qui connait depuis quelques ann√©es un engouement
important, que ce soit d'un point de vue acad√©mique ou op√©rationnel. 
Certaines des applications de ces techniques sont devenues incontournables
dans nos t√¢ches quotidiennes, notamment les moteurs de recherche, la traduction
automatique et plus r√©cemment les _chatbots_.

## R√©sum√© de la partie

Cette partie du cours est consacr√©e √† l'analyse des donn√©es textuelles avec
des exemples de üìñ pour s'amuser. Elle est une introduction progressive
√† ce sujet en se concentrant sur des concepts de base, n√©cessaires √†
la compr√©hension ult√©rieure de principes plus avanc√©s et de techniques
sophistiqu√©es[^2]. Cette partie pr√©sente principalement:

- Les enjeux de nettoyage de champs textuels
et d'analyse de fr√©quence. Il s'agit de NLP un
peu _old school_ mais dont la compr√©hension est n√©cessaire pour aller
plus loin ;
- La mod√©lisation du langage, selon plusieurs approches. 

[^2]: Par exemple, le concept d'_embedding_ - transformation d'un champ
textuel en un vecteur num√©rique multidimensionnel - aujourd'hui central
dans le NLP n'est √©voqu√© qu'√† quelques reprises. Avant d'en arriver
au sujet des _embeddings_, il est pr√©cieux de comprendre les apports et les
limites de concepts comme
le sac de mot (_bag of words_) ou la distance
TF-IDF (_term frequency - inverse document frequency_). Dans une
optique introductive, ce cours se focalise donc sur ces derniers pour faciliter
l'ouverture ult√©rieure de la boite de Pandore que sont les _embeddings_. 

### Nettoyages textuels et analyse de fr√©quences

`Python` est un excellent outil pour l'analyse de donn√©es textuelles. 
Les m√©thodes de base ou les librairies sp√©cialis√©es
comme `NLTK` et `SpaCy` permettent d'effectuer ces t√¢ches de mani√®re
tr√®s efficace. Les ressources en ligne sur le sujet sont tr√®s 
nombreuses. `Python` est bien mieux outill√© que `R` pour l'analyse de
donn√©es textuelles. 

Dans un premier temps, cette partie propose
de revenir sur la mani√®re de structurer et nettoyer un corpus 
textuel au travers de l'approche *bag of words* (sac de mots). 
Elle vise √† montrer comment transformer un corpus en outil propre √† une 
analyse statistique :

* Elle propose d'abord une introduction aux enjeux du nettoyage des donn√©es
textuelles √† travers l'analyse du *Comte de Monte Cristo* d'Alexandre Dumas
[ici](/content/NLP/01_intro.qmd) qui permet de synth√©tiser rapidement l'information disponible
dans un large volume de donn√©es (√† l'image de la @fig-wordcloud-dumas)
* Elle propose ensuite une s√©rie d'exercices sur le nettoyage de textes √† partir des
oeuvres d'Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant √† distinguer la 
sp√©cificit√© du vocabulaire employ√© par chaque auteurs (par exemple @fig-waffle-fear). Ces exercices sont 
disponibles [dans le deuxi√®me chapitre](/content/NLP/01_exoclean.html) de la partie.


### Mod√©lisation du langage

La suite de cette partie proposera une introduction aux enjeux de mod√©lisation
du langage. Ceux-ci sont tr√®s √† la mode du fait du succ√®s de `ChatGPT`. N√©anmoins, avant
d'en arriver aux LLM, il est n√©cessaire de passer par quelques mod√©lisations 
pr√©liminaires. 

Nous proposerons d'abord d'explorer une approche alternative, prenant en compte
le contexte d'apparition d'un mot. L'introduction √† la
_Latent Dirichlet Allocation_ (LDA) sera l'occasion de pr√©senter la mod√©lisation
de documents sous la forme de *topics*.

Enfin, nous introduirons aux enjeux de la transformation de champs textuels
sous forme de vecteurs num√©riques. Pour cela, nous pr√©senterons le principe
de `Word2Vec` qui permet ainsi, par exemple,
malgr√© une distance syntaxique importante,
de dire que s√©mantiquement `Homme` et `Femme` sont proches.
Ce chapitre est une passerelle vers le concept d'_embedding_, v√©ritable
r√©volution r√©cente du NLP, et qui permet de rapprocher des corpus
non seulement sur leur proximit√© syntaxique (partagent-ils par exemple des mots
communs ?) mais aussi sur leur proximit√© s√©mantique (partagent-ils un th√®me ou un sens commun ?).[^embedding]

[^embedding]: Un exemple d'int√©r√™t de ce type d'approche est la @fig-relevanc-table-embedding.

Comme l'illustre la figure suivante, emprunt√©e √† [Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder), les concepts que nous allons aborder
dans cette partie sont indispensables pour √™tre en mesure
d'entra√Æner ou r√©utiliser ult√©rieurement un mod√®le
sophistiqu√© d'analyse du langage:

::: {#fig-encoder}

![Illustration transformer architecture](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png){#fig-encoder-decoder}

Illustration of the original transformer architecture proposed in [Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)
(source: [Sebastien Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder))
:::


## Pour aller plus loin {-}

La recherche dans le domaine du NLP est tr√®s active. Il est donc recommand√©
de faire preuve de curiosit√© pour en apprendre plus car une ressource
unique ne compilera pas l'ensemble des connaissances, _a fortiori_ dans
un champ de recherche aussi dynamique que le NLP. 

Pour approfondir les comp√©tences √©voqu√©es dans ce cours, je recommande vivement 
ce [cours d'`HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt). 

Pour comprendre l'architecture interne d'un LLM,
ce [post de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
est tr√®s utile. 


Ces chapitres n'√©puisent pas les cas d'usage du NLP pour les _data scientists_. Par exemple,
dans le domaine de la statistique publique, un des principaux cas d'usage du NLP est l'utilisation
de techniques de classification automatique pour transformer des r√©ponses libres dans des questionnaires
en champs pr√©d√©finis dans une nomenclature. 

Voici un exemple sur un projet de classification automatis√©e des professions dans la typologie
des nomenclatures d'activit√©s:

::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
viewof activite = Inputs.text( 
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Pr√©diction de l'activit√©
    </caption>
    <tr>
      <th style="text-align:center;">Libell√© (NA2008)</th>
      <th>Probabilit√©</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

:::

::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = f"https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite}"
import requests
data = requests.get(urlApe).json()

# Extract 'IC' value
IC = data['IC']
data.pop('IC', None)

df = pd.DataFrame(data.values())
df['indice_confiance'] = IC
df
```

:::