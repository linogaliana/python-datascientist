---
title: "Partie 4 : Natural Language Processing (NLP)"
title-en: "Part 4: Natural Language Processing (NLP)"
description: |
  Cette partie du cours introduit le traitement automatique du langage (NLP), un domaine scientifique √† la crois√©e de la linguistique et des statistiques devenu, du fait de l'engouement envers les IA g√©n√©ratives, central dans le domaine de la _data science_. A travers des exemples litt√©raires, dcette partie explore d‚Äôabord des m√©thodes classiques comme l‚Äôanalyse fr√©quentiste et le traitement de corpus textuels sous la forme de _bag of words_. Ensuite, elle aborde la mod√©lisation du langage qui ouvre la voie √† des approches plus originales. L‚Äôobjectif de chapitre est de rappeler quelques √©l√©ments g√©n√©raux sur le vaste domaine qu'est le NLP.
description-en: |
  This part of the course introduces automatic language processing (NLP), a scientific field at the crossroads of linguistics and statistics that has become central to the field of _data science_ as a result of the craze for generative AI. Using literary examples, this section first explores classic methods such as frequentist analysis and the processing of textual corpora in the form of _bag of words_. It then looks at language modelling, which opens the way to more original approaches. The aim of this chapter is to recall a few general points about the vast field of NLP
categories:
  - Introduction
  - NLP
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/nlp.png
---

::: {.content-visible when-profile="fr"}

# Introduction

Les parties pr√©c√©dentes √©taient consacr√©es √† l'acquisition de comp√©tences transversales pour la valorisation des donn√©es. De mani√®re naturelle, nous nous sommes jusqu'√† pr√©sent plut√¥t consacr√©s √† la valorisation de donn√©es structur√©es, d'une dimension certes modeste mais qui ouvraient d√©j√† √©norm√©ment de probl√©matiques √† creuser. Cette partie propose maintenant de se consacrer √† un sujet dont il n'est pas √©vident _a priori_ que les ordinateurs s'emparent, source de d√©bats philosophiques s√©culaires, de Platon √† Saussure : le __langage humain__ et sa richesse.

En faisant l'analogie entre langue et langage, c'est-√†-dire en d√©finissant ce dernier comme la capacit√© d'expression et de communication d'une pens√©e par le biais de signes et en d√©finissant la langue comme la mise en ≈ìuvre conventionnelle de cette capacit√©, on peut se placer dans les traces de la linguistique et repr√©senter le langage sous une forme de donn√©es.
Ceci ouvre la voie √† l'analyse statistique ou algorithmique. N√©anmoins, m√™me s'il existe des r√©gularit√©s statistiques, comment des ordinateurs, qui au fond ne connaissent que le _0_ et le _1_, peuvent-ils s'approprier cet objet √©minemment complexe qu'est le langage et qu'un humain met lui-m√™me des ann√©es √† comprendre et s'approprier ?[^blague]

[^blague]: En √©crivant ces lignes j'ai demand√© √† `ChatGPT` de me faire une blague.

    > Pourquoi les programmeurs d√©testent-ils la plage ?
    > Parce que le sable change tout le temps de paradigme !

    Ne l'ayant pas comprise, je lui ai demand√© de me l'expliquer. Voici sa r√©ponse :

    > Bien s√ªr, je serais ravi de l'expliquer ! Cette blague joue sur le double sens du mot "paradigme" et fait un jeu de mots avec les notions li√©es √† la programmation et √† la plage.
    >
    > Dans le domaine de la programmation, un "paradigme" d√©signe un mod√®le ou une m√©thode de programmation, comme la programmation orient√©e objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec diff√©rents paradigmes de programmation.
    >
    > D'autre part, en contexte de plage, "paradigme" est utilis√© pour signifier "mar√©e qui monte et descend", se r√©f√©rant √† l'action naturelle du sable et de l'eau sur la plage.
    >
    > La blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu'ils doivent s'adapter √† de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les mar√©es. En associant les deux sens du mot "paradigme", la blague cr√©e une situation absurde et amusante, faisant sourire ceux qui comprennent les r√©f√©rences √† la programmation et √† la plage.

    Finalement, j'ai trouv√© son explication plus dr√¥le que sa blague.

:::

::: {.content-visible when-profile="en"}

# Introduction

The previous sections focused on acquiring cross-functional skills for working with data. Naturally, we have so far mostly focused on structured data‚Äîmodest in size but already rich in analytical potential. This new section turns to a subject that, at first glance, may seem unlikely to be handled by computers‚Äîa topic of centuries-old philosophical debate, from Plato to Saussure: the richness of __human language__.

By drawing an analogy between "language" and "tongue"‚Äîthat is, defining the former as the capacity to express and communicate thought through signs, and the latter as the conventional implementation of that capacity‚Äîwe align ourselves with the field of linguistics and treat language as data.
This opens the door to statistical and algorithmic analysis. Yet, even if statistical regularities exist, how can computers‚Äîultimately limited to just _0_ and _1_‚Äîgrasp such a complex object as language, which takes humans years to understand and master?[^blague]

[^blague]: While writing these lines, I asked `ChatGPT` to tell me a joke.

    > Why do programmers hate the beach?
    > Because the sand keeps changing paradigms!

    Not understanding it, I asked for an explanation. Here's what I got:

    > Of course, I'd be happy to explain! This joke plays on the double meaning of the word "paradigm" and is a pun involving programming and the beach.
    >
    > In programming, a "paradigm" refers to a model or method of programming, such as object-oriented or functional programming. Programmers are often familiar with different paradigms.
    >
    > On the other hand, in a beach context, "paradigm" is humorously misused to mean "tide"‚Äîalluding to the natural shifting of sand and water.
    >
    > The joke plays on the humorous frustration programmers might feel when adapting to new programming paradigms, just like how sand constantly shifts with the tides. By combining both meanings of "paradigm," the joke creates an absurd and amusing scenario that will resonate with those familiar with programming and beaches.

    In the end, I found the explanation funnier than the joke itself.

:::


::: {.content-visible when-profile="fr"}
# Le traitement automatique du langage

Le traitement automatique du langage ‚Äî traduction fran√ßaise du concept de _natural language processing_ (NLP) ‚Äî est l'ensemble des techniques permettant aux ordinateurs de comprendre, analyser, synth√©tiser et g√©n√©rer le langage humain[^1].

[^1]: Le concept de _natural language processing_ tend √† remplacer celui de _text mining_ dont le p√©rim√®tre √©tait plus sp√©cifique. Le _text mining_ renvoie √† la recherche d'information ‚Äî la fouille ‚Äî dans un ensemble de textes. C'est l'un des champs d'applications du _natural language processing_ mais ce n'est pas l'unique. L'accroissement des ressources de calcul et les progr√®s dans la formalisation du langage ont permis d'√©largir le champ des domaines o√π la linguistique computationnelle intervient.

Il s'agit d'un champ disciplinaire √† l'intersection de la statistique et de la linguistique qui conna√Æt depuis quelques ann√©es un engouement important, que ce soit d'un point de vue acad√©mique, op√©rationnel ou industriel.
Certaines des applications de ces techniques sont devenues incontournables dans nos t√¢ches quotidiennes, notamment les moteurs de recherche, la traduction automatique et plus r√©cemment les _chatbots_, dont le d√©veloppement conna√Æt depuis l'√©mergence de `ChatGPT` en d√©cembre 2022 un rythme fou.
:::

::: {.content-visible when-profile="en"}
# Natural Language Processing

Natural Language Processing (NLP) refers to the set of techniques that allow computers to understand, analyze, synthesize, and generate human language[^1].

[^1]: The concept of "natural language processing" has tended to replace the more specific term "text mining." Text mining refers to the search for ‚Äî or mining of ‚Äî information within a set of texts. While it is one application area of NLP, it is not the only one. Advances in computing power and in the formalization of language have expanded the scope of fields where computational linguistics is applied.

NLP is a disciplinary field at the intersection of statistics and linguistics, which has experienced significant growth in recent years ‚Äî academically, operationally, and industrially.
Some applications of these techniques have become essential in our daily lives, such as search engines, machine translation, and more recently, chatbots, whose development has accelerated rapidly since the launch of `ChatGPT` in December 2022.
:::


::: {.content-visible when-profile="fr"}

# R√©sum√© de la partie

Cette partie du cours est consacr√©e √† l'analyse des donn√©es textuelles avec
des exemples de üìñ pour s'amuser. Elle est une introduction progressive
√† ce sujet en se concentrant sur des concepts de base, n√©cessaires √†
la compr√©hension ult√©rieure de principes plus avanc√©s et de techniques
sophistiqu√©es[^2]. Cette partie pr√©sente principalement :

- Les enjeux de nettoyage de champs textuels
et d'analyse de fr√©quence. Il s'agit de NLP un
peu _old school_ mais dont la compr√©hension est n√©cessaire pour aller
plus loin ;
- La mod√©lisation du langage, selon plusieurs approches.

[^2]: Par exemple, le concept d'_embedding_ - transformation d'un champ
textuel en un vecteur num√©rique multidimensionnel - aujourd'hui central
dans le NLP n'est √©voqu√© qu'√† quelques reprises.

Avant d'en arriver
au sujet des _embeddings_, il est pr√©cieux de comprendre les apports et les
limites de concepts comme
le sac de mot (_bag of words_) ou la distance
TF-IDF (_term frequency - inverse document frequency_). L'un des apports principaux des grands mod√®les de langage, √† savoir la richesse de la f√™netre contextuelle leur permettant de mieux saisir les nuances textuelles et l'intentionalit√© du locuteur, s'√©clairent lorsqu'on saisit les limites du NLP traditionnel.

Dans une
optique introductive, ce cours se focalise donc sur les approches fr√©quentistes, notamment l'approche sac de mot, pour faciliter
l'ouverture ult√©rieure de la boite de Pandore que sont les _embeddings_.

:::

::: {.content-visible when-profile="en"}

# Section Summary

This part of the course is dedicated to text data analysis with üìñ examples for fun. It serves as a gradual introduction to the topic by focusing on basic concepts necessary for later understanding of more advanced principles and sophisticated techniques[^2]. This section mainly covers:

- The challenges of cleaning textual fields
and frequency analysis. This is somewhat _old school_ NLP but understanding it is essential to progress further;
- Language modeling using several approaches.

[^2]: For example, the concept of _embedding_‚Äîthe transformation of a text field into a multidimensional numeric vector‚Äîis central in NLP today but is only briefly mentioned here.

Before diving into the topic of _embeddings_, it's important to understand the contributions and limitations of concepts like
the bag of words or TF-IDF (_term frequency - inverse document frequency_). One of the main benefits of large language models‚Äînamely the richness of their contextual window that allows them to better grasp textual nuances and speaker intentionality‚Äîbecomes clearer when the limitations of traditional NLP are understood.

As an introductory perspective, this course focuses on frequency-based approaches, especially the bag-of-words approach, to ease into the later exploration of the Pandora‚Äôs box that is _embeddings_.

:::


::: {.content-visible when-profile="fr"}

## Nettoyages textuels et analyse de fr√©quences

`Python` est un excellent outil pour l'analyse de donn√©es textuelles.
Les m√©thodes de base de transformation de donn√©es textuelles ou de dictionnaires, associ√©es √† des librairies sp√©cialis√©es
comme `NLTK` et `SpaCy`, permettent d'effectuer des t√¢ches de normalisation et d'analyse de donn√©es textuelles de mani√®re
tr√®s efficace. `Python` est bien mieux outill√© que `R` pour l'analyse de
donn√©es textuelles.
Les ressources en ligne sur le sujet sont tr√®s
nombreuses et la meilleure des √©coles dans le domaine reste la pratique sur un corpus √† nettoyer.

Dans un premier temps, cette partie propose
de revenir sur la mani√®re de structurer et nettoyer un corpus
textuel au travers de l'approche *bag of words* (sac de mots).
Elle vise √† montrer comment transformer un corpus en outil propre √† une
analyse statistique :

* Elle propose d'abord une introduction aux enjeux du nettoyage des donn√©es
textuelles √† travers l'analyse du *Comte de Monte Cristo* d'Alexandre Dumas
[ici](/content/NLP/01_intro.qmd) qui permet de synth√©tiser rapidement l'information disponible
dans un large volume de donn√©es (√† l'image de la @fig-wordcloud-dumas)
* Elle propose ensuite une s√©rie d'exercices sur le nettoyage de textes √† partir des
oeuvres d'Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant √† distinguer la
sp√©cificit√© du vocabulaire employ√© par chaque auteurs (par exemple @fig-waffle-fear). Ces exercices sont
disponibles [dans le deuxi√®me chapitre](/content/NLP/01_exoclean.html) de la partie.

Cette analyse fr√©quentiste permet de prendre du recul sur la nature des donn√©es textuelles et sur les enjeux r√©currents dans la r√©duction de dimension de corpus en langue naturelle. Comme la statistique descriptive entra√Æne naturellement la mod√©lisation, cette approche fr√©quentiste va g√©n√©ralement amener rapidement √† vouloir synth√©tiser quelques lois derri√®re nos corpus textuels.

:::

::: {.content-visible when-profile="en"}

## Text Cleaning and Frequency Analysis

`Python` is an excellent tool for text data analysis.
Basic methods for transforming textual data or dictionaries, combined with specialized libraries
such as `NLTK` and `SpaCy`, make it possible to perform normalization and text data analysis
very efficiently. `Python` is much better equipped than `R` for text data analysis.
There is a wealth of online resources on this subject, and the best way to learn remains hands-on practice with a corpus to clean.

This section first revisits how to structure and clean a textual corpus
through the *bag of words* approach.
It aims to demonstrate how to turn a corpus into a tool suitable for
statistical analysis:

* It first introduces the challenges of text data cleaning
through an analysis of *The Count of Monte Cristo* by Alexandre Dumas
[here](/content/NLP/01_intro.qmd), which helps to quickly summarize the available information
in a large volume of text data (as illustrated by @fig-wordcloud-dumas)
* It then offers a series of exercises on text cleaning based on the
works of Edgar Allan Poe, Mary Shelley, and H.P. Lovecraft, aiming to highlight
the specificity of each author's vocabulary (for example, @fig-waffle-fear). These exercises are
available [in the second chapter](/content/NLP/01_exoclean.html) of the section.

This frequency-based analysis provides perspective on the nature of text data and recurring issues in dimensionality reduction of natural language corpora. Just as descriptive statistics naturally lead to modeling, this frequency approach typically quickly leads to the desire to identify underlying rules behind our text corpora.

:::


::: {.content-visible when-profile="fr"}

## Mod√©lisation du langage

La suite de cette partie proposera une introduction aux enjeux de mod√©lisation
du langage. Ceux-ci sont tr√®s √† la mode du fait du succ√®s de `ChatGPT`. N√©anmoins, avant
d'en arriver aux grands mod√®les de langage (LLM), ces r√©seaux de neurone ayant des milliards de param√®tres et entra√Æn√©s sur des volumes massifs de donn√©es, il est n√©cessaire de passer par quelques mod√©lisations
pr√©liminaires.

Nous proposerons d'abord d'explorer une approche alternative, prenant en compte
le contexte d'apparition d'un mot. L'introduction √† la
_Latent Dirichlet Allocation_ (LDA) sera l'occasion de pr√©senter la mod√©lisation
de documents sous la forme de *topics*. Celle-ci est n√©anmoins pass√©e de mode au profit des m√©thodes li√©es au concept d'_embedding_.

Nous introduirons ainsi √† la fin de cette partie du cours les enjeux de la transformation de champs textuels
sous forme de vecteurs num√©riques. Pour cela, nous pr√©senterons le principe
de `Word2Vec` qui permet ainsi, par exemple,
malgr√© une distance syntaxique importante,
de dire que s√©mantiquement `Homme` et `Femme` sont proches.
Ce chapitre est une passerelle vers le concept d'_embedding_, v√©ritable
r√©volution r√©cente du NLP, et qui permet de rapprocher des corpus
non seulement sur leur proximit√© syntaxique (partagent-ils par exemple des mots
communs ?) mais aussi sur leur proximit√© s√©mantique (partagent-ils un th√®me ou un sens commun ?)[^embedding]. Ce passage par `Word2Vec` permettra aux curieux de pouvoir ensuite passer aux mod√®les de type _transformers_, les mod√®les faisant aujourd'hui office de r√©f√©rence dans le domaine du NLP.

[^embedding]: Un exemple d'int√©r√™t de ce type d'approche est la @fig-relevanc-table-embedding.

:::

::: {.content-visible when-profile="en"}

## Language Modeling

The remainder of this section introduces the challenges of language modeling. These are currently very popular due to the success of `ChatGPT`. However, before delving into large language models (LLMs)‚Äîthose neural networks with billions of parameters trained on massive data volumes‚Äîit's important to first understand some preliminary modeling techniques.

We begin by exploring an alternative approach that takes into account the context in which a word appears. The introduction of
_Latent Dirichlet Allocation_ (LDA) serves as an opportunity to present document modeling through *topics*. However, this approach has fallen out of favor in comparison to methods related to the concept of _embedding_.

Toward the end of this course section, we will introduce the challenge of transforming textual fields
into numeric vector forms. To do so, we will present the principle
behind `Word2Vec`, which allows us, for instance,
despite significant syntactic distance,
to identify that semantically, `Man` and `Woman` are closely related.
This chapter serves as a bridge to the concept of _embedding_, a major recent revolution in NLP. It enables the comparison of corpora
not only by syntactic similarity (e.g., do they share common words?)
but also by semantic similarity (e.g., do they share a theme or meaning?)[^embedding]. Covering `Word2Vec` will give curious learners a solid foundation to then explore transformer-based models, which are now the benchmark in NLP.

[^embedding]: An example of the value of this approach can be seen in @fig-relevanc-table-embedding.

:::


::: {.content-visible when-profile="fr"}

# Pour aller plus loin {-}

La recherche dans le domaine du NLP est tr√®s active. Il est donc recommand√©
de faire preuve de curiosit√© pour en apprendre plus car une ressource
unique ne compilera pas l'ensemble des connaissances, _a fortiori_ dans
un champ de recherche aussi dynamique que le NLP.

Pour approfondir les comp√©tences √©voqu√©es dans ce cours, je recommande vivement
ce [cours d'`HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt).

Pour comprendre l'architecture interne d'un LLM,
ce [post de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
est tr√®s utile.

Ces chapitres n'√©puisent pas les cas d'usage du NLP pour les _data scientists_. Ils n'en sont que la surface √©merg√©e de l'iceberg.
Par exemple,
dans le domaine de la statistique publique, un des principaux cas d'usage du NLP est l'utilisation
de techniques de classification automatique pour transformer des r√©ponses libres dans des questionnaires
en champs pr√©d√©finis dans une nomenclature.
Il s'agit donc d'une adaptation, un peu sp√©cifique √† la statistique publique, grande utilisatrice de nomenclatures normalis√©es, de probl√©matiques de classification multi-niveaux.

Voici un exemple sur un projet de classification automatis√©e des professions dans la typologie
des nomenclatures d'activit√©s (les PCS) √† partir d'un mod√®le entra√Æn√© par la librairie `Fasttext` :

:::

::: {.content-visible when-profile="en"}

# To Go Further {-}

Research in the field of NLP is highly active. It is therefore advisable
to stay curious and explore additional resources, as no single source
can compile all knowledge‚Äîespecially in a field as dynamic as NLP.

To deepen the skills discussed in this course, I strongly recommend
this [course by `HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt).

To understand the internal architecture of an LLM,
this [post by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
is very helpful.

These chapters only scratch the surface of NLP use cases for _data scientists_.
For instance,
in public statistics, one major NLP use case involves using automatic classification techniques to convert free-text answers in surveys
into predefined fields within a nomenclature.
This is a specific adaptation to public statistics, a heavy user of standardized nomenclatures, of multi-level classification problems.

Here is an example from a project on automated job classification using the PCS (socio-professional categories) typology,
based on a model trained with the `Fasttext` library:

:::

::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
viewof activite = Inputs.text(
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Pr√©diction de l'activit√©
    </caption>
    <tr>
      <th style="text-align:center;">Libell√© (NA2008)</th>
      <th>Probabilit√©</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

:::

::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = f"https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite}"
import requests
data = requests.get(urlApe).json()

# Extract 'IC' value
IC = data['IC']
data.pop('IC', None)

df = pd.DataFrame(data.values())
df['indice_confiance'] = IC
df
```

:::
