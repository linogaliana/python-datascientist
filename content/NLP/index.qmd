---
title: "Partie 4 : Natural Language Processing (NLP)"
title-en: "Part 4: Natural Language Processing (NLP)"
author: Lino Galiana
description: |
  Cette partie du cours introduit le traitement automatique du langage (NLP), un domaine scientifique à la croisée de la linguistique et des statistiques devenu, du fait de l'engouement envers les IA génératives, central dans le domaine de la _data science_. A travers des exemples littéraires, dcette partie explore d’abord des méthodes classiques comme l’analyse fréquentiste et le traitement de corpus textuels sous la forme de _bag of words_. Ensuite, elle aborde la modélisation du langage qui ouvre la voie à des approches plus originales. L’objectif de chapitre est de rappeler quelques éléments généraux sur le vaste domaine qu'est le NLP.
description-en: |
  This part of the course introduces automatic language processing (NLP), a scientific field at the crossroads of linguistics and statistics that has become central to the field of _data science_ as a result of the craze for generative AI. Using literary examples, this section first explores classic methods such as frequentist analysis and the processing of textual corpora in the form of _bag of words_. It then looks at language modelling, which opens the way to more original approaches. The aim of this chapter is to recall a few general points about the vast field of NLP
categories:
  - Introduction
  - NLP
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/nlp.png
---

::: {.content-visible when-profile="fr"}

# Introduction

Les parties précédentes étaient consacrées à l'acquisition de compétences transversales pour la valorisation des données. De manière naturelle, nous nous sommes jusqu'à présent plutôt consacrés à la valorisation de données structurées, d'une dimension certes modeste mais qui ouvraient déjà énormément de problématiques à creuser. Cette partie propose maintenant de se consacrer à un sujet dont il n'est pas évident _a priori_ que les ordinateurs s'emparent, source de débats philosophiques séculaires, de Platon à Saussure : le __langage humain__ et sa richesse.

En faisant l'analogie entre langue et langage, c'est-à-dire en définissant ce dernier comme la capacité d'expression et de communication d'une pensée par le biais de signes et en définissant la langue comme la mise en oeuvre conventionnelle de cette capacité, on peut se placer dans les traces de la linguistique et représenter le langage sous une forme de données.
Ceci ouvre la voie à l'analyse statistique ou algorithmique. Néanmoins, même s'il existe des régularités statistiques, comment des ordinateurs, qui au fond ne connaissent que le _0_ et le _1_, peuvent-ils s'approprier cet objet éminemment complexe qu'est le langage et qu'un humain met lui-même des années à comprendre et s'approprier ?[^blague]

[^blague]: En écrivant ces lignes j'ai demandé à `ChatGPT` de me faire une blague.

    > Pourquoi les programmeurs détestent-ils la plage ?
    > Parce que le sable change tout le temps de paradigme !

    Ne l'ayant pas comprise, je lui ai demandé de me l'expliquer. Voici sa réponse :

    > Bien sûr, je serais ravi de l'expliquer ! Cette blague joue sur le double sens du mot "paradigme" et fait un jeu de mots avec les notions liées à la programmation et à la plage.
    >
    > Dans le domaine de la programmation, un "paradigme" désigne un modèle ou une méthode de programmation, comme la programmation orientée objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec différents paradigmes de programmation.
    >
    > D'autre part, en contexte de plage, "paradigme" est utilisé pour signifier "marée qui monte et descend", se référant à l'action naturelle du sable et de l'eau sur la plage.
    >
    > La blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu'ils doivent s'adapter à de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les marées. En associant les deux sens du mot "paradigme", la blague crée une situation absurde et amusante, faisant sourire ceux qui comprennent les références à la programmation et à la plage.

    Finalement, j'ai trouvé son explication plus drôle que sa blague.

:::

::: {.content-visible when-profile="en"}

# Introduction

The previous sections focused on acquiring cross-functional skills for working with data. Naturally, we have so far mostly focused on structured data—modest in size but already rich in analytical potential. This new section turns to a subject that, at first glance, may seem unlikely to be handled by computers—a topic of centuries-old philosophical debate, from Plato to Saussure: the richness of __human language__.

By drawing an analogy between "language" and "tongue"—that is, defining the former as the capacity to express and communicate thought through signs, and the latter as the conventional implementation of that capacity—we align ourselves with the field of linguistics and treat language as data.
This opens the door to statistical and algorithmic analysis. Yet, even if statistical regularities exist, how can computers—ultimately limited to just _0_ and _1_—grasp such a complex object as language, which takes humans years to understand and master?[^blague-en]

[^blague-en]: While writing these lines, I asked `ChatGPT` to tell me a joke.

    > Why do programmers hate the beach?
    > Because the sand keeps changing paradigms!

    Not understanding it, I asked for an explanation. Here's what I got:

    > Of course, I'd be happy to explain! This joke plays on the double meaning of the word "paradigm" and is a pun involving programming and the beach.
    >
    > In programming, a "paradigm" refers to a model or method of programming, such as object-oriented or functional programming. Programmers are often familiar with different paradigms.
    >
    > On the other hand, in a beach context, "paradigm" is humorously misused to mean "tide"—alluding to the natural shifting of sand and water.
    >
    > The joke plays on the humorous frustration programmers might feel when adapting to new programming paradigms, just like how sand constantly shifts with the tides. By combining both meanings of "paradigm," the joke creates an absurd and amusing scenario that will resonate with those familiar with programming and beaches.

    In the end, I found the explanation funnier than the joke itself.

:::


::: {.content-visible when-profile="fr"}
# Le traitement automatique du langage

Le traitement automatique du langage - traduction française du concept de _natural language processing_ (NLP) - est l'ensemble des techniques permettant aux ordinateurs de comprendre, analyser, synthétiser et générer le langage humain.

Le concept de _natural language processing_ tend à remplacer celui de _text mining_ dont le périmètre était plus spécifique. Le _text mining_ renvoie à la recherche d'information — la fouille — dans un ensemble de textes. C'est l'un des champs d'applications du _natural language processing_ mais ce n'est pas l'unique. L'accroissement des ressources de calcul et les progrès dans la formalisation du langage ont permis d'élargir le champ des domaines où la linguistique computationnelle intervient.

Il s'agit d'un champ disciplinaire à l'intersection de la statistique et de la linguistique qui connaît depuis quelques années un engouement important, que ce soit d'un point de vue académique, opérationnel ou industriel. Certaines des applications de ces techniques sont devenues incontournables dans nos tâches quotidiennes, notamment les moteurs de recherche, la traduction automatique et plus récemment les _chatbots_, dont le développement connaît depuis l'émergence de `ChatGPT` en décembre 2022 un rythme fou.
:::

::: {.content-visible when-profile="en"}
# Natural Language Processing

Natural Language Processing (NLP) refers to the set of techniques that allow computers to understand, analyze, synthesize, and generate human language[^1].

[^1]: The concept of "natural language processing" has tended to replace the more specific term "text mining." Text mining refers to the search for — or mining of — information within a set of texts. While it is one application area of NLP, it is not the only one. Advances in computing power and in the formalization of language have expanded the scope of fields where computational linguistics is applied.

NLP is a disciplinary field at the intersection of statistics and linguistics, which has experienced significant growth in recent years — academically, operationally, and industrially.
Some applications of these techniques have become essential in our daily lives, such as search engines, machine translation, and more recently, chatbots, whose development has accelerated rapidly since the launch of `ChatGPT` in December 2022.
:::


::: {.content-visible when-profile="fr"}

# Résumé de la partie

Cette partie du cours est consacrée à l'analyse des données textuelles avec des exemples de livres pour s'amuser. Elle est une introduction progressive
à ce sujet en se concentrant sur des concepts de base, nécessaires à
la compréhension ultérieure de principes plus avancés et de techniques
sophistiquées[^2-fr]. Cette partie présente principalement :

- Les enjeux de nettoyage de champs textuels
et d'analyse de fréquence. Il s'agit de NLP un
peu _old school_ mais dont la compréhension est nécessaire pour aller
plus loin ;
- La modélisation du langage, selon plusieurs approches.

[^2-fr]: Par exemple, le concept d'_embedding_ - transformation d'un champ
textuel en un vecteur numérique multidimensionnel - aujourd'hui central
dans le NLP n'est évoqué qu'à quelques reprises.

Avant d'en arriver
au sujet des _embeddings_, il est précieux de comprendre les apports et les
limites de concepts comme
le sac de mot (_bag of words_) ou la distance
TF-IDF (_term frequency - inverse document frequency_). L'un des apports principaux des grands modèles de langage, à savoir la richesse de la fênetre contextuelle leur permettant de mieux saisir les nuances textuelles et l'intentionalité du locuteur, s'éclairent lorsqu'on saisit les limites du NLP traditionnel.

Dans une
optique introductive, ce cours se focalise donc sur les approches fréquentistes, notamment l'approche sac de mot, pour faciliter
l'ouverture ultérieure de la boite de Pandore que sont les _embeddings_.

:::

::: {.content-visible when-profile="en"}

# Section Summary

This part of the course is dedicated to text data analysis with 📖 examples for fun. It serves as a gradual introduction to the topic by focusing on basic concepts necessary for later understanding of more advanced principles and sophisticated techniques[^2]. This section mainly covers:

- The challenges of cleaning textual fields
and frequency analysis. This is somewhat _old school_ NLP but understanding it is essential to progress further;
- Language modeling using several approaches.

[^2]: For example, the concept of _embedding_—the transformation of a text field into a multidimensional numeric vector—is central in NLP today but is only briefly mentioned here.

Before diving into the topic of _embeddings_, it's important to understand the contributions and limitations of concepts like
the bag of words or TF-IDF (_term frequency - inverse document frequency_). One of the main benefits of large language models—namely the richness of their contextual window that allows them to better grasp textual nuances and speaker intentionality—becomes clearer when the limitations of traditional NLP are understood.

As an introductory perspective, this course focuses on frequency-based approaches, especially the bag-of-words approach, to ease into the later exploration of the Pandora’s box that is _embeddings_.

:::


::: {.content-visible when-profile="fr"}

## Nettoyages textuels et analyse de fréquences

`Python` est un excellent outil pour l'analyse de données textuelles.
Les méthodes de base de transformation de données textuelles ou de dictionnaires, associées à des librairies spécialisées
comme `NLTK` et `SpaCy`, permettent d'effectuer des tâches de normalisation et d'analyse de données textuelles de manière
très efficace. `Python` est bien mieux outillé que `R` pour l'analyse de
données textuelles.
Les ressources en ligne sur le sujet sont très
nombreuses et la meilleure des écoles dans le domaine reste la pratique sur un corpus à nettoyer.

Dans un premier temps, cette partie propose
de revenir sur la manière de structurer et nettoyer un corpus
textuel au travers de l'approche *bag of words* (sac de mots).
Elle vise à montrer comment transformer un corpus en outil propre à une
analyse statistique :

* Elle propose d'abord une introduction aux enjeux du nettoyage des données
textuelles à travers l'analyse du *Comte de Monte Cristo* d'Alexandre Dumas
[ici](/content/NLP/01_intro.qmd) qui permet de synthétiser rapidement l'information disponible
dans un large volume de données (à l'image des _wordcloud_)
* Elle propose ensuite une série d'exercices sur le nettoyage de textes à partir des
oeuvres d'Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant à distinguer la
spécificité du vocabulaire employé par chaque auteurs. Ces exercices sont
disponibles [dans le deuxième chapitre](/content/NLP/02_exoclean.qmd) de la partie.

Cette analyse fréquentiste permet de prendre du recul sur la nature des données textuelles et sur les enjeux récurrents dans la réduction de dimension de corpus en langue naturelle. Comme la statistique descriptive entraîne naturellement la modélisation, cette approche fréquentiste va généralement amener rapidement à vouloir synthétiser quelques lois derrière nos corpus textuels.

:::

::: {.content-visible when-profile="en"}

## Text Cleaning and Frequency Analysis

`Python` is an excellent tool for text data analysis.
Basic methods for transforming textual data or dictionaries, combined with specialized libraries
such as `NLTK` and `SpaCy`, make it possible to perform normalization and text data analysis
very efficiently. `Python` is much better equipped than `R` for text data analysis.
There is a wealth of online resources on this subject, and the best way to learn remains hands-on practice with a corpus to clean.

This section first revisits how to structure and clean a textual corpus
through the *bag of words* approach.
It aims to demonstrate how to turn a corpus into a tool suitable for
statistical analysis:

* It first introduces the challenges of text data cleaning
through an analysis of *The Count of Monte Cristo* by Alexandre Dumas
[here](/content/NLP/01_intro.qmd), which helps to quickly summarize the available information
in a large volume of text data (as illustrated by wordclouds)
* It then offers a series of exercises on text cleaning based on the
works of Edgar Allan Poe, Mary Shelley, and H.P. Lovecraft, aiming to highlight
the specificity of each author's vocabulary. These exercises are
available [in the second chapter](/content/NLP/02_exoclean.qmd) of the section.

This frequency-based analysis provides perspective on the nature of text data and recurring issues in dimensionality reduction of natural language corpora. Just as descriptive statistics naturally lead to modeling, this frequency approach typically quickly leads to the desire to identify underlying rules behind our text corpora.

:::


::: {.content-visible when-profile="fr"}

## Modélisation du langage

La suite de cette partie proposera une introduction aux enjeux de modélisation
du langage. Ceux-ci sont très à la mode du fait du succès de `ChatGPT`. Néanmoins, avant
d'en arriver aux grands modèles de langage (LLM), ces réseaux de neurone ayant des milliards de paramètres et entraînés sur des volumes massifs de données, il est nécessaire de passer par quelques modélisations
préliminaires.

Nous proposerons d'abord d'explorer une approche alternative, prenant en compte
le contexte d'apparition d'un mot. L'introduction à la
_Latent Dirichlet Allocation_ (LDA) sera l'occasion de présenter la modélisation
de documents sous la forme de *topics*. Celle-ci est néanmoins passée de mode au profit des méthodes liées au concept d'_embedding_.

Nous introduirons ainsi à la fin de cette partie du cours les enjeux de la transformation de champs textuels
sous forme de vecteurs numériques. Pour cela, nous présenterons le principe
de `Word2Vec` qui permet ainsi, par exemple,
malgré une distance syntaxique importante,
de dire que sémantiquement `Homme` et `Femme` sont proches.
Ce chapitre est une passerelle vers le concept d'_embedding_, véritable
révolution récente du NLP, et qui permet de rapprocher des corpus
non seulement sur leur proximité syntaxique (partagent-ils par exemple des mots
communs ?) mais aussi sur leur proximité sémantique (partagent-ils un thème ou un sens commun ?). Ce passage par `Word2Vec` permettra aux curieux de pouvoir ensuite passer aux modèles de type _transformers_, les modèles faisant aujourd'hui office de référence dans le domaine du NLP.



:::

::: {.content-visible when-profile="en"}

## Language Modeling

The remainder of this section introduces the challenges of language modeling. These are currently very popular due to the success of `ChatGPT`. However, before delving into large language models (LLMs)—those neural networks with billions of parameters trained on massive data volumes—it's important to first understand some preliminary modeling techniques.

We begin by exploring an alternative approach that takes into account the context in which a word appears. The introduction of
_Latent Dirichlet Allocation_ (LDA) serves as an opportunity to present document modeling through *topics*. However, this approach has fallen out of favor in comparison to methods related to the concept of _embedding_.

Toward the end of this course section, we will introduce the challenge of transforming textual fields
into numeric vector forms. To do so, we will present the principle
behind `Word2Vec`, which allows us, for instance,
despite significant syntactic distance,
to identify that semantically, `Man` and `Woman` are closely related.
This chapter serves as a bridge to the concept of _embedding_, a major recent revolution in NLP. It enables the comparison of corpora
not only by syntactic similarity (e.g., do they share common words?)
but also by semantic similarity (e.g., do they share a theme or meaning?). Covering `Word2Vec` will give curious learners a solid foundation to then explore transformer-based models, which are now the benchmark in NLP.


:::


:::: {.content-visible when-profile="fr"}

# Pour aller plus loin {-}

La recherche dans le domaine du NLP est très active. Il est donc recommandé de faire preuve de curiosité pour en apprendre plus car une ressource unique ne compilera pas l'ensemble des connaissances, _a fortiori_ dans un champ de recherche aussi dynamique que le NLP.

::: {.callout-tip}
Dans le domaine de la statistique publique, un des principaux cas d'usage du NLP est l'utilisation de techniques de classification automatique pour transformer des réponses libres dans des questionnaires en champs prédéfinis dans une nomenclature.
Il s'agit donc d'une adaptation, un peu spécifique à la statistique publique, grande utilisatrice de nomenclatures normalisées, de problématiques de classification multi-niveaux.

Voici un exemple sur un projet de classification automatisée des professions dans la typologie des nomenclatures d'activités (les PCS) à partir d'un modèle entraîné par la librairie `Fasttext` :
:::

```{ojs}
html`
<div>${viewof activite}</div>
<div>${table}</div>
`
```

::::

:::: {.content-visible when-profile="en"}

# To Go Further {-}

Research in the field of NLP is highly active. It is therefore advisable to stay curious and explore additional resources, as no single source can compile all knowledge—especially in a field as dynamic as NLP.

::: {.callout-tip}
In public statistics, one major NLP use case involves using automatic classification techniques to convert free-text answers in surveys
into predefined fields within a nomenclature.
This is a specific adaptation to public statistics, a heavy user of standardized nomenclatures, of multi-level classification problems.

Here is an example from a project on automated job classification using the PCS (socio-professional categories) typology,
based on a model trained with the `Fasttext` library:
:::

```{ojs}
html`
<div>${viewof activite}</div>
<div>${table}</div>
`
```

::::

::: {.content-visible when-format="html"}

```{ojs}
//| echo: false
viewof activite = Inputs.text(
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
table = d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Prédiction de l'activité
    </caption>
    <tr>
      <th style="text-align:center;">Libellé (NA2008)</th>
      <th>Probabilité</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

:::

::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = (
    "https://codification-ape-test.lab.sspcloud.fr/"
    f"predict?nb_echos_max=3&prob_min=0&text_feature={activite}"
)

try:
    # requête
    resp = requests.get(urlApe, timeout=10)
    resp.raise_for_status()  # lève une erreur si code HTTP != 200
    data = resp.json()

    # récupération de IC
    IC = data.pop("IC", None)

    # transformation en DataFrame
    df = pd.DataFrame(data.values())
    df["indice_confiance"] = IC

    print(df)

except requests.exceptions.RequestException as e:
    print("Erreur lors de l'appel API :", e)
    df = pd.DataFrame()  # DataFrame vide en cas d'échec

except (ValueError, KeyError) as e:
    print("Erreur lors du parsing des données :", e)
    df = pd.DataFrame()
```

:::
