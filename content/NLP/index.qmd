---
title: "Partie 4 : Natural Language Processing (NLP)"
date: 2020-10-14T13:00:00Z
slug: "nlp"
categories:
  - Introduction
  - NLP
description: |
  L'un des grands avantages comparatifs de Python par rapport aux
  langages concurrents (R notamment) est dans
  la richesse des librairies de Traitement du Langage Naturel (mieux
  connu sous son acronyme anglais : NLP pour natural langage processing).
  Cette partie vise √† illustrer la richesse de cet √©cosyst√®me √† partir
  de quelques exemples litt√©raires :  Dumas, Poe, Shelley, Lovecraft.
image: nlp.png
---

Les parties pr√©c√©dentes √©taient consacr√©es √† l'acquisition de comp√©tences
transversales pour la valorisation des donn√©es. De mani√®re naturelle, 
nous nous sommes jusqu'√† pr√©sent plut√¥t consacr√©s
√† la valorisation de donn√©es structur√©es, d'une
dimension certes modeste mais qui ouvraient d√©j√† √©norm√©ment de
probl√©matiques √† creuser. Cette partie propose maintenant de se
consacrer √† un sujet dont il n'est pas √©vident _a priori_ que
les ordinateurs s'emparent: le langage humain et sa richesse. 

En effet, si la linguistique propose certes de repr√©senter
de mani√®re conceptuelle le langage sous une forme de donn√©es, comment
des ordinateurs, qui au fond ne connaissent que le 0 et le 1, peuvent-ils
s'approprier cet objet √©minemment complexe qu'est le langage et qu'un 
humain met lui-m√™me des ann√©es √† comprendre et s'approprier ?[^blague]

[^blague]: En √©crivant ces lignes j'ai demand√© √† `ChatGPT` de me faire une blague, la communication entre l'humain et la machine est perfectible :

    > Pourquoi les programmeurs d√©testent-ils la plage ?
    > Parce que le sable change tout le temps de paradigme !

    Ne l'ayant pas comprise du fait de mon esprit obtus, je lui
    ai demand√© de me l'expliquer. Voici sa r√©ponse

    > Bien s√ªr, je serais ravi de l'expliquer ! Cette blague joue sur le double sens du mot "paradigme" et fait un jeu de mots avec les notions li√©es √† la programmation et √† la plage.
    >
    > Dans le domaine de la programmation, un "paradigme" d√©signe un mod√®le ou une m√©thode de programmation, comme la programmation orient√©e objet, la programmation fonctionnelle, etc. Les programmeurs sont souvent familiers avec diff√©rents paradigmes de programmation.
    >
    > D'autre part, en contexte de plage, "paradigme" est utilis√© pour signifier "mar√©e qui monte et descend", se r√©f√©rant √† l'action naturelle du sable et de l'eau sur la plage.
    >
    > La blague joue sur la frustration humoristique que peuvent ressentir les programmeurs lorsqu'ils doivent s'adapter √† de nouveaux paradigmes de programmation, tout comme le sable de la plage qui change constamment avec les mar√©es. En associant les deux sens du mot "paradigme", la blague cr√©e une situation absurde et amusante, faisant sourire ceux qui comprennent les r√©f√©rences √† la programmation et √† la plage.
   
    Finalement, j'ai trouv√© son explication plus dr√¥le que sa blague.  
  

Le traitement du langage naturel - traduction fran√ßaise du concept de
_natural language processing_ (NLP) - est l'ensemble des techniques
permettant aux ordinateurs de comprendre, analyser synth√©tiser et
g√©n√©rer le langage humain[^1]. 

[^1]: Le concept de _natural language processing_ tend √† remplacer celui
de _text mining_ dont le p√©rim√®tre √©tait plus sp√©cifique. Le _text mining_
renvoie √† la recherche d'information - la fouille - dans un ensemble de 
textes. C'est l'un des champs d'applications
du _natural language processing_ mais ce n'est pas l'unique. 

Il s'agit d'un champ disciplinaire √† l'intersection de la statistique
et de la linguistique qui connait depuis quelques ann√©es un engouement
important que ce soit d'un point de vue acad√©mique ou op√©rationnel. 
Certaines des applications de ces techniques sont devenues incontournables
dans nos t√¢ches quotidiennes, notamment les moteurs de recherche, la traduction
automatique et plus r√©cemment les _chatbots_. 

Cette partie du cours est consacr√©e √† l'analyse des donn√©es textuelles avec
des exemples de üìñ pour s'amuser. Elle est une introduction progressive
√† ce sujet en se concentrant sur des concepts de base, n√©cessaires √†
la compr√©hension ult√©rieure de principes plus avanc√©s et de techniques
sophistiqu√©es[^2]. 

[^2]: Par exemple, le concept d'_embedding_ - transformation d'un champ
textuel en un vecteur num√©rique multidimensionnel - aujourd'hui central
dans le NLP n'est √©voqu√© qu'√† quelques reprises. Avant d'en arriver
au sujet des _embeddings_, il est pr√©cieux de comprendre les apports et les
limites de concepts comme
le sac de mot (_bag of words_) ou la distance
TF-IDF (_term frequency - inverse document frequency_). Dans une
optique introductive, ce cours se focalise donc sur ces derniers pour faciliter
l'ouverture ult√©rieure de la boite de Pandore que sont les _embeddings_. 

## R√©sum√© de la partie {.unnumbered}

`Python` est un excellent outil pour l'analyse de donn√©es textuelles. 
Les m√©thodes de base ou les librairies sp√©cialis√©es
comme `NLTK` et `SpaCy` permettent d'effectuer ces t√¢ches de mani√®re
tr√®s efficace. Les ressources en ligne sur le sujet sont tr√®s 
nombreuses. `Python` est bien mieux outill√© que `R` pour l'analyse de
donn√©es textuelles. 

Dans un premier temps, cette partie propose
de revenir sur la mani√®re de structurer et nettoyer un corpus 
textuel au travers de l'approche *bag of words* (sac de mots). 
Elle vise √† montrer comment transformer un corpus en outil propre √† une 
analyse statistique :

* Elle propose d'abord une introduction aux enjeux du nettoyage des donn√©es
textuelles √† travers l'analyse du *Comte de Monte Cristo* d'Alexandre Dumas
[ici](/01_intro) qui permet de synth√©tiser rapidement l'information disponible
dans un large volume de donn√©es (√† l'image de la @fig-wordcloud-dumas)
* Elle propose ensuite une s√©rie d'exercices sur le nettoyage de textes √† partir des
oeuvres d'Edgar Allan Poe, Mary Shelley et H.P. Lovecraft visant √† distinguer la 
sp√©cificit√© du vocabulaire employ√© par chaque auteurs (par exemple @fig-waffle-fear). Ces exercices sont 
disponibles [dans le deuxi√®me chapitre](/02_exoclean) de la partie.

Ensuite, nous proposerons d'explorer une approche alternative, prenant en compte
le contexte d'apparition d'un mot. L'introduction √† la
_Latent Dirichlet Allocation_ (LDA) sera l'occasion de pr√©senter la mod√©lisation
de documents sous la forme de *topics*.

Enfin, nous introduirons aux enjeux de la transformation de champs textuels
sous forme de vecteurs num√©riques. Pour cela, nous pr√©senterons le principe
de `Word2Vec` qui permet ainsi, par exemple,
malgr√© une distance syntaxique importante,
de dire que s√©mantiquement `Homme` et `Femme` sont proches.
Ce chapitre est une passerelle vers le concept d'_embedding_, v√©ritable
r√©volution r√©cente du NLP, et qui permet de rapprocher des corpus
non seulement sur leur proximit√© syntaxique (partagent-ils par exemple des mots
communs ?) mais aussi sur leur proximit√© s√©mantique (partagent-ils un th√®me ou un sens commun ?).[^embedding]

[^embedding]: Un exemple d'int√©r√™t de ce type d'approche est la @fig-relevanc-table-embedding.

Comme l'illustre la figure suivante, emprunt√©e √† [Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder), les concepts que nous allons aborder
dans cette partie sont indispensables pour √™tre en mesure
d'entra√Æner ou r√©utiliser ult√©rieurement un mod√®le
sophistiqu√© d'analyse du langage:

::: {#fig-encoder}

![Illustration transformer architecture](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F81c2aa73-dd8c-46bf-85b0-90e01145b0ed_1422x1460.png){#fig-encoder-decoder}

Illustration of the original transformer architecture proposed in [Attention Is All You Need, 2017](https://arxiv.org/abs/1706.03762)
(source: [Sebastien Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder))
:::


## Pour aller plus loin {-}

La recherche dans le domaine du NLP est tr√®s active. Il est donc recommand√©
de faire preuve de curiosit√© pour en apprendre plus car une ressource
unique ne compilera pas l'ensemble des connaissances, _a fortiori_ dans
un champ de recherche aussi dynamique que le NLP. 

Pour approfondir les comp√©tences √©voqu√©es dans ce cours, je recommande vivement 
ce [cours d'`HuggingFace`](https://huggingface.co/course/chapter1/2?fw=pt). 

Pour comprendre l'architecture interne d'un LLM,
ce [post de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder)
est tr√®s utile. 