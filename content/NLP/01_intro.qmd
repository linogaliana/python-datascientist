---
title: "Nettoyer et structurer l'information dans les données textuelles"
title-en: "Cleaning and structuring information in textual data"
author: Lino Galiana
description: |
  Les corpus textuels étant des objets de très grande dimension où le ratio signal/bruit est faible, il est nécessaire de mettre en oeuvre une série d'étapes de nettoyage de texte. Ce chapitre va explorer quelques méthodes classiques de nettoyage en s'appuyant sur des corpus littéraires: le _Comte de Monte Cristo_ d'Alexandre Dumas et des auteurs fantastiques anglo-saxons (Lovecraft, Poe, Shelley).
description-en: |
  As text corpora are very large objects with a low signal-to-noise ratio where the signal-to-noise ratio is low, it is necessary to carry out a series of a series of text cleaning steps. This chapter will explore some classic cleaning methods based on literary corpora literary corpora: Alexandre Dumas' _Comte de Monte Cristo_ and Anglo-Saxon fantasy writers (Lovecraft, Poe, Shelley).
categories:
  - NLP
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/python-reading.jfif
echo: false
include-in-header:
  - text: |
      <style>
      .cell-output-stdout code {
        word-break: break-wor !important;
        white-space: pre-wrap !important;
      }
      </style>
---

{{< badges
    printMessage="true"
>}}



:::: {.content-visible when-profile="fr"}

# Introduction

## Rappel

Comme évoqué dans l'[introduction de cette partie](/content/nlp/index.qmd) sur le traitement automatique du langage, l'objectif principal des techniques que nous allons explorer est la représentation synthétique du langage.

Le *natural language processing* (NLP) ou
*traitement automatisé du langage* (TAL) en Français,
vise à extraire de l'information de textes à partir d'une analyse statistique du contenu.
Cette définition permet d'inclure de nombreux champs d'applications au sein
du NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ).

Cette approche implique de transformer un texte, qui est une information compréhensible par un humain, en un nombre, information appropriée pour un ordinateur dans le cadre d'une approche statistique ou algorithmique.

Transformer une information textuelle en valeurs numériques propres à une analyse statistique n'est pas une tâche évidente. Les données textuelles sont **non structurées** puisque l'information cherchée, qui est propre à chaque analyse, est perdue au milieu d'une grande masse d'informations qui doit, de plus, être interprétée dans un certain contexte (un même mot ou une phrase n'ayant pas la même signification selon le contexte).

Si cette tâche n'était pas assez difficile comme ça, on peut ajouter d'autres difficultés propres à l'analyse textuelle car ces données sont :

* **bruitées** : ortographe, fautes de frappe...
* **changeantes** : la langue évolue avec de nouveaux mots, sens...
* **complexes** : structures variables, accords...
* **ambiguës** : synonymie, polysémie, sens caché...
* **propres à chaque langue** : il n'existe pas de règle de passage unique entre deux langues
* de **grande dimension** : des combinaisons infinies de séquences de mots


## Objectif du chapitre

Dans ce chapitre, nous allons nous restreindre aux
méthodes fréquentistes dans le paradigme _bag of words_. Celles-ci sont un peu _old school_ par rapport aux approches plus raffinées que nous évoquerons ultérieurement. Néanmoins, les présenter nous permettra d'évoquer un certain nombre d'enjeux typiques des données textuelles qui restent centraux dans le NLP moderne.

Le principal enseignement à retenir de cette partie est que les données textuelles étant à très haute dimension - le langage étant un objet riche - nous avons besoin de méthodes pour réduire le bruit de nos corpus textuels afin de mieux prendre en compte le signal en leur sein.

Cette partie est une introduction s'appuyant sur quelques ouvrages classiques de la littérature française ou anglo-saxonne. Seront notamment présentées quelques librairies faisant parti de la boite à outil minimale des _data scientists_: `NLTK` et `SpaCy`. Les chapitres suivants permettront de se focaliser sur la modélisation du langage.

::: {.callout-note}
## La librairie SpaCy

`NTLK` est la librairie historique d'analyse textuelle en `Python`. Elle existe
depuis les années 1990. L'utilisation industrielle du NLP dans le monde
de la _data science_ est néanmoins plus récente et doit beaucoup à la collecte
accrue de données non structurées par les réseaux sociaux. Cela a amené à
un renouvellement du champ du NLP, tant dans le monde de la recherche que dans
sa mise en application dans l'industrie de la donnée.

Le _package_ [`spaCy`](https://spacy.io/) est l'un des packages qui a permis
cette industrialisation des méthodes de NLP. Conçu autour du concept
de _pipelines_ de données, il est beaucoup plus pratique à mettre en oeuvre
pour une chaîne de traitement de données textuelles mettant en oeuvre
plusieurs étapes de transformation des données.

:::

::::

:::: {.content-visible when-profile="en"}

# Introduction

## Recap

As mentioned in the [introduction to this section](/content/nlp/index.qmd) on natural language processing, the main goal of the techniques we will explore is the synthetic representation of language.

Natural Language Processing (NLP) aims to extract information from text through statistical content analysis.
This definition includes a wide range of NLP applications (translation, sentiment analysis, recommendation, monitoring, etc.).

This approach involves transforming a text—understandable by humans—into a number, which is the appropriate format for a computer in the context of statistical or algorithmic approaches.

Turning textual information into numerical values suitable for statistical analysis is no easy task. Text data are **unstructured** since the sought-after information—specific to each analysis—is hidden in a large mass of information that must also be interpreted within a certain context (the same word or phrase can have different meanings depending on the context).

If that wasn’t already difficult enough, there are additional challenges specific to text analysis, as this data is:

* **noisy**: spelling, typos...
* **evolving**: language changes with new words, meanings...
* **complex**: variable structures, agreements...
* **ambiguous**: synonymy, polysemy, hidden meanings...
* **language-specific**: no single set of rules applies across languages
* **high-dimensional**: infinite combinations of word sequences

## Chapter Objective

In this chapter, we will focus on
frequency-based methods within the _bag of words_ paradigm. These are somewhat _old school_ compared to the more sophisticated approaches we'll cover later. However, introducing them allows us to address several typical challenges of text data that remain central in modern NLP.

The main takeaway from this section is that since text data is very high-dimensional—language is a rich object—we need methods to reduce the noise in our text corpora to better capture the signal.

This part serves as an introduction, drawing on classic works of French and English literature. It will also present some key libraries that form part of the essential toolkit for _data scientists_: `NLTK` and `SpaCy`. The following chapters will then focus on language modeling.

::: {.callout-note}
## The SpaCy Library

`NLTK` is the historical text analysis library in `Python`, dating back to the 1990s. The industrial application of NLP in the world of
_data science_ is more recent and owes a lot to the increased collection
of unstructured data by social networks. This has led to a renewal of the NLP field, both in research and in its industrial application.

The [`spaCy`](https://spacy.io/) package is one of the tools that enabled
this industrialization of NLP methods. Designed around the concept of data _pipelines_, it is much more convenient to use
for a text data processing chain that involves
multiple transformation steps.

:::

::::



::: {.content-visible when-profile="fr"}

## Méthode

L’analyse textuelle vise à transformer le texte en données
numériques manipulables. Pour cela il est nécessaire de se fixer
une unité sémantique minimale.
Cette unité textuelle peut être le mot ou encore une séquence de *n*
mots (un *ngram*) ou encore une chaîne de caractères (e.g. la
ponctuation peut être signifiante). On parle de **token**.

On peut ensuite utiliser diverses techniques (_clustering_,
classification supervisée) suivant l’objectif poursuivi pour exploiter
l’information transformée. Mais les étapes de nettoyage de texte sont indispensables.
Sinon un algorithme sera incapable de détecter une information pertinente dans l'infini des possibles.

Les _packages_ suivants seront utiles au cours de ce chapitre:

:::

::: {.content-visible when-profile="en"}

## Method

Text analysis aims to transform text into manipulable numerical data. To do this, it is necessary to define
a minimal semantic unit.
This textual unit can be a word, a sequence of *n* words (an *ngram*), or even a string of characters (e.g., punctuation can be meaningful). This is called a **token**.

Various techniques (such as _clustering_ or supervised classification) can then be used depending on the objective, in order to exploit the transformed information. However, text cleaning steps are essential.
Otherwise, an algorithm will be unable to detect meaningful information in the infinite range of possibilities.

The following _packages_ will be useful throughout this chapter:

:::

```{python}
#| echo: true
#| output: false
!pip install pywaffle
!pip install spacy
!pip install plotnine
!pip install great_tables
!pip install wordcloud
```


::: {.content-visible when-profile="fr"}

# Bases d'exemple

## Le [*Comte de Monte Cristo*](https://fr.wikipedia.org/wiki/Le_Comte_de_Monte-Cristo)

La base d'exemple est le [*Comte de Monte Cristo*](https://fr.wikipedia.org/wiki/Le_Comte_de_Monte-Cristo) d'Alexandre Dumas.
Il est disponible
gratuitement sur le site
[http://www.gutenberg.org _(Project Gutemberg)_](http://www.gutenberg.org/ebooks/author/492) comme des milliers
d'autres livres du domaine public.

La manière la plus simple de le récupérer
est de télécharger avec le _package_ `request` le fichier texte et le retravailler
légèrement pour ne conserver que le corpus du livre :

:::

::: {.content-visible when-profile="en"}

# Example Dataset

## [*The Count of Monte Cristo*](https://en.wikipedia.org/wiki/The_Count_of_Monte_Cristo)

The example dataset is [*The Count of Monte Cristo*](https://en.wikipedia.org/wiki/The_Count_of_Monte_Cristo) by Alexandre Dumas.
It is available for free on the
[http://www.gutenberg.org _(Project Gutenberg)_](http://www.gutenberg.org/ebooks/author/492) website, along with thousands
of other public domain books.

The simplest way to retrieve it
is to use the `request` package to download the text file and slightly
clean it to retain only the core content of the book:

:::

{{< include "_import_dumas.qmd" >}}

::: {.content-visible when-profile="fr"}

## Le corpus anglo-saxon

Nous allons utiliser une base anglo-saxonne présentant trois auteurs de la littérature fantastique:

* [Edgar Allan Poe](https://fr.wikipedia.org/wiki/Edgar_Allan_Poe), (_EAP_) ;
* [HP Lovecraft](https://fr.wikipedia.org/wiki/H._P._Lovecraft) (_HPL_) ;
* [Mary Wollstonecraft Shelley](https://fr.wikipedia.org/wiki/Mary_Shelley) (_MWS_).

Les données sont disponibles sur un CSV mis à disposition sur [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). L'URL pour les récupérer directement est
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Le fait d'avoir un corpus confrontant plusieurs auteurs nous permettra de comprendre la manière dont les nettoyages de données textuelles favorisent les analyses comparatives.

Nous pouvons utiliser le code suivant pour lire et préparer ces données:

:::

::: {.content-visible when-profile="en"}

## The Anglo-Saxon Corpus

We will use an Anglo-Saxon corpus featuring three authors of gothic literature:

* [Edgar Allan Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe), (_EAP_);
* [HP Lovecraft](https://en.wikipedia.org/wiki/H._P._Lovecraft), (_HPL_);
* [Mary Wollstonecraft Shelley](https://en.wikipedia.org/wiki/Mary_Shelley), (_MWS_).

The data is available in a CSV file provided on [`GitHub`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). The direct URL for retrieval is
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Having a corpus that compares multiple authors will allow us to understand how text data cleaning facilitates comparative analysis.

We can use the following code to read and prepare this data:

:::

{{< include "_import_horror.qmd" >}}

::: {.content-visible when-profile="fr"}
Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite :
:::

::: {.content-visible when-profile="en"}
The dataset thus pairs an author with a sentence they wrote:
:::

```{python}
#| echo: true
horror.head()
```

::: {.content-visible when-profile="fr"}

On peut se rendre compte que les extraits des 3 auteurs ne sont
pas forcément équilibrés dans le jeu de données.
Si on utilise ultérieurement ce corpus pour de la modélisation, il sera nécessaire de tenir compte de ce déséquilibre.

:::

::: {.content-visible when-profile="en"}

We can observe that the excerpts from the 3 authors are
not necessarily balanced within the dataset.
If this corpus is later used for modeling, it will be important to account for this imbalance.

:::


```{python}
#| echo: true
(
  horror
  .value_counts('Author')
  .plot(kind = "barh")
)
```


:::: {.content-visible when-profile="fr"}

# Premières analyses de fréquence

L'approche usuelle en statistique, qui consiste à faire une analyse descriptive avant de mettre en oeuvre une modélisation, s'applique également à l'analyse de données textuelles. La fouille de documents textuels implique ainsi, en premier lieu, une analyse statistique afin de déterminer la structure du corpus.

Avant de s'adonner à une analyse systématique du champ lexical de chaque
auteur, on va se focaliser dans un premier temps sur un unique mot, le mot *fear*.

## Exploration ponctuelle

::: {.callout-tip}

L'exercice ci-dessous présente une représentation graphique nommée
*waffle chart*. Il s'agit d'une approche préférable aux
camemberts (_pie chart_) qui sont des graphiques manipulables car l'oeil humain se laisse
facilement berner par cette représentation graphique qui ne respecte pas
les proportions.

:::

::::

:::: {.content-visible when-profile="en"}

# Initial Frequency Analysis

The standard approach in statistics—starting with descriptive analysis before modeling—also applies to text data analysis. Text mining therefore begins with a statistical analysis to determine the structure of the corpus.

Before diving into a systematic analysis of each author’s lexical field, we will first focus on a single word: *fear*.

## Targeted Exploration

::: {.callout-tip}

The exercise below presents a graphical representation called a
*waffle chart*. This is a better alternative to
pie charts, which are misleading because the human eye can be easily deceived by their shape, which does not accurately represent proportions.

:::

::::


{{< include "01_intro/exercise1.qmd" >}}



```{python}
#| output: false

#1. Compter le nombre de phrase pour chaque auteur avec fear
def nb_occurrences(word, train_data):
    train_data['wordtoplot'] = train_data['Text'].str.contains(word).astype(int)
    table = train_data.groupby('Author').sum(numeric_only = True)
    data = table.to_dict()['wordtoplot']
    return table

table = nb_occurrences("fear", horror)
```

::: {.content-visible when-profile="fr"}
Le comptage obtenu devrait être le suivant
:::

::: {.content-visible when-profile="en"}
The resulting count should be as follows
:::

```{python}
table.head()
```

```{python}
#| output: false
import matplotlib.pyplot as plt
from pywaffle import Waffle

#2. Faire un graphique d'occurences avc pywaffle
def graph_occurrence(word, train_data):
    table = nb_occurrences(word, train_data)
    data = table.to_dict()['wordtoplot']
    fig = plt.figure(
        FigureClass=Waffle,
        rows=15,
        values=data,
        title={'label': 'Utilisation du mot "%s" par les auteurs' %word, 'loc': 'left'},
        labels=[f"{k} ({v})" for k, v in data.items()]
    )
    return fig

fig = graph_occurrence("fear", horror)
```

::: {.content-visible when-profile="fr"}
Ceci permet d'obtenir le _waffle chart_ suivant :
:::

::: {.content-visible when-profile="en"}
This produces the following _waffle chart_:
:::


```{python}
#| echo: false
#| label: fig-waffle-fear
#| fig-cap: "Répartition du terme fear dans le corpus de nos trois auteurs"
#| output: true

fig.get_figure()
```

::: {.content-visible when-profile="fr"}
On remarque ainsi de manière très intuitive
le déséquilibre de notre jeu de données
lorsqu'on se focalise sur le terme _"peur"_
où Mary Shelley représente près de 50%
des observations.

Si on reproduit cette analyse avec le terme _"horror"_, on retrouve la figure suivante:
:::

::: {.content-visible when-profile="en"}
This clearly highlights the imbalance in our dataset
when focusing on the term _"fear"_,
with Mary Shelley accounting for nearly 50%
of the observations.

If we repeat the analysis with the term _"horror"_, we get the following figure:
:::


```{python}
#| output: false

#3. Graphe d'occurences avec le mot horror
fig = graph_occurrence("horror", horror)
```

```{python}
#| label: fig-waffle-horror
#| fig-cap: "Répartition du terme horror dans le corpus de nos trois auteurs"
fig.get_figure()
```


:::: {.content-visible when-profile="fr"}

## Transformation d'un texte en _tokens_

Dans l'exercice précédent, nous faisions une recherche ponctuelle, qui ne passe pas vraiment à l'échelle. Pour généraliser cette approche, on découpe généralement un corpus en unités sémantiques indépendantes : les _tokens_.

::: {.callout-tip}
Nous allons avoir besoin d'importer un certain nombre de corpus prêts à l'emploi pour utiliser les librairies `NTLK` ou `SpaCy`. Les instructions ci-dessous permettront de récupérer toutes ces ressources
:::

Pour récupérer tous nos corpus `NLTK` prêts à l'emploi, nous faisons

::::

:::: {.content-visible when-profile="en"}

## Converting Text into _Tokens_

In the previous exercise, we performed a one-off search, which doesn't scale well. To generalize this approach, a corpus is typically broken down into independent semantic units: _tokens_.

::: {.callout-tip}
We will need to import several ready-to-use corpora to work with the `NLTK` or `SpaCy` libraries. The instructions below will help you retrieve all these resources.
:::

To retrieve all our ready-to-use `NLTK` corpora, we do the following

::::



```{python}
#| output: false
#| echo: true
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('genesis')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

::: {.content-visible when-profile="fr"}
En ce qui concerne `SpaCy`, il est nécessaire d'utiliser
la ligne de commande:
:::

::: {.content-visible when-profile="en"}
For `SpaCy`, it is necessary to use
the following command line:
:::



```{python}
#| output: false
#| echo: true
!python -m spacy download fr_core_news_sm
!python -m spacy download en_core_web_sm
```

::: {.content-visible when-profile="fr"}
Plutôt que d'implémenter soi-même un _tokenizer_ inefficace, il est plus approprié d'en appliquer un issu d'une librairie spécialisée. Historiquement, le plus simple était de prendre le _tokenizer_ de `NLTK`, la librairie historique de _text mining_ en `Python`:
:::

::: {.content-visible when-profile="en"}
Rather than implementing an inefficient _tokenizer_ yourself, it is more appropriate to use one from a specialized library. Historically, the simplest choice was to use the tokenizer from `NLTK`, the classic `Python` text mining library:
:::


```{python}
#| echo: true
#| output: false
from nltk.tokenize import word_tokenize
word_tokenize(dumas[10000:10500])
```

::: {.content-visible when-profile="fr"}
Comme on le voit, cette librairie ne fait pas les choses dans le détail et a quelques incohérences: `j'y étais` est séparé en 4 sèmes (`['j', "'", 'y', 'étais']`) là où `l'acheter` reste un unique sème. `NLTK` est en effet une librairie anglo-saxonne et l'algorithme de séparation n'est pas toujours adapté aux règles grammaticales françaises. Il vaut mieux dans ce cas privilégier `SpaCy`, la librairie plus récente pour faire ce type de tâche. En plus d'être très bien documentée, elle est mieux adaptée pour les langues non anglo-saxonnes. En l'occurrence, comme le montre l'[exemple de la documentation](https://spacy.io/usage/linguistic-features#tokenization) sur les _tokenizers_, l'algorithme de séparation présente un certain raffinement

![Exemple d'algorithme de tokenisation](https://spacy.io/images/tokenization.svg)
Celui-ci peut être appliqué de cette manière:
:::

::: {.content-visible when-profile="en"}

As we can see, this library lacks detail and has some inconsistencies: `j'y étais` is split into 4 tokens (`['j', "'", 'y', 'étais']`) whereas `l'acheter` remains a single token. `NLTK` is an English-centric library, and its tokenization algorithm is not always well-suited to French grammar rules. In such cases, it is better to use `SpaCy`, the more modern library for this kind of task. Besides being well-documented, it is better adapted to non-English languages. As shown in the [documentation example](https://spacy.io/usage/linguistic-features#tokenization) on tokenizers, its algorithm provides a certain level of sophistication:

![Example of a tokenization algorithm](https://spacy.io/images/tokenization.svg)

It can be applied in the following way:
:::

```{python}
#| echo: true
#| label: tokenizer-french-space
import spacy
from spacy.tokenizer import Tokenizer
nlp = spacy.load("fr_core_news_sm")
doc = nlp(dumas[10000:10500])

text_tokenized = []
for token in doc:
    text_tokenized += [token.text]

", ".join(text_tokenized)
```


::: {.content-visible when-profile="fr"}
Comme on peut le voir, il reste encore beaucoup d'éléments polluants notre structuration de corpus, à commencer par la ponctuation. Nous allons néanmoins pouvoir facilement retirer ceux-ci ultérieurement, comme nous le verrons.
:::

::: {.content-visible when-profile="en"}
As we can see, there are still many elements cluttering our corpus structure, starting with punctuation. However, we will be able to easily remove these later on, as we will see.
:::


::: {.content-visible when-profile="fr"}
## Le nuage de mot: une première analyse généralisée

A ce stade, nous n'avons encore aucune appréhension de la structure de notre corpus : nombre de mots, mots les plus représentés, etc.

Pour se faire une idée de la structure de notre corpus,
on peut commencer par compter la distribution des mots dans l'oeuvre de Dumas. Commençons par le début de l'oeuvre, à savoir les 30 000 premiers mots et comptons les mots uniques :
:::

::: {.content-visible when-profile="en"}
## Word Cloud: A First Generalized Analysis
At this point, we still have no clear sense of the structure of our corpus: word count, most frequent words, etc.

To get an idea of the corpus structure,
we can start by counting word distribution in Dumas' work. Let’s begin with the first 30,000 words and count the unique words:
:::


```{python}
#| echo: true
from collections import Counter

doc = nlp(dumas[:30000])

# Extract tokens, convert to lowercase and filter out punctuation and spaces
tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]

# Count the frequency of each token
token_counts = Counter(tokens)
```

::: {.content-visible when-profile="fr"}
Nous avons déjà de nombreux mots différents dans le début de l'oeuvre.
:::

::: {.content-visible when-profile="en"}
There are already many different words in the beginning of the work.
:::

```{python}
#| echo: true
len(token_counts)
```

::: {.content-visible when-profile="fr"}
Nous voyons la haute dimensionnalité du corpus puisque nous avons près de 1500 mots différents sur les 30 000 premiers mots de l'oeuvre de Dumas.
:::

::: {.content-visible when-profile="en"}
We can observe the high dimensionality of the corpus, with nearly 1,500 unique words in the first 30,000 words of Dumas' work.
:::


```{python}
#| echo: true
token_count_all = list(token_counts.items())

# Create a DataFrame from the list of tuples
token_count_all = pd.DataFrame(token_count_all, columns=['word', 'count'])
```

::: {.content-visible when-profile="fr"}
Si on regarde la distribution de la fréquence des mots, exercice que nous prolongerons ultérieurement en évoquant la [loi de Zipf](https://fr.wikipedia.org/wiki/Loi_de_Zipf), nous pouvons voir que de nombreux mots sont unique (près de la moitié des mots), que la densité de fréquence descend vite et qu'il faudrait se concentrer un peu plus sur la queue de distribution que ne le permet la figure suivante :
:::

::: {.content-visible when-profile="en"}
If we look at the distribution of word frequencies—an analysis we will extend later when discussing [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)—we can see that many words are unique (nearly half), that the frequency density drops off quickly, and that we should focus more on the tail of the distribution than the following figure allows:
:::



```{python}
#| echo: true
from plotnine import *
(
  ggplot(token_count_all) +
  geom_histogram(aes(x = "count")) +
  scale_x_log10()
)
```

::: {.content-visible when-profile="fr"}
Maintenant, si on regarde les 25 mots les plus fréquents, on peut voir que ceux-ci ne sont pas très intéressants pour analyser le sens de notre document :
:::

::: {.content-visible when-profile="en"}
Now, if we look at the 25 most frequent words, we can see that they are not very informative for analyzing the meaning of our document:
:::


```{python}
#| echo: true
# Sort the tokens by frequency in descending order
sorted_token_counts = token_counts.most_common(25)
sorted_token_counts = pd.DataFrame(sorted_token_counts, columns=['word', 'count'])
```

```{python}
from great_tables import GT, md
import polars as pl

def create_bar(prop_fill: float, max_width: int, height: int) -> str:
    """Create divs to represent prop_fill as a bar."""
    width = round(max_width * prop_fill, 2)
    px_width = f"{width}px"
    return f"""\
    <div style="width: {max_width}px; background-color: lightgrey;">\
        <div style="height:{height}px;width:{px_width};background-color:green;"></div>\
    </div>\
    """

sorted_token_counts['count_pct'] = sorted_token_counts['count']/sorted_token_counts['count'].max()

df = (
  pl.from_pandas(sorted_token_counts)
  .with_columns(
      pl.col("count_pct")
        .map_elements(lambda x: create_bar(x, max_width=75, height=20))
        .alias("count_pct_bar")
    )
  .with_columns(
      pl.col("count_pct")
        .map_elements(lambda x: f"__{x}__")
        .alias("count_pct")
  )
)

(
  GT(df)
  .cols_hide("count_pct")
  .cols_label(**{"count_pct_bar": "", "word": "Mot", "count": "Nombre d'occurrences"})
  .tab_source_note(md("_Nombre d'apparitions sur les 30 000 premiers caractères du Comte de Monte Cristo_"))
  .fmt_markdown("word")
)
```

::: {.content-visible when-profile="fr"}
Si on représente graphiquement ce classement
:::

::: {.content-visible when-profile="en"}
If we represent this ranking graphically
:::

```{python}
#| echo: true
(
    ggplot(sorted_token_counts, aes(x='word', y='count')) +
    geom_point(stat='identity', size = 3, color = "red") +
    scale_x_discrete(
      limits=sorted_token_counts.sort_values("count")["word"].tolist()
    ) +
    coord_flip() +
    theme_minimal() +
    labs(title='Word Frequency', x='Word', y='Count')
)
```

::: {.content-visible when-profile="fr"}

Nous nous concentrerons ultérieurement sur ces mots-valises car il sera important d'en tenir compte pour les analyses approfondies de nos documents.

Nous avons pu, par ces décomptes de mots, avoir une première intutition de la nature de notre corpus. Néanmoins, une approche un peu plus visuelle serait pertinente pour avoir un peu plus d'intuitions.
Les nuages de mots (*wordclouds*) sont des représentations graphiques assez pratiques pour visualiser
les mots les plus fréquents, lorsqu'elles ne sont pas utilisées à tort et à travers.
Les _wordclouds_ sont très simples à implémenter en `Python`
avec le module `Wordcloud`. Quelques paramètres de mise en forme
permettent même d'ajuster la forme du nuage à
une image.

:::

::: {.content-visible when-profile="en"}

We will focus on these filler words later on, as it will be important to account for them in our deeper document analyses.

Through these word counts, we’ve gained a first intuition about the nature of our corpus. However, a more visual approach would be helpful to gain further insight.
*Word clouds* are convenient graphical representations for visualizing
the most frequent words—when used appropriately.
Word clouds are very easy to implement in `Python`
with the `Wordcloud` module. Some formatting parameters
even allow the shape of the cloud to be adjusted to
match an image.

:::


{{< include "01_intro/exercise2.qmd" >}}

::: {.content-visible when-profile="fr"}

Si nous n'en étions pas convaincus, ces visualisations montrent clairement qu'il est nécessaire de nettoyer notre texte. Par exemple, en ce qui concerne l'oeuvre du Dumas, le nom
du personnage principal, Dantès, est ainsi masqué par un certain nombre d'articles ou mots de liaison qui perturbent l'analyse.
En ce qui concerne le corpus anglo-saxon, ce sont des termes similaires, comme *"the"*, *"of"*, etc.

Ces mots sont des
*stop words*.
Ceci est une démonstration par l'exemple qu'il vaut mieux nettoyer le texte avant de
l'analyser (sauf si on est intéressé
par la loi de Zipf, cf. exercice suivant).

## Aparté: la loi de Zipf

Zipf, dans les années 1930, a remarqué une régularité statistique dans *Ulysse*, l'oeuvre de Joyce. Le mot le plus fréquent apparaissait $x$ fois, le deuxième mot le plus fréquent 2 fois moins, le suivant 3 fois moins que le premier, etc. D'un point de vue statistique, cela signifie que la fréquence d'occurrence $f(n_i)$ d'un mot est
liée à son rang $n_i$ dans l'ordre des fréquences par une loi de la forme

$$f(n_i) = c/n_i$$

où $c$ est une constante.

Plus généralement, on peut dériver la loi de Zipf d'une distribution exponentiellement décroissante des fréquences : $f(n_i) = cn_{i}^{-k}$. Sur le plan empirique, cela signifie qu'on peut utiliser les régressions poissonniennes pour estimer les paramètres de la loi, ce qui prend la spécification suivante

$$
\mathbb{E}\bigg( f(n_i)|n_i \bigg) = \exp(\beta_0 + \beta_1 \log(n_i))
$$

Les modèles linéaires généralisés (GLM) permettent de faire ce type de régression. En `Python`, ils sont disponibles par le biais du _package_ `statsmodels`, dont les sorties sont très inspirées des logiciels payants spécialisés dans l'économétrie comme `Stata`.

:::

::: {.content-visible when-profile="en"}

If it wasn't already obvious, these visualizations clearly show the need to clean our text. For instance, in the case of Dumas' work, the name of the main character, Dantès, is obscured by various articles or connecting words that interfere with the analysis.
In the Anglo-Saxon corpus, similar terms like *"the"*, *"of"*, etc., dominate.

These words are called
*stop words*.
This is a clear example of why text should be cleaned before
analysis (unless one is interested
in Zipf's law, see the next exercise).

## Aside: Zipf's Law

In the 1930s, Zipf observed a statistical regularity in Joyce’s *Ulysses*. The most frequent word appeared $x$ times, the second most frequent word appeared half as often, the third a third as often, and so on. Statistically, this means that the frequency of occurrence $f(n_i)$ of a word is
related to its rank $n_i$ in the frequency order by a law of the form:

$$f(n_i) = c/n_i$$

where $c$ is a constant.

More generally, Zipf’s law can be derived from an exponentially decreasing frequency distribution: $f(n_i) = cn_{i}^{-k}$. Empirically, this means that we can use Poisson regressions to estimate the law’s parameters, following the specification:

$$
\mathbb{E}\bigg( f(n_i)|n_i \bigg) = \exp(\beta_0 + \beta_1 \log(n_i))
$$

Generalized linear models (GLMs) allow us to perform this type of regression. In `Python`, they are available via the `statsmodels` package, whose outputs are heavily inspired by specialized econometric software such as `Stata`.

:::

```{python}
#| echo: true
count_words = pd.DataFrame({'counter' : horror
    .groupby('Author')
    .apply(lambda s: ' '.join(s['Text']).split())
    .apply(lambda s: Counter(s))
    .apply(lambda s: s.most_common())
    .explode()}
)
count_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)
count_words = count_words.reset_index()

count_words = count_words.assign(
    tot_mots_auteur = lambda x: (x.groupby("Author")['count'].transform('sum')),
    freq = lambda x: x['count'] /  x['tot_mots_auteur'],
    rank = lambda x: x.groupby("Author")['count'].transform('rank', ascending = False)
)
```

::: {.content-visible when-profile="fr"}
Commençons par représenter la relation entre la fréquence et le rang:
:::

::: {.content-visible when-profile="en"}
Let’s begin by visualizing the relationship between frequency and rank:
:::

```{python}
#| output: false
#| echo: true
from plotnine import *

g = (
    ggplot(count_words) +
    geom_point(aes(y = "freq", x = "rank", color = 'Author'), alpha = 0.4) +
    scale_x_log10() + scale_y_log10() +
    theme_minimal()
)
```

::: {.content-visible when-profile="fr"}
Nous avons bien, graphiquement, une relation log-linéaire entre les deux :
:::

::: {.content-visible when-profile="en"}
We do indeed observe a log-linear relationship between the two in the plot:
:::

```{python}
g
```

::: {.content-visible when-profile="fr"}
Avec `statsmodels`, vérifions plus formellement cette relation:
:::

::: {.content-visible when-profile="en"}
Using `statsmodels`, let’s formally verify this relationship:
:::


```{python}
#| echo: true
import statsmodels.api as sm
import numpy as np

exog = sm.add_constant(np.log(count_words['rank'].astype(float)))

model = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()

# Afficher les résultats du modèle
print(model.summary())
```

::: {.content-visible when-profile="fr"}
Le coefficient de la régression est presque 1 ce qui suggère bien une relation
quasiment log-linéaire entre le rang et la fréquence d'occurrence d'un mot.
Dit autrement, le mot le plus utilisé l'est deux fois plus que le deuxième mot le plus fréquent qui l'est trois plus que le troisième, etc. On retrouve bien empiriquement cette loi sur ce corpus de trois auteurs.
:::

::: {.content-visible when-profile="en"}
The regression coefficient is close to 1, which suggests a nearly log-linear relationship
between rank and word frequency.
In other words, the most used word occurs twice as often as the second most frequent word, which occurs three times more than the third, and so on. This law is indeed empirically observed in this corpus of three authors.
:::


::: {.content-visible when-profile="fr"}
# Nettoyage de textes

## Retirer les _stop words_

Nous l'avons vu, que ce soit en Français ou Anglais,
un certain nombre de mots de liaisons, nécessaires sur le plan grammatical mais peu porteur d'information, nous empêchent de saisir les principaux mots vecteurs d'information dans notre corpus.

Il est donc nécessaire de nettoyer notre corpus en retirant ces termes. Ce travail de nettoyage va d'ailleurs au-delà d'un simple retrait de mots. C'est également l'occasion de retirer d'autres sèmes gênants, par exemple la ponctuation.

Commençons par télécharger le corpus de _stopwords_
:::

::: {.content-visible when-profile="en"}
# Text Cleaning

## Removing _Stop Words_

As we have seen, whether in French or English,
a number of connecting words, while grammatically necessary, carry little informational value and prevent us from identifying the main information-bearing words in our corpus.

Therefore, it is necessary to clean our corpus by removing such terms. This cleaning process goes beyond simply removing words; it's also an opportunity to eliminate other problematic tokens, such as punctuation.

Let’s start by downloading the _stopwords_ corpus
:::

```{python}
#| echo: true
import nltk
nltk.download('stopwords')
```

::: {.content-visible when-profile="fr"}
La liste des _stopwords_ anglais dans `NLTK`
est la suivante:
:::

::: {.content-visible when-profile="en"}
The list of English _stopwords_ in `NLTK`
is as follows:
:::



```{python}
#| echo: true
from nltk.corpus import stopwords
", ".join(stopwords.words("english"))
```

::: {.content-visible when-profile="fr"}
Celle de `SpaCy` est plus riche (nous avons déjà téléchargé le corpus `en_core_web_sm` en question):
:::

::: {.content-visible when-profile="en"}
The list provided by `SpaCy` is more comprehensive (we have already downloaded the `en_core_web_sm` corpus in question):
:::

```{python}
#| echo: true
nlp_english = spacy.load('en_core_web_sm')
stop_words_english = nlp_english.Defaults.stop_words
", ".join(stop_words_english)
```

::: {.content-visible when-profile="fr"}
Si cette fois on prend la liste des _stopwords_ français dans `NLTK`:
:::

::: {.content-visible when-profile="en"}
This time, if we look at the list of French _stopwords_ in `NLTK`:
:::

```{python}
#| echo: true
", ".join(stopwords.words("french"))
```

::: {.content-visible when-profile="fr"}
On voit que celle-ci n'est pas très riche et mériterait d'être plus complète. Celle de `SpaCy` correspond mieux à ce qu'on attend
:::

::: {.content-visible when-profile="en"}
We can see that this list is not very extensive and could benefit from being more complete. The one from `SpaCy` is more in line with what we would expect.
:::

```{python}
#| echo: true
stop_words_french = nlp.Defaults.stop_words
", ".join(stop_words_french)
```

{{< include "01_intro/exercise3.qmd" >}}


:::: {.content-visible when-profile="fr"}

## Racinisation et lemmatisation

Pour aller plus loin dans l'harmonisation d'un texte, il est possible de
mettre en place des classes d'équivalence entre des mots. Par exemple, quand on désire faire une analyse fréquentiste, on peut être intéressé par considérer que _"cheval"_ et _"chevaux"_ sont équivalents. Selon les cas, différentes formes d’un même mot (pluriel,
singulier, conjugaison) pourront être considérées comme équivalentes et seront remplacées par une
même forme dite canonique.

Il existe deux approches dans le domaine :

* la **lemmatisation** qui requiert la connaissance des statuts
grammaticaux (exemple : _"chevaux"_ devient _"cheval"_) ;
* la **racinisation** (*stemming*) plus fruste mais plus rapide, notamment
en présence de fautes d’orthographes. Dans ce cas, _"chevaux"_ peut devenir _"chev"_
mais être ainsi confondu avec _"chevet"_ ou _"cheveux"_.

Cette approche a l'avantage de réduire la taille du vocabulaire à maîtriser
pour l'ordinateur et le modélisateur. Il existe plusieurs algorithmes de
*stemming*, notamment le *Porter Stemming Algorithm* ou le
*Snowball Stemming Algorithm*.

::: {.callout-note}
Pour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,
télécharger celui-ci grâce aux commandes suivantes :

~~~python
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
~~~

:::

::::

:::: {.content-visible when-profile="en"}

## Stemming and Lemmatization

To go further in text harmonization, it is possible to establish equivalence classes between words. For example, when conducting frequency analysis, one might want to treat _"cheval"_ and _"chevaux"_ as equivalent. Depending on the context, different forms of the same word (plural,
singular, conjugated) can be treated as equivalent and replaced with a
canonical form.

There are two main approaches:

* **Lemmatization**, which requires knowledge of grammatical roles
(example: _"chevaux"_ becomes _"cheval"_);
* **Stemming**, which is more rudimentary but faster, especially
when dealing with spelling errors. In this case, _"chevaux"_ might become _"chev"_,
but that could also match _"chevet"_ or _"cheveux"_.

This approach has the advantage of reducing the vocabulary size that both the computer and modeler must handle. Several stemming algorithms exist, including the *Porter Stemming Algorithm* and the *Snowball Stemming Algorithm*.

::: {.callout-note}
To access the necessary corpus for lemmatization, you need to download it the first time using the following commands:

~~~python
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
~~~

:::

::::


::: {.content-visible when-profile="fr"}

Prenons cette chaine de caractère,

```{python}
dumas[1030:1200]
```

La version racinisée est la suivante:

```{python}
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(language='french')

stemmed = [stemmer.stem(word) for word in word_tokenize(dumas[1030:1200])]
",".join(stemmed)
```

A ce niveau, les mots commencent à être moins intelligibles par un humain
mais peuvent rester intelligible pour la machine. Ce choix n'est néanmoins pas neutre et sa pertinence dépend du cas d'usage.

Les lemmatiseurs permettront des harmonisations plus subtiles. Ils s'appuient sur des bases de connaissance, par exemple _WordNet_, une base
lexicographique ouverte. Par exemple, les mots *"women"*, *"daughters"*
et *"leaves"* seront ainsi lemmatisés de la manière suivante :

:::

::: {.content-visible when-profile="en"}

Let’s take this character string:

```{python}
dumas[1030:1200]
```

The stemmed version is as follows:

```{python}
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(language='french')

stemmed = [stemmer.stem(word) for word in word_tokenize(dumas[1030:1200])]
",".join(stemmed)
```

At this stage, the words become less intelligible for humans
but can still be understandable for machines. This choice is not trivial, and its relevance depends on the specific use case.

Lemmatizers allow for more nuanced harmonization. They rely on knowledge bases, such as _WordNet_, an open lexical database. For instance, the words *"women"*, *"daughters"*, and *"leaves"* will be lemmatized as follows:

:::

```{python}
#| echo: true
from nltk.stem import WordNetLemmatizer
lemm = WordNetLemmatizer()

for word in ["women","daughters", "leaves"]:
    print(f"The lemmatized form of {word} is: {lemm.lemmatize(word)}")
```


:::: {.content-visible when-profile="fr"}

::: {.callout-tip}
## Exercice 4 : Lemmatisation avec nltk

Sur le modèle précédent, utiliser un `WordNetLemmatizer` sur le corpus `dumas[1030:1200]` et observer le résultat.

:::

La version lemmatisée de ce petit morceau de l'oeuvre de Dumas est la suivante:

```{python}
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

lemmatized = [lemmatizer.lemmatize(word) for word in word_tokenize(dumas[1030:1200])]
", ".join(lemmatized)
```

## Limite

Dans les approches fréquentistes, où on recherche la proximité entre des textes par la co-occurrence de termes, cette question de la création de classes d'équivalence est fondamentale. Les mots sont identiques ou différents, il n'y a pas de différence subtile entre eux. Par exemple, on devra soit déclarer que _"python"_ et _"pythons"_ sont équivalents, soient qu'ils sont différents, sans différence de degré entre _"pythons"_, _"anaconda"_ ou _"table"_ par rapport à _"python"_. Les approches modernes, non plus exclusivement basées sur des fréquences d'apparition, permettront d'introduire de la subtilité dans la synthétisation de l'information présente dans les données textuelles.

::::

:::: {.content-visible when-profile="en"}

::: {.callout-tip}
## Exercise 4: Lemmatization with nltk

Following the previous example, use a `WordNetLemmatizer` on the `dumas[1030:1200]` corpus and observe the result.

:::

The lemmatized version of this small excerpt from Dumas' work is as follows:

```{python}
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

lemmatized = [lemmatizer.lemmatize(word) for word in word_tokenize(dumas[1030:1200])]
", ".join(lemmatized)
```

## Limitation

In frequency-based approaches, where the goal is to find similarity between texts based on term co-occurrence, the question of forming equivalence classes is fundamental. Words are either identical or different—there is no subtle gradation. For instance, one must decide whether _"python"_ and _"pythons"_ are equivalent or not, without any nuance distinguishing _"pythons"_, _"anaconda"_, or _"table"_ from _"python"_. Modern approaches, which no longer rely solely on word frequency, allow for more nuance in synthesizing the information present in textual data.

::::

