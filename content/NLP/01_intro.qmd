---
title: "Quelques √©l√©ments pour comprendre les enjeux du NLP"
date: 2023-07-15T13:00:00Z
slug: nlpintro
type: book
tags:
  - NLP
  - nltk
  - Litt√©rature
  - preprocessing
  - Tutoriel
categories:
  - NLP
  - Tutoriel
description: |
  Les corpus textuels √©tant des objets de tr√®s grande dimension
  o√π le ratio signal/bruit est faible, il est n√©cessaire de mettre
  en oeuvre une s√©rie d'√©tapes de nettoyage de texte. Ce chapitre va
  explorer quelques m√©thodes classiques de nettoyage en s'appuyant
  sur le _Comte de Monte Cristo_ d'Alexandre Dumas. 
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_openfood_clean.png
---

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/NLP/01_intro.qmd")
```
:::


Cette partie est une introduction
√† la question du nettoyage de donn√©es textuelles.
Il s'agit de montrer la logique, quelques exemples
avec `Python` <i class="fab fa-python"></i>
et s'amuser avec comme base d'exemple un livre formidable üìñ :
[*Le Comte de Monte Cristo*](https://fr.wikipedia.org/wiki/Le_Comte_de_Monte-Cristo).

L'objectif est de d√©couvrir les principaux enjeux du nettoyage de donn√©es en NLP
et les enjeux de l'analyse de fr√©quence. 


## Base d'exemple

La base d'exemple est le *Comte de Monte Cristo* d'Alexandre Dumas.
Il est disponible
gratuitement sur le site
[Project Gutemberg](http://www.gutenberg.org/ebooks/author/492) comme des milliers
d'autres livres du domaine public. 

La mani√®re la plus simple de le r√©cup√©rer
est de t√©l√©charger avec le _package_ `Request` le fichier texte et le retravailler
l√©g√®rement pour ne conserver que le corpus du livre : 

```{python}
from urllib import request

url = "https://www.gutenberg.org/files/17989/17989-0.txt"
response = request.urlopen(url)
raw = response.read().decode('utf8')

dumas = raw.split("*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***")[1].split("*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***")[0]

import re

def clean_text(text):
    text = text.lower() # mettre les mots en minuscule
    text = " ".join(text.split())
    return text

dumas = clean_text(dumas)

dumas[10000:10500]
```


## La particularit√© des donn√©es textuelles

### Objectif

Le *natural language processing* (NLP) ou
*traitement automatis√© du langage* (TAL) en Fran√ßais,
vise √† extraire de l'information de textes √† partir d'une analyse statistique du contenu. 
Cette d√©finition permet d'inclure de nombreux champs d'applications au sein
du NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ). 

Cette approche implique de transformer un texte, qui est une information compr√©hensible par un humain, en un nombre, information appropri√©e pour un ordinateur dans le cadre d'une approche statistique ou algorithmique. 

Transformer une information textuelle en valeurs num√©riques propres √† une analyse statistique n'est pas une t√¢che √©vidente. Les donn√©es textuelles sont **non structur√©es** puisque l'information cherch√©e, qui est propre √† chaque analyse, est perdue au milieu d'une grande masse d'informations qui doit, de plus, √™tre interpr√©t√©e dans un certain contexte (un m√™me mot ou une phrase n'ayant pas la m√™me signification selon le contexte). 

Si cette t√¢che n'√©tait pas assez difficile comme √ßa, on peut ajouter d'autres difficult√©s propres √† l'analyse textuelle car ces donn√©es sont :

* bruit√©es : ortographe, fautes de frappe...
* changeantes : la langue √©volue avec de nouveaux mots, sens...
* complexes : structures variables, accords...
* ambigues : synonymie, polys√©mie, sens cach√©...
* propres √† chaque langue : il n'existe pas de r√®gle de passage unique entre deux langues
* de grande dimension : des combinaisons infinies de s√©quences de mots

### M√©thode

L‚Äôanalyse textuelle vise √† transformer le texte en donn√©es
num√©riques manipulables. Pour cela il est n√©cessaire de se fixer
une unit√© s√©mantique minimale. 
Cette unit√© textuelle peut √™tre le mot ou encore une s√©quence de *n*
mots (un *n-gramme*) ou encore une cha√Æne de caract√®res (e.g. la
ponctuation peut √™tre signifiante). On parle de **token**. 

On peut ensuite utiliser diverses techniques (_clustering_,
classification supervis√©e) suivant l‚Äôobjectif poursuivi pour exploiter
l‚Äôinformation transform√©e. Mais les √©tapes de nettoyage de texte sont indispensables.
Sinon un algorithme sera incapable de d√©tecter une information pertinente dans l'infini des possibles. 



## Nettoyer un texte

Les *wordclouds* sont des repr√©sentations graphiques assez pratiques pour visualiser
les mots les plus fr√©quents, lorsqu'elles ne sont pas utilis√©es √† tord et √† travers. 
Les _wordcloud_ sont tr√®s simples √† impl√©menter en `Python`
avec le module `Wordcloud`. Quelques param√®tres de mise en forme
permettent m√™me d'ajuster la forme du nuage √†
une image :


```{python}
#| echo: true

import wordcloud
import numpy as np
import io
import requests
import PIL
import matplotlib.pyplot as plt

img = "https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/NLP/book.png"
book_mask = np.array(PIL.Image.open(io.BytesIO(requests.get(img).content)))

def make_wordcloud(corpus):
    wc = wordcloud.WordCloud(background_color="white", max_words=2000, mask=book_mask, contour_width=3, contour_color='steelblue')
    wc.generate(corpus)
    return wc

wordcloud_dumas = make_wordcloud(dumas)
```

```{python}
#| echo: true
#| fig-cap: Nuage de mot produit √† partir du Comte de Monte Cristo
#| label: fig-wordcloud-dumas
plt.imshow(wordcloud_dumas, interpolation='bilinear')
plt.axis("off")
```


Cela montre clairement qu'il est n√©cessaire de nettoyer notre texte. Le nom
du personnage principal, Dant√®s, est ainsi masqu√© par un certain nombre
d'articles ou mots de liaison qui perturbent l'analyse. Ces mots sont des 
*stop-words*. 

La librairie `NLTK` (*Natural Language ToolKit*), librairie
de r√©f√©rence dans le domaine du NLP, permet de facilement retirer ces
stopwords (cela pourrait √©galement √™tre fait avec 
la librairie plus r√©cente, `spaCy`). Avant cela, il est n√©cessaire
de transformer notre texte en le d√©coupant par unit√©s fondamentales (les _tokens_).

Les exemples suivants, extraits de @galianafuzzy, montrent l'int√©r√™t du
nettoyage de textes lorsqu'on d√©sire comparer des corpus
entre eux. En l'occurrence, il s'agit de comparer un corpus de
noms de produits dans des collectes automatis√©es de produits 
de supermarch√© (_scanner-data_) avec des noms de produits
dans les donn√©es de l'`OpenFoodFacts`, une base de donn√©es
contributive. Sans nettoyage, le bruit l'emporte sur le signal
et il est impossible de d√©celer des similarit√©s entre les jeux
de donn√©es. Le nettoyage permet d'harmoniser
un peu ces jeux de donn√©es pour avoir une chance d'√™tre en 
mesure de les comparer. 

::: {layout-ncol=2}
![`OpenFoodFacts` avant nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_openfood_start.png)

![_Scanner-data_ avant nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_relevanc_start.png)
:::

::: {layout-ncol=2}
![`OpenFoodFacts` apr√®s nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_openfood_clean.png)

![_Scanner-data_ apr√®s nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_relevanc_clean.png)
:::


### Tokenisation

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```

Lors de la premi√®re utilisation de `NLTK`, il est n√©cessaire de t√©l√©charger
quelques √©l√©ments n√©cessaires √† la tokenisation, notamment la ponctuation.
Pour cela, il est recommand√© d'utiliser la commande suivante :

~~~python
import nltk
nltk.download('punkt')
~~~

```{=html}
</div>
```
:::

```{python}
import nltk
nltk.download('punkt')
```

La tokenisation consiste √† d√©couper un texte en morceaux. Ces morceaux
pourraient √™tre des phrases, des chapitres, des n-grammes ou des mots. C'est
cette derni√®re option que l'on va choisir, plus simple pour retirer les 
*stopwords* :

```{python}
import nltk

words = nltk.word_tokenize(dumas, language='french')
words[1030:1050]
```

On remarque que les mots avec apostrophes sont li√©s en un seul, ce qui est
peut-√™tre faux sur le plan de la grammaire mais peu avoir un sens pour une 
analyse statistique. Il reste des signes de ponctuation qu'on peut √©liminer
avec la m√©thode `isalpha`: 

```{python}
words = [word for word in words if word.isalpha()]
words[1030:1050]
```




### Retirer les stop-words

Le jeu de donn√©es est maintenant propre. On peut d√©sormais retirer les 
mots qui n'apportent pas de sens et servent seulement √† faire le 
lien entre deux pr√©positions. On appelle ces mots des
*stop words* dans le domaine du NLP. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```
Lors de la premi√®re utilisation de `NLTK`, il est n√©cessaire de t√©l√©charger
les stopwords. 

~~~python
import nltk
nltk.download('stopwords')
~~~

```{=html}
</div>
```
:::

Comme indiqu√© ci-dessus, pour t√©l√©charger
le corpus de stopwords[^2], il est
n√©cessaire d'ex√©cuter la ligne de
commande suivante :

```{python}
import nltk
nltk.download('stopwords')
```

[^2]: Le corpus de _stop-words_ de `NLTK`
est relativement limit√©. Il est recommand√©
de privil√©gier celui de `SpaCy`, plus
complet, pour √©liminer plus de mots
valises. 

```{python}
from nltk.corpus import stopwords
print(stopwords.words("french"))

stop_words = set(stopwords.words('french'))


words = [w for w in words if not w in stop_words]
print(words[1030:1050])
```

Ces retraitements commencent √† porter leurs fruits puisque des mots ayant plus
de sens commencent √† se d√©gager, notamment les noms des personnages
(Fernand, Merc√©d√®s, Villefort, etc.)


```{python}
#| echo: true
wc = make_wordcloud(' '.join(words))

fig = plt.figure()

plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
```


### *Stemming*

Pour r√©duire la complexit√© d'un texte, on peut tirer partie de
_"classes d'√©quivalence"_ : on peut
consid√©rer que diff√©rentes formes d‚Äôun m√™me mot (pluriel,
singulier, conjugaison) sont √©quivalentes et les remplacer par une
m√™me forme dite canonique. Il existe deux approches dans le domaine :

* la **lemmatisation** qui requiert la connaissance des statuts
grammaticaux (exemple : chevaux devient cheval)
* la **racinisation** (*stemming*) plus fruste mais plus rapide, notamment
en pr√©sence de fautes d‚Äôorthographes. Dans ce cas, chevaux peut devenir chev
mais √™tre ainsi confondu avec chevet ou cheveux

La racinisation est plus simple √† mettre en oeuvre car elle peut s'appuyer sur
des r√®gles simples pour extraire la racine d'un mot. 


Pour r√©duire un mot dans sa forme "racine", c'est-√†-dire en s'abstrayant des
conjugaisons ou variations comme les pluriels, on applique une m√©thode de
*stemming*. Le but du *stemming* est de regrouper de
nombreuses variantes d‚Äôun mot comme un seul et m√™me mot.
Par exemple, une fois que l‚Äôon applique un stemming, "chats" et "chat" 
deviennent un m√™me mot. 

Cette approche a l'avantage de r√©duire la taille du vocabulaire √† ma√Ætriser
pour l'ordinateur et le mod√©lisateur. Il existe plusieurs algorithmes de 
*stemming*, notamment le *Porter Stemming Algorithm* ou le
*Snowball Stemming Algorithm*. Nous pouvons utiliser ce dernier en Fran√ßais :

```{python}
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(language='french')

stemmed = [stemmer.stem(word) for word in words]
print(stemmed[1030:1050])
```

A ce niveau, les mots commencent √† √™tre moins intelligibles par un humain. 
La machine prendra le relais, on lui a pr√©par√© le travail

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```
Il existe aussi le _stemmer_ suivant : 

~~~python
from nltk.stem.snowball import FrenchStemmer
stemmer = FrenchStemmer()
~~~
```{=html}
</div>
```
:::


## Reconnaissance des entit√©s nomm√©es

Cette √©tape n'est pas une √©tape de pr√©paration mais illustre la capacit√© 
des librairies `Python` a extraire du sens d'un texte. La librairie 
`spaCy` permet de faire de la reconnaissance d'entit√©s nomm√©es
(_name entity recognition_, NER), ce qui peut
√™tre pratique pour extraire rapidement certains personnages de notre oeuvre.

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> La librairie <code>SpaCy</code></h3>
```

`NTLK` est la librairie historique d'analyse textuelle en `Python`. Elle existe
depuis les ann√©es 1990. L'utilisation industrielle du NLP dans le monde
de la _data science_ est n√©anmoins plus r√©cente et doit beaucoup √† la collecte
accrue de donn√©es non structur√©es par les r√©seaux sociaux. Cela a amen√© √† 
un renouvelement du champ du NLP, tant dans le monde de la recherche que dans
sa mise en application dans l'industrie de la donn√©e.

Le _package_ [`spaCy`](https://spacy.io/) est l'un des packages qui a permis
cette industrialisation des m√©thodes de NLP. Con√ßu autour du concept
de _pipelines_ de donn√©es, il est beaucoup plus pratique √† mettre en oeuvre
pour une cha√Æne de traitement de donn√©es textuelles mettant en oeuvre
plusieurs √©tapes de transformation des donn√©es. 

```{=html}
</div>
```
:::

Voici un exemple de reconnaissance d'entit√©s nomm√©es
sur les premi√®res phrases de l'ouvrage

```{python}
#!pip install deplacy
!python -m spacy download fr_core_news_sm
import spacy
import spacy
from spacy import displacy

nlp=spacy.load("fr_core_news_sm")
doc = nlp(dumas[15000:17000])
displacy.render(doc, style="ent", jupyter=True)
```

La reconnaissance d'entit√©s nomm√©es diposnible
par d√©faut est souvent d√©cevante; il est
souvent n√©cessaire d'enrichir les r√®gles par d√©faut
par des r√®gles _ad hoc_, propres √† chaque corpus.


## Repr√©sentation d'un texte sous forme vectorielle

Une fois nettoy√©, le texte est plus propice √† une repr√©sentation vectorielle.
En fait, implicitement, on a depuis le d√©but adopt√© une d√©marche *bag of words*.
Il s'agit d'une repr√©sentation, sans souci de contexte (ordre des mots, contexte d'utilisation),
o√π chaque *token* repr√©sente un √©l√©ment dans un vocabulaire de taille $|V|$.
On peut ainsi avoir une repr√©sentation matricielle les occurrences de
chaque *token* dans plusieurs documents (par exemple plusieurs livres,
chapitres, etc.) pour, par exemple, en d√©duire une forme de similarit√©. 


Afin de r√©duire la dimension de la matrice *bag of words*,
on peut s'appuyer sur des pond√©rations.
On √©limine ainsi certains mots tr√®s fr√©quents ou au contraire tr√®s rares.
La pond√©ration la plus simple est bas√©e sur la fr√©quence des mots dans le document.
C'est l'objet de la m√©trique **tf-idf** (term frequency - inverse document frequency)
abord√©e dans un prochain chapitre.

## R√©f√©rences
