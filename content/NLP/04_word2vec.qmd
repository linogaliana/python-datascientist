---
title: "M√©thodes de vectorisation : comptages et word embeddings"
date: 2020-10-29T13:00:00Z
draft: false
weight: 40
slug: word2vec
type: book
tags:
  - NLP
  - Litt√©rature
  - Topics Modelling
  - Word2Vec
categories:
  - Tutoriel
  - NLP
description: |
  Pour pouvoir utiliser des donn√©es textuelles dans des algorithmes
  de _machine learning_, il faut les vectoriser, c'est √† dire transformer
  le texte en donn√©es num√©riques. Dans ce TP, nous allons comparer
  diff√©rentes m√©thodes de vectorisation, √† travers une t√¢che de pr√©diction :
  _peut-on pr√©dire un auteur litt√©raire √† partir d'extraits de ses textes ?_
  Parmi ces m√©thodes, on va notamment explorer le mod√®le `Word2Vec`, qui
  permet d'exploiter les structures latentes d'un texte en construisant
  des _word embeddings_ (plongements de mots).
eval: false
image: featured_nlp_intro.png
---

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/NLP/04_word2vec.qmd", ssp_cloud_service="pytorch")
```
:::

Cette page approfondit certains aspects pr√©sent√©s dans la
[partie introductive](#nlp). Apr√®s avoir travaill√© sur le
*Comte de Monte Cristo*, on va continuer notre exploration de la litt√©rature
avec cette fois des auteurs anglophones:

* Edgar Allan Poe, (EAP) ;
* HP Lovecraft (HPL) ;
* Mary Wollstonecraft Shelley (MWS).

Les donn√©es sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent √™tre requ√©t√©es via l'url 
<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.

Le but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quents utilis√©s par les auteurs, de les repr√©senter graphiquement puis on va ensuite essayer de pr√©dire quel texte correspond √† quel auteur √† partir de diff√©rents mod√®les de vectorisation, notamment les *word embeddings*.

Ce notebook est librement inspir√© de  : 

* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras
* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg
* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook


::: {.callout-warning title="Packages √† installer"}
Comme dans la [partie pr√©c√©dente](#nlp), il faut t√©l√©charger des librairies 
sp√©cialis√©ees pour le NLP, ainsi que certaines de leurs d√©pendances.

```{python}
!pip install spacy gensim sentence_transformers pandas matplotlib seaborn
```

Ensuite, comme nous allons utiliser la librairie `spaCy` avec un corpus de textes
en Anglais, il convient de t√©l√©charger le mod√®le NLP pour l'Anglais. Pour cela, 
on peut se r√©f√©rer √† [la documentation de `spacy`](https://spacy.io/usage/models),
extr√™mement bien faite.

```{python}
!python -m spacy download en_core_web_sm
```
:::

```{python}
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gensim

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV, cross_val_score

from gensim.models.word2vec import Word2Vec
import gensim.downloader
from sentence_transformers import SentenceTransformer
```

## Nettoyage des donn√©es

Nous allons ainsi √† nouveau utiliser le jeu de donn√©es `spooky`:

```{python}
data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'
spooky_df = pd.read_csv(data_url)
```

Le jeu de donn√©es met ainsi en regard un auteur avec une phrase qu'il a √©crite:

```{python}
spooky_df.head()
```


### Preprocessing

En NLP, la premi√®re √©tape est souvent celle du *preprocessing*, qui inclut notamment les √©tapes de tokenization et de nettoyage du texte. Comme celles-ci ont √©t√© vues en d√©tail dans le pr√©c√©dent chapitre, on se contentera ici d'un *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les m√©thodes de vectorisation bas√©es sur des comptages).

Jusqu'√† pr√©sent, nous avons utilis√© principalement `nltk` pour le 
*preprocessing* de donn√©es textuelles. Cette fois, nous proposons
d'utiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de
*pipelines* de *preprocessing*. 

Pour initialiser le processus de nettoyage,
on va utiliser le corpus `en_core_web_sm` (voir plus
haut pour l'installation de ce corpus):


```{python}
import spacy
nlp = spacy.load('en_core_web_sm')
```

On va utiliser un `pipe` `spacy` qui permet d'automatiser, et de parall√©liser,
un certain nombre d'op√©rations. Les *pipes* sont l'√©quivalent, en NLP, de
nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s'agit donc d'un outil
tr√®s appropri√© pour industrialiser un certain nombre d'op√©rations de
*preprocessing* :

```{python}
def clean_docs(texts, remove_stopwords=False, n_process = 4):
    
    docs = nlp.pipe(texts, 
                    n_process=n_process,
                    disable=['parser', 'ner',
                             'lemmatizer', 'textcat'])
    stopwords = nlp.Defaults.stop_words

    docs_cleaned = []
    for doc in docs:
        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]
        if remove_stopwords:
            tokens = [tok for tok in tokens if tok not in stopwords]
        doc_clean = ' '.join(tokens)
        docs_cleaned.append(doc_clean)
        
    return docs_cleaned
```

On applique la fonction `clean_docs` √† notre colonne `pandas`.
Les `pandas.Series` √©tant it√©rables, elles se comportent comme des listes et
fonctionnent ainsi tr√®s bien avec notre `pipe` `spacy`

```{python}
spooky_df['text_clean'] = clean_docs(spooky_df['text'])
```

```{python}
spooky_df.head()
```


### Encodage de la variable √† pr√©dire

On r√©alise un simple encodage de la variable √† pr√©dire :
il y a trois cat√©gories (auteurs), repr√©sent√©es par des entiers 0, 1 et 2.

Pour cela, on utilise le `LabelEncoder` de `scikit` d√©j√† pr√©sent√© 
dans la [partie mod√©lisation](#preprocessing). On va utiliser la m√©thode
`fit_transform` qui permet, en un tour de main, d'appliquer √† la fois
l'entra√Ænement (`fit`), √† savoir la cr√©ation d'une correspondance entre valeurs
num√©riques et _labels_, et l'appliquer (`transform`) √† la m√™me colonne.

```{python}
le = LabelEncoder()
spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])
```

On peut v√©rifier les classes de notre `LabelEncoder` :

```{python}
le.classes_
```

### Construction des bases d'entra√Ænement et de test

On met de c√¥t√© un √©chantillon de test (20 %) avant toute analyse (m√™me descriptive).
Cela permettra d'√©valuer nos diff√©rents mod√®les toute √† la fin de mani√®re tr√®s rigoureuse,
puisque ces donn√©es n'auront jamais utilis√©es pendant l'entra√Ænement.

Notre √©chantillon initial n'est pas √©quilibr√© (*balanced*) : on retrouve plus d'oeuvres de
certains auteurs que d'autres. Afin d'obtenir un mod√®le qui soit √©valu√© au mieux, nous allons donc stratifier notre √©chantillon de mani√®re √† obtenir une r√©partition similaire d'auteurs dans nos
ensembles d'entra√Ænement et de test.

Aper√ßu du premier √©l√©ment de `X_train` : 

```{python}
X_train[0]
```

On peut aussi v√©rifier qu'on est capable de retrouver
la correspondance entre nos auteurs initiaux avec
la m√©thode `inverse_transform`

```{python}
print(y_train[0], le.inverse_transform([y_train[0]])[0])
```

## Statistiques exploratoires

### R√©partition des labels

Refaisons un graphique que nous avons d√©j√† produit pr√©c√©demment pour voir
la r√©partition de notre corpus entre auteurs :

```{python}
#| output: hide
fig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')
fig
```


```{python}
#| echo: false
fig.get_figure()
```

On observe une petite asym√©trie : les passages des livres d'Edgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d'entra√Ænement, ce qui peut √™tre probl√©matique dans le cadre d'une t√¢che de classification.
L'√©cart n'est pas dramatique, mais on essaiera d'en tenir compte dans l'analyse en choisissant une m√©trique d'√©valuation pertinente. 

### Mots les plus fr√©quemment utilis√©s par chaque auteur

On va supprimer les *stopwords* pour r√©duire le bruit dans notre jeu
de donn√©es.

```{python}
# Suppression des stop words
X_train_no_sw = clean_docs(X_train, remove_stopwords=True)
X_train_no_sw = np.array(X_train_no_sw)
```

Pour visualiser rapidement nos corpus, on peut utiliser la technique des
nuages de mots d√©j√† vue √† plusieurs reprises. 

Vous pouvez essayer de faire vous-m√™me les nuages ci-dessous
ou cliquer sur la ligne ci-dessous pour afficher le code ayant
g√©n√©r√© les figures :

::: {.cell .markdown}
```{=html}
<details><summary><code>Cliquer pour afficher le code</code> üëá</summary>
```

```{python}
def plot_top_words(initials, ax, n_words=20):
    # Calcul des mots les plus fr√©quemment utilis√©s par l'auteur
    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]
    all_tokens = ' '.join(texts).split()
    counts = Counter(all_tokens)
    top_words = [word[0] for word in counts.most_common(n_words)]
    top_words_counts = [word[1] for word in counts.most_common(n_words)]
    
    # Repr√©sentation sous forme de barplot
    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)
    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')
```

```{python}
#| output: hide

initials_to_author = {
    'EAP': 'Edgar Allen Poe',
    'HPL': 'H.P. Lovecraft',
    'MWS': 'Mary Wollstonecraft Shelley'
}

fig, axs = plt.subplots(3, 1, figsize = (12,12))

plot_top_words('EAP', ax = axs[0])
plot_top_words('HPL', ax = axs[1])
plot_top_words('MWS', ax = axs[2])
```

```{=html}
</details>
```
:::


```{python}
#| eval: false
#| echo: false
axs[0].get_figure()
axs[1].get_figure()
axs[2].get_figure()
```

Beaucoup de mots se retrouvent tr√®s utilis√©s par les trois auteurs.
Il y a cependant des diff√©rences notables : le mot _"life"_
est le plus employ√© par MWS, alors qu'il n'appara√Æt pas dans les deux autres tops.
De m√™me, le mot _"old"_ est le plus utilis√© par HPL
l√† o√π les deux autres ne l'utilisent pas de mani√®re surrepr√©sent√©e.

Il semble donc qu'il y ait des particularit√©s propres √† chacun des auteurs
en termes de vocabulaire,
ce qui laisse penser qu'il est envisageable de pr√©dire les auteurs √† partir
de leurs textes dans une certaine mesure.

## Pr√©diction sur le set d'entra√Ænement

Nous allons √† pr√©sent v√©rifier cette conjecture en comparant
plusieurs mod√®les de vectorisation,
_i.e._ de transformation du texte en objets num√©riques
pour que l'information contenue soit exploitable dans un mod√®le de classification.

### D√©marche

Comme nous nous int√©ressons plus √† l'effet de la vectorisation qu'√† la t√¢che de classification en elle-m√™me,
nous allons utiliser un algorithme de classification simple (un SVM lin√©aire), avec des param√®tres non fine-tun√©s (c'est-√†-dire des param√®tres pas n√©cessairement choisis pour √™tre les meilleurs de tous).

```{python}
clf = LinearSVC(max_iter=10000, C=0.1, dual="auto")
```

Ce mod√®le est connu pour √™tre tr√®s performant sur les t√¢ches de classification de texte, et nous fournira donc un bon mod√®le de r√©f√©rence (*baseline*). Cela nous permettra √©galement de comparer de mani√®re objective l'impact des m√©thodes de vectorisation sur la performance finale.

Pour les deux premi√®res m√©thodes de vectorisation
(bas√©es sur des fr√©quences et fr√©quences relatives des mots),
on va simplement normaliser les donn√©es d'entr√©e, ce qui va permettre au SVM de converger plus rapidement, ces mod√®les √©tant sensibles aux diff√©rences d'√©chelle dans les donn√©es.

On va √©galement _fine-tuner_ via _grid-search_
certains hyperparam√®tres li√©s √† ces m√©thodes de vectorisation : 

- on teste diff√©rents _ranges_ de `n-grams` (unigrammes et unigrammes + bigrammes)
- on teste avec et sans _stop-words_

Afin d'√©viter le surapprentissage,
on va √©valuer les diff√©rents mod√®les via validation crois√©e, calcul√©e sur 4 blocs.

On r√©cup√®re √† la fin le meilleur mod√®le selon une m√©trique sp√©cifi√©e.
On choisit le `score F1`,
moyenne harmonique de la pr√©cision et du rappel,
qui donne un poids √©quilibr√© aux deux m√©triques, tout en p√©nalisant fortement le cas o√π l'une des deux est faible.
Pr√©cis√©ment, on retient le `score F1 *micro-averaged*` :
les contributions des diff√©rentes classes √† pr√©dire sont agr√©g√©es,
puis on calcule le `score F1` sur ces donn√©es agr√©g√©es.
L'avantage de ce choix est qu'il permet de tenir compte des diff√©rences
de fr√©quences des diff√©rentes classes.

### Pipeline de pr√©diction

On va utiliser un *pipeline* `scikit` ce qui va nous permettre d'avoir
un code tr√®s concis pour effectuer cet ensemble de t√¢ches coh√©rentes. 
De plus, cela va nous assurer de g√©rer de mani√®re coh√©rentes nos diff√©rentes
transformations (cf. [partie sur les pipelines](#pipelines))

Pour se faciliter la vie, on d√©finit une fonction `fit_vectorizers` qui
int√®gre dans un *pipeline* g√©n√©rique une m√©thode d'estimation `scikit`
et fait de la validation crois√©e en cherchant le meilleur mod√®le
(en excluant/incluant les *stopwords* et avec unigrammes/bigrammes)

```{python}
def fit_vectorizers(vectorizer):
    pipeline = Pipeline(
    [
        ("vect", vectorizer()),
        ("scaling", StandardScaler(with_mean=False)),
        ("clf", clf),
    ]
    )

    parameters = {
        "vect__ngram_range": ((1, 1), (1, 2)),  # unigrams or bigrams
        "vect__stop_words": ("english", None)
    }

    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',
                               cv=4, n_jobs=4, verbose=1)
    grid_search.fit(X_train, y_train)

    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

    print(f"CV scores {grid_search.cv_results_['mean_test_score']}")
    print(f"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}")
    
    return grid_search
```

## Approche _bag-of-words_

On commence par une approche __"bag-of-words"__, 
i.e. qui revient simplement √† repr√©senter chaque document par un vecteur
qui compte le nombre d'apparitions de chaque mot du vocabulaire dans le document.

Illustrons d'abord le principe √† l'aide d'un exemple simple.

```{python}
corpus = [
    'Un premier document √† propos des chats.',
    'Un second document qui parle des chiens.'
]

vectorizer = CountVectorizer()
vectorizer.fit(corpus)
```

L'objet `vectorizer` a √©t√© "entra√Æn√©" (*fit*) sur notre corpus d'exemple contenant deux documents. Il a notamment appris le vocabulaire complet du corpus, dont on peut afficher l'ordre.

```{python}
vectorizer.get_feature_names_out()
```

L'objet `vectorizer` entra√Æn√© peut maintenant vectoriser le corpus initial, selon l'ordre du vocabulaire affich√© ci-dessus.

```{python}
X = vectorizer.transform(corpus)
print(X.toarray())
```

Quel score `F1` obtient-on finalement avec cette m√©thode de vectorisation sur notre probl√®me de classification d'auteurs ?

```{python}
cv_bow = fit_vectorizers(CountVectorizer)
```

## TF-IDF

On s'int√©resse ensuite √† l'approche __TF-IDF__,
qui permet de tenir compte des fr√©quences *relatives* des mots.

Ainsi, pour un mot donn√©, on va multiplier la fr√©quence d'apparition du mot dans le document (calcul√© comme dans la m√©thode pr√©c√©dente) par un terme qui p√©nalise une fr√©quence √©lev√©e du mot dans le corpus. L'image ci-dessous, emprunt√©e √† Chris Albon, illustre cette mesure:

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/tfidf.png)

*Source: [Towards Data Science](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)*

La vectorisation `TF-IDF` permet donc de limiter l'influence des *stop-words*
et donc de donner plus de poids aux mots les plus salients d'un document.
Illustrons cela √† nouveau avec notre corpus d'exemple de deux documents.

```{python}
corpus = [
    'Un premier document √† propos des chats.',
    'Un second document qui parle des chiens.'
]

vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
```

L√† encore, le vectoriseur a "appris" le vocabulaire du corpus.

```{python}
vectorizer.get_feature_names_out()
```

Et peut √™tre utilis√© pour calculer les scores TF-IDF de chacun des termes des documents.

```{python}
X = vectorizer.transform(corpus)
print(X.toarray())
```

On remarque que "chats" et "chiens" poss√®dent les scores les plus √©lev√©s, ce sont bien les termes les plus distinctifs. A l'inverse, les termes qui reviennent dans les deux documents ("un", "document", "des") ont un score inf√©rieur, car ils sont beaucoup pr√©sents dans le corpus relativement.

Quel score `F1` obtient-on avec cette m√©thode de vectorisation sur notre probl√®me de classification d'auteurs ?

```{python}
cv_tfidf = fit_vectorizers(TfidfVectorizer)
```

On observe clairement que la performance de classification est bien sup√©rieure,
ce qui montre la pertinence de cette technique.

## Word2vec avec averaging

On va maintenant explorer les techniques de vectorisation bas√©es sur les
*embeddings* de mots, et notamment la plus populaire : `Word2Vec`.

L'id√©e derri√®re est simple, mais a r√©volutionn√© le NLP :
au lieu de repr√©senter les documents par des
vecteurs *sparse* de tr√®s grande dimension (la taille du vocabulaire)
comme on l'a fait jusqu'√† pr√©sent,
on va les repr√©senter par des vecteurs *dense* (continus)
de dimension r√©duite (en g√©n√©ral, autour de 100-300).

Chacune de ces dimensions va repr√©senter un facteur latent,
c'est √† dire une variable inobserv√©e,
de la m√™me mani√®re que les composantes principales produites par une ACP.

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/w2v_vecto.png)

*Source: [Medium](https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d)*


__Pourquoi est-ce int√©ressant ?__
Pour de nombreuses raisons, mais pour r√©sumer :
cela permet de beaucoup mieux capturer la similarit√© s√©mantique entre les documents.

Par exemple, un humain sait qu'un document contenant le mot _"Roi"_
et un autre document contenant le mot _"Reine"_ ont beaucoup de chance
d'aborder des sujets semblables.

Pourtant, une vectorisation de type comptage ou TF-IDF
ne permet pas de saisir cette similarit√© :
le calcul d'une mesure de similarit√© (norme euclidienne ou similarit√© cosinus)
entre les deux vecteurs ne prendra en compte la similarit√© des deux concepts, puisque les mots utilis√©s sont diff√©rents.

A l'inverse, un mod√®le `word2vec` bien entra√Æn√© va capter
qu'il existe un facteur latent de type _"royaut√©"_,
et la similarit√© entre les vecteurs associ√©s aux deux mots sera forte.

La magie va m√™me plus loin : le mod√®le captera aussi qu'il existe un
facteur latent de type _"genre"_,
et va permettre de construire un espace s√©mantique dans lequel les
relations arithm√©tiques entre vecteurs ont du sens ;
par exemple :
$$\text{king} - \text{man} + \text{woman} ‚âà \text{queen}$$

__Comment ces mod√®les sont-ils entra√Æn√©s ?__
Via une t√¢che de pr√©diction r√©solue par un r√©seau de neurones simple.

L'id√©e fondamentale est que la signification d'un mot se comprend
en regardant les mots qui apparaissent fr√©quemment dans son voisinage.

Pour un mot donn√©, on va donc essayer de pr√©dire les mots
qui apparaissent dans une fen√™tre autour du mot cible.

En r√©p√©tant cette t√¢che de nombreuses fois et sur un corpus suffisamment vari√©,
on obtient finalement des *embeddings* pour chaque mot du vocabulaire,
qui pr√©sentent les propri√©t√©s discut√©es pr√©c√©demment.


```{python}
X_train_tokens = [text.split() for text in X_train]
w2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, 
                     min_count=1, workers=4)
```


```{python}
w2v_model.wv.most_similar("mother")
```

On voit que les mots les plus similaires √† _"mother"_
sont souvent des mots li√©s √† la famille, mais pas toujours.

C'est li√© √† la taille tr√®s restreinte du corpus sur lequel on entra√Æne le mod√®le,
qui ne permet pas de r√©aliser des associations toujours pertinentes.


L'*embedding* (la repr√©sentation vectorielle) de chaque document correspond √† la moyenne des *word-embeddings* des mots qui le composent : 


```{python}
def get_mean_vector(w2v_vectors, words):
    words = [word for word in words if word in w2v_vectors]
    if words:
        avg_vector = np.mean(w2v_vectors[words], axis=0)
    else:
        avg_vector = np.zeros_like(w2v_vectors['hi'])
    return avg_vector

def fit_w2v_avg(w2v_vectors):
    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)
                                for words in X_train_tokens])
    
    scores = cross_val_score(clf, X_train_vectors, y_train, 
                         cv=4, scoring='f1_micro', n_jobs=4)

    print(f"CV scores {scores}")
    print(f"Mean F1 {np.mean(scores)}")
    return scores
```


```{python}
cv_w2vec = fit_w2v_avg(w2v_model.wv)
```

La performance chute fortement ;
la faute √† la taille tr√®s restreinte du corpus, comme annonc√© pr√©c√©demment.

## Word2vec pr√©-entra√Æn√© + averaging

Quand on travaille avec des corpus de taille restreinte,
c'est g√©n√©ralement une mauvaise id√©e d'entra√Æner son propre mod√®le `word2vec`.

Heureusement, des mod√®les pr√©-entra√Æn√©s sur de tr√®s gros corpus sont disponibles.
Ils permettent de r√©aliser du *transfer learning*,
c'est-√†-dire de b√©n√©ficier de la performance d'un mod√®le qui a √©t√© entra√Æn√© sur une autre t√¢che ou bien sur un autre corpus.

L'un des mod√®les les plus connus pour d√©marrer est le `glove_model` de
`Gensim` (Glove pour _Global Vectors for Word Representation_)[^1]:

> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. 
>
> _Source_ : https://nlp.stanford.edu/projects/glove/

[^1]: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. _GloVe: Global Vectors for Word Representation_. 

On peut le charger directement gr√¢ce √† l'instruction suivante : 

```{python}
#| output: hide
glove_model = gensim.downloader.load('glove-wiki-gigaword-200')
```

Par exemple, la repr√©sentation vectorielle de roi est l'objet
multidimensionnel suivant :

```{python}
glove_model['king']
```

Comme elle est peu intelligible, on va plut√¥t rechercher les termes les
plus similaires. Par exemple,

```{python}
glove_model.most_similar('mother')
```

On peut retrouver notre formule pr√©c√©dente

$$\text{king} - \text{man} + \text{woman} ‚âà \text{queen}$$
dans ce plongement de mots:

```{python}
glove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])
```

Vous pouvez vous r√©f√©rer √† [ce tutoriel](https://jalammar.github.io/illustrated-word2vec/)
pour en d√©couvrir plus sur `Word2Vec`.

Faisons notre apprentissage par transfert :

```{python}
cv_w2vec_transfert = fit_w2v_avg(glove_model)
```

La performance remonte substantiellement.
Cela √©tant, on ne parvient pas √† faire mieux que les approches basiques,
on arrive √† peine aux performances de la vectorisation par comptage.

En effet, pour rappel, les performances sont les suivantes :

```{python}
perfs = pd.DataFrame(
    [np.mean(cv_bow.cv_results_['mean_test_score']),
     np.mean(cv_tfidf.cv_results_['mean_test_score']),
    np.mean(cv_w2vec),
    np.mean(cv_w2vec_transfert)],
    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pr√©-entra√Æn√©', 'Word2Vec pr√©-entra√Æn√©'],
    columns = ["Mean F1 score"]
).sort_values("Mean F1 score",ascending = False)
perfs
```

Les performences limit√©es du mod√®le *Word2Vec* sont cette fois certainement dues √† la mani√®re dont
les *word-embeddings* sont exploit√©s : ils sont moyenn√©s pour d√©crire chaque document. 

Cela a plusieurs limites : 

- on ne tient pas compte de l'ordre et donc du contexte des mots
- lorsque les documents sont longs, la moyennisation peut cr√©er
des repr√©sentation bruit√©es.

## Contextual embeddings

Les *embeddings* contextuels visent √† pallier les limites des *embeddings*
traditionnels √©voqu√©es pr√©c√©demment.

Cette fois, les mots n'ont plus de repr√©sentation vectorielle fixe,
celle-ci est calcul√©e dynamiquement en fonction des mots du voisinage, et ainsi de suite.
Cela permet de tenir compte de la structure des phrases
et de tenir compte du fait que le sens d'un mot est fortement d√©pendant des mots
qui l'entourent. 
Par exemple, dans les expressions "le pr√©sident Macron" et "le camembert Pr√©sident" le mot pr√©sident n'a pas du tout le m√™me r√¥le.

Ces *embeddings* sont produits par des architectures tr√®s complexes,
de type Transformer (`BERT`, etc.).


```{python}
#| output: hide
model = SentenceTransformer('all-mpnet-base-v2')
```


```{python}
X_train_vectors = model.encode(X_train)
```


```{python}
scores = cross_val_score(clf, X_train_vectors, y_train, 
                         cv=4, scoring='f1_micro', n_jobs=4)

print(f"CV scores {scores}")
print(f"Mean F1 {np.mean(scores)}")
```

```{python}
perfs = pd.concat(
  [perfs,
  pd.DataFrame(
    [np.mean(scores)],
    index = ['Contextual Embedding'],
    columns = ["Mean F1 score"])]
).sort_values("Mean F1 score",ascending = False)
perfs
```


Verdict : on fait tr√®s l√©g√®rement mieux que la vectorisation TF-IDF.
On voit donc l'importance de tenir compte du contexte.

__Mais pourquoi, avec une m√©thode tr√®s compliqu√©e, ne parvenons-nous pas √† battre une m√©thode toute simple ?__

On peut avancer plusieurs raisons : 

- le `TF-IDF` est un mod√®le simple, mais toujours tr√®s performant
(on parle de _"tough-to-beat baseline"_).
- la classification d'auteurs est une t√¢che tr√®s particuli√®re et tr√®s ardue,
qui ne fait pas justice aux *embeddings*. Comme on l'a dit pr√©c√©demment, ces derniers se r√©v√®lent particuli√®rement pertinents lorsqu'il est question de similarit√© s√©mantique entre des textes (_clustering_, etc.).

Dans le cas de notre t√¢che de classification, il est probable que
certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de mani√®re pertinente,
ce que ne permettent pas de capter les *embeddings* qui accordent √† tous les mots la m√™me importance.

## Aller plus loin

- Nous avons entra√Æn√© diff√©rents mod√®les sur l'√©chantillon d'entra√Ænement par validation crois√©e, mais nous n'avons toujours pas utilis√© l'√©chantillon test que nous avons mis de c√¥t√© au d√©but. R√©aliser la pr√©diction sur les donn√©es de test, et v√©rifier si l'on obtient le m√™me classement des m√©thodes de vectorisation.
- Faire un *vrai* split train/test : faire l'entra√Ænement avec des textes de certains auteurs, et faire la pr√©diction avec des textes d'auteurs diff√©rents. Cela permettrait de neutraliser la pr√©sence de noms de lieux, de personnages, etc.
- Comparer avec d'autres algorithmes de classification qu'un SVM
- (Avanc√©) : fine-tuner le mod√®le d'embeddings contextuels sur la t√¢che de classification
