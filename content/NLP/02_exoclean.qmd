---
title: "L'analyse fréquentiste par l'approche bag-of-words : intérêt et limites"
title-en: "Frequentist analysis using the bag-of-words approach: forces and limitations"
author: Lino Galiana
categories:
  - NLP
description: |
  Ce chapitre approfondit l’analyse fréquentiste appliquée aux données textuelles en présentant les enjeux du domaine de l'extraction d'informations (_information retrieval_). Après avoir présenté le nettoyage et la vectorisation des textes via TF-IDF, il aborde les limites de l'approche _bag of words_ et ouvre la voie aux _embeddings_, vecteurs représentant les objets textuels par proximité de sens, objet du prochain chapitre.
description-en: This chapter takes a closer look at frequentist analysis applied to textual data by presenting the challenges in the field of information retrieval. After presenting the cleaning and vectorisation of texts using TF-IDF, it discusses the limitations of the _bag of words_ approach and paves the way for _embeddings_, vectors representing textual objects by proximity of meaning, the subject of the next chapter.
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/bag_of_words.jfif
echo: false
---


{{< badges
    printMessage="true"
>}}


::: {.content-visible when-profile="fr"}
Pour avancer dans ce chapitre, nous avons besoin de quelques installations préalables:
:::

::: {.content-visible when-profile="en"}
To move forward in this chapter, we need to perform some preliminary installations:
:::

```{python}
#| echo: true
#| output: false
!pip install spacy
!python -m spacy download en_core_web_sm
!python -m spacy download fr_core_news_sm
```

::: {.content-visible when-profile="fr"}
Il est également utile de définir la fonction suivante, issue de notre chapitre précédent :
:::

::: {.content-visible when-profile="en"}
It is also useful to define the following function, taken from our previous chapter:
:::

```{python}
#| echo: true
def clean_text(doc):
    # Tokenize, remove stop words and punctuation, and lemmatize
    cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    # Join tokens back into a single string
    cleaned_text = ' '.join(cleaned_tokens)
    return cleaned_text
```

::: {.content-visible when-profile="fr"}

# Introduction

Nous avons vu, précédemment, l'intérêt de nettoyer les données pour dégrossir le volume d'information présent dans nos données non structurées. L'objectif de ce chapitre est d'approfondir notre compréhension de l'approche fréquentiste appliquée aux données textuelles. Nous allons évoquer la manière dont cette analyse fréquentiste permet de synthétiser l'information présente dans un corpus textuel. Nous allons également voir la manière dont on peut raffiner l'approche _bag of words_ en tenant compte de l'ordre de la proximité des termes dans une phrase.

## Données

Nous allons reprendre le jeu de données anglo-saxon du chapitre précédent, à savoir
des textes des auteurs fantastiques [Edgar Allan Poe](https://fr.wikipedia.org/wiki/Edgar_Allan_Poe) (_EAP_), [HP Lovecraft](https://fr.wikipedia.org/wiki/H._P._Lovecraft) (_HPL_) et [Mary Wollstonecraft Shelley](https://fr.wikipedia.org/wiki/Mary_Shelley) (_MWS_).

:::

::: {.content-visible when-profile="en"}

# Introduction

Previously, we saw the importance of cleaning data to filter down the volume of information present in unstructured data. The goal of this chapter is to deepen our understanding of the frequency-based approach applied to text data. We will explore how this frequentist analysis helps summarize the information contained within a text corpus. We’ll also look at how to refine the _bag of words_ approach by taking into account the order or proximity of terms within a sentence.

## Data

We will reuse the Anglo-Saxon dataset from the previous chapter, which includes
texts from gothic authors [Edgar Allan Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) (_EAP_), [HP Lovecraft](https://en.wikipedia.org/wiki/H._P._Lovecraft) (_HPL_), and [Mary Wollstonecraft Shelley](https://en.wikipedia.org/wiki/Mary_Shelley) (_MWS_).

:::


{{< include "_import_horror.qmd" >}}

```{python}
# Function to clean the text
def clean_text(doc):
    # Tokenize, remove stop words and punctuation, and lemmatize
    cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    # Join tokens back into a single string
    cleaned_text = ' '.join(cleaned_tokens)
    return cleaned_text

```

```{python}
import spacy
nlp_english = spacy.load('en_core_web_sm')
stopwords = nlp_english.Defaults.stop_words
```



```{python}
#| label: clean-stopwords-horror
docs = nlp_english.pipe(horror["Text"])
cleaned_texts = [clean_text(doc) for doc in docs]
horror['preprocessed_text'] = cleaned_texts
```

::: {.content-visible when-profile="fr"}

# La mesure TF-IDF (_term frequency - inverse document frequency_)

## La matrice documents - termes

Comme nous l'avons évoqué précédemment, nous construisons une représentation synthétique de notre corpus comme un sac de mots dans lesquels on pioche plus ou moins fréquemment des mots selon leur fréquence d'apparition. C'est bien sûr une représentation simpliste de la réalité : les séquences de mots ne sont pas une suite aléatoire indépendante de mots.

Cependant, avant d'évoquer ces enjeux, il nous reste à aller au bout de l'approche sac de mots. La représentation la plus caractéristique de ce paradigme est la matrice document-terme, principalement utilisée pour comparer des corpus. Celle-ci consiste à créer une matrice où chaque document est représenté par la présence ou l'absence des termes de notre corpus.  L’idée est de compter le nombre de fois où les mots (les termes, en colonne) sont présents dans chaque phrase ou libellé (le document, en ligne). Cette matrice fournit alors une représentation numérique des données textuelles.

Considérons un corpus constitué des trois phrases suivantes :

* _“La pratique du tricot et du crochet”_
* _“Transmettre la passion du timbre”_
* _“Vivre de sa passion”_

La matrice document-terme associée à ce corpus est la suivante :

|                                     | crochet | de | du | et | la | passion | pratique | sa | timbre | transmettre | tricot | vivre |
| ----------------------------------- | :-------: | :--: | :--: | :--: | :--: | :-------: | :--------: | :--: | :------: | :-----------: | :------: | :-----: |
| La pratique du tricot et du crochet | 1       | 0  | 2  | 1  | 1  | 0       | 1        | 0  | 0      | 0           | 1      | 0     |
| Transmettre sa passion du timbre    | 0       | 0  | 1  | 0  | 0  | 1       | 0        | 1  | 1      | 1           | 0      | 0     |
| Vivre de sa passion                 | 0       | 1  | 0  | 0  | 0  | 1       | 0        | 1  | 0      | 0           | 0      | 1     |

Chaque phrase du corpus est associée à un vecteur numérique. Par exemple,
la phrase _"La pratique du tricot et du crochet"_, qui n'a pas de sens en soi pour une machine, devient un vecteur numérique intelligible pour elle égal à `[1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0]`. Ce vecteur numérique est une représentation creuse (_sparse_) du langage puisque chaque document (ligne) ne comportera qu'une petite partie du vocabulaire total (l'ensemble des colonnes). Pour tous les mots qui n'apparaîtront pas dans le document, on aura des 0, d'où un vecteur _sparse_. Comme nous le verrons par la suite, cette représentation numérique diffère grandement des approches modernes d'_embeddings_, basées sur l'idée de représentation denses.

:::

::: {.content-visible when-profile="en"}

# The TF-IDF Measure (_term frequency - inverse document frequency_)

## The Document-Term Matrix

As mentioned earlier, we construct a synthetic representation of our corpus as a bag of words, where words are sampled more or less frequently depending on their appearance frequency. This is, of course, a simplified representation of reality: word sequences are not just random independent words.

However, before addressing those limitations, we should complete the bag-of-words approach. The most characteristic representation of this paradigm is the document-term matrix, mainly used to compare corpora. It involves creating a matrix where each document is represented by the presence or absence of terms in our corpus. The idea is to count how often words (terms, in columns) appear in each sentence or phrase (documents, in rows). This matrix then becomes a numerical representation of the text data.

Consider a corpus made up of the following three sentences:

* The practice of knitting and crocheting
* Passing on the passion for stamps
* Living off one's passion"



The corresponding document-term matrix is:

| Sentence                                | and | crocheting | for | knitting | living | of | one's | on | passion | passing | practice | stamps | the |
|-----------------------------------------|:---:|:----------:|:---:|:--------:|:------:|:--:|:-----:|:--:|:-------:|:--------:|:--------:|:------:|:---:|
| The practice of knitting and crocheting |  1  |     1      |  0  |    1     |   0    | 1  |   0   | 0  |    0    |    0     |    1     |   0    |  1  |
| Passing on the passion for stamps       |  0  |     0      |  1  |    0     |   0    | 0  |   0   | 1  |    1    |    1     |    0     |   1    |  1  |
| Living off one's passion                |  0  |     0      |  0  |    0     |   1    | 0  |   1   | 0  |    1    |    0     |    0     |   0    |  0  |


Each sentence in the corpus is associated with a numeric vector. For instance,
the sentence _"La pratique du tricot et du crochet"_, which is meaningless to a machine on its own, becomes a numeric vector it can interpret: `[1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0]`. This numeric vector is a _sparse_ representation of language, since each document (row) will only contain a small portion of the total vocabulary (all columns). Words that do not appear in a document are represented as zeros, hence a _sparse_ vector. As we’ll see later, this numeric representation is very different from modern _embedding_ approaches, which are based on dense representations.

:::


::: {.content-visible when-profile="fr"}

## Utilisation pour l'extraction d'informations

Différents documents peuvent alors être rapprochés sur la base de ces mesures. C'est l'une des manières de procéder des moteurs de recherche même si les meilleurs utilisent des approches bien plus sophistiquées. La métrique [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) (_term frequency–inverse document frequency_)
permet de calculer un score de proximité entre un terme de recherche et un
document à partir de deux composantes :

:::

::: {.content-visible when-profile="en"}

## Use for Information Retrieval

Different documents can then be compared based on these measures. This is one of the methods used by search engines, although the most advanced ones rely on far more sophisticated approaches. The [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) metric (_term frequency–inverse document frequency_)
allows for calculating a relevance score between a search term and a document using two components:

:::

$$
\text{tf-idf}(t, d, D) = \text{tf}(t, d) \times \text{idf}(t, D)
$$

::: {.content-visible when-profile="fr"}

avec $t$ un terme particulier (par exemple un mot), $d$ un document particulier et $D$ l'ensemble de documents dans le corpus.

* La partie `tf` calcule une fonction croissante de la fréquence du terme de recherche dans le document à l'étude ;
* La partie `idf` calcule une fonction inversement proportionnelle à la fréquence du terme dans l'ensemble des documents (ou corpus).

* La première partie (_term-frequency_, TF) est la fréquence d'apparition du terme terme $t$ dans le document $d$. Il existe des mesures de normalisation pour éviter de biaiser la mesure en cas de documents longs.

:::

::: {.content-visible when-profile="en"}

Let $t$ be a specific term (e.g., a word), $d$ a specific document, and $D$ the entire set of documents in the corpus.

* The `tf` component computes a function that increases with the frequency of the search term in the document under consideration;
* The `idf` component computes a function that decreases with the frequency of the term across the entire document set (or corpus).

* The first part (_term frequency_, TF) is the frequency of occurrence of term $t$ in document $d$. There are normalization strategies available to avoid biasing the score in favor of longer documents.

:::

$$
\text{tf}(t, d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}
$$

::: {.content-visible when-profile="fr"}

où $f_{t,d}$ est le nombre brut de fois que le terme $t$ apparaît dans le document $d$  et le dénominateur est le nombre de termes dans le document $d$.

* La seconde partie (_inverse document frequency_, IDF) mesure la rareté, ou au contraire l'aspect commun, d'un terme dans l'ensemble du corpus. Si $N$ est le nombre total de documents dans le corpus $D$, cette partie de la mesure sera

:::

::: {.content-visible when-profile="en"}

where $f_{t,d}$ is the raw count of how many times term $t$ appears in document $d$, and the denominator is the total number of terms in document $d$.

* The second part (_inverse document frequency_, IDF) measures the rarity—or conversely, the commonness—of a term across the corpus. If $N$ is the total number of documents in the corpus $D$, this part of the metric is given by

:::

$$
\text{idf}(t, D) = \log \left( \frac{N}{|\{d \in D : t \in d\}|} \right)
$$

::: {.content-visible when-profile="fr"}

Le dénominateur $( |\{d \in D : t \in d\}| )$ correspond au nombre de documents contenant le terme $t$, s'il apparaît. Plus le mot est rare, plus sa présence dans un document sera surpondérée.

De nombreux moteurs de recherche utilisent cette logique pour rechercher les documents les plus pertinents pour répondre à des termes de recherche. C'est notamment le cas d' [`ElasticSearch`](https://www.elastic.co/fr/elasticsearch), le logiciel qui permet d'implémenter des moteurs de recherche efficaces. Pour classer les documents les plus pertinents par rapport à des termes de recherche, ce logiciel utilise notamment la mesure de distance [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) qui est une version sophistiquée de la mesure TF-IDF.

## Exemple

Prenons une illustration à partir d'un petit corpus. Le code suivant implémente une mesure TF-IDF. Celle-ci est légèrement différente de celle définie ci-dessus pour s'assurer de ne pas effectuer de division par zéro.

:::

::: {.content-visible when-profile="en"}

The denominator $( |\{d \in D : t \in d\}| )$ corresponds to the number of documents in which the term $t$ appears. The rarer the word, the more its presence in a document is given additional weight.

Many search engines use this logic to find the most relevant documents in response to a search query. One notable example is [`ElasticSearch`](https://www.elastic.co/elasticsearch), the software used to implement powerful search engines. To rank the most relevant documents for a given search term, it uses the [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) distance metric, which is a more advanced version of the TF-IDF measure.

## Example

Let’s illustrate this with a small corpus. The following code implements a TF-IDF metric. It slightly deviates from the standard definition to avoid division by zero.

:::

```{python}
#| echo: true
import numpy as np

# Documents d'exemple
documents = [
    "Le corbeau et le renard",
    "Rusé comme un renard",
    "Le chat est orange comme un renard"
]

# Tokenisation
def preprocess(doc):
    return doc.lower().split()

tokenized_docs = [preprocess(doc) for doc in documents]

# Term frequency (TF)
def term_frequency(term, tokenized_doc):
    term_count = tokenized_doc.count(term)
    return term_count / len(tokenized_doc)

# Inverse document frequency (DF)
def document_frequency(term, tokenized_docs):
    return sum(1 for doc in tokenized_docs if term in doc)

# Calculate inverse document frequency (IDF)
def inverse_document_frequency(word, corpus):
    # Normalisation avec + 1 pour éviter la division par zéro
    count_of_documents = len(corpus) + 1
    count_of_documents_with_word = sum([1 for doc in corpus if word in doc]) + 1
    idf = np.log10(count_of_documents/count_of_documents_with_word) + 1
    return idf

# Calculate TF-IDF scores in each document
def tf_idf_term(term):
  tf_idf_scores = pd.DataFrame(
    [
      [
      term_frequency(term, doc),
      inverse_document_frequency(term, tokenized_docs)
      ] for doc in tokenized_docs
    ],
    columns = ["TF", "IDF"]
  )
  tf_idf_scores["TF-IDF"] = tf_idf_scores["TF"] * tf_idf_scores["IDF"]
  return tf_idf_scores
```

::: {.content-visible when-profile="fr"}

Commençons par calculer TF-IDF du mot "chat" pour chaque document. De manière
naturelle, c'est le troisième document, le seul où apparaît le mot qui a la valeur maximale :

:::

::: {.content-visible when-profile="en"}

Let’s begin by computing the TF-IDF score of the word "cat" for each document. Naturally,
it is the third document—the only one where the word appears—that has the highest score:

:::

```{python}
#| echo: true
tf_idf_term("chat")
```

::: {.content-visible when-profile="fr"}

Qu'en est-il du terme renard qui apparaît dans tous les documents (dont la partie $\text{idf}$ est donc égale à 1) ? Dans ce cas, c'est le document où le mot est le plus fréquent, en l'occurrence le 2e, qui a la mesure maximale.

:::

::: {.content-visible when-profile="en"}

What about the term "renard" (fox in French) which appears in all the documents (making the $\text{idf}$ component equal to 1)? In this case, the document where the word appears most frequently—in this example, the second document—has the highest score.

:::

```{python}
#| echo: true
tf_idf_term("renard")
```


::: {.content-visible when-profile="fr"}

## Application

L'exemple précédent ne passait pas très bien à l'échelle. Heureusement, `Scikit` propose une implémentation de la recherche par vecteur TF-IDF que nous pouvons explorer avec un nouvel exercice.

:::

::: {.content-visible when-profile="en"}

## Application

The previous example didn't scale very well. Fortunately, `Scikit` provides an implementation of TF-IDF vector search, which we can explore in a new exercise.

:::

{{< include "02_exoclean/exercise1.qmd" >}}



::: {.content-visible when-profile="fr"}

On remarque que les scores les plus élevés sont soient des extraits courts où le mot apparait une seule fois, soit des extraits plus longs où le mot _"fear"_ apparaît plusieurs fois.

# Un premier enrichissement de l'approche sac de mots : les *n-grams*

Nous avons évoqué deux principales limites à l'approche sac de mot : l'absence de prise en compte du contexte et la représentation _sparse_ du langage qui rend les rapprochements entre texte parfois moyennement pertinents. Dans le paradigme du sac de mots, il est néanmoins possible de prendre en compte la séquence d'enchainement de sèmes (_tokens_) par le biais des _ngrams_.

Pour rappel, jusqu'à présent, dans l'approche _bag of words_, l'ordre des mots n'avait pas d'importance.
On considère qu'un texte est une collection de
mots tirés indépendamment, de manière plus ou moins fréquente en fonction de leur probabilité
d'occurrence. Cependant, tirer un mot particulier n'affecte pas les chances de tirer certains mots
ensuite, de manière conditionnelle.

Une manière d'introduire des liens entre les séries de _tokens_ sont les _n-grams_.
On s'intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Le calcul de _n-grams_ [^ngrams] constitue la méthode la plus simple pour tenir compte du contexte.

[^ngrams]: On parle de _bigrams_ pour les co-occurences de mots deux-à-deux, _trigrams_ pour les co-occurences trois-à-trois, etc.

Pour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire :

:::

::: {.content-visible when-profile="en"}

We observe that the highest scores correspond either to short excerpts where the word appears once, or to longer excerpts where the word _"fear"_ appears multiple times.

# An Initial Enhancement of the Bag-of-Words Approach: *n-grams*

We previously identified two main limitations of the bag-of-words approach: its disregard for context and its sparse representation of language, which sometimes leads to weak similarity matches between texts. However, within the bag-of-words paradigm, it is possible to account for the sequence of tokens using _n-grams_.

To recap, in the traditional _bag of words_ approach, word order doesn't matter.
A text is treated as a collection of words drawn independently, with varying frequencies based on their occurrence probabilities. Drawing a specific word doesn't affect the likelihood of subsequent words.

A way to introduce relationships between sequences of _tokens_ is through _n-grams_.
This method considers not only word frequencies but also which words follow others. It's particularly useful for disambiguating homonyms. The computation of _n-grams_ [^ngrams] is the simplest method for incorporating context.

[^ngrams]: We use the term _bigrams_ for two-word co-occurrences, _trigrams_ for three-word ones, etc.

To carry out this type of analysis, we need to download an additional corpus:

:::


```{python}
#| echo: true
import nltk
nltk.download('genesis')
nltk.corpus.genesis.words('english-web.txt')
```


::: {.content-visible when-profile="fr"}

`NLTK` offre des méthodes pour tenir compte du contexte. Pour ce faire, nous calculons les n-grams, c'est-à-dire l'ensemble des co-occurrences successives de mots n-à-n.  En général, on se contente de bi-grams, au mieux de tri-grams :

* les modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;
* les performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de données initiale).

On va, rapidement, regarder dans quel contexte apparaît le mot `fear` dans
l'oeuvre d'Edgar Allan Poe (EAP). Pour cela, on transforme d'abord
le corpus EAP en tokens `NLTK` :

:::

::: {.content-visible when-profile="en"}

`NLTK` provides methods for incorporating context. To do this, we compute n-grams—that is, sequences of n consecutive word co-occurrences. Generally, we limit ourselves to bigrams or at most trigrams:

* Classification models, sentiment analysis, document comparison, etc., that rely on n-grams with large n quickly face sparse data issues, reducing their predictive power;
* Performance drops quickly as n increases, and data storage costs increase substantially (roughly n times larger than the original dataset).

Let’s quickly examine the context in which the word `fear` appears
in the works of Edgar Allan Poe (EAP). To do this, we first transform the EAP corpus into `NLTK` tokens:

:::

```{python}
#| echo: true
eap_clean = horror.loc[horror["Author"] == "EAP"]
eap_clean = ' '.join(eap_clean['Text'])
tokens = eap_clean.split()
print(tokens[:10])
text = nltk.Text(tokens)
print(text)
```

::: {.content-visible when-profile="fr"}
Vous aurez besoin des fonctions `BigramCollocationFinder.from_words` et `BigramAssocMeasures.likelihood_ratio` :
:::

::: {.content-visible when-profile="en"}
You will need the functions `BigramCollocationFinder.from_words` and `BigramAssocMeasures.likelihood_ratio`:
:::

```{python}
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
```

{{< include "02_exoclean/exercise2.qmd" >}}


::: {.content-visible when-profile="fr"}

Avec la méthode `concordance` (question 1),
la liste devrait ressembler à celle-ci :

:::

::: {.content-visible when-profile="en"}

Using the `concordance` method (question 1),
the list should look like this:

:::

```{python}
#| include: true


# 1. Methode concordance
print("Exemples d'occurences du terme 'fear' :")
text.concordance("fear")
print('\n')
```

::: {.content-visible when-profile="fr"}

Même si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d'informations.

La `collocation` consiste à trouver les bi-grammes qui
apparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées,
il s'agit de sélectionner, à partir d'un modèle statistique, les "meilleures".
On obtient donc avec cette méthode (question 2):

:::

::: {.content-visible when-profile="en"}

Although it is easy to see the words that appear before and after, this list is rather hard to interpret because it combines a lot of information.

`Collocation` involves identifying bigrams that
frequently occur together. Among all observed word pairs,
the idea is to select the "best" ones based on a statistical model.
Using this method (question 2), we get:

:::

```{python}
# 2. Modélisation des meilleures collocations
bcf = BigramCollocationFinder.from_words(text)
bcf.nbest(BigramAssocMeasures.likelihood_ratio, 20)
```

::: {.content-visible when-profile="fr"}

Si on modélise les meilleures collocations :

:::

::: {.content-visible when-profile="en"}

If we model the best collocations:

:::

```{python}
# 3. Modélisation des meilleures collocations (qui apparaissent 5+)
finder = nltk.BigramCollocationFinder.from_words(text)
finder.apply_freq_filter(5)
bigram_measures = nltk.collocations.BigramAssocMeasures()
collocations = finder.nbest(bigram_measures.jaccard, 15)

for collocation in collocations :
    c = ' '.join(collocation)
    print(c)
```

::: {.content-visible when-profile="fr"}

Cette liste a un peu plus de sens,
on a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble
(*Chess Player* par exemple).

En ce qui concerne les _collocations_ du mot fear :

:::

::: {.content-visible when-profile="en"}

This list is a bit more meaningful,
including character names, places, and frequently used expressions
(like *Chess Player* for example).

As for the _collocations_ of the word *fear*:

:::

```{python}
# 4. collocations du mot fear
bigram_measures = nltk.collocations.BigramAssocMeasures()

def collocations_word(word = "fear"):
    # Ngrams with a specific name
    name_filter = lambda *w: word not in w
    # Bigrams
    finder = BigramCollocationFinder.from_words(
                nltk.corpus.genesis.words('english-web.txt'))
    # only bigrams that contain 'fear'
    finder.apply_ngram_filter(name_filter)
    # return the 100 n-grams with the highest PMI
    print(finder.nbest(bigram_measures.likelihood_ratio,100))

collocations_word("fear")
```

::: {.content-visible when-profile="fr"}

Si on mène la même analyse pour le terme *love*, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe :

:::

::: {.content-visible when-profile="en"}

If we perform the same analysis for the term *love*, we logically find subjects that are commonly associated with the verb:

:::

```{python}
collocations_word("love")
```


::: {.content-visible when-profile="fr"}

# Quelques applications

Nous venons d'évoquer un premier cas d'application de l'approche _bag of words_ qui est le rapprochement de textes par leurs termes communs. Ce n'est pas le seul cas d'application de l'approche précédente. Nous allons en évoquer deux qui nous amènent vers la modélisation du langage : la reconnaissance d'entités nommées et la classification.

## Reconnaissance des entités nommées

La [reconnaissance d'entités nommées](https://fr.wikipedia.org/wiki/Reconnaissance_d%27entit%C3%A9s_nomm%C3%A9es), également connue sous l'acronyme NER pour _named entity recognition_, est une méthode d'extraction d'information permettant d'identifier, dans un texte, la nature de certains termes dans une certain classification : lieu, personne, quantité, etc.

Pour illustrer cela,
reprenons le _Comte de Monte Cristo_ et regardons sur un petit morceau de cette oeuvre ce qu'implique la reconnaissance d'entités nommées :

:::

::: {.content-visible when-profile="en"}

# Some Applications

We just discussed an initial application of the _bag of words_ approach: grouping texts based on shared terms. However, this is not the only use case. We will now explore two additional applications that lead us toward language modeling: named entity recognition and classification.

## Named Entity Recognition

[Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) is an information extraction technique used to identify the type of certain terms in a text, such as locations, people, quantities, etc.

To illustrate this,
let’s return to *The Count of Monte Cristo* and examine a short excerpt from the work to see how named entity recognition operates:

:::

{{< include "_import_dumas.qmd" >}}

```{python}
#| output: false
!python -m spacy download fr_core_news_sm
```

```{python}
#| echo: true
import spacy
from spacy import displacy

nlp = spacy.load("fr_core_news_sm")
doc = nlp(dumas[15000:17000])
# displacy.render(doc, style="ent", jupyter=True)
```

::: {.content-visible when-profile="fr"}

La reconnaissance d'entités nommées disponible
par défaut dans les librairies généralistes est souvent décevante ; il est
souvent nécessaire d'enrichir les règles par défaut
par des règles _ad hoc_, propres à chaque corpus.

En pratique, récemment,
l'approche de reconnaissance d'entités nommées a été utilisée
par Etalab pour [pseudonymiser des documents administratifs](https://guides.etalab.gouv.fr/pseudonymisation/#sommaire). Il s'agit d'identifier certaines informations sensibles (état civil, adresse...) par reconnaissance d'entités pour les remplacer par des pseudonymes.

:::

::: {.content-visible when-profile="en"}

The named entity recognition provided
by default in general-purpose libraries is often underwhelming; it is
frequently necessary to supplement the default rules
with _ad hoc_ rules specific to each corpus.

In practice, named entity recognition was recently
used by Etalab to [pseudonymize administrative documents](https://guides.etalab.gouv.fr/pseudonymisation/#sommaire). This involves identifying certain sensitive information (such as civil status, address, etc.) through entity recognition and replacing it with pseudonyms.

:::


::: {.content-visible when-profile="fr"}

## Classification de données textuelles : l'algorithme `Fasttext`

`Fasttext` est un réseau de neurone à une couche développé par Meta en 2016 pour faire de la classification de texte ou de la modélisation de langage. Comme nous allons pouvoir le voir, ce modèle va nous amener à faire la transition avec une modélisation plus raffinée du langage, bien que celle de `Fasttext` soit beaucoup plus frustre que celle des grands modèles de langage (LLM). L'un des principaux cas d'utilisation de `Fasttext` est la classification supervisée de données textuelles. Il s'agit, à partir d'un texte, de déterminer sa catégorie d'appartenance. Par exemple, à partir d'un texte de chanson, s'il s'agit de rap ou de rock. Ce modèle est supervisé puisqu'il apprend à reconnnaître les _features_, en l'occurrence des morceaux de texte, qui permettent d'avoir une bonne performance de prédiction sur le jeu d'entraînement puis de test.

Le concept de _feature_ peut sembler étonnant pour des données textuelles, qui sont, par essence, non structurées. Pour des données structurées, la démarche évoquée dans la partie [modélisation](/content/course/modelisation/index.qmd) apparaîssait naturelle : nous avions des variables observées dans nos données pour les _features_ et l'algorithme de classification consistait à trouver la combinaison entre elles permettant de prédire, au mieux, le _label_. Avec des données textuelles, ce concept de _feature_ observé n'est plus naturel. Il est nécessaire de le construire à partir d'un texte. L'information destructurée devient de l'information structurée. C'est là qu'interviennent les concepts que nous avons vu jusqu'à présent.

`FastText` est un _"sac de n-gram"_. Il considère donc que les _features_ sont à construire à partir des mots de notre corpus mais aussi des _ngrams_ à plusieurs niveaux. L'architecture générale de `FastText` ressemble à celle-ci :

![Illustration de l'architecture de `FastText`](https://raw.githubusercontent.com/InseeFrLab/formation-mlops/main/slides/img/diag-fasttext.png)

Ce qui nous intéresse ici est la partie gauche de ce diagramme, la _"feature extraction"_ car la partie _embedding_ correspond à des concepts que nous verrons lors des prochains chapitres. Avec l'exemple de cette figure, on voit que le texte _"Business engineering and services"_ est tokenisé comme nous avons pu le voir plus tôt en mots. Mais `Fasttext` créé également des ngrams à plusieurs niveaux. Par exemple, il va créer des bigrams de mots : _"Business engineering"_, _"engineering and"_, _"and services"_. mais aussi des quadrigrammes de caractères _"busi"_, _"usin"_ et _"sine"_. Ensuite, `Fasttext` transformera tous ces termes en vecteurs numériques. Contrairement à ce que nous avons vu jusqu'à présent, ces vecteurs ne sont pas des fréquences d'apparition dans le corpus (principe de la matrice origine-document), ce sont des plongements de mots (_word embedding_). Nous découvrirons leurs principes dans les prochains chapitres.

Ce modèle `Fasttext` est très utilisé dans la statistique publique car de nombreuses sources de données textuelles sont à hiérarchiser dans des nomenclatures agrégées.

:::

::: {.content-visible when-profile="en"}

## Text Data Classification: The `Fasttext` Algorithm

`Fasttext` is a single-layer neural network developed by Meta in 2016 for text classification and language modeling. As we will see, this model serves as a bridge to more refined forms of language modeling, although `Fasttext` remains far simpler than large language models (LLMs). One of the main use cases of `Fasttext` is supervised text classification: determining a text’s category. For example, identifying whether a song’s lyrics belong to the rap or rock genre. This is a supervised model because it learns to recognize _features_—in this case, pieces of text—that lead to good prediction performance on both training and test sets.

The concept of a _feature_ might seem odd for text data, which is inherently unstructured. For structured data, as discussed in the [modeling section](/content/course/modelisation/index.qmd), the approach was straightforward: features were observed variables, and the classification algorithm identified the best combination to predict the label. With text data, we must build features from the text itself—turning unstructured data into structured form. This is where the concepts we've covered so far come into play.

`FastText` uses a _"bag of n-grams"_ approach. It considers that features are derived not only from words in the corpus but also from multiple levels of n-grams. The general architecture of `FastText` looks like this:

![Diagram of `FastText` architecture](https://raw.githubusercontent.com/InseeFrLab/formation-mlops/main/slides/img/diag-fasttext.png)

What interests us here is the left side of the diagram—_"feature extraction"_—since the _embedding_ part relates to concepts we will cover in upcoming chapters. In the figure's example, the text _"Business engineering and services"_ is tokenized into words as we’ve seen earlier. But `Fasttext` also creates multiple levels of n-grams. For instance, it generates word bigrams: _"Business engineering"_, _"engineering and"_, _"and services"_; and also character four-grams like _"busi"_, _"usin"_, and _"sine"_. Then, `Fasttext` transforms all these items into numeric vectors. Unlike the term frequency representations we've seen, these vectors are not based on corpus frequency (as in document-term matrices) but are word embeddings. We'll explore this concept in future chapters.

`Fasttext` is widely used in public statistics, as many textual data sources need to be classified into aggregated nomenclatures.

:::



:::: {.content-visible when-format="html"}

::: {.content-visible when-profile="fr"}

Voici un exemple d'utilisation d'un tel modèle pour la classification d'activités

:::

::: {.content-visible when-profile="en"}

Here is an example of how such a model can be used for activity classification

:::


```{ojs}
//| echo: false
viewof activite = Inputs.text(
  {label: '', value: 'data scientist', width: 800}
)
```


```{ojs}
//| echo: false
d3.json(urlApe).then(res => {
  var IC, results;

  ({ IC, ...results } = res);

  IC = parseFloat(IC);

  const rows = Object.values(results).map(obj => {
    return `
    <tr>
      <td>${obj.code} | ${obj.libelle}</td>
      <td>${obj.probabilite.toFixed(3)}</td>
    </tr>
  `;
  }).join('');

  const confidenceRow = `<tr>
    <td colspan="2" style="text-align:left; "><em>Indice de confiance : ${IC.toFixed(3)}</em></td>
  </tr>`;

  const tableHTML = html`
  <table>
    <caption>
      Prédiction de l'activité
    </caption>
    <tr>
      <th style="text-align:center;">Libellé (NA2008)</th>
      <th>Probabilité</th>
    </tr>
      ${rows}
      ${confidenceRow}
  </table>`;

  // Now you can use the tableHTML as needed, for example, inserting it into the DOM.
  // For example, assuming you have a container with the id "tableContainer":
  return tableHTML;
});
```

```{ojs}
//| echo: false
activite_debounce = debounce(viewof activite, 2000)
urlApe = `https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite_debounce}`
```

```{ojs}
//| echo: false
import {debounce} from "@mbostock/debouncing-input"
```

::::

:::: {.content-hidden when-format="html"}

```{python}
import requests
import pandas as pd

activite = "data scientist"
urlApe = f"https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=${activite}"
import requests
data = requests.get(urlApe).json()

# Extract 'IC' value
IC = data['IC']
data.pop('IC', None)

df = pd.DataFrame(data.values())
df['indice_confiance'] = IC
df
```

::::

:::: {.content-visible when-format="ipynb"}

::: {.content-visible when-profile="fr"}

Pour voir une démonstration interactive d'un tel modèle, se rendre sur la [page du site](/content/NLP/02_exoclean.qmd) associée à ce notebook.

:::

::: {.content-visible when-profile="en"}

To see an interactive demonstration of such a model, visit the [corresponding site page](/content/NLP/02_exoclean.qmd) linked to this notebook.

:::

::::

