{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f292a2",
   "metadata": {},
   "source": [
    "#  Préparation des données pour construire un modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd96277",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/modelisation/0_preprocessing.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![nbviewer](https://img.shields.io/badge/visualize-nbviewer-blue)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/notebooks/course/modelisation/0_preprocessing.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=%C2%ABpython-datascientist%C2%BB&resources.requests.memory=%C2%AB4Gi%C2%BB)\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=notebooks/course/modelisation/0_preprocessing.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/notebooks/course/modelisation/0_preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc42ca4",
   "metadata": {},
   "source": [
    "Ce chapitre utilise le jeu de données présenté dans l'[introduction\n",
    "de cette partie](https://linogaliana-teaching.netlify.app/modelisation/):\n",
    "les données de vote aux élections présidentielles US\n",
    "croisées à des variables socio-démographiques. Le code \n",
    "est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/master/content/modelisation/get_data.py)\n",
    "mais l'exercice 1 permet, à ceux qui le désirent, d'essayer de reproduire cela. \n",
    "\n",
    "Le guide utilisateur de `scikit` est une référence précieuse,\n",
    "à consulter régulièrement. La partie sur le *preprocessing* est\n",
    "disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9897915",
   "metadata": {},
   "source": [
    "# Construction de la base de données\n",
    "\n",
    "Les sources étant éclatées, le code pour construire une base combinant toutes ces\n",
    "sources est directement fourni. Le travail de construction d'une base unique\n",
    "est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,\n",
    "pour [réviser `pandas`](#pandas)   :\n",
    "\n",
    "1. Télécharger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_02_sldl_500k.zip)\n",
    "2. Exclure les Etats suivants: \"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"\n",
    "3. Importer les résultats des élections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv)\n",
    "4. Importer les bases disponibles sur le site de l'USDA en faisant attention à renommer les variables de code FIPS de manière identique\n",
    "dans les 4 bases\n",
    "5. *Merger* ces 4 bases dans une base unique de caractéristiques socio-économiques\n",
    "6. *Merger* aux données électorales à partir du code FIPS\n",
    "7. *Merger* au shapefile à partir du code FIPS. Faire attention aux 0 à gauche dans certains codes. Il est\n",
    "recommandé d'utiliser la méthode `str.lstrip` pour les retirer\n",
    "8. Importer les données des élections 2000 à 2016 à partir du [MIT Election Lab](https://electionlab.mit.edu/data)?\n",
    "Les données peuvent être directement requêtées depuis l'url\n",
    "<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>\n",
    "9. Créer une variable `share` comptabilisant la part des votes pour chaque candidat. \n",
    "Ne garder que les colonnes `\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"`\n",
    "10. Faire une conversion `long` to `wide` avec la méthode `pivot_table` pour garder une ligne\n",
    "par comté x année avec en colonnes les résultats de chaque candidat dans cet état.\n",
    "11. Merger à partir du code FIPS au reste de la base. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d029bf",
   "metadata": {},
   "source": [
    "La carte choropleth suivante permet de visualiser rapidement les résultats\n",
    "(l'Alaska et Hawaï ont été exclus). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84794558",
   "metadata": {},
   "source": [
    "Les cartes choropleth peuvent donner une impression fallacieuse ayant servi \n",
    "de justification pour contester les résultats du vote. \n",
    "Le [GIF \"Land does not vote, people do\"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF)\n",
    "qui avait eu un certain succès en 2020 propose un autre mode de visualisation.\n",
    "En tenant compte de la population,\n",
    "on obtient la carte suivante:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f9d56",
   "metadata": {},
   "source": [
    "## Explorer la structure des données\n",
    "\n",
    "La première étape nécessaire à suivre avant de modéliser est de déterminer les variables à inclure dans le modèle. Les fonctionalités de `pandas` sont, à ce niveau, suffisantes pour explorer des structures simples. Néanmoins, lorsqu'on est face à un jeu de données présentant de nombreuses variables explicatives (*features* en machine learning, *covariates* en économétrie), il est souvent judicieux d'avoir une première étape de sélection de variable, ce que nous verrons par la suite [**LIEN**]  \n",
    "\n",
    "Avant d'être en mesure de sélectionner le meilleur ensemble de variables explicatives, nous allons prendre un nombre restreint et arbitraire de variables. La première tâche est de représenter les relations entre les données, notamment leur relation à la variable que l'on va chercher à expliquer (le score du parti républicain) ainsi que les relations entre les variables ayant vocation à expliquer la variable dépendante. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e270e3",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['winner'] not in index\"\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/geopandas/geodataframe.py\", line 1299, in __getitem__\n",
    "##     result = super(GeoDataFrame, self).__getitem__(key)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/frame.py\", line 3461, in __getitem__\n",
    "##     indexer = self.loc._get_listlike_indexer(key, axis=1)[1]\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1314, in _get_listlike_indexer\n",
    "##     self._validate_read_indexer(keyarr, indexer, axis)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/indexing.py\", line 1377, in _validate_read_indexer\n",
    "##     raise KeyError(f\"{not_found} not in index\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36020d5",
   "metadata": {},
   "source": [
    "Créer un DataFrame plus petit avec les variables `winner` et `unemployment`, `median_age`, `asian`, `black`, `white_not_latino_population`,`latino_population`, `gini_coefficient`, `less_than_high_school`, `adult_obesity`, `median_earnings_2010_dollars` et ensuite :\n",
    "\n",
    "1. Représenter une matrice de corrélation graphique\n",
    "1. Choisir quelques variables (pas plus de 4 ou 5) et représenter une matrice de nuages de points\n",
    "2. (optionnel) Refaire ces figures avec `plotly`\n",
    "\n",
    "La matrice de corrélation construite avec `seaborn` aura l'aspect suivant:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2e68b",
   "metadata": {},
   "source": [
    "Il serait également possible de la construire directement à partir de `pandas`\n",
    "donne, avec les fonctionalités de `pandas`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b2594",
   "metadata": {},
   "source": [
    "En ce qui concerne, la matrice de nuage de point :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e10c81",
   "metadata": {},
   "source": [
    "`plotly` offre également la possibilité de faire une matrice de corrélation. \n",
    "Cela donnera un résultat proche de celui-ci:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216e7ca",
   "metadata": {},
   "source": [
    "Avec `plotly`, le résultat devrait ressembler au graphique suivant:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475e924",
   "metadata": {},
   "source": [
    "## Transformer les données\n",
    "\n",
    "Les différences d'échelle ou de distribution entre les variables peuvent \n",
    "diverger des hypothèses sous-jacentes dans les modèles. Par exemple, dans le cadre\n",
    "de la régression linéaire, les variables catégorielles ne sont pas traitées à la même\n",
    "enseigne que les variables ayant valeur dans $\\mathbb{R}$. Il est ainsi \n",
    "souvent nécessaire d'appliquer des tâches de *preprocessing*, c'est-à-dire \n",
    "des tâches de modification de la distribution des données pour les rendre\n",
    "cohérentes avec les hypothèses des modèles.\n",
    "\n",
    "### Standardisation\n",
    "\n",
    "La standardisation consiste à transformer des données pour que la distribution empirique suive une loi $\\mathcal{N}(0,1)$. Pour être performants, la plupart des modèles de machine learning nécessitent d'avoir des données dans cette distribution. \n",
    "\n",
    "Pour un statisticien, le terme `normalization` dans le vocable `scikit` peut avoir un sens contre-intuitif. On s'attendrait à ce que la normalisation consiste à transformer une variable de manière à ce que $X \\sim \\mathcal{N}(0,1)$. C'est, en fait, la **standardisation** en `scikit`. \n",
    "\n",
    "La **normalisation** consiste à modifier les données de manière à avoir une norme unitaire. La raison est expliquée plus bas\n",
    "\n",
    "1. Standardiser la variable `Median_Household_Income_2019` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation\n",
    "2. Créer `scaler`, un `Transformer` que vous construisez sur les 1000 premières lignes de votre DataFrame. Vérifier la moyenne et l'écart-type de chaque colonne sur ces mêmes observations.\n",
    "3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `Median_Household_Income_2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd34d95",
   "metadata": {},
   "source": [
    "La standardisation permet d'obtenir la modification suivante de la distribution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae84223",
   "metadata": {},
   "source": [
    "On obtient bien une distribution centrée à zéro et on pourrait vérifier que la variance empirique soit bien égale à 1. On pourrait aussi vérifier que ceci est vrai également quand on transforme plusieurs colonnes à la fois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1179988",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: could not convert string to float: 'Vance County'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\", line 667, in fit\n",
    "##     return self.partial_fit(X, y)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\", line 696, in partial_fit\n",
    "##     X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/base.py\", line 420, in _validate_data\n",
    "##     X = check_array(X, **check_params)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
    "##     return f(**kwargs)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
    "##     array = np.asarray(array, order=order, dtype=dtype)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/generic.py\", line 1993, in __array__\n",
    "##     return np.asarray(self._values, dtype=dtype)\n",
    "```\n",
    "\n",
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scaler' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "```\n",
    "## Moyenne de chaque variable sur 1000 premières observations\n",
    "```\n",
    "\n",
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scaler' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "```\n",
    "## Ecart-type de chaque variable sur 1000 premières observations\n",
    "```\n",
    "\n",
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scaler' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "Les paramètres qui seront utilisés pour une standardisation ultérieure\n",
    "sont stockés dans les attributs `.mean_` et `.scale_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e040cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70ee14",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scaler' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45694c25",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scaler' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "Une fois appliqués à un autre `DataFrame`, on peut remarquer que la distribution\n",
    "n'est pas exactement centrée-réduite dans le `DataFrame` sur lequel les\n",
    "paramètres n'ont pas été estimés.\n",
    "C'est normal, l'échantillon initial n'était pas aléatoire,\n",
    "les moyennes et variances de cet échantillon n'ont pas de raison de\n",
    "coïncider avec les moments de l'échantillon complet. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58e161",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "La **normalisation** est l'action de transformer les données de manière à obtenir une norme ($\\mathcal{l}_1$ ou $\\mathcal{l}_2$) unitaire. Autrement dit, avec la norme adéquate, la somme des éléments est égale à 1. Par défaut, la norme est dans $\\mathcal{l}_2$. Cette transformation est particulièrement utilisée en classification de texte ou pour effectuer du *clustering*\n",
    "\n",
    "1. Normaliser la variable `median_earnings_2010_dollars` (ne pas écraser les valeurs !) et regarder l'histogramme avant/après normalisation\n",
    "2. Vérifier que la norme $\\mathcal{l}_2$ est bien égale à 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941025",
   "metadata": {},
   "source": [
    "` preprocessing.Normalizer` n'accepte pas les valeurs manquantes, alors que `preprocessing.StandardScaler()` s'en accomode (dans la version `0.22` de scikit). Pour pouvoir aisément appliquer le *normalizer*, il faut\n",
    "\n",
    "* retirer les valeurs manquantes du DataFrame avec la méthode `dropna`: `df.dropna(how = \"any\")`;\n",
    "* ou les imputer avec un modèle adéquat. `scikit` permet de le faire ([info](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values)) \n",
    "\n",
    "### Encodage des valeurs catégorielles\n",
    "\n",
    "Les données catégorielles doivent être recodées sous forme de valeurs numériques pour être intégrables dans le cadre d'un modèle. Cela peut être fait de plusieurs manières:\n",
    "\n",
    "* `LabelEncoder`: transforme un vecteur `[\"a\",\"b\",\"c\"]` en vecteur numérique `[0,1,2]`. Cette approche a l'inconvénient d'introduire un ordre dans les modalités, ce qui n'est pas toujours désiré\n",
    "* `pandas.get_dummies` effectue une opération de *dummy expansion*. Un vecteur de taille *n* avec *K* catégories sera transformé en matrice de taille $n \\times K$ pour lequel chaque colonne sera une variable *dummy* pour la modalité *k*. Il y a ici $K$ modalité, il y a donc multicollinéarité. Avec une régression linéaire avec constante, il convient de retirer une modalité avant l'estimation.\n",
    "* `OrdinalEncoder`: une version généralisée du `LabelEncoder`. `OrdinalEncoder` a vocation à s'appliquer sur des matrices ($X$), alors que `LabelEncoder` est plutôt pour un vecteur ($y$)\n",
    "* `OneHotEncoder`: une version généralisée (et optimisée) de la *dummy expansion*. Il a plutôt vocation à s'appliquer sur les *features* ($X$) du modèle\n",
    "\n",
    "Prendra les variables `state_name` et `county_name` dans `votes`\n",
    "1. Appliquer à `state_name` un `LabelEncoder`\n",
    "2. Regarder la *dummy expansion* de `state_name`\n",
    "3. Appliquer un `OrdinalEncoder` à `df[['state_name', 'county_name']]` ainsi qu'un `OneHotEncoder`\n",
    "\n",
    "Le résultat du *label encoding* est relativement intuitif, notamment quand on le met en relation avec le vecteur initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c86b50",
   "metadata": {},
   "source": [
    "```\n",
    "## array([[23, 'Missouri'],\n",
    "##        [25, 'Nebraska'],\n",
    "##        [30, 'New York'],\n",
    "##        ...,\n",
    "##        [41, 'Texas'],\n",
    "##        [41, 'Texas'],\n",
    "##        [41, 'Texas']], dtype=object)\n",
    "```\n",
    "\n",
    "L'expansion par variables dichotomiques également:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1dbec",
   "metadata": {},
   "source": [
    "```\n",
    "##       Alabama  Arizona  Arkansas  ...  West Virginia  Wisconsin  Wyoming\n",
    "## 0           0        0         0  ...              0          0        0\n",
    "## 1           0        0         0  ...              0          0        0\n",
    "## 2           0        0         0  ...              0          0        0\n",
    "## 3           0        0         0  ...              0          0        0\n",
    "## 4           0        0         0  ...              0          0        0\n",
    "## ...       ...      ...       ...  ...            ...        ...      ...\n",
    "## 3103        0        0         0  ...              0          0        0\n",
    "## 3104        0        0         0  ...              0          0        0\n",
    "## 3105        0        0         0  ...              0          0        0\n",
    "## 3106        0        0         0  ...              0          0        0\n",
    "## 3107        0        0         0  ...              0          0        0\n",
    "## \n",
    "## [3108 rows x 49 columns]\n",
    "```\n",
    "\n",
    "Le résultat du *ordinal encoding* est cohérent avec celui du *label encoding*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd0ab1",
   "metadata": {},
   "source": [
    "```\n",
    "## array([23., 25., 30., ..., 41., 41., 41.])\n",
    "```\n",
    "\n",
    "Enfin, on peut noter que `scikit` optimise l'objet nécessaire pour stocker le résultat d'un modèle de transformation. Par exemple, le résultat de l'encoding *One Hot* est un objet très volumineux. Dans ce cas, scikit utilise une matrice *Sparse*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79785cd",
   "metadata": {},
   "source": [
    "```\n",
    "## <3108x1892 sparse matrix of type '<class 'numpy.float64'>'\n",
    "## \twith 6216 stored elements in Compressed Sparse Row format>\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
