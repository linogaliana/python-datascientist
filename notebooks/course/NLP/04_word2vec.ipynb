{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32045de2",
   "metadata": {},
   "source": [
    "#  Prédiction dauteurs littéraires à partir de différentes méthodes de vectorisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eef992",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=«python-datascientist»&resources.requests.memory=«4Gi»&security.allowlist.enabled=false&init.personalInit=«https://raw.githubusercontent.com/linogaliana/python-datascientist/master/init_onyxia.sh»)\n",
    "<br>\n",
    "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFkAAABZCAMAAABi1XidAAAB8lBMVEX///9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olJXmsrmZYH1olL1olL0nFf1olJXmsrmZYH1olJXmsq8dZb1olJXmsrmZYH1olJXmspXmspXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olLeaIVXmsrmZYH1olL1olL1olJXmsrmZYH1olLna31Xmsr1olJXmsr1olJXmsrmZYH1olLqoVr1olJXmsr1olJXmsrmZYH1olL1olKkfaPobXvviGabgadXmsqThKuofKHmZ4Dobnr1olJXmsr1olJXmspXmsr1olJXmsrfZ4TuhWn1olL1olJXmsqBi7X1olJXmspZmslbmMhbmsdemsVfl8ZgmsNim8Jpk8F0m7R4m7F5nLB6jbh7jbiDirOEibOGnKaMhq+PnaCVg6qWg6qegKaff6WhnpKofKGtnomxeZy3noG6dZi+n3vCcpPDcpPGn3bLb4/Mb47UbIrVa4rYoGjdaIbeaIXhoWHmZYHobXvpcHjqdHXreHLroVrsfG/uhGnuh2bwj2Hxk17yl1vzmljzm1j0nlX1olL3AJXWAAAAbXRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hgYGBkcHBwcXl8gICAgoiIkJCQlJicnJ2goKCmqK+wsLC4usDAwMjP0NDQ1NbW3Nzg4ODi5+3v8PDw8/T09PX29vb39/f5+fr7+/z8/Pz9/v7+zczCxgAABC5JREFUeAHN1ul3k0UUBvCb1CTVpmpaitAGSLSpSuKCLWpbTKNJFGlcSMAFF63iUmRccNG6gLbuxkXU66JAUef/9LSpmXnyLr3T5AO/rzl5zj137p136BISy44fKJXuGN/d19PUfYeO67Znqtf2KH33Id1psXoFdW30sPZ1sMvs2D060AHqws4FHeJojLZqnw53cmfvg+XR8mC0OEjuxrXEkX5ydeVJLVIlV0e10PXk5k7dYeHu7Cj1j+49uKg7uLU61tGLw1lq27ugQYlclHC4bgv7VQ+TAyj5Zc/UjsPvs1sd5cWryWObtvWT2EPa4rtnWW3JkpjggEpbOsPr7F7EyNewtpBIslA7p43HCsnwooXTEc3UmPmCNn5lrqTJxy6nRmcavGZVt/3Da2pD5NHvsOHJCrdc1G2r3DITpU7yic7w/7Rxnjc0kt5GC4djiv2Sz3Fb2iEZg41/ddsFDoyuYrIkmFehz0HR2thPgQqMyQYb2OtB0WxsZ3BeG3+wpRb1vzl2UYBog8FfGhttFKjtAclnZYrRo9ryG9uG/FZQU4AEg8ZE9LjGMzTmqKXPLnlWVnIlQQTvxJf8ip7VgjZjyVPrjw1te5otM7RmP7xm+sK2Gv9I8Gi++BRbEkR9EBw8zRUcKxwp73xkaLiqQb+kGduJTNHG72zcW9LoJgqQxpP3/Tj//c3yB0tqzaml05/+orHLksVO+95kX7/7qgJvnjlrfr2Ggsyx0eoy9uPzN5SPd86aXggOsEKW2Prz7du3VID3/tzs/sSRs2w7ovVHKtjrX2pd7ZMlTxAYfBAL9jiDwfLkq55Tm7ifhMlTGPyCAs7RFRhn47JnlcB9RM5T97ASuZXIcVNuUDIndpDbdsfrqsOppeXl5Y+XVKdjFCTh+zGaVuj0d9zy05PPK3QzBamxdwtTCrzyg/2Rvf2EstUjordGwa/kx9mSJLr8mLLtCW8HHGJc2R5hS219IiF6PnTusOqcMl57gm0Z8kanKMAQg0qSyuZfn7zItsbGyO9QlnxY0eCuD1XL2ys/MsrQhltE7Ug0uFOzufJFE2PxBo/YAx8XPPdDwWN0MrDRYIZF0mSMKCNHgaIVFoBbNoLJ7tEQDKxGF0kcLQimojCZopv0OkNOyWCCg9XMVAi7ARJzQdM2QUh0gmBozjc3Skg6dSBRqDGYSUOu66Zg+I2fNZs/M3/f/Grl/XnyF1Gw3VKCez0PN5IUfFLqvgUN4C0qNqYs5YhPL+aVZYDE4IpUk57oSFnJm4FyCqqOE0jhY2SMyLFoo56zyo6becOS5UVDdj7Vih0zp+tcMhwRpBeLyqtIjlJKAIZSbI8SGSF3k0pA3mR5tHuwPFoa7N7reoq2bqCsAk1HqCu5uvI1n6JuRXI+S1Mco54YmYTwcn6Aeic+kssXi8XpXC4V3t7/ADuTNKaQJdScAAAAAElFTkSuQmCC)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=/__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "\n",
    "Cette page approfondit certains aspects présentés dans la\n",
    "[partie introductive](#nlp). Après avoir travaillé sur le\n",
    "*Comte de Monte Cristo*, on va continuer notre exploration de la littérature\n",
    "avec cette fois des auteurs anglophones:\n",
    "\n",
    "* Edgar Allan Poe, (EAP) ;\n",
    "* HP Lovecraft (HPL) ;\n",
    "* Mary Wollstonecraft Shelley (MWS).\n",
    "\n",
    "Les données sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent être requétées via l'url \n",
    "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
    "\n",
    "Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir de différents modèles de vectorisation, notamment les *word embeddings*.\n",
    "\n",
    "Ce notebook est librement inspiré de  : \n",
    "\n",
    "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras\n",
    "* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg\n",
    "* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243500f",
   "metadata": {},
   "source": [
    "Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments\n",
    "pour que nos librairies de NLP puissent fonctionner correctement.\n",
    "\n",
    "En premier lieu, il convient d'installer les librairies adéquates\n",
    "(`spacy`, `gensim` et `sentence_transformers`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy gensim sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641b6fa",
   "metadata": {},
   "source": [
    "Ensuite, comme nous allons utiliser également `spacy`, il convient de télécharger\n",
    "le corpus Anglais. Pour cela, on peut se référer à\n",
    "[la documentation de `spacy`](https://spacy.io/usage/models),\n",
    "extrèmement bien faite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f95dd8",
   "metadata": {},
   "source": [
    "- Idéalement, il faut installer le module via la ligne de commande. Dans\n",
    "une cellule de notebook `Jupyter`, faire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59af40",
   "metadata": {},
   "source": [
    "- Sans accès à la ligne de commande (depuis une instance `Docker` par exemple),\n",
    "faire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec707744",
   "metadata": {},
   "source": [
    "- Sinon, il est également possible d'installer le module en faisant pointer\n",
    "`pip install` depuis le fichier adéquat sur\n",
    "[`Github`](https://github.com/explosion/spacy-models). Pour cela, taper\n",
    "\n",
    "~~~python\n",
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\n",
    "~~~\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53f5df",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: Unable to convert function return value to a Python type! The signature was\n",
    "## \t() -> handle\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/spacy/__init__.py\", line 11, in <module>\n",
    "##     from thinc.api import prefer_gpu, require_gpu, require_cpu  # noqa: F401\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/thinc/api.py\", line 2, in <module>\n",
    "##     from .initializers import normal_init, uniform_init, glorot_uniform_init, zero_init\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/thinc/initializers.py\", line 4, in <module>\n",
    "##     from .backends import Ops\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/thinc/backends/__init__.py\", line 7, in <module>\n",
    "##     from .ops import Ops\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/thinc/backends/ops.py\", line 11, in <module>\n",
    "##     from ..util import get_array_module, is_xp_array, to_numpy\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/thinc/util.py\", line 38, in <module>\n",
    "##     import tensorflow.experimental.dlpack\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/__init__.py\", line 41, in <module>\n",
    "##     from tensorflow.python.tools import module_util as _module_util\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 46, in <module>\n",
    "##     from tensorflow.python import data\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\n",
    "##     from tensorflow.python.data import experimental\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py\", line 97, in <module>\n",
    "##     from tensorflow.python.data.experimental import service\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py\", line 353, in <module>\n",
    "##     from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 26, in <module>\n",
    "##     from tensorflow.python.data.experimental.ops import compression_ops\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 20, in <module>\n",
    "##     from tensorflow.python.data.util import structure\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py\", line 26, in <module>\n",
    "##     from tensorflow.python.data.util import nest\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py\", line 40, in <module>\n",
    "##     from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 28, in <module>\n",
    "##     from tensorflow.python.framework import constant_op\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 29, in <module>\n",
    "##     from tensorflow.python.eager import execute\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 27, in <module>\n",
    "##     from tensorflow.python.framework import dtypes\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py\", line 33, in <module>\n",
    "##     _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1dd1f0",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'gensim'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89384e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6500a",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'gensim'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a540e7",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'sentence_transformers'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "# Nettoyage des données\n",
    "\n",
    "Nous allons ainsi à nouveau utiliser le jeu de données `spooky`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f43544",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "spooky_df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559321bc",
   "metadata": {},
   "source": [
    "Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a617e2",
   "metadata": {},
   "source": [
    "```\n",
    "##         id                                               text author\n",
    "## 0  id26305  This process, however, afforded me no means of...    EAP\n",
    "## 1  id17569  It never once occurred to me that the fumbling...    HPL\n",
    "## 2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
    "## 3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
    "## 4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
    "```\n",
    "\n",
    "Pour rappel, nous avons les auteurs suivants dans notre corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e548e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "initials_to_author = {\n",
    "    'EAP': 'Edgar Allen Poe',\n",
    "    'HPL': 'H.P. Lovecraft',\n",
    "    'MWS': 'Mary Wollstonecraft Shelley'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d56f2",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "En NLP, la première étape est souvent celle du *preprocessing*, qui inclut notamment les étapes de tokenization et de nettoyage du texte. Comme celles-ci ont été vues en détail dans le précédent chapitre, on se contentera ici d'un *preprocessing* minimaliste : suppression de la ponctuation et suppression des *stop words* (pour la visualisation et les méthodes de vectorisation basées sur des comptages).\n",
    "\n",
    "Jusqu'à présent, nous avons utilisé principalement `nltk` pour le \n",
    "*preprocessing* de données textuelles. Cette fois, nous proposons\n",
    "d'utiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de\n",
    "*pipeline* de *preprocessing*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f348d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c940f6",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'spacy' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "On va utiliser un `pipe` `spacy` qui permet d'automatiser, et de paralléliser,\n",
    "un certain nombre d'opérations. Les *pipes* sont l'équivalent, en NLP, de\n",
    "nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s'agit donc d'un outil\n",
    "très approprié pour industrialiser un certain nombre d'opérations de\n",
    "*preprocessing*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(texts, remove_stopwords=False, n_process = 4):\n",
    "    \n",
    "    docs = nlp.pipe(texts, \n",
    "                    n_process=n_process,\n",
    "                    disable=['parser', 'ner',\n",
    "                             'lemmatizer', 'textcat'])\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "    docs_cleaned = []\n",
    "    for doc in docs:\n",
    "        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n",
    "        if remove_stopwords:\n",
    "            tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "        doc_clean = ' '.join(tokens)\n",
    "        docs_cleaned.append(doc_clean)\n",
    "        \n",
    "    return docs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7395f",
   "metadata": {},
   "source": [
    "On applique la fonction `clean_docs` à notre colonne `pandas`.\n",
    "Les `pandas.Series` étant itérables, elles se comportent comme des listes et\n",
    "fonctionnent ainsi très bien avec notre `pipe` `spacy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aebc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_df['text_clean'] = clean_docs(spooky_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb674b",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'nlp' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 3, in clean_docs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a80fe",
   "metadata": {},
   "source": [
    "```\n",
    "##         id                                               text author\n",
    "## 0  id26305  This process, however, afforded me no means of...    EAP\n",
    "## 1  id17569  It never once occurred to me that the fumbling...    HPL\n",
    "## 2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
    "## 3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
    "## 4  id12958  Finding nothing else, not even gold, the Super...    HPL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00dfd09",
   "metadata": {},
   "source": [
    "## Encodage de la variable à prédire\n",
    "\n",
    "On réalise un simple encodage de la variable à prédire :\n",
    "il y a trois catégories, représentées par des entiers de 0 à 2.\n",
    "\n",
    "Pour cela, on utilise le `LabelEncoder` de `scikit` déjà présenté \n",
    "dans la [partie modélisation](#preprocessing). On va utiliser la méthode\n",
    "`fit_transform` qui permet, en un tour de main, d'appliquer à la fois\n",
    "l'entraînement (`fit`), à savoir la création d'une correspondance entre valeurs\n",
    "numériques et _labels_, et l'appliquer (`transform`) à la même colonne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9c666",
   "metadata": {},
   "source": [
    "On peut vérifier les classes de notre `LabelEncoder`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a34ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8729b10",
   "metadata": {},
   "source": [
    "```\n",
    "## array(['EAP', 'HPL', 'MWS'], dtype=object)\n",
    "```\n",
    "\n",
    "## Construction des bases d'entraînement et de test\n",
    "\n",
    "On met de côté un échantillon de test (20 %) avant toute analyse (même descriptive).\n",
    "Cela permettra d'évaluer nos différents modèles toute à la fin de manière très rigoureuse,\n",
    "puisque ces données n'auront jamais utilisées pendant l'entraînement.\n",
    "\n",
    "Notre échantillon initial n'est pas balancé puisqu'on retrouve plus d'oeuvres de\n",
    "certains auteurs que d'autres. On va donc stratifier notre échantillon\n",
    "pour s'assurer qu'on a bien une répartition similaire d'auteurs dans nos\n",
    "ensembles d'entraînement et de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n",
    "                                                    spooky_df['author_encoded'].values, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=33,\n",
    "                                                    stratify = spooky_df['author_encoded'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9698a42",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 'text_clean'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/frame.py\", line 3458, in __getitem__\n",
    "##     indexer = self.columns.get_loc(key)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
    "##     raise KeyError(key) from err\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004728ca",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "On peut aussi vérifier qu'on est capable de retrouver\n",
    "la correspondance entre nos auteurs initiaux avec\n",
    "la méthode `inverse_transform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aabc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0], le.inverse_transform([y_train[0]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2c266",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'y_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "# Statistiques exploratoires\n",
    "\n",
    "## Répartition des labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9e902",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'y_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "On observe une petite asymétrie : les passages des livres d'Edgar Allen Poe sont plus nombreux dans notre corpus d'entraînement, ce qui peut être problématique dans le cadre d'une tâche de classification.\n",
    "L'écart n'est pas dramatique, mais on essaiera d'en tenir compte dans l'analyse en choisissant une métrique d'évaluation pertinente. \n",
    "\n",
    "## Mots les plus fréquemment utilisés par chaque auteur\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des stop words\n",
    "X_train_no_sw = clean_docs(X_train, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508243d",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d909c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_sw = np.array(X_train_no_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded9746",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train_no_sw' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "Pour visualiser rapidement nos corpus, on peut utiliser la technique des\n",
    "nuages de mots déjà vue à plusieurs reprises. \n",
    "\n",
    "Vous pouvez essayer de faire vous-même les nuages ci-dessous\n",
    "ou cliquer sur la ligne ci-dessous pour afficher le code ayant\n",
    "généré les figures:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508b30f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_top_words(initials, n_words=20):\n",
    "    # Calcul des mots les plus fréquemment utilisés par l'auteur\n",
    "    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n",
    "    all_tokens = ' '.join(texts).split()\n",
    "    counts = Counter(all_tokens)\n",
    "    top_words = [word[0] for word in counts.most_common(n_words)]\n",
    "    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n",
    "    \n",
    "    # Représentation sous forme de barplot\n",
    "    plt.style.use('dark_background')\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.barplot(x=top_words, y=top_words_counts)\n",
    "    plt.title(f'Most Common Words used by {initials_to_author[initials]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc28a2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words('EAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539c5fe",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train_no_sw' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 3, in plot_top_words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words('HPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3abf16",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train_no_sw' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 3, in plot_top_words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c005d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words('MWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d55d26",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train_no_sw' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 3, in plot_top_words\n",
    "```\n",
    "\n",
    "Beaucoup de mots se retrouvent très utilisés par les trois auteurs.\n",
    "Il y a cependant des différences notables : le mot _\"life\"_\n",
    "est le plus employé par MWS, alors qu'il n'apparaît pas dans les deux autres tops.\n",
    "De même, le mot _\"old\"_ est le plus utilisé par HPL\n",
    "là où les deux autres ne l'utilisent pas de manière surreprésentée.\n",
    "\n",
    "Il semble donc qu'il y ait des particularités propres à chacun des auteurs\n",
    "en termes de vocabulaire,\n",
    "ce qui laisse penser qu'il est envisageable de prédire les auteurs à partir\n",
    "de leurs textes dans une certaine mesure.\n",
    "\n",
    "## Prédiction sur le set d'entraînement\n",
    "\n",
    "Nous allons à présent vérifier cette conjecture en comparant\n",
    "plusieurs modèles de vectorisation,\n",
    "_i.e._ de transformation du texte en objets numériques\n",
    "pour que l'information contenue soit exploitable dans un modèle de classification.\n",
    "\n",
    "Comme nous nous intéressons plus à l'effet de la vectorisation qu'à la tâche de classification en elle-même,\n",
    "nous allons utiliser un algorithme de classification simple (un SVM linéaire), avec des paramètres non fine-tunés.\n",
    "\n",
    "Ce modèle est connu pour être très performant sur les tâches de classification de texte, et nous fournira donc une bonne *baseline*. Cela nous permettra également de comparer de manière objective l'impact des méthodes de vectorisation sur la performance finale.\n",
    "\n",
    "On va utiliser au maximum les objets de type pipeline de `sklearn`,\n",
    "qui permettent de réaliser des analyses reproductibles\n",
    "et de fine-tuner proprement les différents hyperparamètres.\n",
    "\n",
    "Pour les deux premières méthodes de vectorisation\n",
    "(basées sur des fréquences et fréquences relatives des mots),\n",
    "on va simplement normaliser les données d'entrée, ce qui va permettre au SVM de converger plus rapidement, ces modèles étant sensibles aux différences d'échelle dans les données.\n",
    "\n",
    "On va également fine-tuner via grid-search certains hyperparamètres liés à ces méthodes de vectorisation : \n",
    "- on teste différents _ranges_ de `n-grams` (unigrammes et unigrammes + bigrammes)\n",
    "- on teste avec et sans _stop-words_\n",
    "\n",
    "Afin d'éviter le surapprentissage,\n",
    "on va évaluer les différents modèles via validation croisée, calculée sur 4 blocs.\n",
    "\n",
    "On récupère à la fin le meilleur modèle selon une métrique spécifiée.\n",
    "On choisit le `score F1`,\n",
    "moyenne harmonique de la précision et du rappel,\n",
    "qui donne un poids équilibré aux deux métriques, tout en pénalisant fortement le cas où l'une des deux est faible.\n",
    "Précisément, on retient le score F1 *micro-averaged* : les contributions des différentes classes à prédire sont aggrégées, puis l'on calcule le score F1 sur ces données aggrégées. L'avantage de ce choix est qu'il permet de tenir compte des différences de fréquences des différentes classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(max_iter=10000, C=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c96b9",
   "metadata": {},
   "source": [
    "On va utiliser un *pipeline* `scikit` ce qui va nous permettre d'avoir\n",
    "un code très concis pour effectuer cet ensemble de tâches cohérentes. \n",
    "De plus, cela va nous assurer de gérer de manière cohérentes nos différentes\n",
    "transformations (cf. [partie sur les pipelines](#pipelines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5134c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizers(vectorizer):\n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", vectorizer()),\n",
    "        (\"scaling\", StandardScaler(with_mean=False)),\n",
    "        (\"clf\", clf),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    parameters = {\n",
    "        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "        \"vect__stop_words\": (\"english\", None)\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n",
    "                               cv=4, n_jobs=4, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n",
    "    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5c576",
   "metadata": {},
   "source": [
    "# Bag-of-words\n",
    "\n",
    "On commence par une approche _\"bag-of-words\"_, \n",
    "i.e. qui revient simplement à représenter chaque document par un vecteur\n",
    "qui compte le nombre d'apparitions de chaque mot du vocabulaire dans le document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vectorizers(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac6b95",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 17, in fit_vectorizers\n",
    "```\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "On s'intéresse ensuite à l'approche TF-IDF,\n",
    "qui permet de tenir compte des fréquences *relatives* des mots.\n",
    "\n",
    "Ainsi, pour un mot donné, on va multiplier la fréquence d'apparition du mot dans le document (calculé comme dans la méthode précédente) par un terme qui pénalise une fréquence élevée du mot dans le corpus.\n",
    "\n",
    "La vectorisation TF-IDF permet donc de limiter l'influence des *stop-words* et donc de donner plus de poids aux mots les plus salients d'un document. On observe clairement que la performance de classification est bien supérieure, ce qui montre la pertinence de cette technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73110182",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_vectorizers(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f73940",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"<string>\", line 17, in fit_vectorizers\n",
    "```\n",
    "\n",
    "# Word2vec + averaging\n",
    "\n",
    "On va maintenant explorer les techniques de vectorisation basées sur les *embeddings* de mots, et notamment la plus populaire : `word2vec`.\n",
    "\n",
    "L'idée derrière est simple, mais a révolutionné le NLP :\n",
    "au lieu de représenter les documents par des\n",
    "vecteurs *sparse* de très grande dimension (la taille du vocabulaire)\n",
    "comme on l'a fait jusqu'à présent,\n",
    "on va les représenter par des vecteurs *dense* (continus)\n",
    "de dimension réduite (en général, autour de 100-300).\n",
    "\n",
    "Chacune de ces dimensions va représenter un facteur latent,\n",
    "c'est à dire une variable inobservée,\n",
    "de la même manière que les composantes principales produites par une ACP.\n",
    "\n",
    "![title](w2v_vecto.png)\n",
    "\n",
    "_Pourquoi est-ce intéressant ?_\n",
    "Pour de nombreuses raisons, mais pour résumer :\n",
    "cela permet de beaucoup mieux capturer la similarité sémantique entre les documents.\n",
    "\n",
    "Par exemple, un humain sait qu'un document contenant le mot _\"Roi\"_\n",
    "et un autre document contenant le mot _\"Reine\"_ ont beaucoup de chance\n",
    "d'aborder des sujets semblables.\n",
    "\n",
    "Pourtant, une vectorisation de type comptage ou TF-IDF\n",
    "ne permet pas de saisir cette similarité :\n",
    "le calcul d'une mesure de similarité (norme euclidienne ou similarité cosine)\n",
    "entre les deux vecteurs ne prendra en compte la similarité des deux concepts, puisque les mots utilisés sont différents.\n",
    "\n",
    "A l'inverse, un modèle `word2vec` bien entraîné va capter\n",
    "qu'il existe un facteur latent de type _\"royauté\"_,\n",
    "et la similarité entre les vecteurs associés aux deux mots sera forte.\n",
    "\n",
    "La magie va même plus loin : le modèle captera aussi qu'il existe un\n",
    "facteur latent de type _\"genre\"_,\n",
    "et va permettre de construire un espace sémantique dans lequel les\n",
    "relations arithmétiques entre vecteurs ont du sens ;\n",
    "par exemple :\n",
    "$$\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}$$\n",
    "\n",
    "_Comment ces modèles sont-ils entraînés ?_\n",
    "Via une tâche de prédiction résolue par un réseau de neurones simple.\n",
    "\n",
    "L'idée fondamentale est que la signification d'un mot se comprend\n",
    "en regardant les mots qui apparaissent fréquemment dans son voisinage.\n",
    "\n",
    "Pour un mot donné, on va donc essayer de prédire les mots\n",
    "qui apparaissent dans une fenêtre autour du mot cible.\n",
    "\n",
    "En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,\n",
    "on obtient finalement des *embeddings* pour chaque mot du vocabulaire,\n",
    "qui présentent les propriétés discutées précédemment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a001c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [text.split() for text in X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878590ca",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8380dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n",
    "                     min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a329e4",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'Word2Vec' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63cfce",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'w2v_model' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "On voit que les mots les plus similaires à _\"mother\"_\n",
    "sont souvent des mots liés à la famille, mais pas toujours.\n",
    "\n",
    "C'est lié à la taille très restreinte du corpus sur lequel on entraîne le modèle,\n",
    "qui ne permet pas de réaliser des associations toujours pertinentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b855c3c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def get_mean_vector(w2v_vectors, words):\n",
    "    words = [word for word in words if word in w2v_vectors]\n",
    "    if words:\n",
    "        avg_vector = np.mean(w2v_vectors[words], axis=0)\n",
    "    else:\n",
    "        avg_vector = np.zeros_like(w2v_vectors['hi'])\n",
    "    return avg_vector\n",
    "\n",
    "def fit_w2v_avg(w2v_vectors):\n",
    "    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n",
    "                                for words in X_train_tokens])\n",
    "    \n",
    "    scores = cross_val_score(clf, X_train_vectors, y_train, \n",
    "                         cv=4, scoring='f1_micro', n_jobs=4)\n",
    "\n",
    "    print(f\"CV scores {scores}\")\n",
    "    print(f\"Mean F1 {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4897c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w2v_avg(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f5975",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'w2v_model' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "La performance chute fortement ;\n",
    "la faute à la taille très restreinte du corpus, comme annoncé précédemment.\n",
    "\n",
    "# Word2vec pré-entraîné + averaging\n",
    "\n",
    "Quand on travaille avec des corpus de taille restreinte,\n",
    "c'est généralement une mauvaise idée d'entraîner son propre modèle `word2vec`.\n",
    "\n",
    "Heureusement, des modèles pré-entraînés sur de très gros corpus sont disponibles.\n",
    "Ils permettent de réaliser du *transfer learning*,\n",
    "c'est à dire de bénéficier de la performance d'un modèle qui a été entraîné sur une autre tâche ou bien un autre corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df552e5a",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gensim' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar('mother')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b310dee",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'glove_model' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w2v_avg(glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa09ff9",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'glove_model' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "La performance remonte substantiellement.\n",
    "Cela étant, on ne parvient pas à faire mieux que les approches basiques,\n",
    "on arrive à peine aux performances de la vectorisation par comptage.\n",
    "\n",
    "La cause est cette fois certainement dans la manière dont on exploite\n",
    "les *embeddings* :\n",
    "afin d'obtenir une représentation vectorielle pour chaque document,\n",
    "on moyennise les vecteurs des différents mots compris dans un document.\n",
    "\n",
    "Cela a plusieurs limites : \n",
    "\n",
    "- on ne tient pas compte de l'ordre et donc du contexte des mots\n",
    "- lorsque les documents sont longs, la moyennisation peut créer\n",
    "des représentation bruitées.\n",
    "\n",
    "# Contextual embeddings\n",
    "\n",
    "Les *embeddings* contextuels visent à pallier les limites des *embeddings*\n",
    "traditionnels évoquées précédemment.\n",
    "\n",
    "Cette fois, les mots n'ont plus de représentation vectorielle fixe,\n",
    "celle-ci est calculée dynamiquement en fonction des mots du voisinage, et ainsi de suite.\n",
    "Cela permet de tenir compte de la structure spatiale des phrases\n",
    "et de tenir compte du fait que le sens d'un mot est fortement dépendant des mots\n",
    "qui l'entourent (ex : dans les phrases \"le président Macron\" et \"le camembert Président\" le mot président n'a pas du tout le même rôle).\n",
    "\n",
    "Ces *embeddings* sont produits par des architectures très complexes,\n",
    "de type Transformer (`BERT`, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb829b5",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'SentenceTransformer' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors = model.encode(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55a25f",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'model' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e54173",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_train_vectors, y_train, \n",
    "                         cv=4, scoring='f1_micro', n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0f20f",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'X_train_vectors' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CV scores {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee9fb2",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scores' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean F1 {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e8a20",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'scores' is not defined\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```\n",
    "\n",
    "Verdict : on fait très légèrement mieux que la vectorisation TF-IDF.\n",
    "On voit donc l'importance de tenir compte du contexte.\n",
    "\n",
    "_Mais pourquoi, avec une méthode très compliquée, ne parvenons-nous pas à battre une méthode toute simple ?_\n",
    "\n",
    "On peut avancer plusieurs raisons : \n",
    "- le TF-IDF est un modèle simple, mais toujours très performant\n",
    "(on parle de _\"tough-to-beat baseline\"_).\n",
    "- la classification d'auteurs est une tâche très particulière et très ardue,\n",
    "qui ne fait pas justice aux *embeddings*. Comme on l'a dit précédemment, ces derniers se révèlent particulièrement pertinent lorsqu'il est question de similarité sémantique entre des textes (clustering, etc.).\n",
    "\n",
    "Dans le cas de notre tâche de classification, il est probable que\n",
    "certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de manière pertinente,\n",
    "ce que ne permettent pas de capter les *embeddings*.\n",
    "\n",
    "## Exercices\n",
    "\n",
    "- Prédictions sur le test set en utilisant les modèles entraînés\n",
    "- Faire un *vrai* split train/test : faire l'entraînement avec des textes de certains auteurs, et faire la prédiction avec des textes d'auteurs différents. Cela permettrait de neutraliser la présence de noms de lieux, de personnages, etc.\n",
    "- Comparer avec d'autres algorithmes de classification qu'un SVM\n",
    "- (Avancé) : fine-tuner le modèle d'embeddings contextuels sur la tâche de classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
