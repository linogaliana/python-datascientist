{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95b786e",
   "metadata": {},
   "source": [
    "#  Nettoyer un texte: approche bag-of-words (exercices)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de490f",
   "metadata": {},
   "source": [
    "Cette page approfondit certains aspects présentés dans la\n",
    "[partie introductive](#nlp). Après avoir travaillé sur le\n",
    "*Comte de Monte Cristo*, on va continuer notre exploration de la littérature\n",
    "avec cette fois des auteurs anglophones:\n",
    "\n",
    "* Edgar Allan Poe, (EAP) ;\n",
    "* HP Lovecraft (HPL) ;\n",
    "* Mary Wollstonecraft Shelley (MWS).\n",
    "\n",
    "Les données sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent être requétées via l'url \n",
    "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
    "\n",
    "Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13c378",
   "metadata": {},
   "source": [
    "Ce notebook librement inspiré de  : \n",
    "\n",
    "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras\n",
    "* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg\n",
    "* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook\n",
    "\n",
    "## Librairies nécessaires\n",
    "\n",
    "Cette page évoquera, les principales librairies pour faire du NLP, notamment: \n",
    "\n",
    "* [WordCloud](https://github.com/amueller/word_cloud)\n",
    "* [nltk](https://www.nltk.org/)\n",
    "* [spacy](https://spacy.io/)\n",
    "* [Keras](https://keras.io/)\n",
    "* [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "Il faudra également installer les librairies `gensim` et `pywaffle`\n",
    "\n",
    "Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments pour que `NTLK` puisse fonctionner correctement. Pour cela, faire:\n",
    "\n",
    "~~~python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05601b",
   "metadata": {},
   "source": [
    "La liste des modules à importer est assez longue, la voici:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fa61d",
   "metadata": {},
   "source": [
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package stopwords to /github/home/nltk_data...\n",
    "## [nltk_data]   Package stopwords is already up-to-date!\n",
    "```\n",
    "\n",
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package punkt to /github/home/nltk_data...\n",
    "## [nltk_data]   Package punkt is already up-to-date!\n",
    "```\n",
    "\n",
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package genesis to /github/home/nltk_data...\n",
    "## [nltk_data]   Unzipping corpora/genesis.zip.\n",
    "```\n",
    "\n",
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package wordnet to /github/home/nltk_data...\n",
    "## [nltk_data]   Unzipping corpora/wordnet.zip.\n",
    "```\n",
    "\n",
    "## Données utilisées\n",
    "\n",
    "1. Importer le jeu de données `spooky` à partir de l'URL <https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv> sous le nom `train`. L'encoding est `latin-1`\n",
    "2. Mettre des majuscules au nom des colonnes\n",
    "3. Retirer le prefix `id` de la colonne `Id`\n",
    "4. Mettre la colonne `Id` en index\n",
    "\n",
    "Une fois n'est pas coutume, la correction de cet exercice ci-dessous:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee512299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "import pandas as pd\n",
    "train = pd.read_csv(url,\n",
    "                    encoding='latin-1')\n",
    "train.columns = train.columns.str.capitalize()\n",
    "                    \n",
    "train['ID'] = train['Id'].str.replace(\"id\",\"\")\n",
    "train = train.set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c4d3f",
   "metadata": {},
   "source": [
    "Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754ee8d",
   "metadata": {},
   "source": [
    "```\n",
    "##                                                       Text Author     ID\n",
    "## Id                                                                      \n",
    "## id26305  This process, however, afforded me no means of...    EAP  26305\n",
    "## id17569  It never once occurred to me that the fumbling...    HPL  17569\n",
    "## id11008  In his left hand was a gold snuff box, from wh...    EAP  11008\n",
    "## id27763  How lovely is spring As we looked from Windsor...    MWS  27763\n",
    "## id12958  Finding nothing else, not even gold, the Super...    HPL  12958\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5b492",
   "metadata": {},
   "source": [
    "On peut se rendre compte que les extraits des 3 auteurs ne sont pas forcément équilibrés dans le jeu de données. Il faudra en tenir compte dans la prédiction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0705cea",
   "metadata": {},
   "source": [
    "```\n",
    "## <AxesSubplot:ylabel='Author'>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b42567",
   "metadata": {},
   "source": [
    "L'approche *bag of words* est présentée de\n",
    "manière plus extensive dans le [chapitre précédent](#nlp).\n",
    "\n",
    "L'idée est d'étudier la fréquence des mots d'un document et la\n",
    "surreprésentation des mots par rapport à un document de\n",
    "référence (appelé *corpus*). Cette approche un peu simpliste mais très\n",
    "efficace : on peut calculer des scores permettant par exemple de faire\n",
    "de classification automatique de document par thème, de comparer la\n",
    "similarité de deux documents. Elle est souvent utilisée en première analyse,\n",
    "et elle reste la référence pour l'analyse de textes mal\n",
    "structurés (tweets, dialogue tchat, etc.). \n",
    "\n",
    "Les analyses tf-idf (*term frequency-inverse document frequency*) ou les\n",
    "constructions d'indices de similarité cosine reposent sur ce type d'approche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81c64a",
   "metadata": {},
   "source": [
    "## Fréquence d'un mot\n",
    "\n",
    "Avant de s'adonner à une analyse systématique du champ lexical de chaque\n",
    "auteur, on va rechercher un unique mot, le mot *fear*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ded189",
   "metadata": {},
   "source": [
    "1. Compter le nombre de phrases, pour chaque auteur, où apparaît le mot `fear`\n",
    "2. Utiliser `pywaffle` pour obtenir les graphiques ci-dessous qui résument\n",
    "de manière synthétique le nombre d'occurrences du mot *\"fear\"* par auteur\n",
    "3. Refaire l'analyse avec le mot *\"horror\"*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2b92",
   "metadata": {},
   "source": [
    "La peur est ainsi plus évoquée par Mary Shelley\n",
    "(sentiment assez naturel face à la créature du docteur Frankenstein) alors\n",
    "que Lovecraft n'a pas volé sa réputation d'écrivain de l'horreur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ec822",
   "metadata": {},
   "source": [
    "## Premier *wordcloud*\n",
    "\n",
    "Pour aller plus loin dans l'analyse du champ lexical de chaque auteur,\n",
    "on peut représenter un `wordcloud` qui permet d'afficher chaque mot avec une\n",
    "taille proportionnelle au nombre d'occurrence de celui-ci\n",
    "\n",
    "1. Faire un wordcloud pour représenter les mots les plus utilisés par chaque auteur\n",
    "2. Calculer les 25 mots plus communs pour chaque auteur et représenter l'histogramme du décompte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb582bd",
   "metadata": {},
   "source": [
    "```\n",
    "## <matplotlib.image.AxesImage object at 0x7f4300dcefa0>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "## <matplotlib.image.AxesImage object at 0x7f4300dd2070>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "## <matplotlib.image.AxesImage object at 0x7f430129dd00>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769405f",
   "metadata": {},
   "source": [
    "```\n",
    "## <seaborn.axisgrid.FacetGrid object at 0x7f4300c3e190>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c35952",
   "metadata": {},
   "source": [
    "Démonstration par l'exemple qu'il vaut mieux nettoyer le texte avant de \n",
    "l'analyser.\n",
    "On voit ici que ce sont des mots communs, comme *\"the\"*, *\"of\"*, etc. sont très\n",
    "présents. Mais ils sont peu porteurs d'information, on peut donc les éliminer\n",
    "avant de faire une analyse syntaxique poussée (sauf si on est intéressé\n",
    "par la loi de Zipf). \n",
    "\n",
    "## Aparté: la loi de Zipf\n",
    "\n",
    "Dans son sens strict, la loi de Zipf prévoit que\n",
    "dans un texte donné, la fréquence d'occurrence $f(n_i)$ d'un mot est\n",
    "liée à son rang $n_i$ dans l'ordre des fréquences par une loi de la forme\n",
    "$f(n_i) = c/n_i$ où $c$ est une constante. Zipf, dans les années 1930, se basait sur l'oeuvre \n",
    "de Joyce, *Ulysse* pour cette affirmation. \n",
    "\n",
    "Plus généralement, on peut dériver la loi de Zipf d'une distribution exponentielle des fréquences: $f(n_i) = cn_{i}^{-k}$. Cela permet d'utiliser la famille des modèles linéaire généralisés, notamment les régressions poissonniennes, pour mesurer les paramètres de la loi. Les modèles linéaire traditionnels en `log` souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d'un modèle gravitaire, où appliquer des OLS est une mauvaise idée, cf. [Galiana et al. (2020)](https://linogaliana.netlify.app/publication/2020-segregation/) pour les limites).\n",
    "\n",
    "On va estimer le modèle suivant par GLM via `statsmodels`:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n",
    "$$\n",
    "\n",
    "Prenons les résultats de l'exercice précédent et enrichissons les du rang et de la fréquence d'occurrence d'un mot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afc917",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "count_words = pd.DataFrame({'counter' : train\n",
    "    .groupby('Author')\n",
    "    .apply(lambda s: ' '.join(s['Text']).split())\n",
    "    .apply(lambda s: Counter(s))\n",
    "    .apply(lambda s: s.most_common())\n",
    "    .explode()}\n",
    ")\n",
    "count_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\n",
    "count_words = count_words.reset_index()\n",
    "\n",
    "count_words = count_words.assign(\n",
    "    freq = lambda x: x['count'] / (x.groupby(\"Author\").transform('sum')['count']),\n",
    "    rank = lambda x: x.groupby(\"Author\").transform('rank', ascending = False)['count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed9cf0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot(y = \"freq\", x = \"rank\", hue = 'Author', data = count_words, fit_reg = False)\n",
    "g.set(xscale=\"log\", yscale=\"log\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec04e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "exog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n",
    "\n",
    "model = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n",
    "\n",
    "# Display model results\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246729f",
   "metadata": {},
   "source": [
    "TO BE COMPLETED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11309906",
   "metadata": {},
   "source": [
    "## Nettoyage d'un texte\n",
    "\n",
    "Les premières étapes dans le nettoyage d'un texte, qu'on a\n",
    "dévelopé au cours du [chapitre précédent](#nlp), sont:\n",
    "\n",
    "* suppression de la ponctuation\n",
    "* suppression des *stopwords*\n",
    "\n",
    "Cela passe par la tokenisation d'un texte, c'est-à-dire la décomposition\n",
    "de celui-ci en unités lexicales (les *tokens*). Ces unités lexicales peuvent être de différentes natures, selon l'analyse que l'on désire procéder. Ici, on va définir les tokens comme des mots.\n",
    "\n",
    "Plutôt que de faire soi-même ce travail de nettoyage, avec des fonctions mal optimisées, on peut utiliser la librairie `nltk` comme détaillé [précédemment](#nlp). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba21e1",
   "metadata": {},
   "source": [
    "Repartir de `train`, notre jeu de données d'entraînement. Pour rappel, `train` a la structure suivante:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6b995",
   "metadata": {},
   "source": [
    "```\n",
    "##                                                       Text  ... wordtoplot\n",
    "## Id                                                          ...           \n",
    "## id26305  This process, however, afforded me no means of...  ...          0\n",
    "## id17569  It never once occurred to me that the fumbling...  ...          0\n",
    "## \n",
    "## [2 rows x 4 columns]\n",
    "```\n",
    "\n",
    "1. Tokeniser chaque phrase avec `nltk`. Le `DataFrame` devrait maintenant avoir cet aspect:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4bab2",
   "metadata": {},
   "source": [
    "```\n",
    "## ID     Author\n",
    "## 00001  MWS       [Idris, was, well, content, with, this, resolv...\n",
    "## 00002  HPL       [I, was, faint, even, fainter, than, the, hate...\n",
    "## dtype: object\n",
    "```\n",
    "\n",
    "2. Retirer les stopwords avec `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6222b",
   "metadata": {},
   "source": [
    "```\n",
    "##       ID Author                                          tokenized\n",
    "## 0  00001    MWS              [Idris, well, content, resolve, mine]\n",
    "## 1  00002    HPL  [I, faint, even, fainter, hateful, modernity, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d87ed",
   "metadata": {},
   "source": [
    "La méthode `apply` est très pratique ici car nous avons une phrase par ligne. Plutôt que de faire un `DataFrame` par auteur, ce qui n'est pas très flexible comme approche, on peut directement appliquer la tokenisation\n",
    "sur notre `DataFrame` grâce à `apply`\n",
    "\n",
    "Ce petit nettoyage permet d'arriver à un texte plus intéressant en termes d'analyse lexicale. Par exemple, si on reproduit l'analyse précédente,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e67118",
   "metadata": {},
   "source": [
    "```\n",
    "## <matplotlib.image.AxesImage object at 0x7f430139e820>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "## <matplotlib.image.AxesImage object at 0x7f42ffb94d00>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "## <matplotlib.image.AxesImage object at 0x7f42ff549520>\n",
    "## (-0.5, 799.5, 499.5, -0.5)\n",
    "```\n",
    "\n",
    "```\n",
    "## <Figure size 1500x1200 with 3 Axes>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2ab2f",
   "metadata": {},
   "source": [
    "Pour aller plus loin dans l'harmonisation d'un texte, il est possible de\n",
    "mettre en place les classes d'équivalence développées dans la \n",
    "[partie précédente](#nlp) afin de remplacer différentes variations d'un même\n",
    "mot par une forme canonique :\n",
    "\n",
    "* la **lemmatisation** qui requiert la connaissance des statuts\n",
    "grammaticaux (exemple : chevaux devient cheval)\n",
    "* la **racinisation** (*stemming*) plus fruste mais plus rapide, notamment\n",
    "en présence de fautes d’orthographes. Dans ce cas, chevaux peut devenir chev\n",
    "mais être ainsi confondu avec chevet ou cheveux \n",
    "\n",
    "La racinisation est généralement plus simple à mettre en oeuvre, quoique\n",
    "plus fruste. Elle est développée dans la [partie précédente](#nlp). \n",
    "\n",
    "La lemmatisation est mise en oeuvre, comme toujours avec NLTK, à travers un\n",
    "modèle. En l'occurrence, un `WordNetLemmatizer`  (WordNet est une base\n",
    "lexicographique ouverte). Par exemple, les mots *\"women\"*, *\"daughters\"*\n",
    "et *\"leaves\"* seront ainsi lemmatisés de la manière suivante:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b9eeb",
   "metadata": {},
   "source": [
    "```\n",
    "## The lemmatized form of women is: woman\n",
    "## The lemmatized form of daughters is: daughter\n",
    "## The lemmatized form of leaves is: leaf\n",
    "```\n",
    "\n",
    "Pour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,\n",
    "télécharger celui-ci grâce aux commandes suivantes:\n",
    "~~~python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961cd81",
   "metadata": {},
   "source": [
    "On va se restreindre au corpus d'Edgar Allan Poe et repartir de la base de données\n",
    "brute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_clean = train[train[\"Author\"] == \"EAP\"]\n",
    "eap_clean = ' '.join(eap_clean['Text'])\n",
    "#Tokenisation naïve sur les espaces entre les mots => on obtient une liste de mots\n",
    "#tokens = eap_clean.split()\n",
    "word_list = nltk.word_tokenize(eap_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541d8e6",
   "metadata": {},
   "source": [
    "1. Utiliser un `WordNetLemmatizer` et observer le résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e43c20",
   "metadata": {},
   "source": [
    "```\n",
    "## This process , however , afforded me no means of ascertaining the dimensions of my dungeon ; as I might make its circuit , and return to the point whence I set out , without being aware of the fact ; so perfectly\n",
    "```\n",
    "\n",
    "```\n",
    "## ---------------------------\n",
    "```\n",
    "\n",
    "```\n",
    "## This process , however , afforded me no mean of ascertaining the dimension of my dungeon ; a I might make it circuit , and return to the point whence I set out , without being aware of the fact ; so perfectly\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ea98c",
   "metadata": {},
   "source": [
    "## TF-IDF: calcul de fréquence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505cc70",
   "metadata": {},
   "source": [
    "Le calcul [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) (term frequency–inverse document frequency) permet de calculer un score de proximité entre un terme de recherche et un document (c'est ce que font les moteurs de recherche):\n",
    "\n",
    "* La partie `tf` calcule une fonction croissante de la fréquence du terme de recherche dans le document à l'étude;\n",
    "* La partie `idf` calcule une fonction inversement proportionnelle à la fréquence du terme dans l'ensemble des documents (ou corpus).\n",
    "\n",
    "Le score total, obtenu en multipliant les deux composantes, permet ainsi de donner un score d'autant plus élevé que le terme est surréprésenté dans un document (par rapport à l'ensemble des documents). Il existe plusieurs fonctions, qui pénalisent plus ou moins les documents longs, ou qui sont plus ou moins *smooth*.\n",
    "\n",
    "Repartir de `train`. \n",
    "\n",
    "1. Utiliser le vectoriseur TfIdF de `scikit-learn` pour transformer notre corpus en une matrice `document x terms`. Au passage, utiliser l'option `stop_words` pour ne pas provoquer une inflation de la taille de la matrice. Nommer le modèle `tfidf` et le jeu entraîné `tfs`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74687f59",
   "metadata": {},
   "source": [
    "2. Après avoir construit la matrice de documents x terms avec le code suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd18b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "corpus_index = [n for n in list(tfidf.vocabulary_.keys())]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfs.todense(), columns=feature_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95047fd7",
   "metadata": {},
   "source": [
    "```\n",
    "##    aaem   ab  aback  abaft  abandon  ...  zopyrus  zorry  zubmizzion  zuro   á¼\n",
    "## 0   0.0  0.0    0.0    0.0      0.0  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 1   0.0  0.0    0.0    0.0      0.0  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 2   0.0  0.0    0.0    0.0      0.0  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 3   0.0  0.0    0.0    0.0      0.0  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 4   0.0  0.0    0.0    0.0      0.0  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## \n",
    "## [5 rows x 24937 columns]\n",
    "```\n",
    "\n",
    "rechercher les lignes où les termes ayant la structure `abandon` sont non-nuls. Les lignes sont les suivantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945ccda",
   "metadata": {},
   "source": [
    "```\n",
    "## Int64Index([    4,   116,   215,   571,   839,  1042,  1052,  1069,  2247,\n",
    "##              2317,  2505,  3023,  3058,  3245,  3380,  3764,  3886,  4425,\n",
    "##              5289,  5576,  5694,  6812,  7500,  9013,  9021,  9077,  9560,\n",
    "##             11229, 11395, 11451, 11588, 11827, 11989, 11998, 12122, 12158,\n",
    "##             12189, 13666, 15259, 16516, 16524, 16759, 17547, 18019, 18072,\n",
    "##             18126, 18204, 18251],\n",
    "##            dtype='int64')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51542085",
   "metadata": {},
   "source": [
    "```\n",
    "##      aaem   ab  aback  abaft   abandon  ...  zopyrus  zorry  zubmizzion  zuro   á¼\n",
    "## 4     0.0  0.0    0.0    0.0  0.000000  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 116   0.0  0.0    0.0    0.0  0.000000  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 215   0.0  0.0    0.0    0.0  0.235817  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 571   0.0  0.0    0.0    0.0  0.000000  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## 839   0.0  0.0    0.0    0.0  0.285886  ...      0.0    0.0         0.0   0.0  0.0\n",
    "## \n",
    "## [5 rows x 24937 columns]\n",
    "```\n",
    "\n",
    "3. Trouver les 50 extraits où le score TF-IDF est le plus élevé et l'auteur associé. Vous devriez obtenir le classement suivant:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa71b35",
   "metadata": {},
   "source": [
    "```\n",
    "## Author\n",
    "## MWS    22\n",
    "## HPL    15\n",
    "## EAP    13\n",
    "## Name: Text, dtype: int64\n",
    "```\n",
    "\n",
    "et les 10 scores les plus élevés sont les suivants:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf49b09",
   "metadata": {},
   "source": [
    "```\n",
    "## ['We could not fear we did not.' '\"And now I do not fear death.'\n",
    "##  'Be of heart and fear nothing.' 'I smiled, for what had I to fear?'\n",
    "##  'Indeed I had no fear on her account.'\n",
    "##  'I have not the slightest fear for the result.'\n",
    "##  'At length, in an abrupt manner she asked, \"Where is he?\" \"O, fear not,\" she continued, \"fear not that I should entertain hope Yet tell me, have you found him?'\n",
    "##  '\"I fear you are right there,\" said the Prefect.'\n",
    "##  'I went down to open it with a light heart, for what had I now to fear?']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9a6a6",
   "metadata": {},
   "source": [
    "On remarque que les scores les plus élévés sont soient des extraits courts où le mot apparait une seule fois, et des extraits plus longs où le mot fear apprait plusieurs fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd45981",
   "metadata": {},
   "source": [
    "La matrice `document x terms` est un exemple typique de matrice sparse puisque, dans des corpus volumineux, une grande diversité de vocabulaire peut être trouvée.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9b5ed",
   "metadata": {},
   "source": [
    "## Approche contextuelle: les *n-gramms*\n",
    "\n",
    "Pour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire:\n",
    "~~~python\n",
    "import nltk\n",
    "nltk.download('genesis')\n",
    "nltk.corpus.genesis.words('english-web.txt')\n",
    "~~~\n",
    "\n",
    "Il s'agit maintenant de raffiner l'analyse. \n",
    "\n",
    "On s'intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Elle permet aussi d'affiner les modèles \"bag-of-words\". Le calcul de n-grams (bigrams pour les co-occurences de mots deux-à-deux, tri-grams pour les co-occurences trois-à-trois, etc.) constitue la méthode la plus simple pour tenir compte du contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461ee20",
   "metadata": {},
   "source": [
    "nltk offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c'est-à-dire l'ensemble des co-occurrences successives de mots deux-à-deux (bigrams), trois-à-trois (tri-grams), etc.\n",
    "\n",
    "En général, on se contente de bi-grams, au mieux de tri-grams :\n",
    "\n",
    "* les modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;\n",
    "* les performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de donnée initiale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaeaa46",
   "metadata": {},
   "source": [
    "On va, rapidement, regarder dans quel contexte apparaît le mot `fear` dans\n",
    "l'oeuvre d'Edgar Allan Poe (EAP). Pour cela, on transforme d'abord\n",
    "le corpus EAP en tokens `NLTK`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cfb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_clean = train_clean[train_clean[\"Author\"] == \"EAP\"]\n",
    "eap_clean = ' '.join(eap_clean['Text'])\n",
    "#Tokenisation naïve sur les espaces entre les mots => on obtient une liste de mots\n",
    "tokens = eap_clean.split()\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1d57b",
   "metadata": {},
   "source": [
    "1. Utiliser la méthode `concordance` pour afficher le contexte dans lequel apparaît le terme `fear`. La liste devrait ressembler à celle-ci:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600a735",
   "metadata": {},
   "source": [
    "```\n",
    "## Exemples d'occurences du terme 'fear' :\n",
    "```\n",
    "\n",
    "```\n",
    "## Displaying 24 of 24 matches:\n",
    "## lady seventy years age heard express fear never see Marie observation attracte\n",
    "## ingly well I went open light heart I fear The fact business simple indeed I ma\n",
    "##  Geneva seemed resolved give scruple fear wind No one spoken frequenting house\n",
    "## d propeller must entirely remodelled fear serious accident I mean steel rod va\n",
    "## ud rose amazing velocity I slightest fear result He proceeded observing analyz\n",
    "## His third contempt ambition Indeed I fear account The ceiling gloomy looking o\n",
    "## dverted blush extreme recency date I fear right said Prefect This could refast\n",
    "## loud quick unequal spoken apparently fear well anger three four quite right Sa\n",
    "## oughts Question Oinos freely without fear No path trodden vicinity reach happy\n",
    "## ick darkness shutters close fastened fear robbers I knew could see opening doo\n",
    "## ible game antagonist I even went far fear I occasioned much trouble might glad\n",
    "## dame could easily enter unobserved I fear mesmerized adding immediately afterw\n",
    "## here poodle Perhaps said I Legrand I fear artist In left hand little heavy Dut\n",
    "##  strong relish physical philosophy I fear tinctured mind common error age I me\n",
    "## ripods expired The replied entered I fear unusual horror thing The rudder ligh\n",
    "## rdiality In second place impressed I fear indeed impossible make comprehended \n",
    "##  spades whole insisted upon carrying fear seemed trusting either implements wi\n",
    "## ind dreaded whip instantly converted fear This prison like rampart formed limi\n",
    "##  I started hourly dreams unutterable fear find hot breath thing upon face vast\n",
    "## ers deputed search premises Be heart fear nothing I removed bed examined corps\n",
    "##  looked stiff rolled eyes I smiled I fear My first idea mere surprise really r\n",
    "## g memory long time awaking slumber I fear I shall never see Marie But imagine \n",
    "## et lonely I watched minutes somewhat fear wonder The one wrote Jeremiad usury \n",
    "## d garments muddy clotted gore I much fear replied Monsieur Maillard becoming e\n",
    "```\n",
    "\n",
    "Même si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d'information. \n",
    "\n",
    "La `collocation` consiste à trouver les bi-grammes qui\n",
    "apparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées, il s'agit de sélectionner, à partir d'un modèle statistique, les \"meilleures\". \n",
    "\n",
    "2. Sélectionner et afficher les meilleures collocation, par exemple selon le critère du ratio de vraisemblance. \n",
    "\n",
    "Une approche ingénue de la `collocation` amène ainsi à considérer les mots suivants: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bd692",
   "metadata": {},
   "source": [
    "```\n",
    "## [('I', 'could'), ('I', 'felt'), ('main', 'compartment'), ('Chess', 'Player'), ('Let', 'us'), ('I', 'saw'), ('Madame', 'Lalande'), ('At', 'length'), ('New', 'York'), ('Ourang', 'Outang'), ('ha', 'ha'), ('three', 'four'), ('I', 'knew'), ('I', 'say'), ('du', 'Roule'), ('I', 'I'), ('General', 'John'), ('could', 'help'), ('In', 'meantime'), ('let', 'us')]\n",
    "```\n",
    "\n",
    "Si ces mots sont très fortement associés, les expressions sont également peu fréquentes. Il est donc parfois nécessaire d'appliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n",
    "\n",
    "3. Refaire la question précédente mais, avant cela, utiliser un modèle `BigramCollocationFinder` et la méthode `apply_freq_filter` pour ne conserver que les bigrammes présents au moins 5 fois. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc23381",
   "metadata": {},
   "source": [
    "```\n",
    "## Chess Player\n",
    "## Ourang Outang\n",
    "## Brevet Brigadier\n",
    "## Hans Pfaall\n",
    "## Bas Bleu\n",
    "## du Roule\n",
    "## New York\n",
    "## ugh ugh\n",
    "## Tea Pot\n",
    "## gum elastic\n",
    "## hu hu\n",
    "## prodigies valor\n",
    "## Gad Fly\n",
    "## Massa Will\n",
    "## Von Kempelen\n",
    "```\n",
    "\n",
    "Cette liste a un peu plus de sens, on a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble (*Chess Player* par exemple)\n",
    "\n",
    "3. Ne s'intéresser qu'aux *collocations* qui concernent le mot *fear*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8616c",
   "metadata": {},
   "source": [
    "```\n",
    "## [('your', 'word'), ('the', 'word'), ('word', 'again'), ('word', 'of'), ('me', 'word'), ('a', 'word'), ('word', '.\"'), ('word', 'will'), ('word', 'that'), ('word', 'in')]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9e4af",
   "metadata": {},
   "source": [
    "Si on mène la même analyse pour le terme *love*, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07020d61",
   "metadata": {},
   "source": [
    "```\n",
    "## [('love', 'me'), ('love', 'he'), ('will', 'love'), ('I', 'love'), ('love', ','), ('you', 'love'), ('the', 'love')]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
