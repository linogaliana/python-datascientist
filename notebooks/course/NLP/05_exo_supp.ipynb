{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercices supplémentaires"
      ],
      "id": "779ca284-aaf0-4f7d-b9cf-455b6751e9f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p class=\"badges\">\n",
        "\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/NLP/05_exo_supp.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
        "<a href=\"https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/NLP/05_exo_supp.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter\" alt=\"Download\"></a>\n",
        "<a href=\"https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/notebooks/course/NLP/05_exo_supp.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter\" alt=\"nbviewer\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABnotebooks/course/NLP/05_exo_supp.ipynb%C2%BB&security.allowlist.enabled=false\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&amp;color=yellow?logo=Python\" alt=\"Onyxia\"></a><br>\n",
        "<a href=\"https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath={binder_path}\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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\" alt=\"Binder\"></a>\n",
        "<a href=\"http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/notebooks/course/NLP/05_exo_supp.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "<a href=\"https://github.dev/linogaliana/python-datascientist/notebooks/course/NLP/05_exo_supp.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20Visual%20Studio%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc\" alt=\"githubdev\"></a>\n",
        "\n",
        "</p>\n",
        "\n",
        "</p>"
      ],
      "id": "2fba6aba-512f-40f1-90c5-b6b5775b157d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cette page approfondit certains aspects présentés dans les autres tutoriels. Il s’agit d’une suite d’exercice, avec corrections, pour présenter d’autres aspects du NLP ou pratiquer sur des données différentes\n",
        "\n",
        "# Exploration des libellés de l’openfood database"
      ],
      "id": "239e9b83-dc52-4696-8838-2ceeda1f47a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "\n",
        "L’objectif de cet exercice est d’analyser les termes les plus fréquents\n",
        "dans les noms de produits de l’openfood database. Au passage, cela permet de réviser les étapes de *preprocessing* (LIEN XXXXX) et d’explorer les enjeux de reconnaissance d’entités nommées.\n",
        "\n",
        "</div>"
      ],
      "id": "7b40492b-6137-4ad0-889c-219dc87915f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans cet exercice:\n",
        "\n",
        "-   tokenisation (`nltk`)\n",
        "-   retrait des stop words (`nltk`)\n",
        "-   nuage de mots (`wordcloud`)\n",
        "-   reconnaissance du langage (`fasttext`)\n",
        "-   reconnaissance d’entités nommées (`spacy`)\n",
        "\n",
        "le tout sur l’OpenFood Database, une base de données alimentaire qui est enrichie de manière collaborative."
      ],
      "id": "2a66ff7b-6a59-4f05-a87b-2746b049c150"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "\n",
        "Pour pouvoir utiliser les modèles pré-entraînés de `spaCy`, il faut les télécharger. La méthode préconisée est d’utiliser, depuis un terminal, la commande suivante\n",
        "\n",
        "``` python\n",
        "python -m spacy download fr_core_news_sm\n",
        "```\n",
        "\n",
        "Dans un notebook jupyter, il se peut qu’il soit nécessaire de relancer le kernel.\n",
        "\n",
        "Si l’accès à la ligne de commande n’est pas possible, ou si la commande échoue, il est possible de télécharger le modèle pré-entraîné directement depuis une session `Python`\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "```\n",
        "\n",
        "</div>"
      ],
      "id": "dcc5163a-23c8-49e4-be81-a405ee8650ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Importer le modèle de reconnaissance de langage qui sera utilisé par la suite\n",
        "    ainsi que le corpus Français utilisé par `spacy`"
      ],
      "id": "cbd2741c-e94e-4aa5-b727-696d4515d653"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "temp_dir = tempfile.NamedTemporaryFile()\n",
        "temp_dir = temp_dir.name\n",
        "\n",
        "os.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\n",
        "spacy.cli.download('fr_core_news_sm')"
      ],
      "id": "eb19869d-e344-41a6-9e31-89ce83d1301c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Importer les données de l’[openfood database](https://fr.openfoodfacts.org/data) à partir du code suivant"
      ],
      "id": "88a3c479-78ed-484c-86e6-102bde693d07"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\n",
        "df_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n",
        "                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")"
      ],
      "id": "1634d558-81af-4bb0-ab63-7deb4b4b6542"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ces données devraient avoir l’aspect suivant:"
      ],
      "id": "148070fd-e135-43ce-ae2d-908218c92e04"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_openfood.iloc[:2, :5]"
      ],
      "id": "cfa2515d-1ed0-47ef-be15-7ffe9b6521a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Créer une fonction de nettoyage des noms de produits effectuant les\n",
        "    étapes suivantes:\n",
        "\n",
        "-   tokeniser le texte en question\n",
        "-   retirer la ponctuation et les *stopwords*\n",
        "\n",
        "Appliquer cette fonction à l’ensemble des noms de produits (variable\n",
        "`product_name`)\n",
        "\n",
        "1.  Effectuer un nuage de mot sur les libellés avant et après nettoyage\n",
        "    pour comprendre la structure du corpus en question.\n",
        "    Le résultat devrait avoir l’apparence suivante"
      ],
      "id": "4d7ce3ed-35de-4a5d-8c04-b884cdf1ede6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wordcloud as wc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n",
        "    if (by is not None) & (valueby is not None):        \n",
        "        txt = data[data[by]==valueby][yvar].astype(str)\n",
        "    else:\n",
        "        txt = data[yvar].astype(str)\n",
        "    all_text = ' '.join([text for text in txt])\n",
        "    wordcloud = wc.WordCloud(width=800, height=500,\n",
        "                          random_state=21,\n",
        "                      max_words=2000).generate(all_text)\n",
        "    return wordcloud\n",
        "\n",
        "def graph_wordcloud_by(data, by, yvar = \"Text\"):\n",
        "    n_topics = data[by].unique().tolist()\n",
        "    width=20\n",
        "    height=80\n",
        "    rows = len(n_topics)//2\n",
        "    cols = 2\n",
        "    fig=plt.figure(figsize=(width, height))\n",
        "    axes = []\n",
        "    for i in range(cols*rows):\n",
        "        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n",
        "        axes.append( fig.add_subplot(rows, cols, i+1) )\n",
        "        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n",
        "        plt.imshow(b)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n",
        "\n",
        "\n",
        "def wordcount_words(data, yvar, by = None):\n",
        "    plt.figure( figsize=(15,15) )\n",
        "    if by is None:\n",
        "        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n",
        "    else:\n",
        "        graph_wordcloud_by(data, by = by, yvar = yvar)\n",
        "\n",
        "wordcount_words(df_openfood, yvar = \"product_name\")\n",
        "wordcount_words(df_openfood, \"tokenized\")"
      ],
      "id": "91548d28-9550-47fc-bcea-b27000a72ce3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Utiliser la librairie `Fasttext` pour extraire les noms de produits\n",
        "    français\n",
        "\n",
        "-   Appliquer le modèle téléchargé précedemment pour déterminer le langage\n",
        "-   Ne récupérer que les libellés français"
      ],
      "id": "f2a4b8e6-839a-449c-b4c9-cb77af9033a1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "PRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\n",
        "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
        "newcols = ['language','score_language']\n",
        "df_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\n",
        "df_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\n",
        "df_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\n",
        "df_openfood_french.head(2)"
      ],
      "id": "0a24a680-60fd-4dfc-a58e-ac91b4842710"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Visualiser avec `spacy.displacy` le résultat d’une reconnaissance\n",
        "    d’entités nommées sur 50 données aléatoires. Cela vous semble-t-il satisfaisant ?"
      ],
      "id": "599d3875-143e-448b-b986-60b0169d9613"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import fr_core_news_sm\n",
        "\n",
        "nlp = fr_core_news_sm.load()\n",
        "\n",
        "example = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n",
        "\n",
        "from spacy import displacy\n",
        "html = displacy.render(nlp(example), style='ent', page=True)"
      ],
      "id": "7b4a4666-61e1-4a17-ae4a-d85ebc06792c"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(html)"
      ],
      "id": "7fd6916b-e4aa-4ea6-85c8-6d728b863f27"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Récupérer dans un vecteur les entités nommées reconnues par `spaCy`.\n",
        "    Regarder les entités reconnues dans les 20 premiers libellés de produits"
      ],
      "id": "0cdfae45-11a7-4bf6-bb83-896c3ba1c8a9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = []\n",
        "for doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "    # Do something with the doc here\n",
        "    x.append([(ent.text, ent.label_) for ent in doc.ents])\n",
        "    \n",
        "x"
      ],
      "id": "f06ff438-f942-4c97-bd0d-57950a9352ee"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!----\n",
        "# State of the union address\n",
        "\n",
        "\n",
        "Un exercice à venir sur l'analyse des discours des présidents américains \n",
        "inspiré de https://github.com/BuzzFeedNews/2018-01-trump-state-of-the-union\n",
        "---->"
      ],
      "id": "a2ee897d-314e-4edc-a0ca-091fcf6cb4e2"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  }
}