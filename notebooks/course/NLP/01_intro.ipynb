{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3872e5",
   "metadata": {},
   "source": [
    "#  Quelques éléments pour comprendre les enjeux\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0a02a",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABNLP%2001_intro%C2%BB&security.allowlist.enabled=false)\n",
    "<br>\n",
    "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=/__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb)\n",
    "[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist//__w/python-datascientist/python-datascientist/notebooks/course/NLP/01_intro.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4337b",
   "metadata": {},
   "source": [
    "Le NLP est un domaine immense de recherche. Cette page est une introduction\n",
    "fort incomplète à la question. Il s'agit de montrer la logique, quelques exemples\n",
    "avec `Python` <i class=\"fab fa-python\"></i>\n",
    "et s'amuser avec comme base d'exemple un livre formidable :books: :\n",
    "*Le Comte de Monte Cristo*\n",
    "\n",
    "## Base d'exemple\n",
    "\n",
    "La base d'exemple est le *Comte de Monte Cristo* d'Alexandre Dumas.\n",
    "Il est disponible\n",
    "gratuitement sur le site\n",
    "[Project Gutemberg](http://www.gutenberg.org/ebooks/author/492) comme des milliers\n",
    "d'autres livres du domaine public. La manière la plus simple de le récupérer\n",
    "est de télécharger avec le module `urllib` le fichier texte et le retravailler\n",
    "légèrement pour ne conserver que le corpus du livre : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc71704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/17989/17989-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "dumas = raw.split(\"*** START OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[1].split(\"*** END OF THE PROJECT GUTENBERG EBOOK LE COMTE DE MONTE-CRISTO, TOME I ***\")[0]\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # mettre les mots en minuscule\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "dumas = clean_text(dumas)\n",
    "\n",
    "dumas[10000:10500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a3921",
   "metadata": {},
   "source": [
    "```\n",
    "## \" mes yeux. --vous avez donc vu l'empereur aussi? --il est entré chez le maréchal pendant que j'y étais. --et vous lui avez parlé? --c'est-à-dire que c'est lui qui m'a parlé, monsieur, dit dantès en souriant. --et que vous a-t-il dit? --il m'a fait des questions sur le bâtiment, sur l'époque de son départ pour marseille, sur la route qu'il avait suivie et sur la cargaison qu'il portait. je crois que s'il eût été vide, et que j'en eusse été le maître, son intention eût été de l'acheter; mais je lu\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f7e1c",
   "metadata": {},
   "source": [
    "## La particularité des données textuelles\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Le *natural language processing* (NLP) ou\n",
    "*traitement automatisé de la langue* (TAL) en Français, vise à extraire de l'information de textes à partir d'une analyse statistique du contenu. \n",
    "Cette définition permet d'inclure de nombreux champs d'applications au sein\n",
    "du NLP (traduction, analyse de sentiment, recommandation, surveillance, etc. ) ainsi que de méthodes. \n",
    "\n",
    "Cette approche implique de transformer un texte, qui est une information compréhensible par un humain, en un nombre, information appropriée pour un ordinateur et une approche statistique ou algorithmique. \n",
    "\n",
    "Transformer une information textuelle en valeurs numériques propres à une analyse statistique n'est pas une tâche évidente. Les données textuelles sont **non structurées** puisque l'information cherchée, qui est propre à chaque analyse, est perdue au milieu d'une grande masse d'informations qui doit, de plus, être interprétée dans un certain contexte (un même mot ou une phrase n'ayant pas la même signification selon le contexte). \n",
    "\n",
    "Si cette tâche n'était pas assez difficile comme ça, on peut ajouter d'autres difficultés propres à l'analyse textuelle car ces données sont :\n",
    "\n",
    "* bruitées : ortographe, fautes de frappe...\n",
    "* changeantes : la langue évolue avec de nouveaux mots, sens...\n",
    "* complexes : structures variables, accords...\n",
    "* ambigues : synonymie, polysémie, sens caché...\n",
    "* propres à chaque langue : il n'existe pas de règle de passage unique entre deux langues\n",
    "* grande dimension : des combinaisons infinies de séquences de mots\n",
    "\n",
    "### Méthode\n",
    "\n",
    "L’unité textuelle peut être le mot ou encore une séquence de *n*\n",
    "mots (un *n-gramme*) ou encore une chaîne de caractères (e.g. la\n",
    "ponctuation peut être signifiante). On parle de **token**. L’analyse textuelle vise à transformer le texte en données\n",
    "numériques manipulables. \n",
    "\n",
    "On peut ensuite utiliser diverses techniques (clustering,\n",
    "classification supervisée) suivant l’objectif poursuivi pour exploiter\n",
    "l’information transformée. Mais les étapes de nettoyage de texte sont indispensables car sinon un algorithme sera incapable de détecter une information pertinente dans l'infini des possibles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c5bae",
   "metadata": {},
   "source": [
    "## Nettoyer un texte\n",
    "\n",
    "Les *wordclouds* sont des représentations graphiques assez pratiques pour visualiser\n",
    "les mots les plus fréquents. Elles sont très simples à implémenter en `Python`\n",
    "avec le module `wordcloud` qui permet même d'ajuster la forme du nuage à\n",
    "une image :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de0eda",
   "metadata": {},
   "source": [
    "Cela montre clairement qu'il est nécessaire de nettoyer notre texte. Le nom\n",
    "du personnage principal, Dantès, est ainsi masqué par un certain nombre\n",
    "d'articles ou mots de liaison qui perturbent l'analyse. Ces mots sont des \n",
    "*stop-words*. La librairie `NLTK` (*Natural Language ToolKit*), librairie\n",
    "de référence dans le domaine du NLP, permet de facilement retirer ces\n",
    "stopwords (cela pourrait également être fait avec \n",
    "la librairie plus récente, `spaCy`). Avant cela, il est nécessaire\n",
    "de transformer notre texte en le découpant par unités fondamentales (les tokens)\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "La tokenisation consiste à découper un texte en morceaux. Ces morceaux\n",
    "pourraient être des phrases, des chapitres, des n-grammes ou des mots. C'est\n",
    "cette dernière option que l'on va choisir, plus simple pour retirer les \n",
    "*stopwords* :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d55d97",
   "metadata": {},
   "source": [
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package punkt to /github/home/nltk_data...\n",
    "## [nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(dumas, language='french')\n",
    "words[1030:1050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4f710",
   "metadata": {},
   "source": [
    "```\n",
    "## ['que', 'voulez-vous', ',', 'monsieur', 'edmond', ',', 'reprit', \"l'armateur\", 'qui', 'paraissait', 'se', 'consoler', 'de', 'plus', 'en', 'plus', ',', 'nous', 'sommes', 'tous']\n",
    "```\n",
    "\n",
    "On remarque que les mots avec apostrophes sont liés en un seul, ce qui est\n",
    "peut-être faux sur le plan de la grammaire mais peu avoir un sens pour une \n",
    "analyse statistique. Il reste des signes de ponctuation qu'on peut éliminer\n",
    "avec la méthode `isalpha`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88513f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if word.isalpha()]\n",
    "words[1030:1050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924cfa4",
   "metadata": {},
   "source": [
    "```\n",
    "## ['assez', 'sombre', 'obséquieux', 'envers', 'ses', 'supérieurs', 'insolent', 'envers', 'ses', 'subordonnés', 'aussi', 'outre', 'son', 'titre', 'comptable', 'qui', 'est', 'toujours', 'un', 'motif']\n",
    "```\n",
    "\n",
    "Lors de la première utilisation de `NLTK`, il est nécessaire de télécharger\n",
    "quelques éléments nécessaires à la tokenisation, notamment la ponctuation.\n",
    "Pour cela, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3e63a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5defff5",
   "metadata": {},
   "source": [
    "### Retirer les stopwords\n",
    "\n",
    "Le jeu de données est maintenant propre. On peut désormais retirer les \n",
    "*stop words*. \n",
    "\n",
    "Lors de la première utilisation de `NLTK`, il est nécessaire de télécharger\n",
    "les stopwords. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d425aa",
   "metadata": {},
   "source": [
    "```\n",
    "## True\n",
    "## \n",
    "## [nltk_data] Downloading package stopwords to /github/home/nltk_data...\n",
    "## [nltk_data]   Unzipping corpora/stopwords.zip.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53867bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b0583",
   "metadata": {},
   "source": [
    "```\n",
    "## ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ab857",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[1030:1050])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972ba1b",
   "metadata": {},
   "source": [
    "```\n",
    "## ['celui', 'dantès', 'a', 'déposé', 'passant', 'comment', 'paquet', 'déposer', 'danglars', 'rougit', 'passais', 'devant', 'porte', 'capitaine', 'entrouverte', 'vu', 'remettre', 'paquet', 'cette', 'lettre']\n",
    "```\n",
    "\n",
    "Ces retraitements commencent à porter leurs fruits puisque des mots ayant plus\n",
    "de sens commencent à se dégager, notamment les noms des personnages\n",
    "(Fernand, Mercédès, Villefort, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863a683",
   "metadata": {},
   "source": [
    "<!-- KA : ne s'affichait pas sur le site sans que je comprenne pourquoi. J'ai ajouté echo=TRUE... -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8506d",
   "metadata": {},
   "source": [
    "### *Stemming*\n",
    "\n",
    "Pour réduire la complexité d'un texte, on peut tirer partie de\n",
    "\"classes d'équivalence\" : on peut\n",
    "considérer que différentes formes d’un même mot (pluriel,\n",
    "singulier, conjugaison) sont équivalentes et les remplacer par une\n",
    "même forme dite canonique. Il existe deux approches dans le domaine :\n",
    "\n",
    "* la **lemmatisation** qui requiert la connaissance des statuts\n",
    "grammaticaux (exemple : chevaux devient cheval)\n",
    "* la **racinisation** (*stemming*) plus fruste mais plus rapide, notamment\n",
    "en présence de fautes d’orthographes. Dans ce cas, chevaux peut devenir chev\n",
    "mais être ainsi confondu avec chevet ou cheveux\n",
    "\n",
    "La racinisation est plus simple à mettre en oeuvre car elle peut s'appuyer sur\n",
    "des règles simples pour extraire la racine d'un mot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be77a48",
   "metadata": {},
   "source": [
    "Pour réduire un mot dans sa forme \"racine\", c'est-à-dire en s'abstrayant des\n",
    "conjugaisons ou variations comme les pluriels, on applique une méthode de\n",
    "*stemming*. Le but du *stemming* est de regrouper de\n",
    "nombreuses variantes d’un mot comme un seul et même mot.\n",
    "Par exemple, une fois que l’on applique un stemming, \"chats\" et \"chat\" \n",
    "deviennent un même mot. \n",
    "Cette approche a l'avantage de réduire la taille du vocabulaire à maîtriser\n",
    "pour l'ordinateur et le modélisateur. Il existe plusieurs algorithmes de \n",
    "*stemming*, notamment le *Porter Stemming Algorithm* ou le\n",
    "*Snowball Stemming Algorithm*. Nous pouvons utiliser ce dernier en Français :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314df697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed[1030:1050])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e73992",
   "metadata": {},
   "source": [
    "```\n",
    "## ['celui', 'dantes', 'a', 'dépos', 'pass', 'comment', 'paquet', 'dépos', 'danglar', 'roug', 'pass', 'dev', 'port', 'capitain', 'entrouvert', 'vu', 'remettr', 'paquet', 'cet', 'lettr']\n",
    "```\n",
    "\n",
    "A ce niveau, les mots commencent à être moins intelligibles par un humain. \n",
    "La machine prendra le relais, on lui a préparé le travail\n",
    "\n",
    "Il existe aussi le stemmer suivant : \n",
    "\n",
    "~~~python\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmer = FrenchStemmer()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78e8c6",
   "metadata": {},
   "source": [
    "### Reconnaissance des entités nommées\n",
    "\n",
    "Cette étape n'est pas une étape de préparation mais illustre la capacité \n",
    "des librairies `Python` a extraire du sens d'un texte. La librairie \n",
    "`spaCy` permet de faire de la reconnaissance d'entités nommées, ce qui peut\n",
    "être pratique pour extraire rapidement certains personnages de notre oeuvre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113fa1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#!pip install deplacy\n",
    "#!python -m spacy download fr_core_news_sm\n",
    "import pkg_resources,imp\n",
    "imp.reload(pkg_resources)\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(dumas)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b80a2a",
   "metadata": {},
   "source": [
    "## Représentation d'un texte sous forme vectorielle\n",
    "\n",
    "Une fois nettoyé, le texte est plus propice à une représentation vectorielle.\n",
    "En fait, implicitement, on a depuis le début adopté une démarche *bag of words*.\n",
    "Il s'agit d'une représentation, sans souci de contexte (ordre, utilisation),\n",
    "où chaque *token* représente un élément dans un vocabulaire de taille $|V|$.\n",
    "On peut ainsi avoir une représentation matricielle les occurrences de\n",
    "chaque *token* dans plusieurs documents (par exemple plusieurs livres,\n",
    "chapitres, etc.) pour, par exemple, en déduire une forme de similarité. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa7e88",
   "metadata": {},
   "source": [
    "Afin de réduire la dimension de la matrice *bag of words*,\n",
    "on peut s'appuyer sur des pondérations.\n",
    "On élimine ainsi certains mots très fréquents ou au contraire très rares.\n",
    "La pondération la plus simple est basée sur la fréquence des mots dans le document.\n",
    "C'est l'objet de la métrique **tf-idf** (term frequency - inverse document frequency)\n",
    "abordée dans un prochain chapitre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
