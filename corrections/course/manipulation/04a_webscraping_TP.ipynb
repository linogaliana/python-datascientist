{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Webscraping avec python"
      ],
      "id": "60ff0ab6-f475-4aaa-aa01-ad40ae527a5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p class=\"badges\">\n",
        "\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/04a_webscraping_TP.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
        "<a href=\"https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/04a_webscraping_TP.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter\" alt=\"Download\"></a>\n",
        "<a href=\"https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/04a_webscraping_TP.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter\" alt=\"nbviewer\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABmanipulation%2004a_webscraping_TP%C2%BB&security.allowlist.enabled=false\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&amp;color=yellow?logo=Python\" alt=\"Onyxia\"></a><br>\n",
        "<a href=\"https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath={binder_path}\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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\" alt=\"Binder\"></a>\n",
        "<a href=\"http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/04a_webscraping_TP.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "<a href=\"https://github.dev/linogaliana/python-datascientist/notebooks/course/manipulation/04a_webscraping_TP.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20Visual%20Studio%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc\" alt=\"githubdev\"></a>\n",
        "\n",
        "</p>\n",
        "\n",
        "</p>"
      ],
      "id": "60121a3c-982d-48df-b4b0-b0f47d0d16cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le [webscraping](https://fr.wikipedia.org/wiki/Web_scraping) désigne les techniques d’extraction du contenu des sites internet.\n",
        "C’est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n’existant pas forcément sous la forme d’un tableau *Excel*.\n",
        "\n",
        "Ce TP vous présente comment créer et exécuter des robots afin de recupérer rapidement des informations utiles à vos projets actuels ou futurs.\n",
        "Il part de quelques cas d’usages concret.\n",
        "Ce chapitre est très fortement inspiré et réadapté à partir de [celui de Xavier Dupré](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/notebooks/TD2A_Eco_Web_Scraping.html), l’ancien professeur de la matière.\n",
        "\n",
        "# Enjeux\n",
        "\n",
        "Un certain nombre d’enjeux du *webscraping* ne seront évoqués\n",
        "que superficiellement dans le cadre de ce chapitre.\n",
        "\n",
        "## La zone grise de la légalité du *webscraping*\n",
        "\n",
        "En premier lieu, en ce qui concerne la question de la légalité\n",
        "de la récupération d’information par *scraping*, il existe\n",
        "une zone grise. Ce n’est pas parce qu’une information est\n",
        "disponible sur internet, directement ou avec un peu de recherche,\n",
        "qu’elle peut être récupérée et réutilisée.\n",
        "\n",
        "L’excellent cours d’[Antoine Palazzolo](https://inseefrlab.github.io/formation_webscraping/) évoque un certain nombre de cas\n",
        "médiatiques et judiciaires sur cette question.\n",
        "Dans le champ français, la CNIL a publié en 2020\n",
        "de nouvelles directives sur le *webscraping* reprécisant\n",
        "que toute donnée ne peut être réutilisée à l’insu de la personne\n",
        "à laquelle ces données appartiennent. Autrement dit, en principe,\n",
        "les données collectées par *webscraping* sont soumises au\n",
        "RGPD, c’est-à-dire nécessitent le consentement des personnes\n",
        "à partir desquelles la réutilisation des données est faite.\n",
        "\n",
        "Il est donc recommandé d’**être vigilant avec les données récupérées**\n",
        "par *webscraping* pour ne pas se mettre en faute légalement.\n",
        "\n",
        "## Stabilité et fiabilité des informations reçues\n",
        "\n",
        "La récupération de données par *webscraping*\n",
        "est certes pratique mais elle ne correspond pas nécessairement\n",
        "à un usage pensé, ou désiré, par un fournisseur de données.\n",
        "Les données étant coûteuses à collecter et à mettre à disposition,\n",
        "certains sites ne désirent pas nécessairement que celles-ci soient\n",
        "extraites gratuitement et facilement. *A fortiori* lorsque la donnée\n",
        "peut permettre à un concurrent de disposer d’une information\n",
        "utile d’un point de vue commercial (prix d’un produit concurrent, etc.).\n",
        "\n",
        "Les acteurs mettent donc souvent en oeuvre des stratégies pour bloquer ou\n",
        "limiter la quantité de données scrappées. La méthode la plus\n",
        "classique est la détection et le blocage\n",
        "des requêtes faites par des robots plutôt que par des humains.\n",
        "Pour des acteurs spécialisés, cette détection est très facile car\n",
        "de nombreuses preuves permettent d’identifier si une visite du site *web*\n",
        "provient d’un utilisateur\n",
        "humain derrière un navigateur ou d’un robot. Pour ne citer que quelques indices:\n",
        "vitesse de la navigation entre pages, rapidité à extraire la donnée,\n",
        "empreinte digitale du navigateur utilisé, capacité à répondre à des\n",
        "questions aléatoires (captcha)…\n",
        "Les bonnes pratiques, évoquées par la suite, ont pour objectif de faire\n",
        "en sorte qu’un robot se comporte de manière civile en adoptant un comportement\n",
        "proche de celui de l’humain mais sans contrefaire le fait qu’il ne s’agit\n",
        "pas d’un humain.\n",
        "\n",
        "Il convient d’ailleurs\n",
        "d’être prudent quant aux informations reçues par *webscraping*.\n",
        "La donnée étant au coeur du modèle économique de certains acteurs, certains\n",
        "n’hésitent pas à renvoyer des données fausses aux robots\n",
        "plutôt que les bloquer. C’est de bonne guerre!\n",
        "Une autre technique piège s’appelle le *honey pot*. Il s’agit de pages qu’un humain\n",
        "n’irait jamais visiter - par exemple parce qu’elles n’apparaissent pas dans\n",
        "l’interface graphique - mais sur lesquelles un robot, en recherche automatique\n",
        "de contenu, va rester bloquer.\n",
        "\n",
        "Sans aller jusqu’à la stratégie de blocage du *webscraping*, d’autres raisons\n",
        "peuvent expliquer qu’une récupération de données ait fonctionné par\n",
        "le passé mais ne fonctionne plus. La plus fréquente est un changement dans la structure\n",
        "d’un site *web*. Le *webscraping* présente en effet l’inconvénient d’aller chercher\n",
        "de l’information dans une structure très hiérarchisée. Un changement dans cette structure\n",
        "peut suffire à rendre un robot incapable de récupérer du contenu. Or, pour rester\n",
        "attractifs, les sites *web* changent fréquemment ce qui peut facilement\n",
        "rendre inopérant un robot.\n",
        "\n",
        "De manière générale, l’un des principaux messages de ce\n",
        "chapitre, à retenir, est que le\n",
        "**webscraping est une solution de dernier ressort, pour des récupérations ponctuelles de données sans garantie de fonctionnement ultérieur**. Il est préférable de **privilégier les API** lorsque celles-ci sont disponibles.\n",
        "Ces dernières ressemblent à un contrat (formel ou non) entre un fournisseur de données\n",
        "et un utilisateur où sont définis des besoins (les données) mais aussi des\n",
        "conditions d’accès (nombre de requêtes, volumétrie, authentification…) là\n",
        "où le *webscraping* est plus proche du comportement dans le *Far West*.\n",
        "\n",
        "## Les bonnes pratiques\n",
        "\n",
        "La possibilité de récupérer des données par l’intermédiaire\n",
        "d’un robot ne signifie pas qu’on peut se permettre de n’être\n",
        "pas civilisé. En effet, lorsqu’il est non-maîtrisé, le\n",
        "*webscraping* peut ressembler à une attaque informatique\n",
        "classique pour faire sauter un site *web*: le déni de service.\n",
        "Le cours d’[Antoine Palazzolo](https://inseefrlab.github.io/formation_webscraping/) revient\n",
        "sur certaines bonnes pratiques qui ont émergé dans la communauté\n",
        "des *scrapeurs*. Il est recommandé de lire cette ressource\n",
        "pour en apprendre plus sur ce sujet. Y sont évoqués\n",
        "plusieurs conventions, parmi lesquelles :\n",
        "\n",
        "-   Se rendre, depuis la racine du site,\n",
        "    sur le fichier `robots.txt` pour vérifier les consignes\n",
        "    proposées par les développeurs du site *web* pour\n",
        "    cadrer le comportement des robots ;\n",
        "-   Espacer chaque requêtes de plusieurs secondes, comme le ferait\n",
        "    un humain, afin d’éviter de surcharger le site *web* et de le\n",
        "    faire sauter par déni de service ;\n",
        "-   Faire les requêtes dans les heures creuses de fréquentation du\n",
        "    site *web* s’il ne s’agit pas d’un site consulté internationalement.\n",
        "    Par exemple, pour un site en Français, lancer le robot\n",
        "    pendant la nuit en France métropolitaine, est une bonne pratique.\n",
        "    Pour lancer un robot depuis `Python` a une heure programmée\n",
        "    à l’avancer, il existe les `cronjobs`.\n",
        "\n",
        "# Un détour par le Web : comment fonctionne un site ?\n",
        "\n",
        "Même si ce TP ne vise pas à faire un cours de web, il vous faut néanmoins certaines bases sur la manière dont un site internet fonctionne afin de comprendre comment sont structurées les informations sur une page.\n",
        "\n",
        "Un site Web est un ensemble de pages codées en *HTML* qui permet de décrire à la fois le contenu et la forme d’une page *Web*.\n",
        "\n",
        "Pour voir cela, ouvrez n’importe quelle page web et faites un clic-droit dessus.\n",
        "- Sous `Chrome` <i class=\"fab fa-chrome\"></i> : Cliquez ensuite sur *“Affichez le code source de la page”* (<kbd>CTRL</kbd>+<kbd>U</kbd>) ;\n",
        "- Sous `Firefox` <i class=\"fab fa-firefox\"></i> : *“Code source de la page”* (<kbd>CTRL</kbd>+<kbd>MAJ</kbd>+<kbd>K</kbd>) ;\n",
        "- Sous `Edge` <i class=\"fab fa-edge\"></i> : *“Affichez la page source”* (<kbd>CTRL</kbd>+<kbd>U</kbd>) ;\n",
        "- Sous `Safari` <i class=\"fab fa-safari\"></i> : voir comment faire [ici](https://fr.wikihow.com/voir-le-code-source)\n",
        "\n",
        "## Les balises\n",
        "\n",
        "Sur une page web, vous trouverez toujours à coup sûr des éléments comme `<head>`, `<title>`, etc. Il s’agit des codes qui vous permettent de structurer le contenu d’une page *HTML* et qui s’appellent des **balises**.\n",
        "Citons, par exemple, les balises `<p>`, `<h1>`, `<h2>`, `<h3>`, `<strong>` ou `<em>`.\n",
        "Le symbole `< >` est une balise : il sert à indiquer le début d’une partie. Le symbole `</ >` indique la fin de cette partie. La plupart des balises vont par paires, avec une *balise ouvrante* et une *balise fermante* (par exemple `<p>` et `</p>`).\n",
        "\n",
        "### Exemple : les balise des tableaux\n",
        "\n",
        "| Balise      | Description                     |\n",
        "|-------------|---------------------------------|\n",
        "| `<table>`   | Tableau                         |\n",
        "| `<caption>` | Titre du tableau                |\n",
        "| `<tr>`      | Ligne de tableau                |\n",
        "| `<th>`      | Cellule d’en-tête               |\n",
        "| `<td>`      | Cellule                         |\n",
        "| `<thead>`   | Section de l’en-tête du tableau |\n",
        "| `<tbody>`   | Section du corps du tableau     |\n",
        "| `<tfoot>`   | Section du pied du tableau      |\n",
        "\n",
        "**Application : un tableau en HTML**\n",
        "\n",
        "Le code *HTML* du tableau suivant\n",
        "\n",
        "``` {html}\n",
        "<table>\n",
        "<caption> Le Titre de mon tableau </caption>\n",
        "\n",
        "   <tr>\n",
        "      <th>Prénom</th>\n",
        "      <th>Nom</th>\n",
        "      <th>Profession</th>\n",
        "   </tr>\n",
        "   <tr>\n",
        "      <td>Mike </td>\n",
        "      <td>Stuntman</td>\n",
        "      <td>Cascadeur</td>\n",
        "   </tr>\n",
        "   <tr>\n",
        "      <td>Mister</td>\n",
        "      <td>Pink</td>\n",
        "      <td>Gangster</td>\n",
        "   </tr>\n",
        "</table>\n",
        "```\n",
        "\n",
        "Donnera dans le navigateur :"
      ],
      "id": "773a807c-433d-4ee0-b943-cf9efc22d64c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|        |          |            |\n",
        "|--------|----------|------------|\n",
        "| Prénom | Nom      | Profession |\n",
        "| Mike   | Stuntman | Cascadeur  |\n",
        "| Mister | Pink     | Gangster   |\n",
        "\n",
        "Le Titre de mon tableau"
      ],
      "id": "8fc565fe-dec2-457e-b010-d20c88d582de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parent et enfant\n",
        "\n",
        "Dans le cadre du langage HTML, les termes de parent (*parent*) et enfant (*child*) servent à désigner des élements emboîtés les uns dans les autres. Dans la construction suivante, par exemple :\n",
        "\n",
        "``` html\n",
        "< div> \n",
        "    < p>\n",
        "       bla,bla\n",
        "    < /p>\n",
        "< /div>\n",
        "```\n",
        "\n",
        "Sur la page web, cela apparaitra de la manière suivante :\n",
        "\n",
        "    <p>\n",
        "       bla,bla\n",
        "    </p>\n",
        "\n",
        "On dira que l’élément `<div>` est le parent de l’élément `<p>` tandis que l’élément `<p>` est l’enfant de l’élément `<div>`.\n",
        "\n",
        "> *Mais pourquoi apprendre ça pour “scraper” ?*\n",
        "\n",
        "Parce que, pour bien récupérer les informations d’un site internet, il faut pouvoir comprendre sa structure et donc son code HTML. Les fonctions `Python` qui servent au *scraping* sont principalement construites pour vous permettre de naviguer entre les balises.\n",
        "Avec `Python`, vous allez en fait reproduire votre comportement manuel de recherche de manière\n",
        "à l’automatiser.\n",
        "\n",
        "# Scraper avec python: le package `BeautifulSoup`\n",
        "\n",
        "## Les packages disponibles\n",
        "\n",
        "Dans la première partie de ce chapitre,\n",
        "nous allons essentiellement utiliser le package [`BeautifulSoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/),\n",
        "en conjonction avec [`urllib`](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
        "ou `requests`. Ces deux derniers *packages* permettent de récupérer le texte\n",
        "brut d’une page qui sera ensuite\n",
        "inspecté via [`BeautifulSoup4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
        "\n",
        "`BeautifulSoup` sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l’exécution de scripts [Javascript](https://fr.wikipedia.org/wiki/JavaScript), il vous faudra passer par des outils comme [Selenium](https://selenium-python.readthedocs.io/).\n",
        "\n",
        "De même, si vous ne connaissez pas l’URL, il faudra passer par un *framework* comme [Scrapy](https://scrapy.org/), qui passe facilement d’une page à une autre. On appelle\n",
        "cette technique le *“webcrawling”*. `Scrapy` est plus complexe à manipuler que `BeautifulSoup` : si vous voulez plus de détails, rendez-vous sur la page du [tutoriel `Scrapy`](https://doc.scrapy.org/en/latest/intro/tutorial.html).\n",
        "\n",
        "Le *webscraping* est un domaine où la reproductibilité est compliquée à mettre en oeuvre.\n",
        "Une page *web* évolue\n",
        "potentiellement régulièrement et d’une page web à l’autre, la structure peut\n",
        "être très différente ce qui rend certains codes difficilement exportables.\n",
        "Par conséquent, la meilleure manière d’avoir un programme fonctionnel est\n",
        "de comprendre la structure d’une page web et dissocier les éléments exportables\n",
        "à d’autres cas d’usages des requêtes *ad hoc*."
      ],
      "id": "19391801-fec1-4d82-a8bc-381c55231713"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib\n",
        "import bs4\n",
        "import pandas\n",
        "from urllib import request"
      ],
      "id": "d297709b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Récupérer le contenu d’une page HTML\n",
        "\n",
        "On va commencer doucement. Prenons une page *wikipedia*,\n",
        "par exemple celle de la Ligue 1 de football, millésime 2019-2020 : [Championnat de France de football 2019-2020](https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020). On va souhaiter récupérer la liste des équipes, ainsi que les url des pages Wikipedia de ces équipes.\n",
        "\n",
        "Etape :one: : se connecter à la page wikipedia et obtenir le code source.\n",
        "Pour cela, le plus simple est d’utiliser le package `urllib` ou, mieux, `requests`.\n",
        "Nous allons ici utiliser la fonction `request` du *package* `urllib`:"
      ],
      "id": "53db64c8-39ef-4764-83b6-771d8f7b18f1"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_ligue_1 = \"https://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\"\n",
        "    \n",
        "request_text = request.urlopen(url_ligue_1).read()\n",
        "# print(request_text[:1000])    "
      ],
      "id": "7ebf7468"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(request_text)"
      ],
      "id": "3afd5cb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Etape :two: : utiliser le package BeautifulSoup\n",
        "qui permet de rechercher efficacement\n",
        "les balises contenues dans la chaine de caractères\n",
        "renvoyée par la fonction `request`:"
      ],
      "id": "fcab882b-80f1-4ca0-9a0f-e9ee3c30b9fb"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "page = bs4.BeautifulSoup(request_text, \"lxml\")"
      ],
      "id": "fe233c3b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si on *print* l’objet `page` créée avec `BeautifulSoup`,\n",
        "on voit que ce n’est plus une chaine de caractères mais bien une page HTML avec des balises.\n",
        "On peut à présent chercher des élements à l’intérieur de ces balises.\n",
        "\n",
        "## La méthode `find`\n",
        "\n",
        "Par exemple, si on veut connaître le titre de la page, on utilise la méthode `.find` et on lui demande *“title”*"
      ],
      "id": "219f8f9e-5de0-45b4-8f5e-5cefba6ce60e"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(page.find(\"title\"))"
      ],
      "id": "e7668a1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La methode `.find` ne renvoie que la première occurence de l’élément.\n",
        "\n",
        "Pour vous en assurer vous pouvez :\n",
        "\n",
        "-   copier le bout de code source obtenu,\n",
        "-   le coller dans une cellule de votre notebook\n",
        "-   et passer la cellule en *“Markdown”*\n",
        "\n",
        "La cellule avec le copier-coller du code source donne :"
      ],
      "id": "cb1208b8-bafe-4dbc-adcc-cfe8e66bfe5b"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(page.find(\"table\"))"
      ],
      "id": "bba277a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ce qui est le texte source permettant de générer le tableau suivant:"
      ],
      "id": "77b7c301-3024-45e6-bd97-d2dbcc44ecf0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "7f98d2ed-a66f-4b32-83a8-4fb4424e9a5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## La méthode `findAll`\n",
        "\n",
        "Pour trouver toutes les occurences, on utilise `.findAll()`."
      ],
      "id": "1eb2eb41-8f8b-4c8d-90aa-23165d5f5efd"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Il y a\", len(page.findAll(\"table\")), \"éléments dans la page qui sont des <table>\")"
      ],
      "id": "3f2dc319"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercice guidé : obtenir la liste des équipes de Ligue 1\n",
        "\n",
        "Dans le premier paragraphe de la page *“Participants”*,\n",
        "on a le tableau avec les résultats de l’année."
      ],
      "id": "7de141aa-20b4-43e8-9e6e-d531abd86a05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 1 : Récupérer les participants de la Ligue 1</h3>\n",
        "\n",
        "Pour cela, nous allons procéder en 6 étapes:\n",
        "\n",
        "1.  Trouver le tableau\n",
        "2.  Récupérer chaque ligne du table\n",
        "3.  Nettoyer les sorties en ne gardant que le texte sur une ligne\n",
        "4.  Généraliser sur toutes les lignes\n",
        "5.  Récupérer les entêtes du tableau\n",
        "6.  Finalisation du tableau\n",
        "\n",
        "</div>"
      ],
      "id": "3c12a245-33f7-4606-9646-aa2a04d91740"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1️ Trouver le tableau"
      ],
      "id": "6344f097-571f-4cb2-a8df-659471080851"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# on identifie le tableau en question : c'est le premier qui a cette classe \"wikitable sortable\"\n",
        "tableau_participants = page.find('table', {'class' : 'wikitable sortable'})"
      ],
      "id": "bf7313be"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "print(tableau_participants)\n",
        "```"
      ],
      "id": "8ec608e5-3473-4729-ac5e-a10eefa9d740"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2️ Récupérer chaque ligne du tableau.\n",
        "\n",
        "On recherche d’abord toutes les lignes du tableau avec la balise `tr`"
      ],
      "id": "47850425-b225-41a8-9fc3-26517b1071b5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_body = tableau_participants.find('tbody')\n",
        "rows = table_body.find_all('tr')"
      ],
      "id": "92816d38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On obtient une liste où chaque élément est une des lignes du tableau\n",
        "Pour illustrer cela, on va d’abord afficher la première ligne.\n",
        "Celle-ci correspont aux entêtes de colonne:"
      ],
      "id": "69e45f17-1dae-40fe-8bca-4e56ba242013"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(rows[0])"
      ],
      "id": "42401d8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La seconde ligne va correspondre à la ligne du premier club présent dans le tableau:"
      ],
      "id": "39b98ed1-e4f3-4a57-bdab-fe7b6e791c0f"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(rows[1])"
      ],
      "id": "c9cc522b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3️ Nettoyer les sorties en ne gardant que le texte sur une ligne\n",
        "\n",
        "On va utiliser l’attribut `text` afin de se débarasser de toute la couche de HTML qu’on obtient à l’étape 2.\n",
        "\n",
        "Un exemple sur la ligne du premier club :\n",
        "- on commence par prendre toutes les cellules de cette ligne, avec la balise `td`.\n",
        "- on fait ensuite une boucle sur chacune des cellules et on ne garde que le texte de la cellule avec l’attribut `text`.\n",
        "- enfin, on applique la méthode `strip()` pour que le texte soit bien mis en forme (sans espace inutile etc)."
      ],
      "id": "1c8c2d4b-4f89-4331-a88d-36d31ccb3ae6"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = rows[1].find_all('td')\n",
        "print(cols[0])\n",
        "print(cols[0].text.strip())"
      ],
      "id": "a509eb08"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ele in cols : \n",
        "    print(ele.text.strip())"
      ],
      "id": "19b2ca51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4️ Généraliser sur toutes les lignes :"
      ],
      "id": "07efeedf-22cf-4c9f-b800-0a7eaaacaf12"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in rows:\n",
        "    cols = row.find_all('td')\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    print(cols)"
      ],
      "id": "66a2e180"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On a bien réussi à avoir les informations contenues dans le tableau des participants du championnat.\n",
        "Mais la première ligne est étrange : c’est une liste vide …\n",
        "\n",
        "Il s’agit des en-têtes : elles sont reconnues par la balise `th` et non `td`.\n",
        "\n",
        "On va mettre tout le contenu dans un dictionnaire, pour le transformer ensuite en DataFrame pandas :"
      ],
      "id": "b1083f3b-53f4-4602-aa99-78a75c82efa8"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "dico_participants = dict()\n",
        "for row in rows:\n",
        "    cols = row.find_all('td')\n",
        "    cols = [ele.text.strip() for ele in cols]\n",
        "    if len(cols) > 0 : \n",
        "        dico_participants[cols[0]] = cols[1:]\n",
        "dico_participants"
      ],
      "id": "55368233"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_participants = pandas.DataFrame.from_dict(dico_participants,orient='index')\n",
        "data_participants.head()"
      ],
      "id": "ec8e8221"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5️ Récupérer les en-têtes du tableau:"
      ],
      "id": "8fac4524-e7a6-4832-9b8a-4cebc62717e4"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in rows:\n",
        "    cols = row.find_all('th')\n",
        "    print(cols)\n",
        "    if len(cols) > 0 : \n",
        "        cols = [ele.get_text(separator=' ').strip().title() for ele in cols]\n",
        "        columns_participants = cols"
      ],
      "id": "1b43b1ae"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_participants"
      ],
      "id": "a3d3bb4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6️ Finalisation du tableau"
      ],
      "id": "b28d549a-c01c-44f0-ab2b-cb69fe06ff50"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_participants.columns = columns_participants[1:]"
      ],
      "id": "53fb57ed"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_participants.head()"
      ],
      "id": "cc2b4cd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pour aller plus loin\n",
        "\n",
        "### Récupération des localisations des stades\n",
        "\n",
        "Essayez de comprendre pas à pas ce qui est fait dans les étapes qui suivent (la récupération d’informations supplémentaires en naviguant dans les pages des différents clubs)."
      ],
      "id": "9e8d300d-5f0c-43e6-a4d6-c7e6f7f26fd3"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib\n",
        "import pandas as pd\n",
        "import bs4 \n",
        "\n",
        "division=[]\n",
        "equipe=[]\n",
        "stade=[]\n",
        "latitude_stade=[]        \n",
        "longitude_stade=[]     \n",
        "\n",
        "def dms2dd(degrees, minutes, seconds, direction):\n",
        "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
        "    if direction == 'S' or direction == 'O':\n",
        "        dd *= -1\n",
        "    return dd;\n",
        "\n",
        "url_list=[\"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_2019-2020\", \"http://fr.wikipedia.org/wiki/Championnat_de_France_de_football_de_Ligue_2_2019-2020\"]\n",
        "\n",
        "for url_ligue in url_list :\n",
        "       \n",
        "    print(url_ligue)\n",
        "    sock = urllib.request.urlopen(url_ligue).read() \n",
        "    page=bs4.BeautifulSoup(sock)\n",
        "\n",
        "# Rechercher les liens des équipes dans la liste disponible sur wikipedia \n",
        "\n",
        "    for team in page.findAll('span' , {'class' : 'toponyme'}) :  \n",
        "        \n",
        "        # Indiquer si c'est de la ligue 1 ou de la ligue 2\n",
        "        \n",
        "        if url_ligue==url_list[0] :\n",
        "            division.append(\"L1\")\n",
        "        else :\n",
        "            division.append(\"L2\")\n",
        "\n",
        "       # Trouver le nom et le lien de l'équipe\n",
        "            \n",
        "        if team.find('a')!=None :\n",
        "            team_url=team.find('a').get('href')\n",
        "            name_team=team.find('a').get('title')\n",
        "            equipe.append(name_team)\n",
        "            url_get_info = \"http://fr.wikipedia.org\"+team_url\n",
        "            print(url_get_info)\n",
        " \n",
        "       # aller sur la page de l'équipe\n",
        "           \n",
        "            search = urllib.request.urlopen(url_get_info).read()\n",
        "            search_team=bs4.BeautifulSoup(search)\n",
        "\n",
        "       # trouver le stade             \n",
        "            compteur = 0\n",
        "            for stadium in search_team.findAll('tr'):\n",
        "                for x in stadium.findAll('th' , {'scope' : 'row'} ) :\n",
        "                    if x.contents[0].string==\"Stade\" and compteur == 0:\n",
        "                        compteur = 1\n",
        "                        # trouver le lien du stade et son nom\n",
        "                        url_stade=stadium.findAll('a')[1].get('href')\n",
        "                        name_stadium=stadium.findAll('a')[1].get('title')\n",
        "                        stade.append(name_stadium)\n",
        "                        url_get_stade = \"http://fr.wikipedia.org\"+url_stade\n",
        "                        print(url_get_stade)\n",
        "                        \n",
        "                        # Aller sur la page du stade et trouver ses coodronnées géographiques\n",
        "                        \n",
        "                        search_stade = urllib.request.urlopen(url_get_stade).read()\n",
        "                        soup_stade=bs4.BeautifulSoup(search_stade) \n",
        "                        kartographer = soup_stade.find('a',{'class': \"mw-kartographer-maplink\"})\n",
        "                        if kartographer == None :\n",
        "                          latitude_stade.append(None)\n",
        "                          longitude_stade.append(None) \n",
        "                        else :\n",
        "                            for coordinates in kartographer :\n",
        "                                print(coordinates)\n",
        "                                liste =   coordinates.split(\",\")          \n",
        "                                latitude_stade.append(str(liste[0]).replace(\" \", \"\") + \"'\")\n",
        "                                longitude_stade.append(str(liste[1]).replace(\" \", \"\") + \"'\")\n",
        "                            \n",
        "\n",
        "dict = {'division' : division , 'equipe': equipe, 'stade': stade, 'latitude': latitude_stade, 'longitude' : longitude_stade}\n",
        "data = pd.DataFrame(dict)\n",
        "data = data.dropna()"
      ],
      "id": "c16da645"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(5)"
      ],
      "id": "3ae94d73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On va transformer les coordonnées en degrés en coordonnées numériques\n",
        "afin d’être en mesure de faire une carte"
      ],
      "id": "4e85a533-e892-4ebd-857a-1ae2bbde42f1"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def dms2dd(degrees, minutes, seconds, direction):\n",
        "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
        "    if direction in ('S', 'O'):\n",
        "        dd *= -1\n",
        "    return dd\n",
        "\n",
        "def parse_dms(dms):\n",
        "    parts = re.split('[^\\d\\w]+', dms)\n",
        "    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n",
        "    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n",
        "    return lat"
      ],
      "id": "cef431d2"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['latitude'] = data['latitude'].apply(parse_dms)\n",
        "data['longitude'] = data['longitude'].apply(parse_dms)"
      ],
      "id": "49c52648"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tous les éléments sont en place pour faire une belle carte à ce stade. On\n",
        "va utiliser `folium` pour celle-ci, qui est présenté dans la partie\n",
        "[visualisation](#cartotp).\n",
        "\n",
        "### Carte des stades avec `folium`"
      ],
      "id": "f56383e5-fd8f-404f-b019-7c70df46c120"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install geopandas\n",
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "import folium\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "\n",
        "Path(\"leaflet\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "center = gdf[['latitude', 'longitude']].mean().values.tolist()\n",
        "sw = gdf[['latitude', 'longitude']].min().values.tolist()\n",
        "ne = gdf[['latitude', 'longitude']].max().values.tolist()\n",
        "\n",
        "m = folium.Map(location = center, tiles='Stamen Toner')\n",
        "\n",
        "# I can add marker one by one on the map\n",
        "for i in range(0,len(gdf)):\n",
        "    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['stade']).add_to(m) \n",
        "\n",
        "m.fit_bounds([sw, ne])"
      ],
      "id": "3305da3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La carte obtenue doit ressembler à la suivante:"
      ],
      "id": "e233f3ba-4d84-4c2e-88ef-280e876a31cf"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher la carte\n",
        "m"
      ],
      "id": "23ff7bd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Récupérer des informations sur les pokemons\n",
        "\n",
        "Le prochain exercice pour mettre en pratique le *webscraping*\n",
        "consiste à récupérer des informations sur les\n",
        "pokemons à partir du\n",
        "site internet [pokemondb.net](http://pokemondb.net/pokedex/national).\n",
        "\n",
        "## Version non guidée"
      ],
      "id": "723bf01b-631e-4ec5-92bd-bdff117f0ce4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2 : Les pokemon (version non guidée)</h3>\n",
        "\n",
        "Pour cet exercice, nous vous demandons d’obtenir différentes informations sur les pokémons :\n",
        "\n",
        "1.  les informations personnelles des **893** pokemons sur le site internet [pokemondb.net](http://pokemondb.net/pokedex/national).\n",
        "    Les informations que nous aimerions obtenir au final dans un `DataFrame` sont celles contenues dans 4 tableaux :\n",
        "\n",
        "-   Pokédex data\n",
        "-   Training\n",
        "-   Breeding\n",
        "-   Base stats\n",
        "\n",
        "1.  Nous aimerions que vous récupériez également les images de chacun des pokémons et que vous les enregistriez dans un dossier\n",
        "\n",
        "-   Petit indice : utilisez les modules `request` et [`shutil`](https://docs.python.org/3/library/shutil.html)\n",
        "-   Pour cette question, il faut que vous cherchiez de vous même certains éléments, tout n’est pas présent dans le TD.\n",
        "\n",
        "</div>"
      ],
      "id": "97abc981-a58c-4673-ad1b-45d9bbc9534a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour la question 1, l’objectif est d’obtenir le code source d’un tableau comme\n",
        "celui qui suit\n",
        "(Pokemon [Nincada](http://pokemondb.net/pokedex/nincada).)"
      ],
      "id": "912e500a-9956-4eea-8972-864f7d5fdda7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>\n",
        "\n",
        "Pokédex data\n",
        "\n",
        "</h2>\n",
        "\n",
        "<table class=\"vitals-table\">\n",
        "\n",
        "<tbody>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "National №\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "<strong>290</strong>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Type\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "<a class=\"type-icon type-bug\" href=\"/type/bug\">Bug</a> <a class=\"type-icon type-ground\" href=\"/type/ground\">Ground</a>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Species\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "Trainee Pokémon\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Height\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "0.5 m (1′08″)\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Weight\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "5.5 kg (12.1 lbs)\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Abilities\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "<span class=\"text-muted\">1. <a href=\"/ability/compound-eyes\" title=\"The Pokémon's accuracy is boosted.\">Compound Eyes</a></span><br><small class=\"text-muted\"><a href=\"/ability/run-away\" title=\"Enables a sure getaway from wild Pokémon.\">Run Away</a> (hidden ability)</small><br>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Local №\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "042 <small class=\"text-muted\">(Ruby/Sapphire/Emerald)</small><br>111 <small class=\"text-muted\">(X/Y — Central Kalos)</small><br>043 <small class=\"text-muted\">(Omega Ruby/Alpha Sapphire)</small><br>104 <small class=\"text-muted\">(Sword/Shield)</small><br>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "\n",
        "</table>\n",
        "\n",
        "<h2>\n",
        "\n",
        "Training\n",
        "\n",
        "</h2>\n",
        "\n",
        "<table class=\"vitals-table\">\n",
        "\n",
        "<tbody>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "EV yield\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"text\">\n",
        "\n",
        "1 Defense\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Catch rate\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "255 <small class=\"text-muted\">(33.3% with PokéBall, full HP)</small>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Base <a href=\"/glossary#def-friendship\">Friendship</a>\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "70 <small class=\"text-muted\">(normal)</small>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Base Exp.\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "53\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Growth Rate\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "Erratic\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "\n",
        "</table>\n",
        "\n",
        "<h2>\n",
        "\n",
        "Breeding\n",
        "\n",
        "</h2>\n",
        "\n",
        "<table class=\"vitals-table\">\n",
        "\n",
        "<tbody>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Egg Groups\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "<a href=\"/egg-group/bug\">Bug</a>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Gender\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "<span class=\"text-blue\">50% male</span>, <span class=\"text-pink\">50% female</span>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "<a href=\"/glossary#def-eggcycle\">Egg cycles</a>\n",
        "\n",
        "</th>\n",
        "\n",
        "<td>\n",
        "\n",
        "15 <small class=\"text-muted\">(3,599–3,855 steps)</small>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "\n",
        "</table>\n",
        "\n",
        "<h2>\n",
        "\n",
        "Base stats\n",
        "\n",
        "</h2>\n",
        "\n",
        "<table class=\"vitals-table\">\n",
        "\n",
        "<tbody>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "HP\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "31\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "172\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "266\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Attack\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "45\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "85\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "207\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Defense\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "90\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "166\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "306\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Sp. Atk\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "30\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "58\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "174\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Sp. Def\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "30\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "58\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "174\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Speed\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "40\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-barchart\">\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "76\n",
        "\n",
        "</td>\n",
        "\n",
        "<td class=\"cell-num\">\n",
        "\n",
        "196\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "\n",
        "</tbody>\n",
        "\n",
        "<tfoot>\n",
        "\n",
        "<tr>\n",
        "\n",
        "<th>\n",
        "\n",
        "Total\n",
        "\n",
        "</th>\n",
        "\n",
        "<td class=\"cell-total\">\n",
        "\n",
        "<b>266</b>\n",
        "\n",
        "</td>\n",
        "\n",
        "<th class=\"cell-barchart\">\n",
        "\n",
        "</th>\n",
        "\n",
        "<th>\n",
        "\n",
        "Min\n",
        "\n",
        "</th>\n",
        "\n",
        "<th>\n",
        "\n",
        "Max\n",
        "\n",
        "</th>\n",
        "\n",
        "</tr>\n",
        "\n",
        "</tfoot>\n",
        "\n",
        "</table>"
      ],
      "id": "da3f6fcb-4645-4f08-9b19-aac90c2b1817"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour la question 2, l’objectif est d’obtenir\n",
        "l’une des images suivantes:"
      ],
      "id": "91711545-1668-43db-a050-a0561fae3be2"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!---![](featured.jpg)---->"
      ],
      "id": "a4edb885-f091-47a2-aede-d35ce363075d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Version guidée\n",
        "\n",
        "Les prochaines parties permettront de faire l’exercice ci-dessus\n",
        "étape par étape,\n",
        "de manière guidée.\n",
        "\n",
        "Nous souhaitons tout d’abord obtenir les\n",
        "informations personnelles de tous\n",
        "les pokemons sur [pokemondb.net](http://pokemondb.net/pokedex/national).\n",
        "\n",
        "Les informations que nous aimerions obtenir au final pour les pokemons sont celles contenues dans 4 tableaux :\n",
        "\n",
        "-   Pokédex data\n",
        "-   Training\n",
        "-   Breeding\n",
        "-   Base stats\n",
        "\n",
        "Nous proposons ensuite de récupérer et afficher les images.\n",
        "\n",
        "### Etape 1: constituer un DataFrame de caractéristiques"
      ],
      "id": "f101ab24-d676-4dbf-90be-bd1f16350d45"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2b : Les pokémons (version guidée)</h3>\n",
        "\n",
        "Pour récupérer les informations, le code devra être divisé en plusieurs étapes :\n",
        "\n",
        "1.  Trouvez la page principale du site et la transformer en un objet intelligible pour votre code.\n",
        "    Les fonctions suivantes vous seront utiles :\n",
        "\n",
        "-   `urllib.request.Request`\n",
        "-   `urllib.request.urlopen`\n",
        "-   `bs4.BeautifulSoup`\n",
        "\n",
        "1.  Créez une fonction qui permet de récupérer la page d’un pokémon à partir de son nom.\n",
        "\n",
        "2.  A partir de la page de `bulbasaur`, obtenez les 4 tableaux qui nous intéressent :\n",
        "\n",
        "-   on va chercher l’élément suivant : `('table', { 'class' : \"vitals-table\"})`\n",
        "-   puis stocker ses éléments dans un dictionnaire\n",
        "\n",
        "1.  Récupérez par ailleurs la liste de noms des pokémons qui nous permettra de faire une boucle par la suite. Combien trouvez-vous de pokémons ?\n",
        "\n",
        "2.  Ecrire une fonction qui récupère l’ensemble des informations sur les dix premiers pokémons de la liste et les intègre dans un `DataFrame`\n",
        "\n",
        "</div>"
      ],
      "id": "a87e4846-c7ac-4619-afd7-3b0a5b446e72"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A l’issue de la question 3,\n",
        "vous devriez obtenir une liste de caractéristiques proche de celle-ci:\n",
        "\n",
        "La structure est ici en dictionnaire, ce qui est pratique.\n",
        "\n",
        "Enfin, vous les\n",
        "informations sur les dix premiers pokémons de la liste intégrées dans un\n",
        "`DataFrame` prendront l’aspect suivant:\n",
        "\n",
        "### Etape 2: récupérer et afficher des photos de Pokemon\n",
        "\n",
        "Nous aimerions que vous récupériez également les images des 5 premiers pokémons\n",
        "et que vous les enregistriez dans un dossier."
      ],
      "id": "6e049332-69a4-4974-b1e1-4cef0469fc06"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2b : Les pokémons (version guidée)</h3>\n",
        "\n",
        "-   Les URL des images des pokemon prennent la forme *“https://img.pokemondb.net/artwork/{pokemon}.jpg”*.\n",
        "    Utiliser les modules `requests` et `shutil` pour télécharger\n",
        "    et enregistrer en local les images.\n",
        "-   Importer ces images stockées au format JPEG dans `Python` grâce à la fonction `imread` du package `skimage.io`\n",
        "\n",
        "</div>"
      ],
      "id": "edd16ca4-7ce3-4b45-b83a-8f34e9d70fcf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `Selenium` : mimer le comportement d’un utilisateur internet\n",
        "\n",
        "Jusqu’à présent,\n",
        "nous avons raisonné comme si nous connaissions toujours l’url qui nous intéresse.\n",
        "De plus, les pages que nous visitons sont **“statiques”**,\n",
        "elles ne dépendent pas d’une action ou d’une recherche de l’internaute.\n",
        "\n",
        "Nous allons voir à présent comment nous en sortir pour remplir\n",
        "des champs sur un site *web* et récupérer ce qui nous intéresse.\n",
        "La réaction d’un site *web* à l’action d’un utilisateur passe régulièrement par\n",
        "l’usage de `JavaScript` dans le monde du développement *web*.\n",
        "Le *package* [Selenium](https://pypi.python.org/pypi/selenium) permet\n",
        "de reproduire, depuis un code automatisé, le comportement\n",
        "manuel d’un utilisateur. Il permet ainsi\n",
        "d’obtenir des informations du site qui ne sont pas dans le\n",
        "code `HTML` mais qui apparaissent uniquement à la suite de\n",
        "l’exécution de script `JavaScript` en arrière plan.\n",
        "\n",
        "`Selenium` se comporte comme un utilisateur *lambda* sur internet :\n",
        "il clique sur des liens, il remplit des formulaires, etc.\n",
        "\n",
        "## Premier exemple en scrapant un moteur de recherche\n",
        "\n",
        "Dans cet exemple, nous allons essayer d’aller sur le\n",
        "site de [Bing Actualités](https://www.bing.com/news)\n",
        "et entrer dans la barre de recherche un sujet donné.\n",
        "Pour tester, nous allons faire une recherche avec le mot-clé **“Trump”**.\n",
        "\n",
        "L’installation de `Selenium` nécessite d’avoir `Chromium` qui est un\n",
        "navigateur Google Chrome minimaliste.\n",
        "La version de [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/)\n",
        "doit être `>= 2.36` et dépend de la version de `Chrome` que vous avez sur votre environnement\n",
        "de travail. Pour installer cette version minimaliste de `Chrome` sur un environnement\n",
        "`Linux`, vous pouvez\n",
        "vous référer à l’encadré dédié"
      ],
      "id": "8f7e5d3f-ec6d-4420-93c9-9236e4ea3c4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> `Installation de Selenium`</h3>\n",
        "\n",
        "D’abord, il convient d’installer les dépendances.\n",
        "Sur `Colab`, vous pouvez utiliser les commandes suivantes:\n",
        "\n",
        "``` python\n",
        "!sudo apt-get update\n",
        "!sudo apt install -y unzip xvfb libxi6 libgconf-2-4 -y\n",
        "!sudo apt install chromium-chromedriver -y\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "```\n",
        "\n",
        "Si vous êtes sur le `SSP-Cloud`, vous pouvez\n",
        "exécuter les commandes suivantes:\n",
        "\n",
        "``` python\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n",
        "!sudo apt-get update\n",
        "!sudo -E apt-get install -y /tmp/chrome.deb\n",
        "!pip install chromedriver-autoinstaller selenium\n",
        "\n",
        "import chromedriver_autoinstaller\n",
        "chromedriver_autoinstaller.install()\n",
        "```\n",
        "\n",
        "Vous pouvez ensuite installer `Selenium`. Par\n",
        "exemple, depuis une\n",
        "cellule de `Notebook`:\n",
        "\n",
        "``` python\n",
        "!pip install selenium\n",
        "```\n",
        "\n",
        "</div>"
      ],
      "id": "3d88a61f-fe5a-4e62-b145-c32957233798"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Après avoir installé `Chromium`,\n",
        "il est nécessaire d’indiquer à `Python` où\n",
        "le trouver. Si vous êtes sur `Linux` et que vous\n",
        "avez suivi les consignes précédentes, vous\n",
        "pouvez faire:"
      ],
      "id": "f1ad762c-3c12-44ff-aa4c-a89306bf8a0d"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "import selenium\n",
        "path_to_web_driver = \"chromedriver\""
      ],
      "id": "404dcb60"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En premier lieu, il convient d’initialiser le comportement\n",
        "de `Selenium` en répliquant les paramètres\n",
        "du navigateur. Pour cela, on va d’abord initialiser\n",
        "notre navigateur avec quelques options:"
      ],
      "id": "11bff343-c6b5-4938-b10c-f1dc93bc9ff5"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "#chrome_options.add_argument('--verbose') "
      ],
      "id": "73258937"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Puis on lance le navigateur:"
      ],
      "id": "368c2d5b-c98c-4e37-bf7e-f1b175fa825a"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "browser = webdriver.Chrome(executable_path=path_to_web_driver,\n",
        "                           options=chrome_options)"
      ],
      "id": "771333ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On va sur le site de `Bing Actualités`, et on lui indique le mot clé que nous souhaitons chercher.\n",
        "En l’occurrence, on s’intéresse aux actualités de Donald Trump.\n",
        "Après avoir inspecté la page depuis les outils de développement du navigateur,\n",
        "on voit que la barre de recherche est un élement du code appelé `q` (comme *query*).\n",
        "On va ainsi demander à `selenium` de chercher cet élément:"
      ],
      "id": "cbd34a59-1de6-4197-8d34-80a12b30838e"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "browser.get('https://www.bing.com/news')\n",
        "\n",
        "search = browser.find_element(\"name\", \"q\")\n",
        "print(search)\n",
        "print([search.text, search.tag_name, search.id])\n",
        "\n",
        "# on envoie à cet endroit le mot qu'on aurait tapé dans la barre de recherche\n",
        "search.send_keys(\"Trump\")\n",
        "\n",
        "search_button = browser.find_element(\"xpath\", \"//input[@id='sb_form_go']\") \n",
        "search_button.click()"
      ],
      "id": "5cb8d4f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`selenium` permet de capturer l’image qu’on verrait dans le navigateur\n",
        "avec `get_screenshot_as_png`. Cela peut être utile pour vérifier qu’on\n",
        "a fait la bonne action:"
      ],
      "id": "7ec10209-e243-4a79-bbc5-c8f5c85c74bb"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "png = browser.get_screenshot_as_png()"
      ],
      "id": "4afb4bfc"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(png, width='500')"
      ],
      "id": "ab8d2f07"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enfin, on peut extraire les résultats. Plusieurs\n",
        "méthodes sont disponibles. La méthode la plus\n",
        "pratique, lorsqu’elle est disponible,\n",
        "est d’utiliser le `XPath` qui est un chemin\n",
        "non ambigu pour accéder à un élement. En effet,\n",
        "plusieurs éléments peuvent partager la même classe ou\n",
        "le même attribut ce qui peut faire qu’une recherche\n",
        "de ce type peut renvoyer plusieurs échos.\n",
        "Pour déterminer le `XPath` d’un objet, les outils\n",
        "de développeurs de votre site *web* sont pratiques.\n",
        "Par exemple, sous `Firefox`, une fois que vous\n",
        "avez trouvé un élément dans l’inspecteur, vous\n",
        "pouvez faire `click droit > Copier > XPath`."
      ],
      "id": "5d2e182f-37b3-4301-a6c7-e87def5d5709"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "links = browser.find_elements(\"xpath\", \"//div/a[@class='title'][@href]\")\n",
        "\n",
        "results = []\n",
        "for link in links:\n",
        "    try:\n",
        "        url = link.get_attribute('href')\n",
        "    except StaleElementReferenceException as e:\n",
        "        print(\"Issue with '{0}' and '{1}'\".format(url, link))\n",
        "        print(\"It might be due to slow javascript which produces the HTML page.\")\n",
        "    results.append(url)"
      ],
      "id": "847052b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enfin, pour mettre fin à notre session, on demande\n",
        "à `Python` de quitter le navigateur"
      ],
      "id": "dec16fe5-1f90-46ab-89bc-d3f2c4f3f386"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "browser.quit()"
      ],
      "id": "d82ecb88"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On a obtenu les résultats suivants:"
      ],
      "id": "cbd4a143-4c37-4790-a878-676dbdf66547"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(results)"
      ],
      "id": "f558bd68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les autres méthodes utiles de `Selenium`:\n",
        "\n",
        "`find_element(****).click()` \\| Une fois qu’on a trouvé un élément réactif, notamment un bouton, on peut cliquer dessus pour activer une nouvelle page \\|  \n",
        "`find_element(****).send_keys(\"toto\")` \\| Une fois qu’on a trouvé un élément, notamment un champ où s’authentifier, on peut envoyer une valeur, ici *“toto”*.\n",
        "\n",
        "## Utiliser selenium pour jouer à 2048\n",
        "\n",
        "Dans cet exemple, on utilise le module pour que `Python`\n",
        "appuie lui même sur les touches du clavier afin de jouer à 2048.\n",
        "\n",
        "Note : ce bout de code ne donne pas une solution à 2048,\n",
        "il permet juste de voir ce qu’on peut faire avec `Selenium`"
      ],
      "id": "b01f470f-2717-4b22-b487-b162e07f4a63"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "# on ouvre la page internet du jeu 2048\n",
        "\n",
        "browser = webdriver.Chrome(executable_path=path_to_web_driver,\n",
        "                           options=chrome_options)\n",
        "browser.get('https://play2048.co//')\n",
        "\n",
        "# Ce qu'on va faire : une boucle qui répète inlassablement la même chose : haut / droite / bas / gauche\n",
        "\n",
        "# on commence par cliquer sur la page pour que les touches sachent \n",
        "browser.find_element(\"class name\", 'grid-container').click()\n",
        "grid = browser.find_element(\"tag name\", 'body')\n",
        "\n",
        "# pour savoir quels coups faire à quel moment, on crée un dictionnaire\n",
        "direction = {0: Keys.UP, 1: Keys.RIGHT, 2: Keys.DOWN, 3: Keys.LEFT}\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    try: # on vérifie que le bouton \"Try again\" n'est pas là - sinon ça veut dire que le jeu est fini\n",
        "        retryButton = browser.find_element(\"link text\",'Try again')\n",
        "        scoreElem = browser.find_element(\"class name\", 'score-container')\n",
        "        break\n",
        "    except:\n",
        "        #Do nothing.  Game is not over yet\n",
        "        pass\n",
        "    # on continue le jeu - on appuie sur la touche suivante pour le coup d'après\n",
        "    count += 1\n",
        "    grid.send_keys(direction[count % 4]) \n",
        "    time.sleep(0.1)\n",
        "\n",
        "print('Score final : {} en {} coups'.format(scoreElem.text, count))    \n",
        "browser.quit()"
      ],
      "id": "b846d2c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercices supplémentaires\n",
        "\n",
        "## Récupérer les noms et âges des ministres français\n",
        "\n",
        "Pour cet exercice, on propose de scraper la liste des ministres français depuis le [site du gouvernement](https://www.gouvernement.fr/composition-du-gouvernement). L’objectif sera, *in fine* de faire un graphique qui représente la distribution de leurs âges.\n",
        "La solution pour cet exercice a été proposée\n",
        "par [Tien-Thinh](https://github.com/tttienthinh)\n",
        "et [Antoine Palazzolo](https://github.com/antoine-palazz).\n",
        "\n",
        "Pour être en mesure de faire cet exercice, il est\n",
        "recommandé d’installer le package `dateparser`"
      ],
      "id": "c2cc3c37-a7cd-4118-bb45-7876df640135"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install dateparser\n",
        "#depuis un notebook. En ligne de commande, retirer le !"
      ],
      "id": "e28197ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour cet exercice, nous proposons d’utiliser les *packages*\n",
        "suivants:"
      ],
      "id": "3073d7fa-f84c-47fc-a456-94747f7891e2"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "import re, datetime\n",
        "from dateutil.parser import parse as parse_dt\n",
        "import dateparser\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4"
      ],
      "id": "360a65b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous proposons également d’utiliser la fonction suivante\n",
        "pour calculer l’âge à partir de la date de naissance."
      ],
      "id": "06b65ffa-3b4f-4787-b50e-fe95a3a7159b"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def from_birth_to_age(birth):\n",
        "    today = datetime.date.today()\n",
        "    return today.year - birth.year - ((today.month, today.day) < (birth.month, birth.day))"
      ],
      "id": "a95db36a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice : Les ministres français </h3>\n",
        "\n",
        "1.  Créer des variables globales `url_gouvernement` et `url_gouvernement` qui représenteront,\n",
        "    respectivement, la racine de l’URL du site web et le chemin au sein de celui-ci ;\n",
        "2.  Utiliser `bs4` pour récupérer la composition du gouvernement, qui est contenue dans un `<div>`\n",
        "    ayant une classe *ad hoc*. Nommer cet objet `compo`\n",
        "3.  Utiliser `find_all` pour récupérer la liste des ministres dans `compo`. Nommer\n",
        "    cet objet `ministres`\n",
        "4.  Inspecter la structure des champs au sein de `ministres`. Répérer les id `biography`. Comme\n",
        "    la structure est générique, on va écrire une fonction `from_bio_to_age` sur laquelle on va itérer\n",
        "    pour chaque élément de la liste `ministres`. Cette fonction effectuera les opérations suivantes:\n",
        "    -   Remplacer les champs de dates de naissance non numériques (par exemple *“1er”*), en valeur numérique (par exemple 1).\n",
        "    -   Utiliser la regex `[0-3]?\\d \\S* \\d{4}` avec le *package* `re` pour extraire les dates\n",
        "        de naissance. Nommer l’objet `str_date`.\n",
        "    -   Appliquer `dateparser.parse` pour convertir sous forme de date\n",
        "    -   Appliquer `from_birth_to_age` pour transformer cette date de naissance en âge\n",
        "5.  Pour chaque élément de la liste `ministres`, faire une boucle (en introduisant un\n",
        "    `time.sleep(0.25)` entre chaque itération pour ne pas surcharger le site):\n",
        "    -   Récupérer les noms et prénoms, fonctions pour chaque ministre\n",
        "    -   Récupérer l’URL de la photo\n",
        "    -   Créer un URL pour chaque ministre afin d’appliquer la fonction\n",
        "        `from_bio_to_age`\n",
        "6.  Utiliser `matplotlib` ou `seaborn` pour faire un histogramme d’âge\n",
        "\n",
        "</div>"
      ],
      "id": "848995c3-03bb-4647-88bd-f7b8126b6858"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A l’issue de la question 4, on devrait\n",
        "retrouver les informations suivantes:"
      ],
      "id": "df2c9945-7c87-4849-861b-62780c1edabe"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Nous retrouvons ainsi {len(ministres)} ministres.\")"
      ],
      "id": "a0adcc11"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def from_bio_to_age(url):\n",
        "    html = urllib.request.urlopen(url).read()\n",
        "    page = bs4.BeautifulSoup(html)\n",
        "    s = page.find(\"div\", {\"id\":\"biography\"}).text.replace(\"1er\", \"1\") # un peu ad hoc\n",
        "    expression = re.compile(\"[0-3]?\\d \\S* \\d{4}\") # renvoie parfois des dates autres que dates de naissance\n",
        "    str_date = expression.findall(s)[0]\n",
        "    date_de_naissance = dateparser.parse(str_date).date()\n",
        "    return from_birth_to_age(date_de_naissance)"
      ],
      "id": "a303d17c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*In fine*, on obtient une liste dont le premier élément\n",
        "prend la forme suivante:"
      ],
      "id": "3e30898b-19ef-4d27-87d5-c8be633d8b0d"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "liste[0]"
      ],
      "id": "968ddcd6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalement, le `DataFrame` pourra être\n",
        "structuré sous la forme suivante. On va éliminer\n",
        "les âges égaux à 0 sont qui sont des erreurs\n",
        "de scraping:\n",
        "lorsque la date de naissance complète n’est pas disponible\n",
        "sur la biographie d’un ministre."
      ],
      "id": "7f933f49-0246-4e18-a544-f4a38e07627d"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(liste)\n",
        "df = df.loc[df['Age'] != 0]\n",
        "df.head(3)"
      ],
      "id": "28718299"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalement, l’histogramme aura l’aspect suivant:"
      ],
      "id": "1eed9793-ff63-42de-a943-941b4721954f"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(df.Age, bins=np.arange(25, 80, 4))"
      ],
      "id": "32b142f6"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  }
}