{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction à dask grâce aux données DVF\n",
        "\n",
        "Pour essayer les exemples présents dans ce tutoriel :"
      ],
      "id": "1f6cebda-d945-4576-9fad-aa0894836b71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p class=\"badges\">\n",
        "\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/07_dask.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
        "<a href=\"https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/07_dask.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter\" alt=\"Download\"></a>\n",
        "<a href=\"https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/07_dask.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter\" alt=\"nbviewer\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABnotebooks/course/manipulation/07_dask.ipynb%C2%BB&security.allowlist.enabled=false\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&amp;color=yellow?logo=Python\" alt=\"Onyxia\"></a><br>\n",
        "<a href=\"https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath={binder_path}\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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\" alt=\"Binder\"></a>\n",
        "<a href=\"http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/notebooks/course/manipulation/07_dask.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "<a href=\"https://github.dev/linogaliana/python-datascientist/notebooks/course/manipulation/07_dask.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20Visual%20Studio%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc\" alt=\"githubdev\"></a>\n",
        "\n",
        "</p>\n",
        "\n",
        "</p>"
      ],
      "id": "ca5bf202-4f71-4a27-8d27-5d4f102cde62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La documentation complète sur `Dask` se trouve sur <https://docs.dask.org/>.\n",
        "\n",
        "Le projet requiert l’installation de `dask`. Afin d’avoir\n",
        "la distribution complète on utilise la commande suivante:\n",
        "\n",
        "``` shell\n",
        "pip install dask[complete]\n",
        "```\n",
        "\n",
        "# Pourquoi utiliser `Dask` ?\n",
        "\n",
        "On peut se référer à la page https://docs.dask.org/en/stable/why.html\n",
        "\n",
        "Plusieurs points sont mis en avant dans la documentation officielle et sont résumés ci-dessous:\n",
        "- `Dask` ressemble fortement en termes de syntaxe à `pandas` et `numpy` ;\n",
        "- `Dask` peut être utilisé sur un ordinateur seul ou sur un *cloud cluster*. Avec `Dask`, on peut traiter des bases de 100GB sur un ordinateur portable, voire même 1TB sans même avoir besoin d’un cluster *big data* ;\n",
        "- `Dask` requiert peu de temps d’installation puisqu’il peut être installé avec le gestionnaire de *packages* `conda` (il est même livré dans la distribution par défaut d’Anaconda)\n",
        "\n",
        "## Comment `Dask` se compare à `Spark` ?\n",
        "\n",
        "Dans le monde du *big-data*, un écosystème concurrent existe: [`Spark`](https://spark.apache.org/). Globalement, lorsqu’on a compris la logique\n",
        "de l’un, il est très facile de faire la transition vers l’autre si besoin<a name=\"cite_ref-1\"></a>[<sup>\\[1\\]</sup>](#cite_note-1). Pour ma part, j’ai principalement fait du `Spark` sur\n",
        "des [données de téléphonie de plusieurs TB](https://www.linogaliana.fr/publication/2020-segregation/). En fait, la logique sera la même que celle de `Dask` sur données moins volumineuses."
      ],
      "id": "189b1662-fc5e-4ce0-9c34-d3e25b995422"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1)"
      ],
      "id": "b11422e0-6e1d-4be7-87b5-ffdd45b355fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sauf si on doit mettre en oeuvre soi-même l’infrastructure `Spark`, ce qui relève plus des compétences du *data architect* que du *data scientist*.\n",
        "\n",
        "-   `Spark` est écrit en `Scala` à l’origine. Le package [`pyspark`](https://spark.apache.org/docs/latest/api/python/) permet d’écrire en `Python` et s’assure de la traduction en `Python` afin d’interagir avec les machines virtuelles `Java` (JVM) nécessaires pour la parallélisation des opérations `Spark`. `Dask` est quant à lui écrit en Python, ce qui est un écosystème plus léger. Pour gagner en performance, il permet d’interagir avec du code C/C++ entre autres ;\n",
        "-   L’installation de `Spark` est plus lourde que celle de `Dask`\n",
        "-   `Spark` est un projet Apache en lui-même alors que `Dask` intervient comme une composante de l’univers `Python`;\n",
        "-   `Spark` est un peu plus vieux (2010 versus 2014 pour `Dask`) ;\n",
        "-   `Spark` permet de très bien faire des opérations classiques SQL et des ETLs, et proposer ses propres librairies de parallélisation de modèles de *machine learning*. Pour faire du *machine learning* avec `Spark` il faut aller piocher dans `Spark MLLib`. `Dask` permet quant à lui de bien interagir avec `scikit-learn` et de faire de la modélisation.\n",
        "\n",
        "Globalement, il faut retenir que `Dask` comme `Spark` ne sont intéressants que pour des données dont le traitement engendre des problèmes de RAM. Autrement, il\n",
        "vaut mieux se contenter de `pandas`.\n",
        "\n",
        "# Démonstration de quelques *features* de `Dask`\n",
        "\n",
        "## Présentation du `Dask.DataFrame`\n",
        "\n",
        "Nous allons utiliser les [données immobilières `DVF`](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/) pour montrer quelques éléments clefs de `Dask`."
      ],
      "id": "dead4c33-6e1b-431a-ab57-04ebddf62f7a"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import dvf files \n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "\n",
        "def import_dvf():\n",
        "    d_urls = {\"2019\" : 'https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2',\n",
        "                 \"2020\" : \"https://www.data.gouv.fr/fr/datasets/r/90a98de0-f562-4328-aa16-fe0dd1dca60f\",\n",
        "                 \"2021\": \"https://www.data.gouv.fr/fr/datasets/r/817204ac-2202-4b4a-98e7-4184d154d98c\"}\n",
        "    dfs = {}\n",
        "    for k in d_urls.keys():\n",
        "        dfs[k] = pd.read_csv(d_urls[k], sep = \"|\", decimal=\",\")\n",
        "        dfs[k][\"year\"] = k\n",
        "    df = pd.concat(dfs).reset_index()\n",
        "    df = df.drop([\"level_0\", \"level_1\"], axis=1)\n",
        "    return df"
      ],
      "id": "59af4e99-d333-4b7b-9943-3726ad6e7db0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans un premier temps, on va utiliser `pandas` pour\n",
        "importer nos données, ces dernières tenant en mémoire\n",
        "sur un ordinateur normalement doté en RAM:"
      ],
      "id": "27ee7a31-9d87-484f-af5b-51798209e762"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf = import_dvf()\n",
        "dvf.shape\n",
        "dvf.head()"
      ],
      "id": "426ffe96-346c-4f3c-ae7f-c1e8666deb20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ici on travaille sur un `DataFrame` d’environ 10 millions de lignes et 44 variables.\n",
        "L’objet `dvf` est un `pandas.DataFrame`\n",
        "qui tient en mémoire sur le SSP-Cloud."
      ],
      "id": "576e2bc6-9395-47c7-a494-d7bf598a27aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "\n",
        "On aurait pu lire directement les csv dans un `dask.DataFrame` avec le `read_csv` de `dask`. Comme exercice, vous pouvez essayer de le faire.\n",
        "\n",
        "</div>"
      ],
      "id": "261b98ea-18fb-428c-9c33-7d867d117596"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut créer une structure `Dask` directement à partir d’un `DataFrame` `pandas` avec la méthode `from_pandas`."
      ],
      "id": "94938e85-4e31-460b-b463-73c481f7ef5f"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd = dd.from_pandas(dvf, npartitions=10) \n",
        "dvf_dd"
      ],
      "id": "ed1953ac-58df-40a1-a0cb-f73c87e7efc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour souligner la différence avec un `pandas.DataFrame`,\n",
        "l’affichage diffère. Seule la structure du `dask.DataFrame`\n",
        "est affichée et non son contenu car les données\n",
        "`dask` ne sont pas chargées en mémoire."
      ],
      "id": "5f088a4b-abc2-4de6-bf1c-55f54dd0df71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\" role=\"alert\">\n",
        "\n",
        "Attention, `Dask` ne peut créer un `Dask.DataFrame` à partir d’un `pandas.DataFrame` multi-indexé.\n",
        "\n",
        "Dans ce cas il a fallu faire un `reset_index()` pour avoir un unique index.\n",
        "\n",
        "</div>"
      ],
      "id": "c71ae3bf-c516-4a89-a735-2f04ff9d0977"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On a ainsi la structure de notre `dask.DataFrame`, soit environ 10M de lignes, avec 44 colonnes en 10 partitions, soit environ 1M d’observations par partition.\n",
        "\n",
        "Il faut savoir que `Dask` produit des `Array`, `Bag` et `DataFrames`, qui fonctionnent comme dans `Numpy` et `Pandas` (il est possible de créer d’autres structures *ad hoc*, cf plus loin).\n",
        "`Dask`, comme `Spark` et en fait comme la plupart des *frameworks* permettant de\n",
        "traiter des données plus volumineuses que la RAM disponible,\n",
        "repose sur le principe du partitionnement et de la parallélisation\n",
        "des opérations. Les données ne sont jamais importées dans leur\n",
        "ensemble mais par bloc. Un plan des opérations à effectuer est\n",
        "ensuite appliquer sur chaque bloc (nous reviendrons\n",
        "sur ce principe), indépendamment. La particularité de `Dask`,\n",
        "par rapport à `Spark`,\n",
        "est que chaque bloc est un `pandas.DataFrame`, ce qui\n",
        "rend très facile l’application de manipulations de données\n",
        "traditionnelles sur des sources volumineuses:\n",
        "\n",
        "![](https://docs.dask.org/en/stable/_images/dask-dataframe.svg)\n",
        "\n",
        "Le site de `Dask` cite une règle qui est la suivante :\n",
        "\n",
        "> *“Have 5 to 10 times as much RAM as the size of your dataset”*,\n",
        ">\n",
        "> @mckinney2017apache, [*10 things I hate about pandas*](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)\n",
        "\n",
        "Sur disque, en sauvegardant en `CSV`, on\n",
        "obtient une base de 1.4GB. Si l’on suit la règle du pouce donnée plus haut, on va avoir besoin d’une RAM entre 7-14GB pour traiter la donnée, en fonction de nos traitements qui seront plus ou moins intensifs. Autrement dit, si on a moins de 8GB de RAM, il devient intéressant de faire appel à `dask`, sinon il vaut mieux privilégier `pandas` (sauf si on fait des\n",
        "traitements très intensifs en calculs)."
      ],
      "id": "d01e0789-d591-4510-be14-304db15477b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "\n",
        "Le choix du nombre de partition (10) est arbitraire ici. Bien qu’on puisse\n",
        "trouver des règles du pouce pour fixer un nombre optimal de\n",
        "partitions, cela dépend de beaucoup de facteurs et, en pratique,\n",
        "rien ne remplace l’essai-erreur. Par exemple, [la documentation `Dask` recommande des blocs d’environ\n",
        "100MB](https://docs.dask.org/en/stable/dataframe-best-practices.html)\n",
        "ce qui peut convenir pour des ordinateurs à la RAM limitée mais n’a pas\n",
        "forcément de sens pour des machines ayant 16GB de RAM.\n",
        "Un nombre important de partition va permettre de faire des opérations\n",
        "sur des petits blocs de données, ce qui permettra de gagner en vitesse\n",
        "d’exécution. Le prix à payer est beaucoup *d’input/output* car\n",
        "`Dask` va passer du temps à lire beaucoup de blocs de données et écrire\n",
        "des bases intermédiaires.\n",
        "\n",
        "</div>"
      ],
      "id": "7d533b07-0d85-4bb6-bb0b-e6da3ce531af"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut accéder aux index que couvrent les partitions de la manière suivante:"
      ],
      "id": "e2ee4419-de99-4898-b251-5d1f378de6af"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd.divisions"
      ],
      "id": "8e72d524-69bd-465d-80e4-6f52470f3a94"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et on peut directement accéder à une partition grâce aux crochets `[]`:"
      ],
      "id": "9ce3f83d-4011-4441-a1e9-3cd124720c2f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd.partitions[0]"
      ],
      "id": "93ccf466-b4bf-4bf1-bc1b-0cdd14723274"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## La *“lazy evaluation”*\n",
        "\n",
        "`Dask` fait de la **“lazy evaluation”**. Cela signifie que le résultat n’est calculé que si on le demande explicitement. Dans le cas, contraire, ce que l’on appelle un `dask` **task graph** est produit (on verra plus bas comment voir ce *graph*).\n",
        "\n",
        "Pour demander explicitement le résultat d’un calcul, il faut utiliser la\n",
        "méthode `compute`.\n",
        "A noter que certaines méthodes vont déclencher un `compute` directement, comme par exemple `len` ou `head`.\n",
        "\n",
        "Par exemple, pour afficher le contenu des 100 premières lignes :"
      ],
      "id": "2053a0bd-d974-472c-ad87-2d8be9a1e028"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd.loc[0:100,:].compute()"
      ],
      "id": "5328314f-fe7c-4de5-bc74-f428a43dbc0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce qui est pratique avec `dask.dataframe` c’est que de nombreuses méthodes sont semblables à celles de `pandas`. Par exemple, si l’on souhaite connaitre les types de locaux présents dans la base en 2019:"
      ],
      "id": "bbb8ace9-19b8-47b3-8f38-2b8e172ac95b"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd.loc[:,\"Type local\"].value_counts().compute()"
      ],
      "id": "8fda6400-9d24-4f88-ba27-0ed731da049d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A titre de comparaison, comparons les temps de calculs entre `pandas` et `dask` ici:"
      ],
      "id": "3991c6f7-c5ef-4d2a-baa2-8250b5f0ddd9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "dvf_dd.loc[:,\"Type local\"].value_counts().compute()\n",
        "print(timeit.default_timer() - start_time)"
      ],
      "id": "98307093-b4cc-41c8-8ea7-936645a9eaeb"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "dvf.loc[:,\"Type local\"].value_counts()\n",
        "print(timeit.default_timer() - start_time)"
      ],
      "id": "90fbfe3f-cef8-4a56-9f54-f4c6c24ae96b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On se rend compte que le `pandas.DataFrame` a un temps de calcul plus court, mais c’est parce que `dask` va nous servir avant tout à lire des bases dont le traitement excède notre RAM. Donc, cette comparaison n’existera tout simplement pas car le `pandas.DataFrame` n’aura pas été chargé en RAM. **On voit dans cet exemple que lorsque le traitement du `DataFrame` tient en RAM, l’utilisation de `Dask` est inutile**.\n",
        "\n",
        "Les méthodes dans `Dask` peuvent être chainées, comme dans `pandas`, par exemple, on pourra écrire:"
      ],
      "id": "d6c86cb9-c4bf-47c8-bf37-907e7da4007c"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_by_year = dvf_dd.loc[~dvf_dd[\"Surface terrain\"].isna(),[\"Surface terrain\", \"year\"]].groupby(\"year\").mean()"
      ],
      "id": "fb660451-0373-485d-a812-be624e11e6a9"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_by_year.compute()"
      ],
      "id": "64cbf475-28d3-4fed-969d-0e75eb532ca8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le principe de la *lazy evaluation* est donc d’annoncer à `Dask`\n",
        "qu’on va effectuer une série d’opération qui ne vont se réaliser\n",
        "que lorsqu’on fera un appel à `compute`. `Dask`, quant à lui,\n",
        "se chargera d’optimiser les traitements.\n",
        "Comme le plan d’action peut devenir difficile à suivre si on\n",
        "désire effectuer beaucoup d’opérations enchaînées, on peut\n",
        "vouloir visualiser le *graph* de computation de `dask`.\n",
        "Avec celui-ci, on voit toutes les étapes que jusqu’ici `dask` n’a pas *executé*\n",
        "et qu’il va devoir exécuter pour calculer le résultat (`compute()`)."
      ],
      "id": "e404c568-0598-4969-8137-9293399e6e02"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    mean_by_year.dask\n",
        ")"
      ],
      "id": "266d990c-47b0-4f46-9356-763e7e9d2559"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{% rawhtml %}}\n",
        "\n",
        "{{% /rawhtml %}}\n",
        "\n",
        "En l’occurence on voit l’enchainement des étapes\n",
        "`from_pandas()`, `getitem`, `isna`, `inv` et `loc-series` qui résultent de nos filtres sur le `DataFrame`. Ensuite,\n",
        "on voit les étapes de `groupby` et, enfin, pour calculer la moyenne il convient de faire la somme et la division. Toutes ces étapes vont être effectuées quand on appelle `compute()` et pas avant (*lazy evaluation*).\n",
        "\n",
        "Afin de voir la structure du `dask DataFrame` on peut utiliser la méthode `visualize()`"
      ],
      "id": "ce106c68-80f9-4dae-8660-0ea1018da375"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd.visualize() # attention graphviz est requis"
      ],
      "id": "ad07f0be-3aea-4850-92cb-f6ca76ced123"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":warning: `graphviz` est requis. S’il n’est pas installé dans votre environnement, faire `pip install graphviz`\n",
        "\n",
        "Pour construire de véritables *pipelines* de données,\n",
        "le principe du [`pipe` de `pandas`](https://docs.dask.org/en/stable/generated/dask.dataframe.Series.pipe.html) évoqué [dans cette partie du cours](#pandas/#les-pipe) et celui des [pipelines scikit](https://ml.dask.org/compose.html), évoqué [dans un chapitre dédié](#pipeline-scikit/)\n",
        "ont été importés dans `dask`.\n",
        "\n",
        "## Problèmes de lecture dus à des types problématiques\n",
        "\n",
        "La méthode `read_csv` de `dask` va inférer les types du `DataFrame` à partir d’échantillon, et va les implémenter sur tout le `DataFrame` seulement au moment d’une étape `compute`.\n",
        "\n",
        "Il peut donc y avoir des erreurs de types dûs à un échantillon ne prenant pas en compte certains cas particuliers, causant des erreurs dans la lecture du fichier.\n",
        "\n",
        "Dans ce cas, et comme de manière générale avec `pandas`, il peut être recommandé de faire appel au paramètre `dtype` de `read_csv` - qui est un `dict` - (la doc de `dask` nous dit aussi que l’on peut augmenter la taille de l’échantllon `sample`).\n",
        "\n",
        "# Utiliser `dask` avec le format parquet\n",
        "\n",
        "Le format `parquet` tend à devenir le format\n",
        "de référence dans le monde de la *data-science*.\n",
        "Une présentation extensive de celui-ci est disponible\n",
        "dans le [chapitre dédié](#reads3).\n",
        "\n",
        "`dask` permet de lire le format `parquet`, et plus précisément d’utiliser des fonctionnalités spécifiques à ce format. La lecture et l’écriture en `parquet` reposent par défaut sur `pyarrow`. On peut aussi utiliser `fastparquet` et préciser dans la lecture/écriture ce que l’on souhaite des deux."
      ],
      "id": "b66459f4-e8aa-428e-9ca8-42a2dfe82bd7"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_net = dvf.loc[:,[ 'Date mutation', 'Nature mutation', 'Valeur fonciere', 'Commune', \n",
        "       'Code commune', 'Type local', 'Identifiant local', 'Surface reelle bati',\n",
        "       'Nombre pieces principales', 'Nature culture',\n",
        "       'Nature culture speciale', 'Surface terrain', 'year']]"
      ],
      "id": "75d43e5e-b650-458d-8ce3-9d04fc0b37b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On va utiliser l’*engine* par défaut pour\n",
        "l’écriture de `parquet` qui est `pyarrow` (faire `pip install pyarrow` si vous ne l’avez pas déjà installé). `to_parquet` qui est une méthode `pandas` a été également étendue aux objets `dask`:"
      ],
      "id": "36c1ef57-16bd-4f7f-83e6-549b3860601b"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_net.to_parquet(\"dvf/\", partition_cols=\"year\")"
      ],
      "id": "62679575-5c6b-40c2-a40e-4e99d42f0411"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un `DataFrame` partitionné prendra une structure du type\n",
        "de [celle-ci](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery):\n",
        "\n",
        "``` raw\n",
        "path\n",
        "└── to\n",
        "    └── table\n",
        "        ├── gender=male\n",
        "        │   ├── ...\n",
        "        │   │\n",
        "        │   ├── country=US\n",
        "        │   │   └── data.parquet\n",
        "        │   ├── country=CN\n",
        "        │   │   └── data.parquet\n",
        "        │   └── ...\n",
        "        └── gender=female\n",
        "            ├── ...\n",
        "            │\n",
        "            ├── country=US\n",
        "            │   └── data.parquet\n",
        "            ├── country=CN\n",
        "            │   └── data.parquet\n",
        "            └── ...\n",
        "```\n",
        "\n",
        "On peut alors facilement traiter un sous-échantillon des données,\n",
        "par exemple l’année 2019:"
      ],
      "id": "57edadeb-52cc-4299-945d-052e37d5c87a"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_2019 = dd.read_parquet(\"dvf/year=2019/\", columns=[\"Date mutation\", \"Valeur fonciere\"]) # On peut sélectionner directement les deux colonnes"
      ],
      "id": "97b48104-e167-46c4-86e6-9b15b733fd4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lorsqu’il faudra passer à l’échelle, on changera le chemin en `\"dvf/`\n",
        "pour utiliser l’ensemble des données.\n",
        "\n",
        "# A quoi sert `persist` ?\n",
        "\n",
        "Par défaut, `compute` exécute l’ensemble du plan et ne conserve\n",
        "en mémoire que le résultat de celui-ci. Les données intermédiaires\n",
        "ne sont pas conservées. Si on désire réutiliser une partie de celui-ci,\n",
        "par exemple les premières étapes, on devra donc ré-effectuer\n",
        "les calculs.\n",
        "\n",
        "Il est possible de garder une partie des données en mémoire avec `persist()`. Les données sont sauvegardées dans des objets appelés `Futures`. Cela peut être intéressant si un bloc particulier de données est utilisé dans plusieurs `compute` ou si l’on a besoin de voir ce qu’il y a à l’intérieur souvent."
      ],
      "id": "02ecf1a6-925e-499b-b144-9066d3dee27c"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "dvf_dd_mem = dvf_dd.persist()"
      ],
      "id": "6b170dc1-0b4c-4129-923e-dde39e348120"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "dvf_dd_mem.head()"
      ],
      "id": "04e22e9c-bde4-4c3c-ac36-cacbaa5a04cf"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "dvf_dd.head()"
      ],
      "id": "1653df52-9c98-405f-8095-53d7a924a740"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On a bien un temps plus important avec le `dask DataFrame` initial, comparé avec celui sur lequel on a utilisé `persist`.\n",
        "\n",
        "Il existe un autre objet `dask`, les `Array` pour reprendre la logique de `numpy`. De la même manière qu’un `dask DataFrame` est en quelque sorte un ensemble de `pandas DataFrame`, un `dask Array` est un ensemble de `numpy Array` qui sont plus importants en taille que la RAM. On pourra utiliser les opérations courantes `numpy` avec `dask` de la même manière que le `dask DataFrame` réplique la logique du `pandas DataFrame`.\n",
        "\n",
        "#### **Aller plus loin**: Utiliser le decorator `dask.delayed` pour paralléliser du code\n",
        "\n",
        "Il est possible de paralléliser des fonctions par exemple en utilisant le **decorator** `dask.delayed`. Cela permet de rendre les fonctions `lazy`, lorsque l’on appelle la fonction, un delayed object est construit, pour avoir le résultat il faut faire un `compute`. Pour aller plus loin: <https://tutorial.dask.org/03_dask.delayed.html>."
      ],
      "id": "eac7e198-52f3-4a84-aac3-65ec84671249"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask\n",
        "import time \n",
        "# voici le resultat sans le decorator\n",
        "def aire_carre(longueur):\n",
        "    time.sleep(1)\n",
        "    return longueur**2\n",
        "\n",
        "def perimetre_carre(longueur):\n",
        "    time.sleep(1)\n",
        "    return 4*longueur\n",
        "\n",
        "def ajout_aire_perim(a, b):\n",
        "    return a + b\n",
        "start_time = timeit.default_timer()\n",
        "car1 = aire_carre(7)\n",
        "car2 = perimetre_carre(9)\n",
        "car3 = ajout_aire_perim(car1, car2)\n",
        "car3\n",
        "print(timeit.default_timer() - start_time)"
      ],
      "id": "69611011-39b6-4df9-ab03-1c2055e1f7d9"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dask.delayed\n",
        "def aire_carre(longueur):\n",
        "    time.sleep(1)\n",
        "    return longueur**2\n",
        "\n",
        "@dask.delayed\n",
        "def perimetre_carre(longueur):\n",
        "    time.sleep(1)\n",
        "    return 4*longueur\n",
        "\n",
        "@dask.delayed\n",
        "def ajout_aire_perim(a, b):\n",
        "    return a + b"
      ],
      "id": "b379f6b1-2150-4d88-9a91-41996613d25c"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "car1 = aire_carre(7)\n",
        "car2 = perimetre_carre(9)\n",
        "car3 = ajout_aire_perim(car1, car2)"
      ],
      "id": "86124b8e-236f-4af1-ad02-8669207342c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mais en fait rien n’a été calculé, si l’on souhaite le résultat, il faut appeler `compute`"
      ],
      "id": "20d898a3-bae3-42b6-a257-d8bce773f7bd"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "car3.compute()\n",
        "print(timeit.default_timer() - start_time)"
      ],
      "id": "1b634a42-1853-4771-b36e-d8f21e5f472a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ici l’intérêt est assez limité, mais on voit que l’on réduit quand même de 2 à 1 seconde le temps de calcul. Mais l’idée derrière est que l’on a transformé `car3` en un objet `Delayed`. Cela a généré un `task graph` permettant de paralléliser certaines opérations. Ici il est important de noter que les fonctions que l’on parallélise doivent mettre un certain temps, sinon il n’y aura pas de gain de performance (si on retire le `time.sleep` il n’y a pas de gain de performance car le fait de paralléliser rajoute en fait du temps vu que chaque fonction a un temps de calcul trop faible pour que la parallélisation soit intéressante)."
      ],
      "id": "423c67c3-9356-4743-81a1-2aeff14913e8"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "car3.visualize() # on peut visualiser le task graph et voir ce qui est fait en parallèle "
      ],
      "id": "29ebf766-418f-4fdb-bacf-e224e975cfe3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il y a des exercices intéressants dans la doc de `dask` sur les objets `Delayed`, notamment sur la parallélisation de séquence de traitement de données. Ils donnent l’exemple d’un ensemble de csv ayant le même format dont on veut résumer un indicateur final. On peut appliquer le decorator à une fonction permettant de lire le csv, puis utiliser une boucle for pour lire le ficher et appliquer les traitements. Ensuite, il faudra appeler compute sur l’objet final que l’on souhaite.\n",
        "\n",
        "Pour aller plus loin sur l’utilisation de `dask` sur un cluster voir <https://tutorial.dask.org/04_distributed.html>.\n",
        "\n",
        "# Remerciements\n",
        "\n",
        "Ce chapitre a été rédigé avec [Raphaële Adjerad](https://github.com/raphaeleadjerad)."
      ],
      "id": "7269a8b4-d03f-45db-9803-d00f1d88108a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  }
}