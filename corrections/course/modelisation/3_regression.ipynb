{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1629868a",
   "metadata": {},
   "source": [
    "#  Régression: une introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca517a2",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=«python-datascientist»&resources.requests.memory=«4Gi»&security.allowlist.enabled=false&init.personalInit=«https://raw.githubusercontent.com/linogaliana/python-datascientist/master/init_onyxia.sh»)\n",
    "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=/__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb)\n",
    "[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/3_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af163c81",
   "metadata": {},
   "source": [
    "Le précédent chapitre visait à proposer un premier modèle pour comprendre\n",
    "les comtés où le parti Républicain l'emporte. La variable d'intérêt étant\n",
    "bimodale (victoire ou défaite), on était dans le cadre d'un modèle de \n",
    "classification.\n",
    "\n",
    "Maintenant, sur les mêmes données, on va proposer un modèle de régression\n",
    "pour expliquer le score du parti Républicain. La variable est donc continue.\n",
    "Nous ignorerons le fait que ses bornes se trouvent entre 0 et 100 et donc \n",
    "qu'il faudrait, pour être rigoureux, transformer l'échelle afin d'avoir \n",
    "des données dans cet intervalle. \n",
    "\n",
    "Ce chapitre utilise toujours le même jeu de données, présenté dans l'[introduction\n",
    "de cette partie](https://linogaliana-teaching.netlify.app/modelisation/): les données de vote aux élections présidentielles US\n",
    "croisées à des variables socio-démographiques.\n",
    "Le code \n",
    "est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/master/content/course/modelisation/get_data.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/test-preprocessing/content/course/modelisation/get_data.py'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('getdata.py', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25709fb0",
   "metadata": {},
   "source": [
    "```\n",
    "## 3585\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef822f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import getdata\n",
    "votes = getdata.create_votes_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17036b",
   "metadata": {},
   "source": [
    "# Principe général\n",
    "\n",
    "Le principe général de la régression consiste à trouver une loi $h_\\theta(X)$\n",
    "telle que\n",
    "\n",
    "$$\n",
    "h_\\theta(X) = \\mathbb{E}_\\theta(Y|X)\n",
    "$$\n",
    "Cette formalisation est extrêmement généraliste et ne se restreint d'ailleurs\n",
    "par à la régression linéaire. \n",
    "\n",
    "En économétrie, la régression offre une alternative aux méthodes de maximum\n",
    "de vraisemblance et aux méthodes des moments. La régression est un ensemble \n",
    "très vaste de méthodes, selon la famille de modèles\n",
    "(paramétriques, non paramétriques, etc.) et la structure de modèles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa71d68",
   "metadata": {},
   "source": [
    "## La régression linéaire\n",
    "\n",
    "C'est la manière la plus simple de représenter la loi $h_\\theta(X)$ comme \n",
    "combinaison linéaire de variables $X$ et de paramètres $\\theta$. Dans ce\n",
    "cas, \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_\\theta(Y|X) = X\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb41927",
   "metadata": {},
   "source": [
    "Cette relation est encore, sous cette formulation, théorique. Il convient \n",
    "de l'estimer à partir des données observées $y$. La méthode des moindres\n",
    "carrés consiste à minimiser l'erreur quadratique entre la prédiction et \n",
    "les données observées (ce qui explique qu'on puisse voir la régression comme\n",
    "un problème de *Machine Learning*). En toute généralité, la méthode des\n",
    "moindres carrés consiste à trouver l'ensemble de paramètres $\\theta$ \n",
    "tel que\n",
    "\n",
    "$$\n",
    "\\theta = \\arg \\min_{\\theta \\in \\Theta} \\mathbb{E}\\bigg[ \\left( y - h_\\theta(X) \\right)^2 \\bigg]\n",
    "$$\n",
    "\n",
    "Ce qui, dans le cadre de la régression linéaire, s'exprime de la manière suivante:\n",
    "\n",
    "$$\n",
    "\\beta = \\arg\\min \\mathbb{E}\\bigg[ \\left( y - X\\beta \\right)^2 \\bigg]\n",
    "$$\n",
    "\n",
    "Lorsqu'on amène le modèle théorique ($\\mathbb{E}_\\theta(Y|X) = X\\beta$) aux données,\n",
    "on formalise le modèle de la manière suivante:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Avec une certaine distribution du bruit $\\epsilon$ qui dépend\n",
    "des hypothèses faites. Par exemple, avec des \n",
    "$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ i.i.d., l'estimateur $\\beta$ obtenu\n",
    "est équivalent à celui du Maximum de Vraisemblance dont la théorie asymptotique\n",
    "nous assure l'absence de biais, la variance minimale (borne de Cramer-Rao)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07837228",
   "metadata": {},
   "source": [
    "**Exercice 1a : Régression linéaire avec scikit [parcours data-science]**\n",
    "\n",
    "Cet exercice vise à illustrer la manière d'effectuer une régression linéaire avec `scikit`.\n",
    "Dans ce domaine,\n",
    "`statsmodels` est nettement plus complet, ce que montrera l'exercice suivant.\n",
    "L'intérêt principal de faire \n",
    "des régressions avec `scikit` est de pouvoir comparer les résultats d'une régression linéaire\n",
    "avec d'autres modèles de régression. Cependant, le chapitre sur les \n",
    "pipelines montrera qu'on peut très bien insérer, avec quelques efforts\n",
    "de programmation orientée objet, une régression `statsmodels` dans \n",
    "un pipeline `scikit`. \n",
    "\n",
    "L'objectif est d'expliquer le score des Républicains en fonction de quelques\n",
    "variables. Contrairement au chapitre précédent, où on se focalisait sur\n",
    "un résultat binaire (victoire/défaite des Républicains), cette\n",
    "fois on va chercher à modéliser directement le score des Républicains. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb606a7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# packages utiles\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bab57",
   "metadata": {},
   "source": [
    "1. A partir de quelques variables, par exemple, *'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"*, expliquer la variable `per_gop` à l'aide d'un échantillon d'entraînement `X_train` constitué au préalable.\n",
    "\n",
    ":warning: utiliser la variable `Median_Household_Income_2019`\n",
    "en `log` sinon son échelle risque d'écraser tout effet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Régression linéaire de per_gop sur différentes variables explicatives\n",
    "xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"]\n",
    "\n",
    "df2 = votes[[\"per_gop\"] + xvars]\n",
    "df2['log_income'] = np.log(df2[\"Median_Household_Income_2019\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750fc9f",
   "metadata": {},
   "source": [
    "```\n",
    "## <string>:1: SettingWithCopyWarning:\n",
    "## \n",
    "## \n",
    "## A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "## Try using .loc[row_indexer,col_indexer] = value instead\n",
    "## \n",
    "## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab46e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "indices_to_keep = ~df2.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "df2 = df2[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df2.drop([\"Median_Household_Income_2019\",\"per_gop\"], axis = 1),\n",
    "    100*df2[['per_gop']].values.ravel(), test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = ols.predict(X_test)\n",
    "#y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f81cdb",
   "metadata": {},
   "source": [
    "2. Afficher les valeurs des coefficients, constante comprise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859eefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Afficher les valeurs des coefficients\n",
    "print(ols.intercept_, ols.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a668",
   "metadata": {},
   "source": [
    "```\n",
    "## 38.7245742262449 [-3.44264194 -0.47323213 -1.40067009  7.06268493]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf9507",
   "metadata": {},
   "source": [
    "3. Evaluer la pertinence du modèle avec le $R^2$ et la qualité du fit avec le MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff157cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluer la pertinence du modèle\n",
    "\n",
    "rmse = sklearn.metrics.mean_squared_error(y_test, y_pred, squared = False)\n",
    "rsq = sklearn.metrics.r2_score(y_test, y_pred) \n",
    "\n",
    "print('Mean squared error: %.2f'\n",
    "      % rmse)\n",
    "# The coefficient of determination: 1 is perfect prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c3c35",
   "metadata": {},
   "source": [
    "```\n",
    "## Mean squared error: 12.56\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient of determination: %.2f'\n",
    "      % rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5842c16",
   "metadata": {},
   "source": [
    "```\n",
    "## Coefficient of determination: 0.41\n",
    "```\n",
    "\n",
    "4. Représenter un nuage de points des valeurs observées\n",
    "et des erreurs de prédiction. Observez-vous\n",
    "un problème de spécification ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3ed48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#4. Nuage de points des valeurs observées\n",
    "tempdf = pd.DataFrame({\"prediction\": y_pred, \"observed\": y_test,\n",
    "                       \"error\": y_test - y_pred})\n",
    "\n",
    "fig = plt.figure()\n",
    "g = sns.scatterplot(data = tempdf, x = \"observed\", y = \"error\")\n",
    "g.axhline(0, color = \"red\")\n",
    "\n",
    "# La répartition des erreurs n'est clairement pas \n",
    "# aléatoire en fonction de $X$.\n",
    "# Le modèle souffre\n",
    "# donc d'un problème de spécification. \n",
    "\n",
    "\n",
    "g.figure.savefig(\"errorplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceb297",
   "metadata": {},
   "source": [
    "```r\n",
    "knitr::include_graphics(\"errorplot.png\")\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb51c9",
   "metadata": {},
   "source": [
    "**Exercice 1b : Régression linéaire avec scikit [parcours économie]**\n",
    "\n",
    "Cet exercice vise à illustrer la manière d'effectuer une régression linéaire avec `statsmodels` qui offre une interface proche de celle de `R`.\n",
    "\n",
    "L'objectif est d'expliquer le score des Républicains en fonction de quelques\n",
    "variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e3bd3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# packages utiles\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49089bc",
   "metadata": {},
   "source": [
    "1. A partir de quelques variables, par exemple, *'Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"*, expliquer la variable `per_gop`. :warning: utiliser la variable `Median_Household_Income_2019`\n",
    "en `log` sinon son échelle risque d'écraser tout effet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Régression linéaire de per_gop sur différentes variables explicatives\n",
    "xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"]\n",
    "\n",
    "df2 = votes[[\"per_gop\"] + xvars]\n",
    "df2['log_income'] = np.log(df2[\"Median_Household_Income_2019\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318967a",
   "metadata": {},
   "source": [
    "```\n",
    "## <string>:1: SettingWithCopyWarning:\n",
    "## \n",
    "## \n",
    "## A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "## Try using .loc[row_indexer,col_indexer] = value instead\n",
    "## \n",
    "## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fec6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "indices_to_keep = ~df2.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "df2 = df2[indices_to_keep].astype(np.float64)\n",
    "\n",
    "X = sm.add_constant(df2.drop([\"Median_Household_Income_2019\",\"per_gop\"], axis = 1))\n",
    "results = sm.OLS(df2[['per_gop']], X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d654ed",
   "metadata": {},
   "source": [
    "2. Afficher un tableau de régression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Afficher le tableau de régression\n",
    "print(results.summary())\n",
    "# html_snippet = results.summary().as_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9507f",
   "metadata": {},
   "source": [
    "```\n",
    "##                             OLS Regression Results                            \n",
    "## ==============================================================================\n",
    "## Dep. Variable:                per_gop   R-squared:                       0.440\n",
    "## Model:                            OLS   Adj. R-squared:                  0.439\n",
    "## Method:                 Least Squares   F-statistic:                     608.7\n",
    "## Date:                Mon, 06 Dec 2021   Prob (F-statistic):               0.00\n",
    "## Time:                        15:39:17   Log-Likelihood:                 2172.5\n",
    "## No. Observations:                3107   AIC:                            -4335.\n",
    "## Df Residuals:                    3102   BIC:                            -4305.\n",
    "## Df Model:                           4                                         \n",
    "## Covariance Type:            nonrobust                                         \n",
    "## ===================================================================================================================================\n",
    "##                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
    "## -----------------------------------------------------------------------------------------------------------------------------------\n",
    "## const                                                               0.4068      0.147      2.765      0.006       0.118       0.695\n",
    "## Unemployment_rate_2019                                             -0.0345      0.002    -19.306      0.000      -0.038      -0.031\n",
    "## Percent of adults with less than a high school diploma, 2015-19    -0.0051      0.000    -11.333      0.000      -0.006      -0.004\n",
    "## Percent of adults with a bachelor's degree or higher, 2015-19      -0.0140      0.000    -42.380      0.000      -0.015      -0.013\n",
    "## log_income                                                          0.0692      0.013      5.136      0.000       0.043       0.096\n",
    "## ==============================================================================\n",
    "## Omnibus:                      128.336   Durbin-Watson:                   1.932\n",
    "## Prob(Omnibus):                  0.000   Jarque-Bera (JB):              161.315\n",
    "## Skew:                          -0.441   Prob(JB):                     9.35e-36\n",
    "## Kurtosis:                       3.683   Cond. No.                     1.97e+03\n",
    "## ==============================================================================\n",
    "## \n",
    "## Notes:\n",
    "## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "## [2] The condition number is large, 1.97e+03. This might indicate that there are\n",
    "## strong multicollinearity or other numerical problems.\n",
    "```\n",
    "\n",
    "3. Evaluer la pertinence du modèle avec le R^2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calcul du R^2\n",
    "print(\"R2: \", results.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee83d99",
   "metadata": {},
   "source": [
    "```\n",
    "## R2:  0.439751369229638\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b6c46",
   "metadata": {},
   "source": [
    "4. Utiliser l'API `formula` pour régresser le score des républicains en fonction de la variable `Unemployment_rate_2019`, de `Unemployment_rate_2019` au carré et du log de \n",
    "`Median_Household_Income_2019`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aafae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Nouvelle régression avec l'API formula\n",
    "results = smf.ols('per_gop ~ Unemployment_rate_2019 + I(Unemployment_rate_2019**2) + np.log(Median_Household_Income_2019)', data=df2).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cbc05",
   "metadata": {},
   "source": [
    "```\n",
    "##                             OLS Regression Results                            \n",
    "## ==============================================================================\n",
    "## Dep. Variable:                per_gop   R-squared:                       0.115\n",
    "## Model:                            OLS   Adj. R-squared:                  0.114\n",
    "## Method:                 Least Squares   F-statistic:                     133.8\n",
    "## Date:                Mon, 06 Dec 2021   Prob (F-statistic):           1.55e-81\n",
    "## Time:                        15:39:17   Log-Likelihood:                 1461.5\n",
    "## No. Observations:                3107   AIC:                            -2915.\n",
    "## Df Residuals:                    3103   BIC:                            -2891.\n",
    "## Df Model:                           3                                         \n",
    "## Covariance Type:            nonrobust                                         \n",
    "## ========================================================================================================\n",
    "##                                            coef    std err          t      P>|t|      [0.025      0.975]\n",
    "## --------------------------------------------------------------------------------------------------------\n",
    "## Intercept                                3.4689      0.149     23.238      0.000       3.176       3.762\n",
    "## Unemployment_rate_2019                  -0.0454      0.006     -7.515      0.000      -0.057      -0.034\n",
    "## I(Unemployment_rate_2019 ** 2)           0.0011      0.001      2.094      0.036    6.68e-05       0.002\n",
    "## np.log(Median_Household_Income_2019)    -0.2439      0.013    -18.710      0.000      -0.269      -0.218\n",
    "## ==============================================================================\n",
    "## Omnibus:                      334.339   Durbin-Watson:                   1.971\n",
    "## Prob(Omnibus):                  0.000   Jarque-Bera (JB):              448.587\n",
    "## Skew:                          -0.890   Prob(JB):                     3.90e-98\n",
    "## Kurtosis:                       3.547   Cond. No.                     1.41e+03\n",
    "## ==============================================================================\n",
    "## \n",
    "## Notes:\n",
    "## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "## [2] The condition number is large, 1.41e+03. This might indicate that there are\n",
    "## strong multicollinearity or other numerical problems.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb0bbb",
   "metadata": {},
   "source": [
    "Pour sortir une belle table pour un rapport sous $\\LaTeX$, il est possible d'utiliser\n",
    "la méthode [`Summary.as_latex`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex). Pour un rapport HTML, on utilisera [`Summary.as_html`](https://www.statsmodels.org/devel/generated/statsmodels.iolib.summary.Summary.as_latex.html#statsmodels.iolib.summary.Summary.as_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a7034",
   "metadata": {},
   "source": [
    "Les utilisateurs de `R` retrouveront des éléments très familiers avec `statsmodels`,\n",
    "notamment la possibilité d'utiliser une formule pour définir une régression.\n",
    "La philosophie de `statsmodels` est similaire à celle qui a influé sur la construction\n",
    "des packages `stats` et `MASS` de `R`: offrir une librairie généraliste, proposant\n",
    "une large gamme de modèles. Néanmoins, `statsmodels` bénéficie de sa jeunesse\n",
    "par rapport aux packages `R`. Depuis les années 1990, les packages `R` visant \n",
    "à proposer des fonctionalités manquantes dans `stats` et `MASS` se sont\n",
    "multipliés alors que `statsmodels`, enfant des années 2010, n'a eu qu'à\n",
    "proposer un cadre général (les *generalized estimating equations*) pour\n",
    "englober ces modèles.\n",
    "\n",
    "## La régression logistique\n",
    "\n",
    "Ce modèle s'applique à une distribution binaire.\n",
    "Dans ce cas, $\\mathbb{E}\\_{\\theta}(Y|X) = \\mathbb{P}\\_{\\theta}(Y = 1|X)$.\n",
    "La régression logistique peut être vue comme un modèle linéaire en probabilité:\n",
    "\n",
    "$$\n",
    "\\text{logit}\\bigg(\\mathbb{E}\\_{\\theta}(Y|X)\\bigg) = \\text{logit}\\bigg(\\mathbb{P}\\_{\\theta}(Y = 1|X)\\bigg) = X\\beta\n",
    "$$\n",
    "\n",
    "La fonction $\\text{logit}$ est $]0,1[ \\to \\mathbb{R}: p \\mapsto \\log(\\frac{p}{1-p})$.\n",
    "\n",
    "Elle permet ainsi de transformer une probabilité dans $\\mathbb{R}$.\n",
    "Sa fonction réciproque est la sigmoïde ($\\frac{1}{1 + e^{-x}}$),\n",
    "objet central du *Deep Learning*.\n",
    "\n",
    "Il convient de noter que les probabilités ne sont pas observées, c'est l'*outcome*\n",
    "binaire (0/1) qui l'est. Cela amène à voir la régression logistique de deux\n",
    "manières différentes :\n",
    "\n",
    "* En économétrie, on s'intéresse au modèle latent qui détermine le choix de\n",
    "l'outcome. Par exemple, si on observe les choix de participer ou non au marché\n",
    "du travail, on va modéliser les facteurs déterminant ce choix ;\n",
    "* En *Machine Learning*, le modèle latent n'est nécessaire que pour classifier\n",
    "dans la bonne catégorie les observations\n",
    "\n",
    "L'estimation des paramètres $\\beta$ peut se faire par maximum de vraisemblance\n",
    "ou par régression, les deux solutions sont équivalentes sous certaines\n",
    "hypothèses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98b87f",
   "metadata": {},
   "source": [
    "Par défaut, `scikit` applique une régularisation pour pénaliser les modèles\n",
    "peu parcimonieux (comportement différent\n",
    "de celui de `statsmodels`). Ce comportement par défaut est à garder à l'esprit\n",
    "si l'objectif n'est pas de faire de la prédiction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928eac7",
   "metadata": {},
   "source": [
    "**Exercice 2a : Régression logistique [data-scientists]**\n",
    "\n",
    "Avec `scikit`, en utilisant échantillons d'apprentissage et d'estimation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages utiles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793b2b0",
   "metadata": {},
   "source": [
    "1. Evaluer l'effet des variables déjà utilisées sur la probabilité des Républicains\n",
    "de gagner. Affichez la valeur des coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ce386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Modèle logit avec les mêmes variables que précédemment\n",
    "xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"]\n",
    "\n",
    "df2 = votes[[\"per_gop\"] + xvars]\n",
    "df2['log_income'] = np.log(df2[\"Median_Household_Income_2019\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a0e3a",
   "metadata": {},
   "source": [
    "```\n",
    "## <string>:1: SettingWithCopyWarning:\n",
    "## \n",
    "## \n",
    "## A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "## Try using .loc[row_indexer,col_indexer] = value instead\n",
    "## \n",
    "## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eff482",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_keep = ~df2.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "df2 = df2[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df2['y'] = (df2['per_gop']>0.5).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df2.drop([\"Median_Household_Income_2019\",\"y\"], axis = 1),\n",
    "    1*df2[['y']].values.ravel(), test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "#y_pred[:10]\n",
    "print(clf.intercept_, clf.coef_)\n",
    "\n",
    "# Lino : KA a fait un début de corrigé rapidos : à vérifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904e6c6",
   "metadata": {},
   "source": [
    "```\n",
    "## [-14.67252665] [[14.97958856 -0.09357842 -0.0688799  -0.10031329  1.04836814]]\n",
    "```\n",
    "\n",
    "2. Déduire une matrice de confusion et \n",
    "une mesure de qualité du modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Matrice de confusion\n",
    "sklearn.metrics.plot_confusion_matrix(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec1ea1",
   "metadata": {},
   "source": [
    "```\n",
    "## <sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x7f3c719e7e50>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc05d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sc_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)\n",
    "sc_f1 = sklearn.metrics.f1_score(y_pred, y_test)\n",
    "sc_recall = sklearn.metrics.recall_score(y_pred, y_test)\n",
    "sc_precision = sklearn.metrics.precision_score(y_pred, y_test)\n",
    "\n",
    "#print(sc_accuracy)\n",
    "#print(sc_f1)\n",
    "#print(sc_recall)\n",
    "#print(sc_precision)\n",
    "\n",
    "# Lino : KA a fait un début de corrigé rapidos : à vérifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143f1c8",
   "metadata": {},
   "source": [
    "3. Supprimer la régularisation grâce au paramètre `penalty`. Quel effet sur les paramètres estimés ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1baadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Supprimer la régularisation\n",
    "clf2 = LogisticRegression(penalty='none').fit(X_train, y_train)\n",
    "y_pred2 = clf.predict(X_test)\n",
    "print(clf2.intercept_, clf2.coef_)\n",
    "# Les coefficients sont complètement différents\n",
    "\n",
    "# Lino : KA a fait un début de corrigé rapidos : à vérifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2754c",
   "metadata": {},
   "source": [
    "```\n",
    "## [-8314.38925259] [[1.59474244e+04 7.09594153e+00 1.67680395e+00 2.46456453e-02\n",
    "##   2.54864304e+01]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc32fd",
   "metadata": {},
   "source": [
    "**Exercice 2b : Régression logistique [économistes]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages utiles\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fb06c",
   "metadata": {},
   "source": [
    "En utilisant échantillons d'apprentissage et d'estimation :\n",
    "\n",
    "1. Evaluer l'effet des variables déjà utilisées sur la probabilité des Républicains\n",
    "de gagner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Modèle logit avec les mêmes variables que précédemment\n",
    "xvars = ['Unemployment_rate_2019', 'Median_Household_Income_2019', 'Percent of adults with less than a high school diploma, 2015-19', \"Percent of adults with a bachelor's degree or higher, 2015-19\"]\n",
    "\n",
    "df2 = votes[[\"per_gop\"] + xvars]\n",
    "df2['log_income'] = np.log(df2[\"Median_Household_Income_2019\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902bb8e",
   "metadata": {},
   "source": [
    "```\n",
    "## <string>:1: SettingWithCopyWarning:\n",
    "## \n",
    "## \n",
    "## A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "## Try using .loc[row_indexer,col_indexer] = value instead\n",
    "## \n",
    "## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_keep = ~df2.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "df2 = df2[indices_to_keep].astype(np.float64)\n",
    "\n",
    "df2['y'] = (df2['per_gop']>0.5).astype(int)\n",
    "\n",
    "mylogit = smf.logit(formula = 'y ~ Unemployment_rate_2019 + I(Unemployment_rate_2019**2) + np.log(Median_Household_Income_2019)', data=df2[df2['Median_Household_Income_2019']>0]).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921af47f",
   "metadata": {},
   "source": [
    "```\n",
    "## Optimization terminated successfully.\n",
    "##          Current function value: 0.452194\n",
    "##          Iterations 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mylogit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61656c97",
   "metadata": {},
   "source": [
    "```\n",
    "##                            Logit Regression Results                           \n",
    "## ==============================================================================\n",
    "## Dep. Variable:                      y   No. Observations:                 3107\n",
    "## Model:                          Logit   Df Residuals:                     3103\n",
    "## Method:                           MLE   Df Model:                            3\n",
    "## Date:                Mon, 06 Dec 2021   Pseudo R-squ.:                 0.04442\n",
    "## Time:                        15:39:17   Log-Likelihood:                -1405.0\n",
    "## converged:                       True   LL-Null:                       -1470.3\n",
    "## Covariance Type:            nonrobust   LLR p-value:                 3.977e-28\n",
    "## ========================================================================================================\n",
    "##                                            coef    std err          z      P>|z|      [0.025      0.975]\n",
    "## --------------------------------------------------------------------------------------------------------\n",
    "## Intercept                               28.5676      2.577     11.086      0.000      23.517      33.618\n",
    "## Unemployment_rate_2019                  -0.2738      0.110     -2.488      0.013      -0.490      -0.058\n",
    "## I(Unemployment_rate_2019 ** 2)          -0.0019      0.009     -0.207      0.836      -0.020       0.016\n",
    "## np.log(Median_Household_Income_2019)    -2.3735      0.223    -10.648      0.000      -2.810      -1.937\n",
    "## ========================================================================================================\n",
    "```\n",
    "\n",
    "2. Faire un test de ratio de vraisemblance concernant l'inclusion de la variable de (log)-revenu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Faire un test de ratio de vraisemblance \n",
    "logit_h0 = smf.logit(formula = 'y ~ Unemployment_rate_2019 + I(Unemployment_rate_2019**2)', data=df2[df2['Median_Household_Income_2019']>0]).fit()\n",
    "# print(logit_h0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad1d0f",
   "metadata": {},
   "source": [
    "```\n",
    "## Optimization terminated successfully.\n",
    "##          Current function value: 0.470844\n",
    "##          Iterations 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecb58c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "lr = -2*(mylogit.llf - logit_h0.llf)\n",
    "lrdf = (logit_h0.df_resid - mylogit.df_resid)\n",
    "\n",
    "lr_pvalue = stats.chi2.sf(lr, df=lrdf)\n",
    "#print(lr_pvalue)\n",
    "\n",
    "# La pvalue du test de maximum de ratio\n",
    "# de vraisemblance étant proche de  1,\n",
    "# cela signifie que la variable log revenu ajoute,\n",
    "# presque à coup sûr, de l'information au modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca6012",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d42cbb2",
   "metadata": {},
   "source": [
    "La statistique du test est:\n",
    "$$\n",
    "LR = -2\\log\\bigg(\\frac{\\mathcal{L}_{\\theta}}{\\mathcal{L}_{\\theta_0}}\\bigg) = -2(\\mathcal{l}_{\\theta} - \\mathcal{l}_{\\theta_0})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c897044",
   "metadata": {},
   "source": [
    "<!-- ## Modèles linéaires généralisés -->\n",
    "\n",
    "<!-- TO BE COMPLETED -->\n",
    "\n",
    "<!-- ## Autres modèles de Machine Learning -->\n",
    "\n",
    "<!-- TO BE COMPLETED -->\n",
    "\n",
    "<!-- ## Tests d'hypothèses -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
