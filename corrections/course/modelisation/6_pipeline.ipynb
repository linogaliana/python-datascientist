{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317cf096",
   "metadata": {},
   "source": [
    "#  Premier pas vers lindustrialisation avec les pipelines scikit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1fe4e",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=«python-datascientist»&resources.requests.memory=«4Gi»&security.allowlist.enabled=false&init.personalInit=«https://raw.githubusercontent.com/linogaliana/python-datascientist/master/init_onyxia.sh»)\n",
    "<br>\n",
    "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFkAAABZCAMAAABi1XidAAAB8lBMVEX///9XmsrmZYH1olJXmsr1olJXmsrmZYH1olJXmsr1olJXmsrmZYH1olL1olJXmsr1olJXmsrmZYH1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olJXmsrmZYH1olL1olL0nFf1olJXmsrmZYH1olJXmsq8dZb1olJXmsrmZYH1olJXmspXmspXmsr1olL1olJXmsrmZYH1olJXmsr1olL1olJXmsrmZYH1olL1olLeaIVXmsrmZYH1olL1olL1olJXmsrmZYH1olLna31Xmsr1olJXmsr1olJXmsrmZYH1olLqoVr1olJXmsr1olJXmsrmZYH1olL1olKkfaPobXvviGabgadXmsqThKuofKHmZ4Dobnr1olJXmsr1olJXmspXmsr1olJXmsrfZ4TuhWn1olL1olJXmsqBi7X1olJXmspZmslbmMhbmsdemsVfl8ZgmsNim8Jpk8F0m7R4m7F5nLB6jbh7jbiDirOEibOGnKaMhq+PnaCVg6qWg6qegKaff6WhnpKofKGtnomxeZy3noG6dZi+n3vCcpPDcpPGn3bLb4/Mb47UbIrVa4rYoGjdaIbeaIXhoWHmZYHobXvpcHjqdHXreHLroVrsfG/uhGnuh2bwj2Hxk17yl1vzmljzm1j0nlX1olL3AJXWAAAAbXRSTlMAEBAQHx8gICAuLjAwMDw9PUBAQEpQUFBXV1hgYGBkcHBwcXl8gICAgoiIkJCQlJicnJ2goKCmqK+wsLC4usDAwMjP0NDQ1NbW3Nzg4ODi5+3v8PDw8/T09PX29vb39/f5+fr7+/z8/Pz9/v7+zczCxgAABC5JREFUeAHN1ul3k0UUBvCb1CTVpmpaitAGSLSpSuKCLWpbTKNJFGlcSMAFF63iUmRccNG6gLbuxkXU66JAUef/9LSpmXnyLr3T5AO/rzl5zj137p136BISy44fKJXuGN/d19PUfYeO67Znqtf2KH33Id1psXoFdW30sPZ1sMvs2D060AHqws4FHeJojLZqnw53cmfvg+XR8mC0OEjuxrXEkX5ydeVJLVIlV0e10PXk5k7dYeHu7Cj1j+49uKg7uLU61tGLw1lq27ugQYlclHC4bgv7VQ+TAyj5Zc/UjsPvs1sd5cWryWObtvWT2EPa4rtnWW3JkpjggEpbOsPr7F7EyNewtpBIslA7p43HCsnwooXTEc3UmPmCNn5lrqTJxy6nRmcavGZVt/3Da2pD5NHvsOHJCrdc1G2r3DITpU7yic7w/7Rxnjc0kt5GC4djiv2Sz3Fb2iEZg41/ddsFDoyuYrIkmFehz0HR2thPgQqMyQYb2OtB0WxsZ3BeG3+wpRb1vzl2UYBog8FfGhttFKjtAclnZYrRo9ryG9uG/FZQU4AEg8ZE9LjGMzTmqKXPLnlWVnIlQQTvxJf8ip7VgjZjyVPrjw1te5otM7RmP7xm+sK2Gv9I8Gi++BRbEkR9EBw8zRUcKxwp73xkaLiqQb+kGduJTNHG72zcW9LoJgqQxpP3/Tj//c3yB0tqzaml05/+orHLksVO+95kX7/7qgJvnjlrfr2Ggsyx0eoy9uPzN5SPd86aXggOsEKW2Prz7du3VID3/tzs/sSRs2w7ovVHKtjrX2pd7ZMlTxAYfBAL9jiDwfLkq55Tm7ifhMlTGPyCAs7RFRhn47JnlcB9RM5T97ASuZXIcVNuUDIndpDbdsfrqsOppeXl5Y+XVKdjFCTh+zGaVuj0d9zy05PPK3QzBamxdwtTCrzyg/2Rvf2EstUjordGwa/kx9mSJLr8mLLtCW8HHGJc2R5hS219IiF6PnTusOqcMl57gm0Z8kanKMAQg0qSyuZfn7zItsbGyO9QlnxY0eCuD1XL2ys/MsrQhltE7Ug0uFOzufJFE2PxBo/YAx8XPPdDwWN0MrDRYIZF0mSMKCNHgaIVFoBbNoLJ7tEQDKxGF0kcLQimojCZopv0OkNOyWCCg9XMVAi7ARJzQdM2QUh0gmBozjc3Skg6dSBRqDGYSUOu66Zg+I2fNZs/M3/f/Grl/XnyF1Gw3VKCez0PN5IUfFLqvgUN4C0qNqYs5YhPL+aVZYDE4IpUk57oSFnJm4FyCqqOE0jhY2SMyLFoo56zyo6becOS5UVDdj7Vih0zp+tcMhwRpBeLyqtIjlJKAIZSbI8SGSF3k0pA3mR5tHuwPFoa7N7reoq2bqCsAk1HqCu5uvI1n6JuRXI+S1Mco54YmYTwcn6Aeic+kssXi8XpXC4V3t7/ADuTNKaQJdScAAAAAElFTkSuQmCC)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=/__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb)\n",
    "[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist//__w/python-datascientist/python-datascientist/notebooks/course/modelisation/6_pipeline.ipynb)\n",
    "\n",
    "# Pourquoi utiliser les pipelines ?\n",
    "\n",
    "Les chapitres précédents ont permis de montrer des bouts de code \n",
    "épars pour entraîner des modèles ou faire du _preprocessing_.\n",
    "Cette démarche est intéressante pour tâtonner mais risque d'être coûteuse\n",
    "ultérieurement s'il est nécessaire d'ajouter une étape de preprocessing \n",
    "ou de changer d'algorithme.\n",
    "\n",
    "Heureusement, `scikit` propose un excellent outil pour proposer un cadre\n",
    "général pour créer une chaîne de production *machine learning*. Il\n",
    "s'agit des\n",
    "[_pipelines_](https://scikit-learn.org/stable/modules/compose.html). \n",
    "Ils présentent de nombreux intérêts, parmi lesquels:\n",
    "\n",
    "* Ils sont très __pratiques__ et __lisibles__. On rentre des données en entrée, on n'appelle qu'une seule fois les méthodes `fit` et `predict` ce qui permet de s'assurer une gestion cohérente des transformations de variables, par exemple après l'appel d'un `StandardScaler`\n",
    "* La __modularité__ rend aisée la mise à jour d'un pipeline et renforce la capacité à le réutiliser\n",
    "* Ils permettent de facilement chercher les hyperparamètres d'un modèle. Sans *pipeline*, écrire un code qui fait du *tuning* d'hyperparamètres peut être pénible. Avec les *pipelines*, c'est une ligne de code. \n",
    "* La __sécurité__ d'être certain que les étapes de preprocessing sont bien appliquées aux jeux de données désirés avant l'estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de70d6",
   "metadata": {},
   "source": [
    "Un des intérêts des *pipelines* scikit est qu'ils fonctionnent aussi avec\n",
    "des méthodes qui ne sont pas issues de `scikit`.\n",
    "\n",
    "Il est très \n",
    "facile d'introduire un modèle de réseau de neurone `Keras` dans\n",
    "un pipeline `scikit`.\n",
    "\n",
    "Pour introduire un modèle économétrique `statsmodels`\n",
    "c'est un peu plus coûteux mais nous allons proposer des exemples\n",
    "qui peuvent servir de modèle et qui montrent que c'est faisable \n",
    "sans trop de difficulté.\n",
    "\n",
    "# Comment créer un *pipeline*\n",
    "\n",
    "Un *pipeline* est un enchaînement d'opérations qu'on code en enchainant\n",
    "des pairs *(clé, valeur)*:\n",
    "\n",
    "* la clé est le nom du pipeline, cela peut être utile lorsqu'on va\n",
    "représenter le *pipeline* sous forme de diagramme acyclique (visualisation DAG)\n",
    "ou qu'on veut afficher des informations sur une étape\n",
    "* la valeur représente la transformation à mettre en oeuvre dans le *pipeline*\n",
    "(c'est-à-dire, à l'exception de la dernière étape, \n",
    "mettre en oeuvre une méthode `transform` et éventuellement une\n",
    "transformation inverse).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aa082",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n",
    "pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93f97b",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff5047",
   "metadata": {},
   "source": [
    "Il est pratique de visualiser un *pipeline* sous forme de DAG.\n",
    "Pour cela, dans un notebook, on utilise la configuration\n",
    "suivante:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cb48b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850031f7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75700b4d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbfa143",
   "metadata": {},
   "source": [
    "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2f917d18-8af9-4fa3-91e8-947083a1341a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2f917d18-8af9-4fa3-91e8-947083a1341a\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e7c4cfde-f7a5-4150-8752-f9eb687d0a19\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e7c4cfde-f7a5-4150-8752-f9eb687d0a19\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"63e56fc9-a401-4eab-876a-167808b3c904\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"63e56fc9-a401-4eab-876a-167808b3c904\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded0d67",
   "metadata": {},
   "source": [
    "Au sein d'une étape de *pipeline*, les paramètres d'un estimateur\n",
    "sont accessibles avec la notation `<estimator>__<parameter>`.\n",
    "Cela permet de fixer des valeurs pour les arguments des fonctions `scikit`\n",
    "qui sont appelées au sein d'un *pipeline*. \n",
    "C'est cela qui rendra l'approche des pipelines particulièrement utile\n",
    "pour la *grid search*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5606e90",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"reduce_dim__n_components\":[2, 5, 10], \"clf__C\":[0.1, 10, 100]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21d7fb",
   "metadata": {},
   "source": [
    "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4aa80481-965b-45ce-a928-46221fc36680\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4aa80481-965b-45ce-a928-46221fc36680\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())]),\n",
    "             param_grid={'clf__C': [0.1, 10, 100],\n",
    "                         'reduce_dim__n_components': [2, 5, 10]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"00012888-d876-4e12-a490-69f391640143\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"00012888-d876-4e12-a490-69f391640143\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"deffc692-20d9-4c2b-a85a-648b1a8b188b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"deffc692-20d9-4c2b-a85a-648b1a8b188b\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca14e27",
   "metadata": {},
   "source": [
    "## Données utilisées\n",
    "\n",
    "Nous allons utiliser les données de transactions immobilières DVF pour chercher\n",
    "la meilleure manière de prédire, sachant les caractéristiques d'un bien, son\n",
    "prix.\n",
    "\n",
    "Ces données peuvent être importées directement depuis [`data.gouv`](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mutations = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/3004168d-bec4-44d9-a781-ef16f41856a2', sep = \"|\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3a444",
   "metadata": {},
   "source": [
    "```\n",
    "## sys:1: DtypeWarning: Columns (18,23,24,26,28,41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
    "```\n",
    "\n",
    "On propose d'enrichir la base de quelques variables qui pourraient servir\n",
    "ultérieurement:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['Date mutation'] = pd.to_datetime(mutations['Date mutation'], format = \"%d/%m/%Y\")\n",
    "mutations['year'] = mutations['Date mutation'].dt.year\n",
    "mutations['month'] = mutations['Date mutation'].dt.month\n",
    "mutations['dep'] = mutations['Code postal'].astype(str).str[:2]\n",
    "mutations['lprix'] = np.log(mutations[\"Valeur fonciere\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b98f7a",
   "metadata": {},
   "source": [
    "Si vous travaillez avec les données de 2020, n'oubliez pas\n",
    "d'intégrer l'effet\n",
    "confinement strict dans vos modèles. Pour cela, vous pouvez créer une variable\n",
    "indicatrice entre les dates en question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['confinement'] = mutations['Date mutation'].between(pd.to_datetime(\"2020-03-17\"), pd.to_datetime(\"2020-05-03\")).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49d3f1",
   "metadata": {},
   "source": [
    "Les données DVF proposent une observation par transaction. Ces transactions\n",
    "peuvent concerner plusieurs lots.\n",
    "Pour simplifier,\n",
    "on va créer une variable de surface qui agrège les différentes informations\n",
    "de surface disponibles dans le jeu de données. En effet, les variables\n",
    "en question sont très corrélées les unes entre elles :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca3afb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "corr = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "plt.savefig('correlation_matrix_dvf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a127289",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4a1ce",
   "metadata": {},
   "source": [
    "Les agréger revient à supposer que le modèle de fixation des prix est le même\n",
    "entre chaque lot. C'est une hypothèse simplificatrice qu'une personne plus \n",
    "experte du marché immobilier, ou qu'une approche propre de sélection\n",
    "de variable pourrait amener à nier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['surface'] = mutations.loc[:, mutations.columns[mutations.columns.str.startswith('Surface Carrez')].tolist()].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e9d8c",
   "metadata": {},
   "source": [
    "# Préalable : quelques méthodes pour gagner en flexibilité dans le preprocessing\n",
    "\n",
    "Notre *dataframe* comporte des types hétérogènes de variables:\n",
    "\n",
    "* Des variables numériques dont les variances sont très hétérogènes\n",
    "* Des variables textuelles qui mériteraient un recodage sous forme numérique\n",
    "* Des variables discrètes dont les modalités devraient être éclatées (_one hot encoding_)\n",
    "\n",
    "Pour gagner en flexibilité, nous allons proposer certaines méthodes qui permettent\n",
    "d'appliquer les étapes de _preprocessing_ adéquates à un sous-ensemble de \n",
    "variables[^1]. \n",
    "\n",
    "[^1]: Un certain nombre des éléments suivants ont été glannés, par ci par là,\n",
    "depuis `stackoverflow`.\n",
    "\n",
    "Pour cela, il convient d'adopter l'approche de la programmation orientée objet. \n",
    "On va créer des classes avec des méthodes `transform` et `fit_transform`\n",
    "qui pourront ainsi être intégrées directement dans les *pipelines*, comme s'il\n",
    "s'agissait de méthodes issues de `scikit`.\n",
    "\n",
    "La première généralise `LabelEncoder` à un sous-ensemble de colonnes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d978a8",
   "metadata": {},
   "source": [
    "La seconde généralise cette fois le *one hot encoding* à un sous ensemble de \n",
    "fonctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40530d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnOneHotEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = OneHotEncoder(sparse=False).fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = OneHotEncoder(sparse=False).fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d215cb",
   "metadata": {},
   "source": [
    "Les méthodes suivantes vont nous permettre de passer en arguments les noms\n",
    "de colonnes pour intégrer la récupération des bonnes colonnes de nos\n",
    "dataframes dans le pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cb1b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "\n",
    "class Columns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, names=None):\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.names]\n",
    "\n",
    "class Normalize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, func=None, func_param={}):\n",
    "        self.func = func\n",
    "        self.func_param = func_param\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.func != None:\n",
    "            return self.func(X, **self.func_param)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bea1c",
   "metadata": {},
   "source": [
    "# Un premier pipeline: *random forest* sur des variables standardisées\n",
    "\n",
    "Notre premier *pipeline* va nous permettre d'intégrer ensemble:\n",
    "\n",
    "1. Une étape de *preprocessing* avec la standardisation de variables\n",
    "2. Une étape d'estimation du prix en utilisant un modèle de *random forest*\n",
    "\n",
    "Pour le moment, on va prendre comme acquis un certain nombre de variables\n",
    "explicatives (les *features*) et les hyperparamètres du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb015b",
   "metadata": {},
   "source": [
    "## Définition des ensembles train/test\n",
    "\n",
    "Nous allons donc nous restreindre à un sous-ensemble de colonnes dans un\n",
    "premier temps :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['dep', 'Nombre de lots', 'Code type local', 'surface', 'Nombre pieces principales']\n",
    "xvars2 = pd.Series(xvars).str.replace(\" \",\"_\").tolist()\n",
    "\n",
    "mutations2 = mutations.loc[:, xvars + [\"Valeur fonciere\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184076de",
   "metadata": {},
   "source": [
    "Nous allons également ne conserver que les transactions inférieures à 5 millions\n",
    "d'euros (on anticipe que celles ayant un montant supérieur sont des transactions\n",
    "exceptionnelles dont le mécanisme de fixation du prix diffère)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations2  = mutations2.dropna()\n",
    "mutations2 = mutations2.loc[mutations2['Valeur fonciere'] < 5e6] #keep only values below 10 millions\n",
    "\n",
    "mutations2.columns = mutations2.columns.str.replace(\" \", \"_\")\n",
    "numeric_features = mutations2.columns[~mutations2.columns.isin(['dep','Code_type_local', 'confinement'])].tolist()\n",
    "categorical_features = ['dep','Code_type_local']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a3883",
   "metadata": {},
   "source": [
    "Au passage, nous avons abandonné la variable de code postal pour privilégier\n",
    "la commune afin de réduire la dimension de notre jeu de données. Si on voulait\n",
    "vraiment avoir un bon modèle, il faudrait faire autrement car le code postal\n",
    "est probablement un très bon prédicteur du prix d'un bien, une fois que\n",
    "les caractéristiques du bien sont contrôlées.\n",
    "\n",
    "Nous allons stratifier notre échantillonage de train/test par département\n",
    "afin de tenir compte, de manière minimale, de la géographie. \n",
    "Pour accélérer les calculs pour ce tutoriel, nous n'allons considérer que\n",
    "20% des transactions observées sur chaque département.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8744db7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mutations2 = mutations2.groupby('dep').sample(frac = 0.2, random_state = 123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mutations2[xvars2], mutations2[[\"Valeur_fonciere\"]].values.ravel(), test_size = 0.2, random_state = 0, stratify=mutations2[['dep']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df1bdf",
   "metadata": {},
   "source": [
    "## Définition du premier pipeline\n",
    "\n",
    "Nous allons donc partir d'un *random forest* avec des valeurs d'hyperparamètres\n",
    "données. \n",
    "\n",
    "Les _random forest_ sont une méthode d'aggrégation[^2] d'arbres de décision. \n",
    "On calcule $K$ arbres de décision et en tire, par une méthode d'agrégation,\n",
    "une règle de décision moyenne qu'on va appliquer pour tirer une\n",
    "prédiction de nos données. \n",
    "\n",
    "[^2]: En _machine learning_ on retrouve un principe inspiré du\n",
    "_bootstrap_\n",
    "qui permet d'agréger un ensemble d'estimateurs en un estimateur _\"moyennisé\"_.\n",
    "Il s'agit du [*bagging*](https://en.wikipedia.org/wiki/Bootstrap_aggregating).\n",
    "En économétrie, le _bootstrap_ consiste à ré-estimer sur *K* sous-échantillons\n",
    "aléatoires des données un estimateur afin d'en tirer, par exemple, un intervalle\n",
    "de confiance empirique à 95%. Le principe du _bagging_ est le même. On ré-estime\n",
    "_K_ fois notre estimateur (par exemple un arbre de décision) et propose une \n",
    "règle d'agrégation pour en tirer une règle moyennisée et donc une prédiction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cae7a",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)\n",
    "\n",
    "C'est un article de Léo Breiman (2001)[^3], statisticien à Berkeley, qui\n",
    "est à l'origine du succès des *random forests*. L'un des intérêts\n",
    "des *random forest* est qu'il existe des méthodes pour déterminer \n",
    "l'importance relative de chaque variable dans la prédiction. \n",
    "\n",
    "[^3]: Breiman L (2001). _\"Random Forests\". Machine Learning_. 45 (1): 5–32.\n",
    "\n",
    "Pour commencer, nous allons fixer la taille des arbres de décision avec\n",
    "l'hyperparamètre `max_depth = 2`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420133e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038a727",
   "metadata": {},
   "source": [
    "Notre _pipeline_ va intégrer les étapes suivantes:\n",
    "\n",
    "1. __Preprocessing__:\n",
    "    + Les variables numériques vont être standardisées avec un `StandardScaler`.\n",
    "Pour cela, nous allons utiliser la liste `numeric_features` définie précédemment.\n",
    "    + Les variables catégorielles vont être explosées avec un *one hot encoding*\n",
    "(méthode `OneHotEncoder` de `scikit`)\n",
    "Pour cela, nous allons utiliser la liste `categorical_features`\n",
    "2. __Random forest__: nous allons appliquer l'estimateur `regr` défini plus haut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n",
    "        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n",
    "    ])),\n",
    "    ('randomforest', regr)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6387bc04",
   "metadata": {},
   "source": [
    "Nous avons construit ce pipeline sous forme de couches successives. La couche\n",
    "`randomforest` prendra automatiquement le résultat de la couche `features`\n",
    "en _input_. La couche `features` permet d'introduire de manière relativement\n",
    "simple (quand on a les bonnes méthodes) la complexité du *preprocessing*\n",
    "sur données réelles dont les types divergent. \n",
    "\n",
    "On peut visualiser le graphe et ainsi se représenter la manière dont\n",
    "ce _pipeline_ opère:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7851958",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5431",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162e0c7",
   "metadata": {},
   "source": [
    "Maintenant, il ne reste plus qu'à estimer notre modèle sur l'ensemble\n",
    "d'entraînement. C'est très simple avec un _pipeline_ : il suffit d'utiliser\n",
    "de mettre à jour le _pipeline_ avec la méthode `fit`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2563751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c391732",
   "metadata": {},
   "source": [
    "## Variable importance\n",
    "\n",
    "Pour le moment, on va utiliser un *hack* pour récupérer le `DataFrame`\n",
    "suite au preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(X.shape)\n",
    "        #self.shape = shape\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "pipe_deb = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        ('numeric', make_pipeline(Columns(names=numeric_features[:-1]),StandardScaler())),\n",
    "        ('categorical', make_pipeline(Columns(names=categorical_features),OneHotEncoder(sparse=False)))\n",
    "    ])),\n",
    "    (\"debug\", Debug())#,    \n",
    "#    ('randomforest', regr)\n",
    "])\n",
    "\n",
    "X_train_mod = pipe_deb.fit_transform(X_train)\n",
    "features_post_processing = pd.DataFrame(X_train_mod.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6306ae3",
   "metadata": {},
   "source": [
    "Celui-ci ne serait pas nécessaire, en soi, si on voulait utiliser les \n",
    "fonctionalités de `scikit` (le _pipeline_ est géré de manière cohérente)\n",
    "mais c'est nécessaire si on veut utiliser `yellowbrick` pour faire\n",
    "l'analyse d'importance de variables\n",
    "\n",
    "On ne va représenter, parmi notre ensemble important de colonnes, que celles\n",
    "qui ont une importance non nulle. L'importance va être définie à partir \n",
    "de la mesure d'impureté[^3]\n",
    "\n",
    "[^3]: Extrait de [ce blog](https://mljar.com/blog/feature-importance-in-random-forest/):\n",
    "Gini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let’s look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe['randomforest'].feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in pipe['randomforest'].estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.Series(importances, index=features_post_processing.columns)\n",
    "\n",
    "forest_importances = pd.Series(importances, index=features_post_processing.columns)\n",
    "forest_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e273a8",
   "metadata": {},
   "source": [
    "```\n",
    "## 0     0.026812\n",
    "## 1     0.115643\n",
    "## 2     0.000000\n",
    "## 3     0.000000\n",
    "## 4     0.000000\n",
    "##         ...   \n",
    "## 92    0.000000\n",
    "## 93    0.000000\n",
    "## 94    0.000000\n",
    "## 95    0.000000\n",
    "## 96    0.339486\n",
    "## Length: 97, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances[forest_importances>0].plot.bar(yerr=std[forest_importances>0], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba43b03",
   "metadata": {},
   "source": [
    "Idéalement, on utiliserait `yellowbrick` pour représenter l'importance des variables\n",
    "Mais en l'état actuel du *pipeline* on a beaucoup de variables dont le poids\n",
    "est nul qui viennent polluer la visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import FeatureImportances\n",
    "viz = FeatureImportances(pipe['randomforest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c13918",
   "metadata": {},
   "source": [
    "```\n",
    "## /opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
    "##   warnings.warn('From version 0.24, get_params will raise an '\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e028bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027b8f1",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: fit() missing 1 required positional argument: 'X'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2baf2",
   "metadata": {},
   "source": [
    "```\n",
    "## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'RandomForestRegressor' object has no attribute 'features_'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/yellowbrick/base.py\", line 241, in show\n",
    "##     self.finalize()\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/yellowbrick/model_selection/importances.py\", line 279, in finalize\n",
    "##     len(self.features_), self.name\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/yellowbrick/utils/wrapper.py\", line 42, in __getattr__\n",
    "##     return getattr(self._wrapped, attr)\n",
    "```\n",
    "\n",
    "## Prédiction\n",
    "\n",
    "L'analyse de l'importance de variables permet de mieux comprendre\n",
    "le fonctionnement interne des *random forests*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "compar = pd.DataFrame([y_test, pipe.predict(X_test)]).T\n",
    "compar.columns = ['obs','pred']\n",
    "compar['diff'] = compar.obs - compar.pred\n",
    "\n",
    "print(\"Le RMSE sur le jeu de test est {:,}\".format(\n",
    "   int(np.sqrt(mean_squared_error(y_test, pipe.predict(X_test))))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1a4b2",
   "metadata": {},
   "source": [
    "```\n",
    "## Le RMSE sur le jeu de test est 464,117\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94e2bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "g = sns.scatterplot(data = compar, x = 'obs', y = 'pred', ax = ax)\n",
    "plt.axvline(1e6, color='r', linestyle = '--')\n",
    "plt.axhline(1e6, color='r', linestyle = '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d8d71",
   "metadata": {},
   "source": [
    "# Références\n",
    "\n",
    "- Breiman L (2001). _\"Random Forests\". Machine Learning_. 45 (1): 5–32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
