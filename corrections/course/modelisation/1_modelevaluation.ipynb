{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Evaluer la qualité d'un modèle\"\n",
        "date: 2020-10-15T13:00:00Z\n",
        "draft: false\n",
        "weight: 20\n",
        "slug: performance\n",
        "tags:\n",
        "  - scikit\n",
        "  - machine learning\n",
        "  - US elections\n",
        "  - model performance\n",
        "  - Modelisation\n",
        "  - Exercice\n",
        "categories:\n",
        "  - Modélisation\n",
        "  - Exercice\n",
        "type: book\n",
        "summary: |\n",
        "  Faire preuve de méthode pour évaluer la qualité d'un modèle \n",
        "  permet de proposer des prédictions plus robustes, ayant\n",
        "  de meilleures performances sur un nouveau jeu de données\n",
        "  (prédictions _out-of-sample_). Décomposer\n",
        "  l'échantillon initial en sous-échantillons d'entraînement\n",
        "  et de tests, faire de la validation croisée, utiliser\n",
        "  les bonnes mesures de performances \n",
        "  peut se faire, grâce à scikit, de manière relativement standardisée.\n",
        "  Cette démarche scientifique est essentielle pour assurer la confiance\n",
        "  dans la qualité d'un modèle, ce qu'a illustré récemment\n",
        "  un [cycle de séminaire de Princeton](https://reproducible.cs.princeton.edu/)\n",
        "---"
      ],
      "id": "ef7a0788"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}"
      ],
      "id": "06dfa218"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: 'asis'\n",
        "#| include: true\n",
        "#| eval: true\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, '../../../../') #insert the utils module\n",
        "from utils import print_badges\n",
        "\n",
        "#print_badges(__file__)\n",
        "print_badges(\"content/course/modelisation/1_modelevaluation.qmd\")"
      ],
      "id": "ffc87cfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Nous allons ici voir des méthodes générales permettant de s'assurer que le modèle\n",
        "de _Machine Learning_ mobilisé est de qualité. \n",
        "\n",
        "# Découper l'échantillon\n",
        "\n",
        "Le chapitre précédent présentait le pipeline simple ci-dessous\n",
        "pour introduire à la notion d'entraînement d'un modèle:\n",
        "\n",
        "![](scikit_predict.png)\n",
        "\n",
        "Ce *pipeline* fait abstraction d'hypothèses exogènes à l'estimation \n",
        "mais qui sont à faire sur des paramètres\n",
        "car elles affectent la performance de la prédiction.\n",
        "Par exemple, de nombreux modèles proposent une pénalisation des modèles\n",
        "non parcimonieux pour éviter le sur-apprentissage. Le choix de la pénalisation\n",
        "idéale dépend de la structure des données et n'est jamais connue, *ex-ante*\n",
        "par le modélisateur. Faut-il pénaliser fortement ou non le modèle ? En l'absence\n",
        "d'argument théorique, on aura tendance à tester plusieurs paramètres de\n",
        "pénalisation et choisir celui qui permet la meilleure prédiction.\n",
        "\n",
        "La notion de __validation croisée__ permettra de généraliser cette approche. Ces paramètres\n",
        "qui affectent la prédiction seront pas la suite appelés des\n",
        "**hyperparamètres**. Comme nous allons le voir, nous allons aboutir à un \n",
        "raffinement de l'approche pour obtenir un *pipeline* ayant plutôt cet aspect:\n",
        "\n",
        "![](scikit_predict2.png)\n",
        "\n",
        "\n",
        "# Le problème du sur-apprentissage\n",
        "\n",
        "Le but du *Machine Learning* est de calibrer l’algorithme sur des exemples\n",
        "connus (données labellisées) afin de généraliser à des\n",
        "exemples nouveaux (éventuellement non labellisés).\n",
        "On vise donc de bonnes qualités\n",
        "prédictives et non un ajustement parfait\n",
        "aux données historiques.\n",
        "\n",
        "Il existe un __arbitrage biais-variance__ dans la qualité d'estimation[^1]. Soit $h(X,\\theta)$ un modèle statistique. On \n",
        "peut décomposer l'erreur d'estimation en deux parties :\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\bigg[(y - h(\\theta,X))^2 \\bigg] = \\underbrace{ \\bigg( y - \\mathbb{E}(h_\\theta(X)) \\bigg)^2}_{\\text{biais}^2} + \\underbrace{\\mathbb{V}\\big(h(\\theta,X)\\big)}_{\\text{variance}}\n",
        "$$\n",
        "\n",
        "Il y a ainsi un compromis à faire entre biais et variance. Un modèle peu parcimonieux, c'est-à-dire proposant un grand nombre de paramètres, va, en général, avoir un faible biais mais une grande variance. En effet, le modèle va tendre à se souvenir d'une combinaison de paramètres à partir d'un grand nombre d'exemples sans être capable d'apprendre la règle qui permette de structurer les données. \n",
        "\n",
        "[^1]! Cette formule permet de bien comprendre la théorie statistique asymptotique, notamment le théorème de Cramer-Rao. Dans la classe des estimateurs sans biais, c'est-à-dire dont le premier terme est nul, trouver l'estimateur à variance minimale revient à trouver l'estimateur qui minimise $\\mathbb{E}\\bigg[(y - h_\\theta(X))^2 \\bigg]$. C'est la définition même de la régression, ce qui, quand on fait des hypothèses supplémentaires sur le modèle statistique, explique le théorème de Cramer-Rao.\n",
        "\n",
        "\n",
        "Par exemple, la ligne verte ci-dessous est trop dépendante des données et risque de produire une erreur plus importante que la ligne noire (qui moyennise plus) sur de nouvelles données. \n",
        "\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/300px-Overfitting.svg.png)\n",
        "\n",
        "\n",
        "Pour renforcer la validité externe d'un modèle, il est ainsi commun, en *Machine Learning*:\n",
        "\n",
        "1. d'estimer un modèle sur un jeu de données (__jeu d'apprentissage__ ou *training set*) mais d'évaluer la performance, et donc la pertinence du modèle, sur d'autres données, qui n'ont pas été mobilisées lors de la phase d'estimation (__jeu de validation, de test__ ou *testing set*) ;\n",
        "2. d'avoir des mesures de performances qui pénalisent fortement les modèles peu parcimonieux (BIC) ou conduire une première phase de sélection de variable (par des méthodes de LASSO...)\n",
        "\n",
        "\n",
        "Pour décomposer un modèle en jeu d'estimation et de test,\n",
        "la meilleure méthode est d'utiliser les fonctionnalités de `scikit` de la manière suivante : \n"
      ],
      "id": "4afb9151"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "from sklearn.model_selection import train_test_split\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(\n",
        "  x, y,\n",
        "  test_size = 0.2,\n",
        "  random_state = 0\n",
        "  )"
      ],
      "id": "490e7f26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La proportion d'observations dans le jeu de test est contrôlée par l'argument `test_size`.\n",
        "La proportion optimale n'existe pas.\n",
        "La règle du pouce habituelle est d'assigner aléatoirement 20 % des observations\n",
        "dans l'échantillon de test pour garder suffisamment d'observations\n",
        "dans l'échantillon d'estimation. \n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Hint </h3>\n",
        "```\n",
        "\n",
        "Lorsqu'on travaille avec des séries temporelles, l'échantillonnage aléatoire des observations n'a pas vraiment de sens. Il vaut mieux tester la qualité de l'observation sur des périodes distinguées. \n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "Avec des données multi-niveaux,\n",
        "comme c'est le cas de données géographiques ou de données individuelles avec des variables de classe,\n",
        "il peut être intéressant d'utiliser un __échantillonnage stratifié__.\n",
        "Cela permet de garder une proportion équivalente de chaque groupe dans les deux jeux de données de test ou d'apprentissage.\n",
        "Ce type d'échantillonnage stratifié est également possible avec `scikit`.\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "L'[exercice sur les SVM](https://linogaliana-teaching.netlify.app/svm/) illustre cette construction et la manière\n",
        "dont elle facilite l'évaluation de la qualité d'un modèle.\n",
        "\n",
        "## Validation croisée\n",
        "\n",
        "Certains algorithmes font intervenir des __hyperparamètres__,\n",
        "c'est-à-dire des paramètres exogènes qui déterminent la prédiction mais ne sont pas estimés.\n",
        "La __validation croisée__ est une méthode permettant de choisir la valeur du paramètre\n",
        "qui optimise la qualité de la prédiction en agrégeant\n",
        "des scores de performance sur des découpages différents de l'échantillon d'apprentissage.\n",
        "\n",
        "La validation croisée permet d'évaluer les performances de modèles différents (SVM, random forest, etc.) ou, couplé à une stratégie de *grid search* de trouver les valeurs des hyperparamètres qui aboutissent à la meilleure prédiction.\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "L'étape de découpage de l'échantillon de validation croisée est à distinguer de l'étape `split_sample_test`. A ce stade, on a déjà partitionné les données en échantillon d'apprentissage et test. C'est l'échantillon d'apprentissage qu'on découpe en sous-morceaux. \n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "La méthode la plus commune est la validation croisée _k-fold_.\n",
        "On partitionne les données en $K$ morceaux et on considère chaque pli, tour à tour, comme un échantillon\n",
        "de test en apprenant sur les $K-1$ échantillons restants. Les $K$ indicateurs ainsi calculés sur les $K$ échantillons de test peuvent être moyennés et\n",
        "comparés pour plusieurs valeurs des hyperparamètres.\n",
        "\n",
        "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
        "\n",
        "Il existe d'autres types de validation croisée, notamment la *leave one out* qui consiste à considérer une fois\n",
        "exactement chaque observation comme l’échantillon de test (une *n-fold cross validation*).\n",
        "\n",
        "# Mesurer la performance\n",
        "\n",
        "Jusqu'à présent, nous avons passé sous silence la question du support de $y$, c'est-à-dire \n",
        "de l'étendue des valeurs de notre variable d'intérêt.\n",
        "En pratique, la distribution des $y$\n",
        "va néanmoins déterminer deux questions cruciales : la méthode et l'indicateur de performance.\n",
        "\n",
        "En apprentissage supervisé, on distingue en général les problèmes de:\n",
        "\n",
        "* Classification : la variable $y$ est discrète\n",
        "* Régression : la variable $y$ est continue\n",
        "\n",
        "Les deux approches ne sont pas sans lien. On peut par exemple voir le modèle économétrique de choix d'offre de travail comme un problème de classification (participation ou non au marché du travail) ou de régression (régression sur un modèle à variable latente)\n",
        "\n",
        "### Classification\n",
        "\n",
        "La plupart des critères de performance sont construits à partir de la **matrice de confusion**:\n",
        "\n",
        "![Image empruntée à https://www.lebigdata.fr/confusion-matrix-definition](https://www.lebigdata.fr/wp-content/uploads/2018/12/confusion-matrix-exemple-768x432.jpg?ezimgfmt=ng:webp/ngcb1)\n",
        "\n",
        "A partir des 4 coins de cette matrice, il existe plusieurs mesure de performance\n",
        "\n",
        "| Critère     | Mesure       |  Calcul |\n",
        "|-------------|--------------|------------------|\n",
        "| *Accuracy*  | Taux de classification correcte | Diagonale du tableau: $\\frac{TP+TN}{TP+FP+FN+FP}$ |\n",
        "| *Precision* | Taux de vrais positifs  |Ligne des prédictions positives: $\\frac{TP}{TP+FP}$ |\n",
        "| *Recall* (rappel)   | Capacité à identifier les labels positifs | Colonne des prédictions positives: $\\frac{TP}{TP+FN}$ |\n",
        "| *F1 Score*  | Mesure synthétique (moyenne harmonique) de la précision et du rappel | $2 \\frac{precision \\times recall}{precision + recall}$  |\n",
        "\n",
        "En présence de classes désequilibrées, la\n",
        "_F-mesure_ est plus pertinente pour évaluer les\n",
        "performances mais l’apprentissage restera\n",
        "mauvais si l’algorithme est sensible à ce\n",
        "problème. Notamment, si on désire avoir une performance équivalente sur les classes minoritaires, il faut généralement les sur-pondérer (ou faire un échantillonnage stratifié) lors de la constitution de l'échantillon d'observation.\n",
        "\n",
        "Il est possible de construire des modèles à partir des probabilités prédites d'appartenir à la classe d'intérêt. Pour cela, on fixe un seuil $c$ tel que\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(y_i=1|X_i) > c \\Rightarrow \\widehat{y}_i = 1 \n",
        "$$\n",
        "\n",
        "Plus on augmente $c$, plus on est sélectif sur le critère d'appartenance à la classe.\n",
        "Le rappel, i.e. le taux de faux négatifs, diminue. Mais on augmente le nombre de positifs manqués. Pour chaque valeur de $c$ correspond une matrice de confusion et donc des mesures de performances.\n",
        "La **courbe ROC** est un outil classique pour représenter en un graphique l’ensemble de ces\n",
        "informations en faisant varier $c$ de 0 à 1:\n",
        "\n",
        "![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)\n",
        "\n",
        "L'aire sous la courbe (**AUC**) permet d'évaluer quantitativement le meilleur modèle au\n",
        "sens de ce critère. L'AUC représente la probabilité que le modèle soit capable de distinguer entre la classe positive et négative. \n",
        "\n",
        "### Régression\n",
        "\n",
        "En Machine Learning, les indicateurs de performance en régression sont les suivants:\n",
        "\n",
        "| Nom | Formule |\n",
        "|-----|---------|\n",
        "| Mean squared error | $MSE = \\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]$ |\n",
        "| Root Mean squared error | $RMSE = \\sqrt{\\mathbb{E}\\left[(y - h_\\theta(X))^2\\right]}$ |\n",
        "| Mean Absolute Error | $MAE = \\mathbb{E} \\bigg[ \\lvert y - h_\\theta(X) \\rvert \\bigg]$ |\n",
        "| Mean Absolute Percentage Error | $MAE = \\mathbb{E}\\left[ \\left\\lvert \\frac{y - h_\\theta(X)}{y} \\right\\rvert \\right]$ |\n",
        "\n",
        "L'économètre se focalise moins sur la qualité de la prédiction et utilisera \n",
        "d'autres critères pour évaluer la qualité d'un modèle (certains, comme le BIC, sont\n",
        "à regarder aussi dans une optique *Machine Learning*): $R^2$, $BIC$,\n",
        "$AIC$, *log-likelihood*, etc. "
      ],
      "id": "d29e507d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}