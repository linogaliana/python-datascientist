{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pr√©paration des donn√©es pour construire un mod√®le\"\n",
        "date: 2020-10-15T13:00:00Z\n",
        "draft: false\n",
        "weight: 10\n",
        "slug: preprocessing\n",
        "tags:\n",
        "  - scikit\n",
        "  - machine learning\n",
        "  - US election\n",
        "  - preprocessing\n",
        "  - Modelisation\n",
        "  - Exercice\n",
        "categories:\n",
        "  - Mod√©lisation\n",
        "  - Exercice\n",
        "type: book\n",
        "summary: |\n",
        "  Afin d'avoir des donn√©es coh√©rentes avec les hypoth√®ses de mod√©lisation,\n",
        "  il est absolument fondamental de prendre le temps de\n",
        "  pr√©parer les donn√©es √† fournir √† un mod√®le. La qualit√© de la pr√©diction\n",
        "  d√©pend fortement de ce travail pr√©alable qu'on appelle _preprocessing_.\n",
        "  Beaucoup de m√©thodes sont disponibles dans `scikit`, ce qui rend ce travail\n",
        "  moins fastidieux et plus fiable. \n",
        "plotly: true\n",
        "bibliography: ../../../../reference.bib\n",
        "---"
      ],
      "id": "cf544331"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}"
      ],
      "id": "3e2eb166"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: 'asis'\n",
        "#| include: true\n",
        "#| eval: true\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, '../../../../') #insert the utils module\n",
        "from utils import print_badges\n",
        "\n",
        "#print_badges(__file__)\n",
        "print_badges(\"content/course/modelisation/0_preprocessing.qmd\")"
      ],
      "id": "afd13c5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Ce chapitre utilise le jeu de donn√©es pr√©sent√© dans l'[introduction\n",
        "de cette partie](https://linogaliana-teaching.netlify.app/modelisation/) :\n",
        "les donn√©es de vote aux √©lections pr√©sidentielles am√©ricaines de 2020 au niveau des comt√©s\n",
        "crois√©es √† des variables socio-d√©mographiques.\n",
        "Le code de consitution de la base de donn√©es\n",
        "est disponible [sur Github](https://github.com/linogaliana/python-datascientist/blob/master/content/course/modelisation/get_data.py).\n",
        "L'exercice 1 permet, √† ceux qui le d√©sirent, d'essayer de le reconstituer pas √† pas. \n",
        "\n",
        "Le guide utilisateur de `scikit` est une r√©f√©rence pr√©cieuse,\n",
        "√† consulter r√©guli√®rement. La partie sur le *preprocessing* est\n",
        "disponible [ici](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "\n",
        "L'objectif de ce chapitre est de pr√©senter quelques √©l√©ments de \n",
        "pr√©paration des donn√©es. Il s'agit d'une √©tape fondamentale, √† ne\n",
        "pas n√©gliger. Les mod√®les reposent sur certaines hypoth√®ses, g√©n√©ralement\n",
        "relatives √† la distribution th√©orique des variables qui y sont int√©gr√©es.\n",
        "\n",
        "Il est n√©cessaire de faire correspondre la distribution empirique\n",
        "√† ces hypoth√®ses ce qui implique un travail de restructuration des donn√©es.\n",
        "Celui-ci permettra d'avoir des r√©sultats de mod√©lisation plus pertinents. \n",
        "Nous verrons dans le chapitre sur les *pipelines* comment industrialiser\n",
        "ces √©tapes de _preprocessing_ afin de se simplifier la vie pour appliquer\n",
        "un mod√®le sur un jeu de donn√©es diff√©rent de celui sur lequel il a √©t√© estim√©. \n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> scikit-learn </h3>\n",
        "```\n",
        "\n",
        "\n",
        "`scikit-learn` est aujourd'hui la librairie de r√©f√©rence dans l'√©cosyst√®me du\n",
        "_Machine Learning_. Il s'agit d'une librairie qui, malgr√© les tr√®s nombreuses\n",
        "m√©thodes impl√©ment√©es, pr√©sente l'avantage d'√™tre un point d'entr√©e unifi√©.\n",
        "Cet aspect unifi√© est l'une des raisons du succ√®s pr√©coce de celle-ci. `R` n'a \n",
        "b√©n√©fici√© que plus r√©cemment d'une librairie unifi√©e,\n",
        "√† savoir [`tidymodels`](https://www.tidymodels.org/).\n",
        "\n",
        "Une autre raison du succ√®s de `scikit` est son approche op√©rationnelle: la mise\n",
        "en production de mod√®les d√©velopp√©s via les _pipelines_ `scikit` est peu co√ªteuse.\n",
        "Un [chapitre sp√©cial de ce cours](/pipeline-scikit) est d√©di√© aux _pipelines_.\n",
        "Avec Romain Avouac, nous proposons un [cours plus avanc√©](https://ensae-reproductibilite.netlify.app/) \n",
        "en derni√®re ann√©e d'ENSAE o√π nous pr√©sentons certains enjeux relatifs\n",
        "√† la mise en production de mod√®les d√©velopp√©s avec `scikit`. \n",
        "\n",
        "Le coeur de l'√©quipe de d√©veloppement de `scikit-learn` est situ√©\n",
        "√† l'[Inria](https://www.inria.fr/fr) üá´üá∑. \n",
        "Pour d√©couvrir la richesse de l'√©cosyst√®me `scikit`, il \n",
        "est recommand√© de suivre le\n",
        "[`MOOC scikit`](https://www.fun-mooc.fr/fr/cours/machine-learning-python-scikit-learn/),\n",
        "d√©velopp√© dans le cadre de l'initiative [`Inria Academy`](https://www.inria.fr/fr/mooc-scikit-learn).\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "# Construction de la base de donn√©es\n",
        "\n",
        "Les sources de donn√©es √©tant diverses, le code qui construit la base finale est directement fourni. Le travail de construction d'une base unique\n",
        "est un peu fastidieux mais il s'agit d'un bon exercice, que vous pouvez tenter,\n",
        "pour [r√©viser `pandas`](#pandas)   :\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 1 : Importer les donn√©es des √©lections US</h3>\n",
        "```\n",
        "\n",
        "\n",
        "__Cet exercice est OPTIONNEL__\n",
        "\n",
        "1. T√©l√©charger et importer le shapefile [depuis ce lien](https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_02_sldl_500k.zip)\n",
        "2. Exclure les Etats suivants: \"02\", \"69\", \"66\", \"78\", \"60\", \"72\", \"15\"\n",
        "3. Importer les r√©sultats des √©lections depuis [ce lien](https://raw.githubusercontent.com/tonmcg/US_County_Level_Election_Results_08-20/master/2020_US_County_Level_Presidential_Results.csv)\n",
        "4. Importer les bases disponibles sur le site de l'USDA en faisant attention √† renommer les variables de code FIPS de mani√®re identique\n",
        "dans les 4 bases\n",
        "5. *Merger* ces 4 bases dans une base unique de caract√©ristiques socio√©conomiques\n",
        "6. *Merger* aux donn√©es √©lectorales √† partir du code FIPS\n",
        "7. *Merger* au shapefile √† partir du code FIPS. Faire attention aux 0 √† gauche dans certains codes. Il est\n",
        "recommand√© d'utiliser la m√©thode `str.lstrip` pour les retirer\n",
        "8. Importer les donn√©es des √©lections 2000 √† 2016 √† partir du [MIT Election Lab](https://electionlab.mit.edu/data)?\n",
        "Les donn√©es peuvent √™tre directement requ√™t√©es depuis l'url\n",
        "<https://dataverse.harvard.edu/api/access/datafile/3641280?gbrecs=false>\n",
        "9. Cr√©er une variable `share` comptabilisant la part des votes pour chaque candidat. \n",
        "Ne garder que les colonnes `\"year\", \"FIPS\", \"party\", \"candidatevotes\", \"share\"`\n",
        "10. Faire une conversion `long` to `wide` avec la m√©thode `pivot_table` pour garder une ligne\n",
        "par comt√© x ann√©e avec en colonnes les r√©sultats de chaque candidat dans cet √©tat.\n",
        "11. Merger √† partir du code FIPS au reste de la base. \n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "Si vous ne faites pas l'exercice 1, pensez √† charger les donn√©es en executant la fonction `get_data.py` :\n"
      ],
      "id": "b43fd2ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "#!pip install --upgrade xlrd #colab bug verson xlrd\n",
        "#!pip install geopandas\n",
        "\n",
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/modelisation/get_data.py'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('getdata.py', 'wb').write(r.content)\n",
        "\n",
        "import getdata\n",
        "votes = getdata.create_votes_dataframes()"
      ],
      "id": "d7195f49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce code introduit une base nomm√©e `votes` dans l'environnement. Il s'agit d'une\n",
        "base rassemblant les diff√©rentes sources. Elle a l'aspect\n",
        "suivant:\n"
      ],
      "id": "dc20e6d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "votes.head(3)"
      ],
      "id": "bdb2fa4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La carte choropl√®the suivante permet de visualiser rapidement les r√©sultats\n",
        "(l'Alaska et Hawa√Ø ont √©t√© exclus). \n"
      ],
      "id": "a4f7220d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "#| echo: true\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# republican : red, democrat : blue\n",
        "color_dict = {'republican': '#FF0000', 'democrats': '#0000FF'}\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12,12))\n",
        "grouped = votes.groupby('winner')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, column='winner', label=key, color=color_dict[key])\n",
        "plt.axis('off')"
      ],
      "id": "c714c724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les cartes choropl√®thes peuvent donner une impression fallacieuse \n",
        "ce qui exiplique que \n",
        "ce type de carte a servi \n",
        "de justification pour contester les r√©sultats du vote.\n",
        "En effet, un biais\n",
        "connu des repr√©sentations choropl√®thes est qu'elles donnent une importance\n",
        "visuelle excessive aux grands espaces. Or, ceux-ci sont souvent des espaces\n",
        "peu denses et influencent donc moins la variable d'int√©r√™t (en l'occurrence\n",
        "le taux de vote en faveur des r√©publicains/d√©mocrates). Une repr√©sentation √† \n",
        "privil√©gier pour ce type de ph√©nom√®nes est les\n",
        "ronds proportionnels (voir @inseeSemiologie, _\"Le pi√®ge territorial en cartographie\"_). \n",
        "\n",
        "\n",
        "Le [GIF \"Land does not vote, people do\"](https://www.core77.com/posts/90771/A-Great-Example-of-Better-Data-Visualization-This-Voting-Map-GIF)\n",
        "qui avait eu un certain succ√®s en 2020 propose un autre mode de visualisation.\n",
        "La carte originale a probablement √©t√© construite avec `JavaScript`. Cependant,\n",
        "on dispose avec `Python` de plusieurs outils\n",
        "pour r√©pliquer, √† faible co√ªt, cette carte \n",
        "gr√¢ce √†\n",
        "l'une des surcouches √† JavaScript vue dans la partie [visualisation](#visualisation). \n",
        "\n",
        "En l'occurrence, on peut utiliser `plotly` pour tenir compte de la population:\n",
        "\n",
        "\n",
        "{{< chart data=\"people_vote\" >}}\n",
        "\n",
        "\n",
        "\n",
        "La Figure a √©t√© obtenue avec le code suivant:\n"
      ],
      "id": "4ab6feba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| warning: false\n",
        "#| output: false\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "\n",
        "centroids = votes.copy()\n",
        "centroids.geometry = centroids.centroid\n",
        "centroids['size'] = centroids['CENSUS_2010_POP'] / 10000  # to get reasonable plotable number\n",
        "\n",
        "color_dict = {\"republican\": '#FF0000', 'democrats': '#0000FF'}\n",
        "centroids[\"winner\"] =  np.where(centroids['votes_gop'] > centroids['votes_dem'], 'republican', 'democrats') \n",
        "\n",
        "\n",
        "centroids['lon'] = centroids['geometry'].x\n",
        "centroids['lat'] = centroids['geometry'].y\n",
        "centroids = pd.DataFrame(centroids[[\"county_name\",'lon','lat','winner', 'CENSUS_2010_POP',\"state_name\"]])\n",
        "groups = centroids.groupby('winner')\n",
        "\n",
        "df = centroids.copy()\n",
        "\n",
        "df['color'] = df['winner'].replace(color_dict)\n",
        "df['size'] = df['CENSUS_2010_POP']/6000\n",
        "df['text'] = df['CENSUS_2010_POP'].astype(int).apply(lambda x: '<br>Population: {:,} people'.format(x))\n",
        "df['hover'] = df['county_name'].astype(str) +  df['state_name'].apply(lambda x: ' ({}) '.format(x)) + df['text']\n",
        "\n",
        "fig_plotly = go.Figure(data=go.Scattergeo(\n",
        "    locationmode = 'USA-states',\n",
        "    lon=df[\"lon\"], lat=df[\"lat\"],\n",
        "    text = df[\"hover\"],\n",
        "    mode = 'markers',\n",
        "    marker_color = df[\"color\"],\n",
        "    marker_size = df['size'],\n",
        "    hoverinfo=\"text\"\n",
        "    ))\n",
        "\n",
        "fig_plotly.update_traces(\n",
        "  marker = {'opacity': 0.5, 'line_color': 'rgb(40,40,40)', 'line_width': 0.5, 'sizemode': 'area'}\n",
        ")\n",
        "\n",
        "fig_plotly.update_layout(\n",
        "        title_text = \"Reproduction of the \\\"Acres don't vote, people do\\\" map <br>(Click legend to toggle traces)\",\n",
        "        showlegend = True,\n",
        "        geo = {\"scope\": 'usa', \"landcolor\": 'rgb(217, 217, 217)'}\n",
        "    )"
      ],
      "id": "24625f2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: false\n",
        "fig_plotly.write_json(\"people_vote.json\")\n",
        "fig_plotly.write_image(\"featured.png\")"
      ],
      "id": "6119b1eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les cercles proportionnels permettent ainsi √† l'oeil de se concentrer sur les \n",
        "zones les plus denses et non sur les grands espaces.\n",
        "\n",
        "# Explorer la structure des donn√©es\n",
        "\n",
        "La premi√®re √©tape n√©cessaire √† suivre avant de se lancer dans la mod√©lisation\n",
        "est de d√©terminer les variables √† inclure dans le mod√®le.\n",
        "\n",
        "Les fonctionnalit√©s de `pandas` sont, √† ce niveau, suffisantes pour explorer des structures simples.\n",
        "N√©anmoins, lorsqu'on est face √† un jeu de donn√©es pr√©sentant de\n",
        "nombreuses variables explicatives (*features* en machine learning, *covariates* en √©conom√©trie),\n",
        "il est souvent judicieux d'avoir une premi√®re √©tape de s√©lection de variables,\n",
        "ce que nous verrons par la suite dans la [partie d√©di√©e](https://linogaliana-teaching.netlify.app/lasso/).  \n",
        "\n",
        "Avant d'√™tre en mesure de s√©lectionner le meilleur ensemble de variables explicatives,\n",
        "nous allons en prendre un nombre restreint et arbitraire.\n",
        "La premi√®re t√¢che est de repr√©senter les relations entre les donn√©es,\n",
        "notamment la relation des variables explicatives\n",
        "√† la variable d√©pendante (le score du parti r√©publicain)\n",
        "ainsi que les relations entre les variables explicatives. \n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2 : Regarder les corr√©lations entre les variables</h3>\n",
        "```\n",
        "\n",
        "\n",
        "1. Cr√©er un DataFrame `df2` plus petit avec les variables `winner`, `votes_gop`, `Unemployment_rate_2019`,\n",
        "`Median_Household_Income_2019`,\n",
        "`Percent of adults with less than a high school diploma, 2015-19`,\n",
        "`Percent of adults with a bachelor's degree or higher, 2015-19`\n",
        "2. Repr√©senter gr√¢ce √† un graphique la matrice de corr√©lation avec `heatmap` de `seaborn`.\n",
        "3. Repr√©senter une matrice de nuages de points des variables de la base `df2` avec `pd.plotting.scatter_matrix`\n",
        "4. (optionnel) Refaire ces figures avec `plotly` qui offre √©galement la possibilit√© de faire une matrice de corr√©lation. \n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "04fe8bb8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: false\n",
        "\n",
        "# 1. Cr√©er le data.frame df2.\n",
        "df2 = votes.set_index(\"GEOID\").loc[: , [\"winner\", \"votes_gop\",\n",
        "          'Unemployment_rate_2019', 'Median_Household_Income_2019',\n",
        "          'Percent of adults with less than a high school diploma, 2015-19',\n",
        "          \"Percent of adults with a bachelor's degree or higher, 2015-19\"]]"
      ],
      "id": "54ff8fca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "# 2. Matrice de corr√©lation graphique\n",
        "g1 = sns.heatmap(df2.drop(\"winner\", axis = 1).corr(), cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "\n",
        "## Construction directement avec pandas √©galement possible\n",
        "g2 = df2.drop(\"winner\", axis = 1).corr().style.background_gradient(cmap='coolwarm').format('{:.2f}')"
      ],
      "id": "89735124",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La matrice construite avec `seaborn` (question 2) aura l'aspect suivant:\n"
      ],
      "id": "0a7ae424"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "g1.figure.get_figure()"
      ],
      "id": "2c9666a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alors que celle construite directement avec `corr` de `pandas`\n",
        "ressemblera plut√¥t √† ce tableau :\n"
      ],
      "id": "405e5267"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "g2"
      ],
      "id": "679feda3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le nuage de point obtenu √† l'issue de la question 3 ressemblera √† :\n"
      ],
      "id": "5897432a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "# 3. Matrice de nuages de points\n",
        "ax = pd.plotting.scatter_matrix(df2, figsize = (15,15))"
      ],
      "id": "801f40d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "ax"
      ],
      "id": "547bfe87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le r√©sultat de la question 4 devrait, quant √† lui,\n",
        "ressembler au graphique suivant :\n"
      ],
      "id": "ccf276f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        " \n",
        "# 4. Matrice de cor√©lation avec plotly\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "htmlsnip2 = px.scatter_matrix(df2)\n",
        "htmlsnip2.update_traces(diagonal_visible=False)\n",
        "#htmlsnip2"
      ],
      "id": "9dbe3c3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pour inclusion dans le site web\n",
        "htmlsnip2.write_json(\"scatter.json\")"
      ],
      "id": "fd8abf99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< chart data=\"scatter\" >}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Transformer les donn√©es\n",
        "\n",
        "Les diff√©rences d'√©chelle ou de distribution entre les variables peuvent \n",
        "diverger des hypoth√®ses sous-jacentes dans les mod√®les.\n",
        "\n",
        "Par exemple, dans le cadre\n",
        "de la r√©gression lin√©aire, les variables cat√©gorielles ne sont pas trait√©es √† la m√™me\n",
        "enseigne que les variables ayant valeur dans $\\mathbb{R}$. Une variable\n",
        "discr√®te (prenant un nombre fini de valeurs) devra √™tre transform√©es en suite de\n",
        "variables 0/1 par rapport √† une modalit√© de r√©f√©rence pour √™tre en ad√©quation\n",
        "avec les hypoth√®ses de la r√©gression lin√©aire.\n",
        "On appelle ce type de transformation\n",
        "*one-hot encoding*, sur lequel nous reviendrons. Il s'agit d'une transformation,\n",
        "parmi d'autres, disponibles dans `scikit` pour mettre en ad√©quation un jeu de\n",
        "donn√©es et des hypoth√®ses math√©matiques. \n",
        "\n",
        "L'ensemble de ces t√¢ches s'appelle le *preprocessing*. L'un des int√©r√™ts\n",
        "d'utiliser `scikit` est qu'on peut consid√©rer qu'une t√¢che de preprocessing\n",
        "est une t√¢che d'apprentissage (on apprend des param√®tres d'une structure \n",
        "de donn√©es) qui est r√©utilisable pour un jeu de donn√©es √† la structure\n",
        "similaire:\n",
        "\n",
        "![](scikit_predict.png)\n",
        "\n",
        "\n",
        "Nous allons voir deux processus tr√®s classiques de *preprocessing* : \n",
        "\n",
        "1. La **standardisation** transforme des donn√©es pour que la distribution empirique suive une loi $\\mathcal{N}(0,1)$.\n",
        "\n",
        "2. La **normalisation**  transforme les donn√©es de mani√®re √† obtenir une norme ($\\mathcal{l}_1$ ou $\\mathcal{l}_2$) unitaire. Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-danger\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-triangle-exclamation\"></i> Warning</h3>\n",
        "```\n",
        "\n",
        "Pour un statisticien,\n",
        "le terme `normalization` dans le vocable `scikit` peut avoir un sens contre-intuitif.\n",
        "On s'attendrait √† ce que la normalisation consiste √† transformer une variable de mani√®re √† ce que $X \\sim \\mathcal{N}(0,1)$.\n",
        "C'est, en fait, la **standardisation** en `scikit` qui fait cela.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Standardisation\n",
        "\n",
        "La standardisation consiste √† transformer des donn√©es pour que la distribution empirique suive une loi $\\mathcal{N}(0,1)$. Pour √™tre performants, la plupart des mod√®les de machine learning n√©cessitent souvent d'avoir des donn√©es dans cette distribution.\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 3: Standardisation</h3>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. Standardiser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.\n",
        "\n",
        "*Note : On obtient bien une distribution centr√©e √† z√©ro et on pourrait v√©rifier que la variance empirique soit bien √©gale √† 1. On pourrait aussi v√©rifier que ceci est vrai √©galement quand on transforme plusieurs colonnes √† la fois.*\n",
        "\n",
        "\n",
        "2. Cr√©er `scaler`, un `Transformer` que vous construisez sur les 1000 premi√®res lignes de votre DataFrame `df2`  √†  l'exception de la variable √† expliquer `winner`. V√©rifier la moyenne et l'√©cart-type de chaque colonne sur ces m√™mes observations.\n",
        "\n",
        "*Note : Les param√®tres qui seront utilis√©s pour une standardisation ult√©rieure sont stock√©s dans les attributs `.mean_` et `.scale_`*\n",
        "\n",
        "On peut voir ces attributs comme des param√®tres entra√Æn√©s sur un certain jeu de\n",
        "donn√©es et qu'on peut r√©utiliser sur un autre, √† condition que les\n",
        "dimensions co√Øncident.\n",
        "\n",
        "3. Appliquer `scaler` sur les autres lignes du DataFrame et comparer les distributions obtenues de la variable `Median_Household_Income_2019`.\n",
        "\n",
        "*Note : Une fois appliqu√©s √† un autre `DataFrame`, on peut remarquer que la distribution n'est pas exactement centr√©e-r√©duite dans le `DataFrame` sur lequel les param√®tres n'ont pas √©t√© estim√©s. C'est normal, l'√©chantillon initial n'√©tait pas al√©atoire, les moyennes et variances de cet √©chantillon n'ont pas de raison de co√Øncider avec les moments de l'√©chantillon complet.*\n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "cc0200cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: false\n",
        "\n",
        "# 1. Standardisation de Median_Household_Income_2019 et histogramme\n",
        "from sklearn import preprocessing\n",
        "df2['y_standard'] = preprocessing.scale(df2['Median_Household_Income_2019'])\n",
        "f, axes = plt.subplots(2, figsize=(10, 10))\n",
        "sns.distplot(df2[\"Median_Household_Income_2019\"] , color=\"skyblue\", ax=axes[0])\n",
        "sns.distplot(df2[\"y_standard\"] , color=\"olive\", ax=axes[1])\n",
        "#print(df2['y_standard'].mean())\n",
        "#print(df2['y_standard'].var())"
      ],
      "id": "5cfe9681",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "#plt.savefig('standardisation.png', bbox_inches='tight')\n",
        "# ax"
      ],
      "id": "c1c57ea7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 2. Cr√©er un scaler\n",
        "df2 = df2.drop(\"winner\", axis = 1)\n",
        "print(\"Moyenne de chaque variable sur 1000 premi√®res observations avant : \", np.array(df2.head(1000).mean(axis=0)))\n",
        "print(\"Ecart-type de chaque variable sur 1000 premi√®res observations avant : \", np.array(df2.head(1000).std(axis=0)))\n",
        "scaler = preprocessing.StandardScaler().fit(df2.head(1000))\n",
        "scaler.transform(df2.head(1000))\n",
        "print(\"Moyenne de chaque variable sur 1000 premi√®res observations apr√®s : \", scaler.transform(df2.head(1000)).mean(axis=0))\n",
        "print(\"Ecart-type de chaque variable sur 1000 premi√®res observations apr√®s : \", scaler.transform(df2.head(1000)).std(axis=0))\n",
        "#print(scaler.mean_)\n",
        "#print(scaler.scale_)"
      ],
      "id": "40484352",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 3. Appliquer le scaler √† toutes les autres lignes\n",
        "X1 = scaler.transform(df2.head(1000))\n",
        "X2 = scaler.transform(df2[1000:])\n",
        "col_pos = df2.columns.get_loc(\"Median_Household_Income_2019\")\n",
        "# X2.mean(axis = 0)\n",
        "# X2.std(axis = 0)\n",
        "f, axes = plt.subplots(2, figsize=(10, 10))\n",
        "sns.distplot(X1[:,col_pos] , color=\"skyblue\", ax=axes[0])\n",
        "sns.distplot(X2[:,col_pos] , color=\"olive\", ax=axes[1])"
      ],
      "id": "0892ffde",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#plt.savefig('standardisation2.png', bbox_inches='tight')\n",
        "#axes"
      ],
      "id": "9ac0f57b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalisation\n",
        "\n",
        "La **normalisation** est l'action de transformer les donn√©es de mani√®re\n",
        "√† obtenir une norme ($\\mathcal{l}_1$ ou $\\mathcal{l}_2$) unitaire.\n",
        "Autrement dit, avec la norme ad√©quate, la somme des √©l√©ments est √©gale √† 1.\n",
        "Par d√©faut, la norme est dans $\\mathcal{l}_2$.\n",
        "Cette transformation est particuli√®rement utilis√©e en classification de texte ou pour effectuer du *clustering*.\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 4 : Normalisation</h3>\n",
        "```\n",
        "\n",
        "\n",
        "1. Normaliser la variable `Median_Household_Income_2019` (ne pas √©craser les valeurs !) et regarder l'histogramme avant/apr√®s normalisation.\n",
        "2. V√©rifier que la norme $\\mathcal{l}_2$ est bien √©gale √† 1.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "c652072b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: false\n",
        "\n",
        "# 1. Normalisation de Median_Household_Income_2019 et histogrammes\n",
        "scaler = preprocessing.Normalizer().fit(df2.dropna(how = \"any\").head(1000))\n",
        "X1 = scaler.transform(df2.dropna(how = \"any\").head(1000))\n",
        "\n",
        "f, axes = plt.subplots(2, figsize=(10, 10))\n",
        "sns.distplot(df2[\"Median_Household_Income_2019\"] , color=\"skyblue\", ax=axes[0])\n",
        "sns.distplot(X1[:,col_pos] , color=\"olive\", ax=axes[1])"
      ],
      "id": "e5267973",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#plt.savefig('normalisation.png', bbox_inches='tight')\n",
        "# axes"
      ],
      "id": "56c4dd76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "#| echo: false\n",
        "\n",
        "# 2. V√©rification de la norme L2\n",
        "np.sqrt(np.sum(X1**2, axis=1))[:10] # L2-norm"
      ],
      "id": "3a229003",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-danger\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-triangle-exclamation\"></i> Warning</h3>\n",
        "```\n",
        "\n",
        "`preprocessing.Normalizer` n'accepte pas les valeurs manquantes, alors que `preprocessing.StandardScaler()` s'en accomode (dans la version `0.22` de scikit). Pour pouvoir ais√©ment appliquer le *normalizer*, il faut\n",
        "\n",
        "* retirer les valeurs manquantes du DataFrame avec la m√©thode `dropna`: `df.dropna(how = \"any\")`;\n",
        "* ou les imputer avec un mod√®le ad√©quat. [`scikit` permet de le faire](https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values).\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Encodage des valeurs cat√©gorielles\n",
        "\n",
        "Les donn√©es cat√©gorielles doivent √™tre recod√©es\n",
        "sous forme de valeurs num√©riques pour √™tre int√©gr√©s aux mod√®les de *machine learning*.\n",
        "Cela peut √™tre fait de plusieurs mani√®res :\n",
        "\n",
        "* `LabelEncoder`: transforme un vecteur `[\"a\",\"b\",\"c\"]` en vecteur num√©rique `[0,1,2]`.\n",
        "Cette approche a l'inconv√©nient d'introduire un ordre dans les modalit√©s, ce qui n'est pas toujours souhaitable\n",
        "\n",
        "* `OrdinalEncoder`: une version g√©n√©ralis√©e du `LabelEncoder` qui a vocation √† s'appliquer sur des matrices ($X$),\n",
        "alors que `LabelEncoder` s'applique plut√¥t √† un vecteur ($y$)\n",
        "\n",
        "* `pandas.get_dummies` effectue une op√©ration de *dummy expansion*.\n",
        "Un vecteur de taille *n* avec *K* cat√©gories sera transform√© en matrice de taille $n \\times K$\n",
        "pour lequel chaque colonne sera une variable *dummy* pour la modalit√© *k*.\n",
        "Il y a ici $K$ modalit√©s et il y a donc multicolin√©arit√©.\n",
        "Avec une r√©gression lin√©aire avec constante,\n",
        "il convient de retirer une modalit√© avant l'estimation.\n",
        "\n",
        "* `OneHotEncoder` est une version g√©n√©ralis√©e (et optimis√©e) de la *dummy expansion*.\n",
        "Il a plut√¥t vocation √† s'appliquer sur les *features* ($X$) du mod√®le\n",
        "\n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 5 : Encoder des variables cat√©gorielles</h3>\n",
        "```\n",
        "\n",
        "\n",
        "1. Cr√©er `df` qui conserve uniquement les variables `state_name` et `county_name` dans `votes`.\n",
        "2. Appliquer √† `state_name` un `LabelEncoder`\n",
        "*Note : Le r√©sultat du label encoding est relativement intuitif, notamment quand on le met en relation avec le vecteur initial.*\n",
        "\n",
        "3. Regarder la *dummy expansion* de `state_name`\n",
        "4. Appliquer un `OrdinalEncoder` √† `df[['state_name', 'county_name']]`\n",
        "*Note : Le r√©sultat du _ordinal encoding_ est coh√©rent avec celui du label encoding*\n",
        "\n",
        "5. Appliquer un `OneHotEncoder` √† `df[['state_name', 'county_name']]`\n",
        "\n",
        "*Note : `scikit` optimise l'objet n√©cessaire pour stocker le r√©sultat d'un mod√®le de transformation. Par exemple, le r√©sultat de l'encoding One Hot est un objet tr√®s volumineux. Dans ce cas, `scikit` utilise une matrice Sparse.*\n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "455be88b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#1. Cr√©ation de df\n",
        "df = votes[[\"state_name\",'county_name']]"
      ],
      "id": "4c819966",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#2. Appliquer un LabelEncoder √† stat_name\n",
        "label_enc = preprocessing.LabelEncoder().fit(df['state_name'])\n",
        "np.column_stack((label_enc.transform(df['state_name']),df['state_name']))"
      ],
      "id": "a30acf2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 3. dummy expansion de state_name\n",
        "pd.get_dummies(df['state_name'])"
      ],
      "id": "571d5296",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 4. OrdinalEncoder\n",
        "ord_enc = preprocessing.OrdinalEncoder().fit(df)\n",
        "# ord_enc.transform(df[['state', 'county']])\n",
        "ord_enc.transform(df)[:,0]"
      ],
      "id": "8ce7a4f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 5. OneHotEncoder\n",
        "onehot_enc = preprocessing.OneHotEncoder().fit(df)\n",
        "onehot_enc.transform(df)"
      ],
      "id": "b4fa6ace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R√©f√©rences\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "6a4d3a2c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}