{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Nettoyer un texte: des exercices pour découvrir l'approche bag-of-words\"\n",
        "date: 2020-10-29T13:00:00Z\n",
        "draft: false\n",
        "weight: 20\n",
        "slug: nlpexo\n",
        "tags:\n",
        "  - NLP\n",
        "  - nltk\n",
        "  - Littérature\n",
        "  - preprocessing\n",
        "  - Exercice\n",
        "categories:\n",
        "  - NLP\n",
        "  - Exercice\n",
        "type: book\n",
        "summary: |\n",
        "  Ce chapitre continue de présenter l'approche de __nettoyage de données__ \n",
        "  du `NLP` en s'appuyant sur le corpus de trois auteurs\n",
        "  anglo-saxons : Mary Shelley, Edgar Allan Poe, H.P. Lovecraft.\n",
        "  Dans cette série d'exercice nous mettons en oeuvre de manière\n",
        "  plus approfondie les différentes méthodes présentées\n",
        "  précedemment.\n",
        "bibliography: ../../../../reference.bib\n",
        "---"
      ],
      "id": "18de4ac6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}"
      ],
      "id": "cf8e589d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: 'asis'\n",
        "#| include: true\n",
        "#| eval: true\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, '../../../../') #insert the utils module\n",
        "from utils import print_badges\n",
        "\n",
        "#print_badges(__file__)\n",
        "print_badges(\"content/course/NLP/02_exoclean.qmd\")"
      ],
      "id": "4661d551",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "Cette page approfondit certains aspects présentés dans la\n",
        "[partie introductive](#nlp). Après avoir travaillé sur le\n",
        "*Comte de Monte Cristo*, on va continuer notre exploration de la littérature\n",
        "avec cette fois des auteurs anglophones :\n",
        "\n",
        "* Edgar Allan Poe, (EAP) ;\n",
        "* HP Lovecraft (HPL) ;\n",
        "* Mary Wollstonecraft Shelley (MWS).\n",
        "\n",
        "Les données sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent être requétées via l'url \n",
        "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
        "\n",
        "Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquemment utilisés par les auteurs, de les représenter graphiquement.\n",
        "On prendra appui sur l'approche *bag of words* présentée dans le chapitre précédent[^1].\n",
        "\n",
        "\n",
        "[^1]: L'approche *bag of words* est déjà, si on la pousse à ses limites, très intéressante. Elle peut notamment\n",
        "faciliter la mise en cohérence de différents corpus\n",
        "par la méthode des appariements flous\n",
        "(cf. [@galianafuzzy](https://epic-davinci-acb57b.netlify.app/#1).\n",
        "Le [chapitre sur ElasticSearch](#elastic) présent dans cette partie du cours présente quelques\n",
        "éléments de ce travail sur les données de l'`OpenFoodFacts`\n",
        "\n",
        "\n",
        "Ce notebook est librement inspiré de  : \n",
        "\n",
        "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras\n",
        "* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg\n",
        "* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook\n",
        "\n",
        "Les chapitres suivants permettront d'introduire aux enjeux de modélisation\n",
        "de corpus textuels. Dans un premier temps, le modèle `LDA` permettra d'explorer\n",
        "le principe des modèles bayésiens à couche cachées pour modéliser les sujets (*topics*)\n",
        "présents dans un corpus et segmenter ces _topics_ selon les mots qui les composent.\n",
        "\n",
        "Le dernier chapitre de la partie visera à\n",
        "prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`.\n",
        "Cela sera un pas supplémentaire dans la formalisation puisqu'il s'agira de\n",
        "représenter chaque mot d'un texte sous forme d'un vecteur de grande dimension, ce\n",
        "qui nous permettra de rapprocher les mots entre eux dans un espace complexe.\n",
        "Cette technique, dite des plongements de mots (_Word Embedding_),\n",
        "permet ainsi de transformer une information complexe difficilement quantifiable\n",
        "comme un mot\n",
        "en un objet numérique qui peut ainsi être rapproché d'autres par des méthodes\n",
        "algébriques. Pour découvrir ce concept, ce [post de blog](https://ssphub.netlify.app/post/word-embedding/)\n",
        "est particulièrement utile. En pratique, la technique des\n",
        "plongements de mots permet d'obtenir des tableaux comme celui-ci:\n",
        "\n",
        "![](word_embedding.png)\n",
        "\n",
        "\n",
        "# Librairies nécessaires\n",
        "\n",
        "Cette page évoquera les principales librairies pour faire du NLP, notamment : \n",
        "\n",
        "* [WordCloud](https://github.com/amueller/word_cloud)\n",
        "* [nltk](https://www.nltk.org/)\n",
        "* [SpaCy](https://spacy.io/)\n",
        "* [Keras](https://keras.io/)\n",
        "* [TensorFlow](https://www.tensorflow.org/)\n",
        "\n",
        "Il faudra également installer les librairies `gensim` et `pywaffle`\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Hint</h3>\n",
        "```\n",
        "\n",
        "Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments pour que `NTLK` puisse fonctionner correctement. Pour cela, faire :\n",
        "\n",
        "~~~python\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('genesis')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "~~~\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "La liste des modules à importer est assez longue, la voici :\n"
      ],
      "id": "1ba784f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: hide\n",
        "#| echo: true\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import base64\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from collections import Counter\n",
        "from time import time\n",
        "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install pywaffle\n",
        "from pywaffle import Waffle\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('genesis')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "id": "44e094ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Données utilisées\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 1 : Importer les données spooky</h3>\n",
        "```\n",
        "\n",
        "\n",
        "*Pour ceux qui ont envie de tester leurs connaissances en pandas*\n",
        "\n",
        "1. Importer le jeu de données `spooky` à partir de l'URL <https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv> sous le nom `train`. L'encoding est `latin-1`\n",
        "2. Mettre des majuscules au nom des colonnes.\n",
        "3. Retirer le prefix `id` de la colonne `Id` et appeler la nouvelle colonne `ID`.\n",
        "4. Mettre l'ancienne colonne `Id` en index. \n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "0eb444cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd"
      ],
      "id": "a4770070",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#1. Import des données\n",
        "url='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
        "train = pd.read_csv(url,encoding='latin-1')"
      ],
      "id": "542bb7d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#2. Majuscules aux noms des colonnes\n",
        "train.columns = train.columns.str.capitalize()"
      ],
      "id": "84186416",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#3. Retirer le prefixe id\n",
        "train['ID'] = train['Id'].str.replace(\"id\",\"\")"
      ],
      "id": "1058e55e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#4. Mettre Id en index\n",
        "train = train.set_index('Id')\n",
        "#train.head()"
      ],
      "id": "4aa80bb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si vous ne faites pas l'exercice 1, pensez à charger les données en executant la fonction `get_data.py` :\n"
      ],
      "id": "f77d95e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/linogaliana/python-datascientist/master/content/course/NLP/get_data.py'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('getdata.py', 'wb').write(r.content)\n",
        "\n",
        "import getdata\n",
        "train = getdata.create_train_dataframes()"
      ],
      "id": "a68dd604",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce code introduit une base nommée `train` dans l'environnement.\n",
        "\n",
        "Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite : \n"
      ],
      "id": "7fe3a472"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train.head()"
      ],
      "id": "88c0267a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "sampsize = train.shape[0]"
      ],
      "id": "b1d59b9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut se rendre compte que les extraits des 3 auteurs ne sont\n",
        "pas forcément équilibrés dans le jeu de données.\n",
        "Il faudra en tenir compte dans la prédiction. \n"
      ],
      "id": "4d9f6141"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "fig = plt.figure()\n",
        "g = sns.barplot(x=['Edgar Allen Poe', 'Mary W. Shelley', 'H.P. Lovecraft'], y=train['Author'].value_counts())"
      ],
      "id": "cd22bf76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "\n",
        "L'approche *bag of words* est présentée de\n",
        "manière plus extensive dans le [chapitre précédent](#nlp).\n",
        "\n",
        "L'idée est d'étudier la fréquence des mots d'un document et la\n",
        "surreprésentation des mots par rapport à un document de\n",
        "référence (appelé *corpus*).\n",
        "\n",
        "Cette approche un peu simpliste mais très\n",
        "efficace : on peut calculer des scores permettant par exemple de faire\n",
        "de classification automatique de document par thème, de comparer la\n",
        "similarité de deux documents. Elle est souvent utilisée en première analyse,\n",
        "et elle reste la référence pour l'analyse de textes mal\n",
        "structurés (tweets, dialogue tchat, etc.). \n",
        "\n",
        "Les analyses tf-idf (*term frequency-inverse document frequency*) ou les\n",
        "constructions d'indices de similarité cosinus reposent sur ce type d'approche.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Fréquence d'un mot\n",
        "\n",
        "Avant de s'adonner à une analyse systématique du champ lexical de chaque\n",
        "auteur, on se focaliser dans un premier temps sur un unique mot, le mot *fear*. \n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-comment\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "L'exercice ci-dessous présente une représentation graphique nommée \n",
        "*waffle chart*. Il s'agit d'une approche préférable aux\n",
        "camemberts qui sont des graphiques manipulables car l'oeil humain se laisse\n",
        "facilement berner par cette représentation graphique qui ne respecte pas\n",
        "les proportions. \n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 2 : Fréquence d'un mot</h3>\n",
        "```\n",
        "\n",
        "\n",
        "1. Compter le nombre de phrases, pour chaque auteur, où apparaît le mot `fear`.\n",
        "2. Utiliser `pywaffle` pour obtenir les graphiques ci-dessous qui résument\n",
        "de manière synthétique le nombre d'occurrences du mot *\"fear\"* par auteur.\n",
        "3. Refaire l'analyse avec le mot *\"horror\"*. \n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "A l'issue de la question 1, vous devriez obtenir le tableau\n",
        "de fréquence suivant:\n"
      ],
      "id": "851ce481"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| include: false\n",
        "\n",
        "#1. Compter le nombre de phrase pour chaque auteur avec fear\n",
        "def nb_occurrences(word, train_data):\n",
        "    train_data['wordtoplot'] = train_data['Text'].str.contains(word).astype(int)\n",
        "    table = train_data.groupby('Author').sum()\n",
        "    data = table.to_dict()['wordtoplot']\n",
        "    return table\n",
        "  \n",
        "table = nb_occurrences(\"fear\", train)"
      ],
      "id": "88880b6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "table.head()"
      ],
      "id": "fda6bf57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "\n",
        "#2. Faire un graphique d'occurences avc pywaffle\n",
        "def graph_occurrence(word, train_data):\n",
        "    table = nb_occurrences(word, train_data)\n",
        "    data = table.to_dict()['wordtoplot']\n",
        "    fig = plt.figure(\n",
        "        FigureClass=Waffle, \n",
        "        rows=15, \n",
        "        values=data, \n",
        "        title={'label': 'Utilisation du mot \"%s\" par les auteurs' %word, 'loc': 'left'},\n",
        "        labels=[\"{0} ({1})\".format(k, v) for k, v in data.items()]\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "fig = graph_occurrence(\"fear\", train)"
      ],
      "id": "dce3fe07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ceci permet d'obtenir le _waffle chart_ suivant:\n"
      ],
      "id": "5ac8d2a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure()"
      ],
      "id": "ffd87bd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque ainsi de manière très intuitive\n",
        "le déséquilibre de notre jeu de données\n",
        "lorsqu'on se focalise sur le terme _\"peur\"_\n",
        "où Mary Shelley représente près de 50%\n",
        "des observations.\n"
      ],
      "id": "83ac2477"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure().savefig(\"featured.png\")"
      ],
      "id": "8d2837b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si on reproduit cette analyse avec le terme _\"horror\"_, on peut\n",
        "en conclure que la peur est plus évoquée par Mary Shelley\n",
        "(sentiment assez naturel face à la créature du docteur Frankenstein) alors\n",
        "que Lovecraft n'a pas volé sa réputation d'écrivain de l'horreur !\n"
      ],
      "id": "153709d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#3. Graphe d'occurences avec le mot horror\n",
        "fig = graph_occurrence(\"horror\", train)"
      ],
      "id": "d0e96a98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure()"
      ],
      "id": "e8a8a721",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Premier *wordcloud*\n",
        "\n",
        "Pour aller plus loin dans l'analyse du champ lexical de chaque auteur,\n",
        "on peut représenter un `wordcloud` qui permet d'afficher chaque mot avec une\n",
        "taille proportionnelle au nombre d'occurrence de celui-ci.\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 3 : Wordcloud</h3>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. En utilisant la fonction `wordCloud`, faire trois nuages de mot pour représenter les mots les plus utilisés par chaque auteur.\n",
        "2. Calculer les 25 mots plus communs pour chaque auteur et représenter les trois histogrammes des décomptes. \n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "acfabb50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "\n",
        "#1. Wordclouds\n",
        "def graph_wordcloud(author, train_data, varname = \"Text\"):\n",
        "  txt = train_data[train_data['Author']==author][varname]\n",
        "  all_text = ' '.join([text for text in txt])\n",
        "  wordcloud = WordCloud(width=800, height=500,\n",
        "                      random_state=21,\n",
        "                      max_words=2000,\n",
        "                      background_color = \"white\",\n",
        "                      colormap='Set2').generate(all_text)\n",
        "  return wordcloud\n",
        "\n",
        "n_topics = [\"HPL\",\"EAP\",\"MWS\"]\n",
        "\n",
        "fig = plt.figure(figsize=(15, 12))\n",
        "for i in range(len(n_topics)):\n",
        "    ax = fig.add_subplot(2,2,i+1)\n",
        "    wordcloud = graph_wordcloud(n_topics[i], train)\n",
        "\n",
        "    ax.imshow(wordcloud)\n",
        "    ax.axis('off')"
      ],
      "id": "001ae3fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le _wordcloud_ pour nos différents auteurs est le suivant:\n"
      ],
      "id": "18c7153f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure()"
      ],
      "id": "e86fb49f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enfin, si on fait un histogramme des fréquences,\n",
        "cela donnera :\n"
      ],
      "id": "2aabba6b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "\n",
        "#2. Histogramme de décompte\n",
        "count_words = pd.DataFrame({'counter' : train\n",
        "    .groupby('Author')\n",
        "    .apply(lambda s: ' '.join(s['Text']).split())\n",
        "    .apply(lambda s: Counter(s))\n",
        "    .apply(lambda s: s.most_common(25))\n",
        "    .explode()}\n",
        ")\n",
        "count_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(),\n",
        "index=count_words.index)\n",
        "count_words = count_words.reset_index()\n",
        "g = sns.FacetGrid(count_words, row=\"Author\")\n",
        "g.map_dataframe(sns.barplot, x=\"word\", y=\"count\")"
      ],
      "id": "39c26be9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "g.figure.get_figure()"
      ],
      "id": "01486ad2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On voit ici que ce sont des mots communs, comme *\"the\"*, *\"of\"*, etc. sont très\n",
        "présents. Mais ils sont peu porteurs d'information, on peut donc les éliminer\n",
        "avant de faire une analyse syntaxique poussée.\n",
        "Ceci est une démonstration par l'exemple qu'il vaut mieux nettoyer le texte avant de \n",
        "l'analyser (sauf si on est intéressé\n",
        "par la loi de Zipf, cf. exercice suivant).\n",
        "\n",
        "\n",
        "\n",
        "## Aparté: la loi de Zipf\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> La loi de Zipf</h3>\n",
        "```\n",
        "\n",
        "Dans son sens strict, la loi de Zipf prévoit que\n",
        "dans un texte donné, la fréquence d'occurrence $f(n_i)$ d'un mot est\n",
        "liée à son rang $n_i$ dans l'ordre des fréquences par une loi de la forme\n",
        "$f(n_i) = c/n_i$ où $c$ est une constante. Zipf, dans les années 1930, se basait sur l'oeuvre \n",
        "de Joyce, *Ulysse* pour cette affirmation. \n",
        "\n",
        "Plus généralement, on peut dériver la loi de Zipf d'une distribution exponentielle des fréquences: $f(n_i) = cn_{i}^{-k}$. Cela permet d'utiliser la famille des modèles linéaires généralisés, notamment les régressions poissonniennes, pour mesurer les paramètres de la loi. Les modèles linéaire traditionnels en `log` souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d'un modèle gravitaire, où appliquer des OLS est une mauvaise idée, cf. [@galiana2020segregation](https://linogaliana.netlify.app/publication/2020-segregation/) pour les limites).\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "Un modèle exponentiel peut se représenter par un modèle de Poisson ou, si \n",
        "les données sont très dispersées, par un modèle binomial négatif. Pour\n",
        "plus d'informations, consulter l'annexe de @galiana2020segregation. \n",
        "La technique économétrique associée pour l'estimation est \n",
        "les modèles linéaires généralisés (GLM) qu'on peut \n",
        "utiliser en `Python` via le\n",
        "package `statsmodels`[^3]:\n",
        "\n",
        "[^3]: La littérature sur les modèles gravitaires, présentée dans @galiana2020segregation, \n",
        "donne quelques arguments pour privilégier les modèles GLM à des modèles log-linéaires\n",
        "estimés par moindres carrés ordinaires. \n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\bigg( f(n_i)|n_i \\bigg) = \\exp(\\beta_0 + \\beta_1 \\log(n_i))\n",
        "$$\n",
        "\n",
        "\n",
        "Prenons les résultats de l'exercice précédent et enrichissons les du rang et de la fréquence d'occurrence d'un mot : \n"
      ],
      "id": "4c301716"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "count_words = pd.DataFrame({'counter' : train\n",
        "    .groupby('Author')\n",
        "    .apply(lambda s: ' '.join(s['Text']).split())\n",
        "    .apply(lambda s: Counter(s))\n",
        "    .apply(lambda s: s.most_common())\n",
        "    .explode()}\n",
        ")\n",
        "count_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)\n",
        "count_words = count_words.reset_index()\n",
        "\n",
        "count_words = count_words.assign(\n",
        "    tot_mots_auteur = lambda x: (x.groupby(\"Author\")['count'].transform('sum')),\n",
        "    freq = lambda x: x['count'] /  x['tot_mots_auteur'],\n",
        "    rank = lambda x: x.groupby(\"Author\")['count'].transform('rank', ascending = False)\n",
        ")"
      ],
      "id": "78eb2d55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Commençons par représenter la relation entre la fréquence et le rang:\n"
      ],
      "id": "f9d4d7c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: true\n",
        "g = sns.lmplot(y = \"freq\", x = \"rank\", hue = 'Author', data = count_words, fit_reg = False)\n",
        "g.set(xscale=\"log\", yscale=\"log\")\n",
        "g"
      ],
      "id": "e87c4eaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous avons bien, graphiquement, une relation log-linéaire entre les deux:\n"
      ],
      "id": "b86c6bdd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "g.figure.get_figure()"
      ],
      "id": "7498270b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avec `statsmodels`, vérifions plus formellement cette relation:\n"
      ],
      "id": "ca17ce19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "exog = sm.add_constant(np.log(count_words['rank'].astype(float)))\n",
        "\n",
        "model = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()\n",
        "\n",
        "# Afficher les résultats du modèle\n",
        "print(model.summary())"
      ],
      "id": "488ca0f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le coefficient de la régression est presque 1 ce qui suggère bien une relation\n",
        "quasiment log-linéaire entre le rang et la fréquence d'occurrence d'un mot. \n",
        "Dit autrement, le mot le plus utilisé l'est deux fois plus que le deuxième\n",
        "mois le plus fréquent qui l'est trois plus que le troisième, etc.\n",
        "\n",
        "# Nettoyage d'un texte\n",
        "\n",
        "Les premières étapes dans le nettoyage d'un texte, qu'on a\n",
        "développé au cours du [chapitre précédent](#nlp), sont :\n",
        "\n",
        "* suppression de la ponctuation\n",
        "* suppression des *stopwords*\n",
        "\n",
        "Cela passe par la tokenisation d'un texte, c'est-à-dire la décomposition\n",
        "de celui-ci en unités lexicales (les *tokens*).\n",
        "Ces unités lexicales peuvent être de différentes natures,\n",
        "selon l'analyse que l'on désire mener.\n",
        "Ici, on va définir les tokens comme étant les mots utilisés.\n",
        "\n",
        "Plutôt que de faire soi-même ce travail de nettoyage,\n",
        "avec des fonctions mal optimisées,\n",
        "on peut utiliser la librairie `nltk` comme détaillé [précédemment](#nlp). \n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 4 : Nettoyage du texte</h3>\n",
        "```\n",
        "\n",
        "\n",
        "Repartir de `train`, notre jeu de données d'entraînement. Pour rappel, `train` a la structure suivante:\n",
        "\n",
        "1. Tokeniser chaque phrase avec `nltk`.\n",
        "2. Retirer les stopwords avec `nltk`.\n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "Pour rappel, au début de l'exercice, le `DataFrame` présente l'aspect suivant:\n"
      ],
      "id": "d12042cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "train.head(2)"
      ],
      "id": "4c02ead1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Après tokenisation, il devrait avoir cet aspect :\n"
      ],
      "id": "e4aad755"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: true\n",
        "#| echo: false\n",
        "\n",
        "#1. Tokenisation\n",
        "train_clean = (train\n",
        "    .groupby([\"ID\",\"Author\"])\n",
        "    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))\n",
        "    .apply(lambda words: [word for word in words if word.isalpha()])\n",
        ")\n",
        "train_clean.head(2)"
      ],
      "id": "6e4f7e58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Après le retrait des stopwords, cela donnera:\n"
      ],
      "id": "3b7ec97b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#2. Enlever les stopwords.\n",
        "from nltk.corpus import stopwords  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "train_clean = (train_clean\n",
        "    .apply(lambda words: [w for w in words if not w in stop_words])\n",
        "    .reset_index(name='tokenized')\n",
        ")\n",
        "\n",
        "train_clean.head(2)"
      ],
      "id": "35fc909f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-lightbulb\"></i> Hint</h3>\n",
        "```\n",
        "\n",
        "La méthode `apply` est très pratique ici car nous avons une phrase par ligne. Plutôt que de faire un `DataFrame` par auteur, ce qui n'est pas une approche très flexible, on peut directement appliquer la tokenisation\n",
        "sur notre `DataFrame` grâce à `apply`, sans le diviser.\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "Ce petit nettoyage permet d'arriver à un texte plus intéressant en termes d'analyse lexicale. Par exemple, si on reproduit l'analyse précédente... :\n"
      ],
      "id": "e4586223"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: true\n",
        "train_clean[\"Text\"] = train_clean['tokenized'].apply(lambda s: \" \".join(map(str, s)))\n",
        "\n",
        "n_topics = [\"HPL\",\"EAP\",\"MWS\"]\n",
        "\n",
        "fig = plt.figure(figsize=(15, 12))\n",
        "for i in range(len(n_topics)):\n",
        "    ax = fig.add_subplot(2,2,i+1)\n",
        "    wordcloud = graph_wordcloud(n_topics[i], train_clean)\n",
        "\n",
        "    ax.imshow(wordcloud)\n",
        "    ax.axis('off')\n",
        "\n",
        "fig"
      ],
      "id": "a6a4aa08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure()"
      ],
      "id": "eb7bc89d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour aller plus loin dans l'harmonisation d'un texte, il est possible de\n",
        "mettre en place les classes d'équivalence développées dans la \n",
        "[partie précédente](#nlp) afin de remplacer différentes variations d'un même\n",
        "mot par une forme canonique :\n",
        "\n",
        "* la **racinisation** (*stemming*) assez fruste mais rapide, notamment\n",
        "en présence de fautes d’orthographe. Dans ce cas, _chevaux_ peut devenir _chev_\n",
        "mais être ainsi confondu avec _chevet_ ou _cheveux_.\n",
        "Cette méthode est généralement plus simple à mettre en oeuvre, quoique\n",
        "plus fruste. \n",
        "\n",
        "* la **lemmatisation** qui requiert la connaissance des statuts\n",
        "grammaticaux (exemple : _chevaux_ devient _cheval_).\n",
        "Elle est mise en oeuvre, comme toujours avec `nltk`, à travers un\n",
        "modèle. En l'occurrence, un `WordNetLemmatizer`  (WordNet est une base\n",
        "lexicographique ouverte). Par exemple, les mots *\"women\"*, *\"daughters\"*\n",
        "et *\"leaves\"* seront ainsi lemmatisés de la manière suivante :\n"
      ],
      "id": "f0f27971"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemm = WordNetLemmatizer()\n",
        "\n",
        "for word in [\"women\",\"daughters\", \"leaves\"]:\n",
        "    print(\"The lemmatized form of %s is: {}\".format(lemm.lemmatize(word)) % word)"
      ],
      "id": "b5358b23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "Pour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,\n",
        "télécharger celui-ci grâce aux commandes suivantes:\n",
        "\n",
        "~~~python\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "~~~\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "On va se restreindre au corpus d'Edgar Allan Poe et repartir de la base de données\n",
        "brute:\n"
      ],
      "id": "d3a3d9bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eap_clean = train[train[\"Author\"] == \"EAP\"]\n",
        "eap_clean = ' '.join(eap_clean['Text'])\n",
        "#Tokenisation naïve sur les espaces entre les mots => on obtient une liste de mots\n",
        "#tokens = eap_clean.split()\n",
        "word_list = nltk.word_tokenize(eap_clean)"
      ],
      "id": "345ddef7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 5 : Lemmatisation avec nltk</h3>\n",
        "```\n",
        "\n",
        "\n",
        "Utiliser un `WordNetLemmatizer` et observer le résultat.\n",
        "\n",
        "Optionnel: Effectuer la même tâche avec `spaCy`\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "Le `WordNetLemmatizer` donnera le résultat suivant:\n"
      ],
      "id": "5c7bd4a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#Exercice 5 : WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "\n",
        "print(\" \".join(word_list[:43]))\n",
        "print(\"---------------------------\")\n",
        "print(lemmatized_output[:209])"
      ],
      "id": "c56baf35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF: calcul de fréquence\n",
        "\n",
        "Le calcul [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) (term _frequency–inverse document frequency_)\n",
        "permet de calculer un score de proximité entre un terme de recherche et un\n",
        "document (c'est ce que font les moteurs de recherche). \n",
        "\n",
        "* La partie `tf` calcule une fonction croissante de la fréquence du terme de recherche dans le document à l'étude ;\n",
        "* La partie `idf` calcule une fonction inversement proportionnelle à la fréquence du terme dans l'ensemble des documents (ou corpus).\n",
        "\n",
        "Le score total, obtenu en multipliant les deux composantes,\n",
        "permet ainsi de donner un score d'autant plus élevé que le terme est surréprésenté dans un document\n",
        "(par rapport à l'ensemble des documents).\n",
        "Il existe plusieurs fonctions, qui pénalisent plus ou moins les documents longs,\n",
        "ou qui sont plus ou moins *smooth*.\n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 6 : TF-IDF: calcul de fréquence</h3>\n",
        "```\n",
        "\n",
        "\n",
        "1. Utiliser le vectoriseur TF-IdF de `scikit-learn` pour transformer notre corpus en une matrice `document x terms`. Au passage, utiliser l'option `stop_words` pour ne pas provoquer une inflation de la taille de la matrice. Nommer le modèle `tfidf` et le jeu entraîné `tfs`.\n",
        "2. Après avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes où les termes ayant la structure `abandon` sont non-nuls. \n",
        "3. Trouver les 50 extraits où le score TF-IDF est le plus élevé et l'auteur associé. Vous devriez obtenir le classement suivant:\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "23bd5d71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "#1. TfIdf de scikit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words=stopwords.words(\"english\"))\n",
        "tfs = tfidf.fit_transform(train['Text'])\n",
        "#print(tfs)"
      ],
      "id": "8a396c0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "corpus_index = [n for n in list(tfidf.vocabulary_.keys())]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfs.todense(), columns=feature_names)\n",
        "\n",
        "df.head()"
      ],
      "id": "d0e3fb49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les lignes où les termes de abandon sont non nuls\n",
        "sont les suivantes :\n"
      ],
      "id": "fa89b4ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: true\n",
        "#| echo: false\n",
        "\n",
        "#2. Lignes où les termes de abandon sont non nuls.\n",
        "tempdf = df.loc[(df.filter(regex = \"abandon\")!=0).any(axis=1)]\n",
        "print(tempdf.index)\n",
        "tempdf.head(5)"
      ],
      "id": "325c8675",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: true\n",
        "#| echo: false\n",
        "\n",
        "#3. 50 extraits avec le TF-IDF le plus élevé.\n",
        "list_fear = df[\"fear\"].sort_values(ascending =False).head(n=50).index.tolist()\n",
        "train.iloc[list_fear].groupby('Author').count()['Text'].sort_values(ascending = False)"
      ],
      "id": "38e21dad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les 10 scores les plus élevés sont les suivants :\n"
      ],
      "id": "541d8a11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(train.iloc[list_fear[:9]]['Text'].values)"
      ],
      "id": "9a83be1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque que les scores les plus élévés sont soient des extraits courts où le mot apparait une seule fois, soit des extraits plus longs où le mot fear apparaît plusieurs fois.\n",
        "\n",
        "\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-info\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-comment\"></i> Note</h3>\n",
        "```\n",
        "\n",
        "\n",
        "La matrice `document x terms` est un exemple typique de matrice _sparse_ puisque, dans des corpus volumineux, une grande diversité de vocabulaire peut être trouvée.  \n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "# Approche contextuelle: les *n-gramms*\n",
        "\n",
        "\n",
        "Pour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire :\n"
      ],
      "id": "15b9660b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "nltk.download('genesis')\n",
        "nltk.corpus.genesis.words('english-web.txt')"
      ],
      "id": "5064fe8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il s'agit maintenant de raffiner l'analyse. \n",
        "\n",
        "On s'intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Elle permet aussi d'affiner les modèles \"bag-of-words\". Le calcul de n-grams (bigrams pour les co-occurences de mots deux-à-deux, tri-grams pour les co-occurences trois-à-trois, etc.) constitue la méthode la plus simple pour tenir compte du contexte.\n",
        "\n",
        "\n",
        "`nltk` offre des methodes pour tenir compte du contexte : pour ce faire, nous calculons les n-grams, c'est-à-dire l'ensemble des co-occurrences successives de mots n-à-n.  En général, on se contente de bi-grams, au mieux de tri-grams :\n",
        "\n",
        "* les modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;\n",
        "* les performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de données initiale).\n",
        "\n",
        "\n",
        "On va, rapidement, regarder dans quel contexte apparaît le mot `fear` dans\n",
        "l'oeuvre d'Edgar Allan Poe (EAP). Pour cela, on transforme d'abord\n",
        "le corpus EAP en tokens `nltk : \n"
      ],
      "id": "07c85815"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "eap_clean = train[train[\"Author\"] == \"EAP\"]\n",
        "eap_clean = ' '.join(eap_clean['Text'])\n",
        "tokens = eap_clean.split()\n",
        "print(tokens[:10])\n",
        "text = nltk.Text(tokens)\n",
        "print(text)"
      ],
      "id": "f3ba0adf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vous aurez besoin des fonctions ` BigramCollocationFinder.from_words` et `BigramAssocMeasures.likelihood_ratio` : \n"
      ],
      "id": "f292125a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures"
      ],
      "id": "011a6f6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-pencil\"></i> Exercice 7  : n-grams et contexte du mot fear</h3>\n",
        "```\n",
        "\n",
        "1. Utiliser la méthode `concordance` pour afficher le contexte dans lequel apparaît le terme `fear`. \n",
        "2. Sélectionner et afficher les meilleures collocation, par exemple selon le critère du ratio de vraisemblance. \n",
        "\n",
        "Lorsque deux mots sont fortement associés, cela est parfois dû au fait qu'ils apparaissent rarement. Il est donc parfois nécessaire d'appliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.\n",
        "\n",
        "3. Refaire la question précédente en utilisant toujours un modèle `BigramCollocationFinder` suivi de la méthode `apply_freq_filter` pour ne conserver que les bigrammes présents au moins 5 fois. Puis, au lieu d'utiliser la méthode de maximum de vraisemblance, testez la méthode `nltk.collocations.BigramAssocMeasures().jaccard`.\n",
        "\n",
        "4. Ne s'intéresser qu'aux *collocations* qui concernent le mot *fear*\n",
        "\n",
        "\n",
        "\n",
        "```{=html}\n",
        "</div>\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "Avec la méthode `concordance` (question 1), \n",
        "la liste devrait ressembler à celle-ci:\n"
      ],
      "id": "ab4f368c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: true\n",
        "#| echo: false\n",
        "\n",
        "# 1. Methode concordance\n",
        "print(\"Exemples d'occurences du terme 'fear' :\")\n",
        "text.concordance(\"fear\")\n",
        "print('\\n')"
      ],
      "id": "66b30e38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Même si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d'informations. \n",
        "\n",
        "La `collocation` consiste à trouver les bi-grammes qui\n",
        "apparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées,\n",
        "il s'agit de sélectionner, à partir d'un modèle statistique, les \"meilleures\". \n",
        "On obtient donc avec cette méthode (question 2):\n"
      ],
      "id": "be6a545a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 2. Modélisation des meilleures collocations\n",
        "bcf = BigramCollocationFinder.from_words(text)\n",
        "bcf.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
      ],
      "id": "fbe529c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si on modélise les meilleures collocations:\n"
      ],
      "id": "5b7e205d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 3. Modélisation des meilleures collocations (qui apparaissent 5+)\n",
        "finder = nltk.BigramCollocationFinder.from_words(text)\n",
        "finder.apply_freq_filter(5)\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "collocations = finder.nbest(bigram_measures.jaccard, 15) \n",
        "\n",
        "for collocation in collocations:\n",
        "    c = ' '.join(collocation)\n",
        "    print(c)"
      ],
      "id": "3237ac96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cette liste a un peu plus de sens,\n",
        "on a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble\n",
        "(*Chess Player* par exemple).\n",
        "\n",
        "En ce qui concerne les _collocations_ du mot fear:\n"
      ],
      "id": "8dcd2bbb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#| echo: false\n",
        "\n",
        "# 4. collocations du mot fear\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "\n",
        "def collocations_word(word = \"fear\"):\n",
        "    # Ngrams with a specific name \n",
        "    name_filter = lambda *w: word not in w\n",
        "    ## Bigrams\n",
        "    finder = BigramCollocationFinder.from_words(\n",
        "                nltk.corpus.genesis.words('english-web.txt'))\n",
        "    # only bigrams that contain 'fear'\n",
        "    finder.apply_ngram_filter(name_filter)\n",
        "    # return the 100 n-grams with the highest PMI\n",
        "    print(finder.nbest(bigram_measures.likelihood_ratio,100))\n",
        "    \n",
        "collocations_word(\"fear\")"
      ],
      "id": "7738f24c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si on mène la même analyse pour le terme *love*, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe :\n"
      ],
      "id": "7e79428e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "collocations_word(\"love\")"
      ],
      "id": "55339599",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Références"
      ],
      "id": "6eeb6d4b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}