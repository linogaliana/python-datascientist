{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercices supplémentaires\n",
        "\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master/course/NLP/05_exo_supp.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
        "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter.png)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master/course/NLP/05_exo_supp.ipynb)\n",
        "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter.png)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master/course/NLP/05_exo_supp.ipynb)\n",
        "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python.png)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?autoLaunch=true&onyxia.friendlyName=%C2%ABpython-datascience%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABcourse/NLP%2005_exo_supp.ipynb%C2%BB&security.allowlist.enabled=false)<br>\n",
        "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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.png)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=course/NLP/05_exo_supp.ipynb)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master/course/NLP/05_exo_supp.ipynb)\n",
        "[![githubdev](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20Visual%20Studio%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc.png)](https://github.dev/linogaliana/python-datascientist/course/NLP/05_exo_supp.ipynb)\n",
        "\n",
        "Cette page approfondit certains aspects présentés dans les autres tutoriels. Il s’agit d’une suite d’exercice, avec corrections, pour présenter d’autres aspects du NLP ou pratiquer sur des données différentes\n",
        "\n",
        "# Exploration des libellés de l’openfood database\n",
        "\n",
        "{{% box status=“exercise” title=“Exercise: les noms de produits dans l’openfood database” icon=“fas fa-pencil-alt” %}}\n",
        "L’objectif de cet exercice est d’analyser les termes les plus fréquents\n",
        "dans les noms de produits de l’openfood database. Au passage, cela permet de réviser les étapes de *preprocessing* (LIEN XXXXX) et d’explorer les enjeux de reconnaissance d’entités nommées.\n",
        "{{% /box %}}\n",
        "\n",
        "Dans cet exercice:\n",
        "\n",
        "-   tokenisation (`nltk`)\n",
        "-   retrait des stop words (`nltk`)\n",
        "-   nuage de mots (`wordcloud`)\n",
        "-   reconnaissance du langage (`fasttext`)\n",
        "-   reconnaissance d’entités nommées (`spacy`)\n",
        "\n",
        "le tout sur l’OpenFood Database, une base de données alimentaire qui est enrichie de manière collaborative.\n",
        "\n",
        "{{% box status=“hint” title=“Hint” icon=“fa fa-lightbulb” %}}\n",
        "Pour pouvoir utiliser les modèles pré-entraînés de `spaCy`, il faut les télécharger. La méthode préconisée est d’utiliser, depuis un terminal, la commande suivante\n",
        "\n",
        "``` python\n",
        "python -m spacy download fr_core_news_sm\n",
        "```\n",
        "\n",
        "Dans un notebook jupyter, il se peut qu’il soit nécessaire de relancer le kernel.\n",
        "\n",
        "Si l’accès à la ligne de commande n’est pas possible, ou si la commande échoue, il est possible de télécharger le modèle pré-entraîné directement depuis une session `Python`\n",
        "\n",
        "``` python\n",
        "import spacy\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "```\n",
        "\n",
        "{{% /box %}}\n",
        "\n",
        "1.  Importer le modèle de reconnaissance de langage qui sera utilisé par la suite\n",
        "    ainsi que le corpus Français utilisé par `spacy`"
      ],
      "id": "0e6f76d0-b15f-449f-af30-8bbd771ebec5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "temp_dir = tempfile.NamedTemporaryFile()\n",
        "temp_dir = temp_dir.name\n",
        "\n",
        "os.system(\"wget -O {} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\".format( \"%s.model.bin\" % temp_dir))\n",
        "spacy.cli.download('fr_core_news_sm')"
      ],
      "id": "20bc66a1-d926-49f7-92c0-bda7dd65f21a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Importer les données de l’[openfood database](https://fr.openfoodfacts.org/data) à partir du code suivant"
      ],
      "id": "deb42a0a-c07e-4fbd-896c-8b42b78a90a9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve('https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv', \"%s.openfood.csv\" % temp_dir)\n",
        "df_openfood = pd.read_csv(\"%s.openfood.csv\" % temp_dir, delimiter=\"\\t\",\n",
        "                          usecols=['product_name'], encoding = 'utf-8', dtype = \"str\")"
      ],
      "id": "5e54343f-c601-48c8-9b6a-1329d9bfcc6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ces données devraient avoir l’aspect suivant:"
      ],
      "id": "df1160d0-c773-4a61-9e7c-b294e24c17f0"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_openfood.iloc[:2, :5]"
      ],
      "id": "f690ff8e-321f-414f-bfad-45eff135f88e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Créer une fonction de nettoyage des noms de produits effectuant les\n",
        "    étapes suivantes:\n",
        "\n",
        "-   tokeniser le texte en question\n",
        "-   retirer la ponctuation et les *stopwords*\n",
        "\n",
        "Appliquer cette fonction à l’ensemble des noms de produits (variable\n",
        "`product_name`)\n",
        "\n",
        "1.  Effectuer un nuage de mot sur les libellés avant et après nettoyage\n",
        "    pour comprendre la structure du corpus en question.\n",
        "    Le résultat devrait avoir l’apparence suivante"
      ],
      "id": "6dec4b7d-27db-4300-97ec-a6a350119122"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wordcloud as wc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def graph_wordcloud(data, by = None, valueby = None, yvar = \"Text\"):\n",
        "    if (by is not None) & (valueby is not None):        \n",
        "        txt = data[data[by]==valueby][yvar].astype(str)\n",
        "    else:\n",
        "        txt = data[yvar].astype(str)\n",
        "    all_text = ' '.join([text for text in txt])\n",
        "    wordcloud = wc.WordCloud(width=800, height=500,\n",
        "                          random_state=21,\n",
        "                      max_words=2000).generate(all_text)\n",
        "    return wordcloud\n",
        "\n",
        "def graph_wordcloud_by(data, by, yvar = \"Text\"):\n",
        "    n_topics = data[by].unique().tolist()\n",
        "    width=20\n",
        "    height=80\n",
        "    rows = len(n_topics)//2\n",
        "    cols = 2\n",
        "    fig=plt.figure(figsize=(width, height))\n",
        "    axes = []\n",
        "    for i in range(cols*rows):\n",
        "        b = graph_wordcloud(data, by = by, valueby = n_topics[i], yvar = yvar)\n",
        "        axes.append( fig.add_subplot(rows, cols, i+1) )\n",
        "        axes[-1].set_title(\"{}\".format(n_topics[i]))  \n",
        "        plt.imshow(b)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n",
        "\n",
        "\n",
        "def wordcount_words(data, yvar, by = None):\n",
        "    plt.figure( figsize=(15,15) )\n",
        "    if by is None:\n",
        "        wordcloud = graph_wordcloud(data, yvar = yvar, by = by)\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('{}.png'.format(yvar), bbox_inches='tight')\n",
        "    else:\n",
        "        graph_wordcloud_by(data, by = by, yvar = yvar)\n",
        "\n",
        "wordcount_words(df_openfood, yvar = \"product_name\")\n",
        "wordcount_words(df_openfood, \"tokenized\")"
      ],
      "id": "e7bf6d2b-4784-42fe-b38d-6e4371d82236"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Utiliser la librairie `Fasttext` pour extraire les noms de produits\n",
        "    français\n",
        "\n",
        "-   Appliquer le modèle téléchargé précedemment pour déterminer le langage\n",
        "-   Ne récupérer que les libellés français"
      ],
      "id": "fe93a9ee-2fb4-4686-9267-25526cef9472"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fasttext\n",
        "\n",
        "PRETRAINED_MODEL_PATH = \"%s.model.bin\" % temp_dir\n",
        "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
        "newcols = ['language','score_language']\n",
        "df_openfood[newcols] = pd.DataFrame(df_openfood['product_name'].astype(str).apply(lambda s: list(model.predict(s))).apply(lambda l: [l[0][0],l[1][0]]).tolist(), columns = newcols)\n",
        "df_openfood['language'] = df_openfood['language'].str.replace(\"__label__\",\"\")\n",
        "df_openfood_french = df_openfood[df_openfood['language'] == \"fr\"]\n",
        "df_openfood_french.head(2)"
      ],
      "id": "a5c4b350-cc00-4b94-a0f6-f07cde0d36f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Visualiser avec `spacy.displacy` le résultat d’une reconnaissance\n",
        "    d’entités nommées sur 50 données aléatoires. Cela vous semble-t-il satisfaisant ?"
      ],
      "id": "6528b424-cf61-4a1f-8079-5b594c563883"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import fr_core_news_sm\n",
        "\n",
        "nlp = fr_core_news_sm.load()\n",
        "\n",
        "example = \" \\n \".join(df_openfood_french['product_name'].astype(\"str\").sample(50))\n",
        "\n",
        "from spacy import displacy\n",
        "html = displacy.render(nlp(example), style='ent', page=True)"
      ],
      "id": "fcd41104-544d-4951-9867-ab77d886dfc3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(html)"
      ],
      "id": "424c176a-f1d8-49fe-949e-ac9c8fe6736d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Récupérer dans un vecteur les entités nommées reconnues par `spaCy`.\n",
        "    Regarder les entités reconnues dans les 20 premiers libellés de produits"
      ],
      "id": "22f65849-5edf-481a-a3f8-97850d9ccbbd"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = []\n",
        "for doc in nlp.pipe(df_openfood_french.head(20)['product_name'].astype(\"unicode\"), disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
        "    # Do something with the doc here\n",
        "    x.append([(ent.text, ent.label_) for ent in doc.ents])\n",
        "    \n",
        "x"
      ],
      "id": "c0560b53-916c-4e00-8a4a-6460b237ba85"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!----\n",
        "# State of the union address\n",
        "\n",
        "\n",
        "Un exercice à venir sur l'analyse des discours des présidents américains \n",
        "inspiré de https://github.com/BuzzFeedNews/2018-01-trump-state-of-the-union\n",
        "---->"
      ],
      "id": "51d55664-b33b-4586-8b34-d3595d69a3d3"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  }
}