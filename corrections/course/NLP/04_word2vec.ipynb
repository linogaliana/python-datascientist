{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"M√©thodes de vectorisation : comptages et word embeddings\"\n",
        "date: 2020-10-29T13:00:00Z\n",
        "draft: false\n",
        "weight: 40\n",
        "slug: word2vec\n",
        "type: book\n",
        "tags:\n",
        "  - NLP\n",
        "  - Litt√©rature\n",
        "  - Topics Modelling\n",
        "  - Word2Vec\n",
        "categories:\n",
        "  - Tutoriel\n",
        "summary: |\n",
        "  Pour pouvoir utiliser des donn√©es textuelles dans des algorithmes\n",
        "  de _machine learning_, il faut les vectoriser, c'est √† dire transformer\n",
        "  le texte en donn√©es num√©riques. Dans ce TP, nous allons comparer\n",
        "  diff√©rentes m√©thodes de vectorisation, √† travers une t√¢che de pr√©diction :\n",
        "  _peut-on pr√©dire un auteur litt√©raire √† partir d'extraits de ses textes ?_\n",
        "  Parmi ces m√©thodes, on va notamment explorer le mod√®le `Word2Vec`, qui\n",
        "  permet d'exploiter les structures latentes d'un texte en construisant\n",
        "  des _word embeddings_ (plongements de mots).\n",
        "eval: false\n",
        "---"
      ],
      "id": "0c392e7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.cell .markdown}"
      ],
      "id": "53f171f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: 'asis'\n",
        "#| include: true\n",
        "#| eval: true\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, '../../../../') #insert the utils module\n",
        "from utils import print_badges\n",
        "\n",
        "#print_badges(__file__)\n",
        "print_badges(\"content/course/NLP/04_word2vec.qmd\")"
      ],
      "id": "6f7c925a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "Cette page approfondit certains aspects pr√©sent√©s dans la\n",
        "[partie introductive](#nlp). Apr√®s avoir travaill√© sur le\n",
        "*Comte de Monte Cristo*, on va continuer notre exploration de la litt√©rature\n",
        "avec cette fois des auteurs anglophones:\n",
        "\n",
        "* Edgar Allan Poe, (EAP) ;\n",
        "* HP Lovecraft (HPL) ;\n",
        "* Mary Wollstonecraft Shelley (MWS).\n",
        "\n",
        "Les donn√©es sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent √™tre requ√©t√©es via l'url \n",
        "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
        "\n",
        "Le but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quents utilis√©s par les auteurs, de les repr√©senter graphiquement puis on va ensuite essayer de pr√©dire quel texte correspond √† quel auteur √† partir de diff√©rents mod√®les de vectorisation, notamment les *word embeddings*.\n",
        "\n",
        "Ce notebook est librement inspir√© de  : \n",
        "\n",
        "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras\n",
        "* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg\n",
        "* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook\n",
        "\n",
        "\n",
        "\n",
        "{{% box status=\"warning\" title=\"Warning\" icon=\"fa fa-exclamation-triangle\" %}}\n",
        "Comme dans la [partie pr√©c√©dente](#nlp), il faut t√©l√©charger quelques √©l√©ments\n",
        "pour que nos librairies de NLP puissent fonctionner correctement.\n",
        "\n",
        "En premier lieu, il convient d'installer les librairies ad√©quates\n",
        "(`spacy`, `gensim` et `sentence_transformers`):\n",
        "\n",
        "~~~python\n",
        "!pip install spacy gensim sentence_transformers\n",
        "~~~\n",
        "\n",
        "Ensuite, comme nous allons utiliser √©galement `spacy`, il convient de t√©l√©charger\n",
        "le corpus Anglais. Pour cela, on peut se r√©f√©rer √†\n",
        "[la documentation de `spacy`](https://spacy.io/usage/models),\n",
        "extr√™mement bien faite.\n",
        "\n",
        "\n",
        "- Id√©alement, il faut installer le module via la ligne de commande. Dans\n",
        "une cellule de notebook `Jupyter`, faire :\n",
        "\n",
        "~~~python\n",
        "!python -m spacy download en_core_web_sm\n",
        "~~~\n",
        "\n",
        "- Sans acc√®s √† la ligne de commande (depuis une instance `Docker` par exemple),\n",
        "faire :\n",
        "\n",
        "~~~python\n",
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "~~~\n",
        "\n",
        "- Sinon, il est √©galement possible d'installer le module en faisant pointer\n",
        "`pip install` vers le fichier ad√©quat sur\n",
        "[`Github`](https://github.com/explosion/spacy-models). Pour cela, taper\n",
        "\n",
        "~~~python\n",
        "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\n",
        "~~~\n",
        "\n",
        "\n",
        "{{% /box %}}\n"
      ],
      "id": "c4d09ab5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "id": "d0ea356a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nettoyage des donn√©es\n",
        "\n",
        "Nous allons ainsi √† nouveau utiliser le jeu de donn√©es `spooky`:\n"
      ],
      "id": "3b160ecf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
        "spooky_df = pd.read_csv(data_url)"
      ],
      "id": "9127f9c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le jeu de donn√©es met ainsi en regard un auteur avec une phrase qu'il a √©crite:\n"
      ],
      "id": "8e4400d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spooky_df.head()"
      ],
      "id": "df439021",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "En NLP, la premi√®re √©tape est souvent celle du *preprocessing*, qui inclut notamment les √©tapes de tokenization et de nettoyage du texte. Comme celles-ci ont √©t√© vues en d√©tail dans le pr√©c√©dent chapitre, on se contentera ici d'un *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les m√©thodes de vectorisation bas√©es sur des comptages).\n",
        "\n",
        "Jusqu'√† pr√©sent, nous avons utilis√© principalement `nltk` pour le \n",
        "*preprocessing* de donn√©es textuelles. Cette fois, nous proposons\n",
        "d'utiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de\n",
        "*pipelines* de *preprocessing*. \n",
        "\n",
        "Pour initialiser le processus de nettoyage,\n",
        "on va utiliser le corpus `en_core_web_sm` (voir plus\n",
        "haut pour l'installation de ce corpus):\n"
      ],
      "id": "c88ecdf4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "id": "fcca64a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On va utiliser un `pipe` `spacy` qui permet d'automatiser, et de parall√©liser,\n",
        "un certain nombre d'op√©rations. Les *pipes* sont l'√©quivalent, en NLP, de\n",
        "nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s'agit donc d'un outil\n",
        "tr√®s appropri√© pour industrialiser un certain nombre d'op√©rations de\n",
        "*preprocessing* :\n"
      ],
      "id": "dc9e8bd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_docs(texts, remove_stopwords=False, n_process = 4):\n",
        "    \n",
        "    docs = nlp.pipe(texts, \n",
        "                    n_process=n_process,\n",
        "                    disable=['parser', 'ner',\n",
        "                             'lemmatizer', 'textcat'])\n",
        "    stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "    docs_cleaned = []\n",
        "    for doc in docs:\n",
        "        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n",
        "        if remove_stopwords:\n",
        "            tokens = [tok for tok in tokens if tok not in stopwords]\n",
        "        doc_clean = ' '.join(tokens)\n",
        "        docs_cleaned.append(doc_clean)\n",
        "        \n",
        "    return docs_cleaned"
      ],
      "id": "ae52d7b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On applique la fonction `clean_docs` √† notre colonne `pandas`.\n",
        "Les `pandas.Series` √©tant it√©rables, elles se comportent comme des listes et\n",
        "fonctionnent ainsi tr√®s bien avec notre `pipe` `spacy`\n"
      ],
      "id": "342376c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spooky_df['text_clean'] = clean_docs(spooky_df['text'])"
      ],
      "id": "cf3ea2d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spooky_df.head()"
      ],
      "id": "82e48912",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encodage de la variable √† pr√©dire\n",
        "\n",
        "On r√©alise un simple encodage de la variable √† pr√©dire :\n",
        "il y a trois cat√©gories (auteurs), repr√©sent√©es par des entiers 0, 1 et 2.\n",
        "\n",
        "Pour cela, on utilise le `LabelEncoder` de `scikit` d√©j√† pr√©sent√© \n",
        "dans la [partie mod√©lisation](#preprocessing). On va utiliser la m√©thode\n",
        "`fit_transform` qui permet, en un tour de main, d'appliquer √† la fois\n",
        "l'entra√Ænement (`fit`), √† savoir la cr√©ation d'une correspondance entre valeurs\n",
        "num√©riques et _labels_, et l'appliquer (`transform`) √† la m√™me colonne.\n"
      ],
      "id": "630ff7a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "le = LabelEncoder()\n",
        "spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])"
      ],
      "id": "0d63c53b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut v√©rifier les classes de notre `LabelEncoder` :\n"
      ],
      "id": "04369b0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "le.classes_"
      ],
      "id": "e5991c57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construction des bases d'entra√Ænement et de test\n",
        "\n",
        "On met de c√¥t√© un √©chantillon de test (20 %) avant toute analyse (m√™me descriptive).\n",
        "Cela permettra d'√©valuer nos diff√©rents mod√®les toute √† la fin de mani√®re tr√®s rigoureuse,\n",
        "puisque ces donn√©es n'auront jamais utilis√©es pendant l'entra√Ænement.\n",
        "\n",
        "Notre √©chantillon initial n'est pas √©quilibr√© (*balanced*) : on retrouve plus d'oeuvres de\n",
        "certains auteurs que d'autres. Afin d'obtenir un mod√®le qui soit √©valu√© au mieux, nous allons donc stratifier notre √©chantillon de mani√®re √† obtenir une r√©partition similaire d'auteurs dans nos\n",
        "ensembles d'entra√Ænement et de test.\n"
      ],
      "id": "6948869a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(spooky_df['text_clean'].values, \n",
        "                                                    spooky_df['author_encoded'].values, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=33,\n",
        "                                                    stratify = spooky_df['author_encoded'].values)"
      ],
      "id": "cc1ee45d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par exemple, les textes d'EAP repr√©sentent 40 % des √©chantillons d'entra√Ænement et de test : \n"
      ],
      "id": "5ed633ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(100*y_train.tolist().count(0)/(len(y_train)))\n",
        "print(100*y_test.tolist().count(0)/(len(y_test)))"
      ],
      "id": "2b427a45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aper√ßu du premier √©l√©ment de `X_train` : \n"
      ],
      "id": "c4999f42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train[0]"
      ],
      "id": "baf74959",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut aussi v√©rifier qu'on est capable de retrouver\n",
        "la correspondance entre nos auteurs initiaux avec\n",
        "la m√©thode `inverse_transform`\n"
      ],
      "id": "1e809801"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(y_train[0], le.inverse_transform([y_train[0]])[0])"
      ],
      "id": "357a114d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistiques exploratoires\n",
        "\n",
        "## R√©partition des labels\n",
        "\n",
        "Refaisons un graphique que nous avons d√©j√† produit pr√©c√©demment pour voir\n",
        "la r√©partition de notre corpus entre auteurs:\n"
      ],
      "id": "dbbd6566"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: hide\n",
        "fig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind='bar')\n",
        "fig"
      ],
      "id": "05bd145b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig.get_figure()"
      ],
      "id": "59b02ccc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On observe une petite asym√©trie : les passages des livres d'Edgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d'entra√Ænement, ce qui peut √™tre probl√©matique dans le cadre d'une t√¢che de classification.\n",
        "L'√©cart n'est pas dramatique, mais on essaiera d'en tenir compte dans l'analyse en choisissant une m√©trique d'√©valuation pertinente. \n",
        "\n",
        "## Mots les plus fr√©quemment utilis√©s par chaque auteur\n",
        "\n",
        "On va supprimer les *stopwords* pour r√©duire le bruit dans notre jeu\n",
        "de donn√©es.\n"
      ],
      "id": "3089d491"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Suppression des stop words\n",
        "X_train_no_sw = clean_docs(X_train, remove_stopwords=True)\n",
        "X_train_no_sw = np.array(X_train_no_sw)"
      ],
      "id": "17c87905",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour visualiser rapidement nos corpus, on peut utiliser la technique des\n",
        "nuages de mots d√©j√† vue √† plusieurs reprises. \n",
        "\n",
        "Vous pouvez essayer de faire vous-m√™me les nuages ci-dessous\n",
        "ou cliquer sur la ligne ci-dessous pour afficher le code ayant\n",
        "g√©n√©r√© les figures :\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "```{=html}\n",
        "<details><summary><code>Cliquer pour afficher le code</code> üëá</summary>\n",
        "```"
      ],
      "id": "615c605c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_top_words(initials, ax, n_words=20):\n",
        "    # Calcul des mots les plus fr√©quemment utilis√©s par l'auteur\n",
        "    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n",
        "    all_tokens = ' '.join(texts).split()\n",
        "    counts = Counter(all_tokens)\n",
        "    top_words = [word[0] for word in counts.most_common(n_words)]\n",
        "    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n",
        "    \n",
        "    # Repr√©sentation sous forme de barplot\n",
        "    ax = sns.barplot(ax = ax, x=top_words, y=top_words_counts)\n",
        "    ax.set_title(f'Most Common Words used by {initials_to_author[initials]}')"
      ],
      "id": "4b328b9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: hide\n",
        "\n",
        "initials_to_author = {\n",
        "    'EAP': 'Edgar Allen Poe',\n",
        "    'HPL': 'H.P. Lovecraft',\n",
        "    'MWS': 'Mary Wollstonecraft Shelley'\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize = (12,12))\n",
        "\n",
        "plot_top_words('EAP', ax = axs[0])\n",
        "plot_top_words('HPL', ax = axs[1])\n",
        "plot_top_words('MWS', ax = axs[2])"
      ],
      "id": "3df85b80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{=html}\n",
        "</details>\n",
        "```\n",
        "\n",
        ":::\n"
      ],
      "id": "790ee210"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: false\n",
        "axs[0].get_figure()\n",
        "axs[1].get_figure()\n",
        "axs[2].get_figure()"
      ],
      "id": "58cb00b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beaucoup de mots se retrouvent tr√®s utilis√©s par les trois auteurs.\n",
        "Il y a cependant des diff√©rences notables : le mot _\"life\"_\n",
        "est le plus employ√© par MWS, alors qu'il n'appara√Æt pas dans les deux autres tops.\n",
        "De m√™me, le mot _\"old\"_ est le plus utilis√© par HPL\n",
        "l√† o√π les deux autres ne l'utilisent pas de mani√®re surrepr√©sent√©e.\n",
        "\n",
        "Il semble donc qu'il y ait des particularit√©s propres √† chacun des auteurs\n",
        "en termes de vocabulaire,\n",
        "ce qui laisse penser qu'il est envisageable de pr√©dire les auteurs √† partir\n",
        "de leurs textes dans une certaine mesure.\n",
        "\n",
        "# Pr√©diction sur le set d'entra√Ænement\n",
        "\n",
        "Nous allons √† pr√©sent v√©rifier cette conjecture en comparant\n",
        "plusieurs mod√®les de vectorisation,\n",
        "_i.e._ de transformation du texte en objets num√©riques\n",
        "pour que l'information contenue soit exploitable dans un mod√®le de classification.\n",
        "\n",
        "## D√©marche\n",
        "\n",
        "Comme nous nous int√©ressons plus √† l'effet de la vectorisation qu'√† la t√¢che de classification en elle-m√™me,\n",
        "nous allons utiliser un algorithme de classification simple (un SVM lin√©aire), avec des param√®tres non fine-tun√©s (c'est-√†-dire des param√®tres pas n√©cessairement choisis pour √™tre les meilleurs de tous).\n"
      ],
      "id": "cedb58d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf = LinearSVC(max_iter=10000, C=0.1)"
      ],
      "id": "fb00f445",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce mod√®le est connu pour √™tre tr√®s performant sur les t√¢ches de classification de texte, et nous fournira donc un bon mod√®le de r√©f√©rence (*baseline*). Cela nous permettra √©galement de comparer de mani√®re objective l'impact des m√©thodes de vectorisation sur la performance finale.\n",
        "\n",
        "<!-- KA : dit plus bas. -->\n",
        "<!-- On va utiliser au maximum les objets de type pipeline de `sklearn`, -->\n",
        "<!-- qui permettent de r√©aliser des analyses reproductibles -->\n",
        "<!-- et de fine-tuner proprement les diff√©rents hyperparam√®tres. -->\n",
        "\n",
        "Pour les deux premi√®res m√©thodes de vectorisation\n",
        "(bas√©es sur des fr√©quences et fr√©quences relatives des mots),\n",
        "on va simplement normaliser les donn√©es d'entr√©e, ce qui va permettre au SVM de converger plus rapidement, ces mod√®les √©tant sensibles aux diff√©rences d'√©chelle dans les donn√©es.\n",
        "\n",
        "On va √©galement _fine-tuner_ via _grid-search_\n",
        "certains hyperparam√®tres li√©s √† ces m√©thodes de vectorisation : \n",
        "\n",
        "- on teste diff√©rents _ranges_ de `n-grams` (unigrammes et unigrammes + bigrammes)\n",
        "- on teste avec et sans _stop-words_\n",
        "\n",
        "Afin d'√©viter le surapprentissage,\n",
        "on va √©valuer les diff√©rents mod√®les via validation crois√©e, calcul√©e sur 4 blocs.\n",
        "\n",
        "On r√©cup√®re √† la fin le meilleur mod√®le selon une m√©trique sp√©cifi√©e.\n",
        "On choisit le `score F1`,\n",
        "moyenne harmonique de la pr√©cision et du rappel,\n",
        "qui donne un poids √©quilibr√© aux deux m√©triques, tout en p√©nalisant fortement le cas o√π l'une des deux est faible.\n",
        "Pr√©cis√©ment, on retient le `score F1 *micro-averaged*` :\n",
        "les contributions des diff√©rentes classes √† pr√©dire sont agr√©g√©es,\n",
        "puis on calcule le `score F1` sur ces donn√©es agr√©g√©es.\n",
        "L'avantage de ce choix est qu'il permet de tenir compte des diff√©rences\n",
        "de fr√©quences des diff√©rentes classes.\n",
        "\n",
        "## Pipeline de pr√©diction\n",
        "\n",
        "On va utiliser un *pipeline* `scikit` ce qui va nous permettre d'avoir\n",
        "un code tr√®s concis pour effectuer cet ensemble de t√¢ches coh√©rentes. \n",
        "De plus, cela va nous assurer de g√©rer de mani√®re coh√©rentes nos diff√©rentes\n",
        "transformations (cf. [partie sur les pipelines](#pipelines))\n",
        "\n",
        "Pour se faciliter la vie, on d√©finit une fonction `fit_vectorizers` qui\n",
        "int√®gre dans un *pipeline* g√©n√©rique une m√©thode d'estimation `scikit`\n",
        "et fait de la validation crois√©e en cherchant le meilleur mod√®le\n",
        "(en excluant/incluant les *stopwords* et avec unigrammes/bigrammes)\n"
      ],
      "id": "33054b02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fit_vectorizers(vectorizer):\n",
        "    pipeline = Pipeline(\n",
        "    [\n",
        "        (\"vect\", vectorizer()),\n",
        "        (\"scaling\", StandardScaler(with_mean=False)),\n",
        "        (\"clf\", clf),\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    parameters = {\n",
        "        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
        "        \"vect__stop_words\": (\"english\", None)\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='f1_micro',\n",
        "                               cv=4, n_jobs=4, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_parameters = grid_search.best_estimator_.get_params()\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "\n",
        "    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n",
        "    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n",
        "    \n",
        "    return grid_search"
      ],
      "id": "49367798",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Approche _bag-of-words_\n",
        "\n",
        "On commence par une approche __\"bag-of-words\"__, \n",
        "i.e. qui revient simplement √† repr√©senter chaque document par un vecteur\n",
        "qui compte le nombre d'apparitions de chaque mot du vocabulaire dans le document.\n"
      ],
      "id": "126a0edf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cv_bow = fit_vectorizers(CountVectorizer)"
      ],
      "id": "285372d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF\n",
        "\n",
        "On s'int√©resse ensuite √† l'approche __TF-IDF__,\n",
        "qui permet de tenir compte des fr√©quences *relatives* des mots.\n",
        "\n",
        "Ainsi, pour un mot donn√©, on va multiplier la fr√©quence d'apparition du mot dans le document (calcul√© comme dans la m√©thode pr√©c√©dente) par un terme qui p√©nalise une fr√©quence √©lev√©e du mot dans le corpus. L'image ci-dessous, emprunt√©e √† Chris Albon, illustre cette mesure:\n",
        "\n",
        "![](https://chrisalbon.com/images/machine_learning_flashcards/TF-IDF_print.png)\n",
        "*Source: [https://chrisalbon](https://chrisalbon.com/code/machine_learning/preprocessing_text/tf-idf/)*\n",
        "\n",
        "La vectorisation `TF-IDF` permet donc de limiter l'influence des *stop-words*\n",
        "et donc de donner plus de poids aux mots les plus salients d'un document.\n",
        "On observe clairement que la performance de classification est bien sup√©rieure,\n",
        "ce qui montre la pertinence de cette technique.\n"
      ],
      "id": "dfb29a75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cv_tfidf = fit_vectorizers(TfidfVectorizer)"
      ],
      "id": "451ad86d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2vec avec averaging\n",
        "\n",
        "On va maintenant explorer les techniques de vectorisation bas√©es sur les\n",
        "*embeddings* de mots, et notamment la plus populaire : `Word2Vec`.\n",
        "\n",
        "L'id√©e derri√®re est simple, mais a r√©volutionn√© le NLP :\n",
        "au lieu de repr√©senter les documents par des\n",
        "vecteurs *sparse* de tr√®s grande dimension (la taille du vocabulaire)\n",
        "comme on l'a fait jusqu'√† pr√©sent,\n",
        "on va les repr√©senter par des vecteurs *dense* (continus)\n",
        "de dimension r√©duite (en g√©n√©ral, autour de 100-300).\n",
        "\n",
        "Chacune de ces dimensions va repr√©senter un facteur latent,\n",
        "c'est √† dire une variable inobserv√©e,\n",
        "de la m√™me mani√®re que les composantes principales produites par une ACP.\n",
        "\n",
        "![](w2v_vecto.png)\n",
        "\n",
        "*Source: https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d*\n",
        "\n",
        "\n",
        "__Pourquoi est-ce int√©ressant ?__\n",
        "Pour de nombreuses raisons, mais pour r√©sumer :\n",
        "cela permet de beaucoup mieux capturer la similarit√© s√©mantique entre les documents.\n",
        "\n",
        "Par exemple, un humain sait qu'un document contenant le mot _\"Roi\"_\n",
        "et un autre document contenant le mot _\"Reine\"_ ont beaucoup de chance\n",
        "d'aborder des sujets semblables.\n",
        "\n",
        "Pourtant, une vectorisation de type comptage ou TF-IDF\n",
        "ne permet pas de saisir cette similarit√© :\n",
        "le calcul d'une mesure de similarit√© (norme euclidienne ou similarit√© cosinus)\n",
        "entre les deux vecteurs ne prendra en compte la similarit√© des deux concepts, puisque les mots utilis√©s sont diff√©rents.\n",
        "\n",
        "A l'inverse, un mod√®le `word2vec` bien entra√Æn√© va capter\n",
        "qu'il existe un facteur latent de type _\"royaut√©\"_,\n",
        "et la similarit√© entre les vecteurs associ√©s aux deux mots sera forte.\n",
        "\n",
        "La magie va m√™me plus loin : le mod√®le captera aussi qu'il existe un\n",
        "facteur latent de type _\"genre\"_,\n",
        "et va permettre de construire un espace s√©mantique dans lequel les\n",
        "relations arithm√©tiques entre vecteurs ont du sens ;\n",
        "par exemple :\n",
        "$$\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}$$\n",
        "\n",
        "__Comment ces mod√®les sont-ils entra√Æn√©s ?__\n",
        "Via une t√¢che de pr√©diction r√©solue par un r√©seau de neurones simple.\n",
        "\n",
        "L'id√©e fondamentale est que la signification d'un mot se comprend\n",
        "en regardant les mots qui apparaissent fr√©quemment dans son voisinage.\n",
        "\n",
        "Pour un mot donn√©, on va donc essayer de pr√©dire les mots\n",
        "qui apparaissent dans une fen√™tre autour du mot cible.\n",
        "\n",
        "En r√©p√©tant cette t√¢che de nombreuses fois et sur un corpus suffisamment vari√©,\n",
        "on obtient finalement des *embeddings* pour chaque mot du vocabulaire,\n",
        "qui pr√©sentent les propri√©t√©s discut√©es pr√©c√©demment.\n"
      ],
      "id": "57d017a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train_tokens = [text.split() for text in X_train]\n",
        "w2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, \n",
        "                     min_count=1, workers=4)"
      ],
      "id": "db2ea4ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w2v_model.wv.most_similar(\"mother\")"
      ],
      "id": "77c33452",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On voit que les mots les plus similaires √† _\"mother\"_\n",
        "sont souvent des mots li√©s √† la famille, mais pas toujours.\n",
        "\n",
        "C'est li√© √† la taille tr√®s restreinte du corpus sur lequel on entra√Æne le mod√®le,\n",
        "qui ne permet pas de r√©aliser des associations toujours pertinentes.\n",
        "\n",
        "\n",
        "L'*embedding* (la repr√©sentation vectorielle) de chaque document correspond √† la moyenne des *word-embeddings* des mots qui le composent : \n"
      ],
      "id": "e7e88f73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_mean_vector(w2v_vectors, words):\n",
        "    words = [word for word in words if word in w2v_vectors]\n",
        "    if words:\n",
        "        avg_vector = np.mean(w2v_vectors[words], axis=0)\n",
        "    else:\n",
        "        avg_vector = np.zeros_like(w2v_vectors['hi'])\n",
        "    return avg_vector\n",
        "\n",
        "def fit_w2v_avg(w2v_vectors):\n",
        "    X_train_vectors = np.array([get_mean_vector(w2v_vectors, words)\n",
        "                                for words in X_train_tokens])\n",
        "    \n",
        "    scores = cross_val_score(clf, X_train_vectors, y_train, \n",
        "                         cv=4, scoring='f1_micro', n_jobs=4)\n",
        "\n",
        "    print(f\"CV scores {scores}\")\n",
        "    print(f\"Mean F1 {np.mean(scores)}\")\n",
        "    return scores"
      ],
      "id": "5f712e43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cv_w2vec = fit_w2v_avg(w2v_model.wv)"
      ],
      "id": "c90fe14c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La performance chute fortement ;\n",
        "la faute √† la taille tr√®s restreinte du corpus, comme annonc√© pr√©c√©demment.\n",
        "\n",
        "# Word2vec pr√©-entra√Æn√© + averaging\n",
        "\n",
        "Quand on travaille avec des corpus de taille restreinte,\n",
        "c'est g√©n√©ralement une mauvaise id√©e d'entra√Æner son propre mod√®le `word2vec`.\n",
        "\n",
        "Heureusement, des mod√®les pr√©-entra√Æn√©s sur de tr√®s gros corpus sont disponibles.\n",
        "Ils permettent de r√©aliser du *transfer learning*,\n",
        "c'est-√†-dire de b√©n√©ficier de la performance d'un mod√®le qui a √©t√© entra√Æn√© sur une autre t√¢che ou bien sur un autre corpus.\n",
        "\n",
        "L'un des mod√®les les plus connus pour d√©marrer est le `glove_model` de\n",
        "`Gensim` (Glove pour _Global Vectors for Word Representation_)[^1]:\n",
        "\n",
        "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
        ">\n",
        "> _Source_: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "[^1]: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. _GloVe: Global Vectors for Word Representation_. \n",
        "\n",
        "On peut le charger directement gr√¢ce √† l'instruction suivante : \n"
      ],
      "id": "c0c6ccbd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: hide\n",
        "glove_model = gensim.downloader.load('glove-wiki-gigaword-200')"
      ],
      "id": "cecac31c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par exemple, la repr√©sentation vectorielle de roi est l'objet\n",
        "multidimensionnel suivant :\n"
      ],
      "id": "2c3a2846"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "glove_model['king']"
      ],
      "id": "3ad5c903",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme elle est peu intelligible, on va plut√¥t rechercher les termes les\n",
        "plus similaires. Par exemple,\n"
      ],
      "id": "1e5c1612"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "glove_model.most_similar('mother')"
      ],
      "id": "9c2577af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut retrouver notre formule pr√©c√©dente\n",
        "\n",
        "$$\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}$$\n",
        "dans ce plongement de mots:\n"
      ],
      "id": "4d2de4b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "glove_model.most_similar(positive = ['king', 'woman'], negative = ['man'])"
      ],
      "id": "53b71b66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vous pouvez vous r√©f√©rer √† [ce tutoriel](https://jalammar.github.io/illustrated-word2vec/)\n",
        "pour en d√©couvrir plus sur `Word2Vec`.\n",
        "\n",
        "Faisons notre apprentissage par transfert :\n"
      ],
      "id": "0c67bb69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cv_w2vec_transfert = fit_w2v_avg(glove_model)"
      ],
      "id": "f326ee8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La performance remonte substantiellement.\n",
        "Cela √©tant, on ne parvient pas √† faire mieux que les approches basiques,\n",
        "on arrive √† peine aux performances de la vectorisation par comptage.\n",
        "\n",
        "En effet, pour rappel, les performances sont les suivantes:\n"
      ],
      "id": "79937186"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perfs = pd.DataFrame(\n",
        "    [np.mean(cv_bow.cv_results_['mean_test_score']),\n",
        "     np.mean(cv_tfidf.cv_results_['mean_test_score']),\n",
        "    np.mean(cv_w2vec),\n",
        "    np.mean(cv_w2vec_transfert)],\n",
        "    index = ['Bag-of-Words','TF-IDF', 'Word2Vec non pr√©-entra√Æn√©', 'Word2Vec pr√©-entra√Æn√©'],\n",
        "    columns = [\"Mean F1 score\"]\n",
        ").sort_values(\"Mean F1 score\",ascending = False)\n",
        "perfs"
      ],
      "id": "20fc3c1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les performences limit√©es du mod√®le *Word2Vec* sont cette fois certainement dues √† la mani√®re dont\n",
        "les *word-embeddings* sont exploit√©s : ils sont moyenn√©s pour d√©crire chaque document. \n",
        "\n",
        "Cela a plusieurs limites : \n",
        "\n",
        "- on ne tient pas compte de l'ordre et donc du contexte des mots\n",
        "- lorsque les documents sont longs, la moyennisation peut cr√©er\n",
        "des repr√©sentation bruit√©es.\n",
        "\n",
        "# Contextual embeddings\n",
        "\n",
        "Les *embeddings* contextuels visent √† pallier les limites des *embeddings*\n",
        "traditionnels √©voqu√©es pr√©c√©demment.\n",
        "\n",
        "Cette fois, les mots n'ont plus de repr√©sentation vectorielle fixe,\n",
        "celle-ci est calcul√©e dynamiquement en fonction des mots du voisinage, et ainsi de suite.\n",
        "Cela permet de tenir compte de la structure des phrases\n",
        "et de tenir compte du fait que le sens d'un mot est fortement d√©pendant des mots\n",
        "qui l'entourent. \n",
        "Par exemple, dans les expressions \"le pr√©sident Macron\" et \"le camembert Pr√©sident\" le mot pr√©sident n'a pas du tout le m√™me r√¥le.\n",
        "\n",
        "Ces *embeddings* sont produits par des architectures tr√®s complexes,\n",
        "de type Transformer (`BERT`, etc.).\n"
      ],
      "id": "f035ffcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: hide\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "id": "b33665c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train_vectors = model.encode(X_train)"
      ],
      "id": "db1e893a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scores = cross_val_score(clf, X_train_vectors, y_train, \n",
        "                         cv=4, scoring='f1_micro', n_jobs=4)\n",
        "\n",
        "print(f\"CV scores {scores}\")\n",
        "print(f\"Mean F1 {np.mean(scores)}\")"
      ],
      "id": "8ac490b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perfs = pd.concat(\n",
        "  [perfs,\n",
        "  pd.DataFrame(\n",
        "    [np.mean(scores)],\n",
        "    index = ['Contextual Embedding'],\n",
        "    columns = [\"Mean F1 score\"])]\n",
        ").sort_values(\"Mean F1 score\",ascending = False)\n",
        "perfs"
      ],
      "id": "27a0f658",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verdict : on fait tr√®s l√©g√®rement mieux que la vectorisation TF-IDF.\n",
        "On voit donc l'importance de tenir compte du contexte.\n",
        "\n",
        "__Mais pourquoi, avec une m√©thode tr√®s compliqu√©e, ne parvenons-nous pas √† battre une m√©thode toute simple ?__\n",
        "\n",
        "On peut avancer plusieurs raisons : \n",
        "\n",
        "- le `TF-IDF` est un mod√®le simple, mais toujours tr√®s performant\n",
        "(on parle de _\"tough-to-beat baseline\"_).\n",
        "- la classification d'auteurs est une t√¢che tr√®s particuli√®re et tr√®s ardue,\n",
        "qui ne fait pas justice aux *embeddings*. Comme on l'a dit pr√©c√©demment, ces derniers se r√©v√®lent particuli√®rement pertinents lorsqu'il est question de similarit√© s√©mantique entre des textes (_clustering_, etc.).\n",
        "\n",
        "Dans le cas de notre t√¢che de classification, il est probable que\n",
        "certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de mani√®re pertinente,\n",
        "ce que ne permettent pas de capter les *embeddings* qui accordent √† tous les mots la m√™me importance.\n",
        "\n",
        "## Aller plus loin\n",
        "\n",
        "- Nous avons entra√Æn√© diff√©rents mod√®les sur l'√©chantillon d'entra√Ænement par validation crois√©e, mais nous n'avons toujours pas utilis√© l'√©chantillon test que nous avons mis de c√¥t√© au d√©but. R√©aliser la pr√©diction sur les donn√©es de test, et v√©rifier si l'on obtient le m√™me classement des m√©thodes de vectorisation.\n",
        "- Faire un *vrai* split train/test : faire l'entra√Ænement avec des textes de certains auteurs, et faire la pr√©diction avec des textes d'auteurs diff√©rents. Cela permettrait de neutraliser la pr√©sence de noms de lieux, de personnages, etc.\n",
        "- Comparer avec d'autres algorithmes de classification qu'un SVM\n",
        "- (Avanc√©) : fine-tuner le mod√®le d'embeddings contextuels sur la t√¢che de classification"
      ],
      "id": "905344dc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}