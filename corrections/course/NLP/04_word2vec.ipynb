{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "991832b4",
=======
   "id": "a1a47818",
>>>>>>> master
=======
   "id": "b89b50bd",
>>>>>>> master
   "metadata": {},
   "source": [
    "#  Prédiction avec Word2Vec et Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "00ef79c1",
=======
   "id": "f56d79aa",
>>>>>>> master
=======
   "id": "1dc8847f",
>>>>>>> master
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
    "[![Download](https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter)](https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter)](https://nbviewer.jupyter.org/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![Onyxia](https://img.shields.io/badge/SSPcloud-Tester%20via%20SSP--cloud-informational&color=yellow?logo=Python)](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/jupyter?onyxia.friendlyName=«python-datascientist»&resources.requests.memory=«4Gi»&security.allowlist.enabled=false&init.personalInit=«https://raw.githubusercontent.com/linogaliana/python-datascientist/master/init_onyxia.sh»)\n",
    "<br>\n",
    "[![Binder](https://img.shields.io/badge/Launch-Binder-E66581.svg?logo=data:image/png;base64,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)](https://mybinder.org/v2/gh/linogaliana/python-datascientist/master?filepath=/__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/linogaliana/python-datascientist/blob/master//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)\n",
    "[![githubdev](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/linogaliana/python-datascientist//__w/python-datascientist/python-datascientist/notebooks/course/NLP/04_word2vec.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "ef4a4f69",
=======
   "id": "aaf35dd6",
>>>>>>> master
=======
   "id": "2a88b9c5",
>>>>>>> master
   "metadata": {},
   "source": [
    "Cette page approfondit certains aspects présentés dans la\n",
    "[partie introductive](#nlp). Après avoir travaillé sur le\n",
    "*Comte de Monte Cristo*, on va continuer notre exploration de la littérature\n",
    "avec cette fois des auteurs anglophones :\n",
    "\n",
    "* Edgar Allan Poe, (EAP) ;\n",
    "* HP Lovecraft (HPL) ;\n",
    "* Mary Wollstonecraft Shelley (MWS).\n",
    "\n",
    "Les données sont disponibles ici : [spooky.csv](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv) et peuvent être requétées via l'url \n",
    "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`."
=======
    "Le but va être dans un premier temps de regarder dans le détail les termes les plus fréquents utilisés par les auteurs, de les représenter graphiquement puis on va ensuite essayer de prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`.\n",
    "\n",
    "Les chapitres précédents permettaient de mieux comprendre les enjeux\n",
    "du nettoyage de documents textuels et la modélisation de documents\n",
    "par une approche fréquentiste. On va ici aller plus loin dans la modélisation\n",
    "en utilisant un réseau de neurone grâce au package `Keras`."
>>>>>>> master
=======
    "Le but va être d'essayer de prédire quel texte correspond à quel auteur à partir d'un modèle `Word2Vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ac7b9",
   "metadata": {},
   "source": [
    "Les chapitres précédents permettaient de mieux comprendre les enjeux\n",
    "du nettoyage de documents textuels et la modélisation de documents\n",
    "par une approche fréquentiste. On va ici aller plus loin dans la modélisation\n",
    "en utilisant un réseau de neurones grâce au package `Keras`."
>>>>>>> master
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "d2c4981d",
=======
   "id": "60bee24a",
>>>>>>> master
=======
   "id": "f4c7ce9b",
>>>>>>> master
   "metadata": {},
   "source": [
    "Ce notebook librement inspiré de  : \n",
    "\n",
    "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras\n",
    "* https://github.com/GU4243-ADS/spring2018-project1-ginnyqg\n",
    "* https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook\n",
    "\n",
    "## Librairies nécessaires\n",
    "\n",
    "Cette page évoquera, les principales librairies pour faire du NLP, notamment: \n",
    "\n",
    "* [WordCloud](https://github.com/amueller/word_cloud)\n",
    "* [nltk](https://www.nltk.org/)\n",
    "* [spacy](https://spacy.io/)\n",
    "* [Keras](https://keras.io/)\n",
<<<<<<< HEAD
    "* [TensorFlow](https://www.tensorflow.org/)\n",
    "\n",
    "Il faudra également installer les librairies `gensim` et `pywaffle`\n",
    "\n",
=======
    "* [TensorFlow](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914fecb",
   "metadata": {},
   "source": [
<<<<<<< HEAD
>>>>>>> master
    "Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments pour que `NTLK` puisse fonctionner correctement. Pour cela, faire:\n",
    "\n",
    "~~~python\n",
=======
    "Comme dans la [partie précédente](#nlp), il faut télécharger quelques éléments pour que `NTLK` puisse fonctionner correctement. Pour cela, faire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907a97b",
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> master
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
<<<<<<< HEAD
    "nltk.download('wordnet')\n",
<<<<<<< HEAD
    "~~~"
=======
    "~~~\n",
    "\n",
=======
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fbfa1",
   "metadata": {},
   "source": [
>>>>>>> master
    "Comme nous allons utiliser également `spacy`, il convient de télécharger\n",
    "le corpus Anglais. Pour cela, on peut se référer à\n",
    "[la documentation de `spacy`](https://spacy.io/usage/models),\n",
    "extrèmement bien faite.\n",
    "\n",
    "- Idéalement, il faut installer le module via la ligne de commande. Dans\n",
    "une cellule de notebook `Jupyter`, faire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0472c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78a34c",
   "metadata": {},
   "source": [
    "- Sans accès à la ligne de commande (depuis une instance `Docker` par exemple),\n",
    "faire :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c239f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2bf33",
   "metadata": {},
   "source": [
    "- Sinon, il est également possible d'installer le module en faisant pointer\n",
    "`pip install` depuis le fichier adéquat sur\n",
    "[`Github`](https://github.com/explosion/spacy-models). Pour cela, taper\n",
    "\n",
    "~~~python\n",
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl\n",
    "~~~\n"
>>>>>>> master
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "722fe4ca",
=======
   "id": "5e710d0d",
>>>>>>> master
=======
   "id": "f48ffa73",
>>>>>>> master
   "metadata": {},
   "source": [
    "La liste des modules à importer est assez longue, la voici:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "bf769b76",
=======
   "id": "25c93cc5",
>>>>>>> master
=======
   "id": "d6d35f34",
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
<<<<<<< HEAD
    "from pywaffle import Waffle\n",
=======
>>>>>>> master
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "18beb285",
=======
   "id": "ac252304",
>>>>>>> master
=======
   "id": "a80bb65b",
   "metadata": {},
   "source": [
    "## Données utilisées\n",
    "\n",
    "*Si vous avez déjà lu la section précédente et importé les données, vous\n",
    "pouvez passer à la section suivante*\n",
    "\n",
    "Le code suivant permet d'importer le jeu de données `spooky`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0151486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url='https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "import pandas as pd\n",
    "train = pd.read_csv(url,\n",
    "                    encoding='latin-1')\n",
    "train.columns = train.columns.str.capitalize()\n",
    "                    \n",
    "train['ID'] = train['Id'].str.replace(\"id\",\"\")\n",
    "train = train.set_index('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6887ab6",
>>>>>>> master
   "metadata": {},
   "source": [
    "Le jeu de données met ainsi en regard un auteur avec une phrase qu'il a écrite:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6439e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c552d",
   "metadata": {},
   "source": [
    "```\n",
    "##                                                       Text Author     ID\n",
    "## Id                                                                      \n",
    "## id26305  This process, however, afforded me no means of...    EAP  26305\n",
    "## id17569  It never once occurred to me that the fumbling...    HPL  17569\n",
    "## id11008  In his left hand was a gold snuff box, from wh...    EAP  11008\n",
    "## id27763  How lovely is spring As we looked from Windsor...    MWS  27763\n",
    "## id12958  Finding nothing else, not even gold, the Super...    HPL  12958\n",
    "```\n",
    "\n",
    "## Prédiction - Modélisation en utilisant Word2vec et Keras\n",
    "\n",
<<<<<<< HEAD
    "Derniere partie du TD, prédire les auteurs à partir des textes\n",
    "qu'on aura dans la base de test.\n",
    "\n",
    "Pour cela, on utilise les modules `spacy`, `Word2vec` et `Keras`\n",
=======
    "Jusqu'à présent, nous avons utilisé principalement `nltk` pour le \n",
    "*preprocessing* de données textuelles. Cette fois, nous proposons\n",
    "d'utiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de\n",
    "*pipeline* de *preprocessing*. \n",
    "\n",
>>>>>>> master
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "55fd5fc3",
   "metadata": {
    "lines_to_next_cell": 2
   },
=======
   "id": "ff209e46",
=======
   "id": "b43f5fba",
>>>>>>> master
   "metadata": {},
>>>>>>> master
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "6b9ad70e",
   "metadata": {},
   "source": [
    "TO BE CONTINUED"
=======
   "id": "02cff1f2",
=======
   "id": "29f83f5f",
>>>>>>> master
   "metadata": {},
   "source": [
    "On va utiliser une fonction standardisée pour nettoyer les champs textuels\n",
    "avec `spacy`. Idéalement, on pourrait utiliser un `pipe` `spacy`. Mais là\n",
    "on va privilégier une fonction :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dabfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "from nltk.corpus import stopwords  \n",
    "stop_words = spacy.lang.en.STOP_WORDS\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = text\n",
    "    doc = nlp(doc, disable=['parser', 'ner','tagger'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return(tokens)\n",
    "\n",
    "train['text_clean'] = train['Text'].apply(preprocess_text)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcef1b7",
   "metadata": {},
   "source": [
    "```\n",
<<<<<<< HEAD
    "## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 'text'\n",
    "## \n",
    "## Detailed traceback: \n",
    "##   File \"<string>\", line 1, in <module>\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/frame.py\", line 3458, in __getitem__\n",
    "##     indexer = self.columns.get_loc(key)\n",
    "##   File \"/opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3363, in get_loc\n",
    "##     raise KeyError(key) from err\n",
    "```\n"
>>>>>>> master
=======
    "## /opt/conda/envs/python-ENSAE/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
    "##   warnings.warn(Warnings.W108)\n",
    "```\n",
    "\n",
    "<!-- TO BE CONTINUED -->"
>>>>>>> master
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
